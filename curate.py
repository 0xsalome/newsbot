#!/usr/bin/env python3
"""
News Curation Bot - Main Script
æ§‹é€ çš„é¡ä¼¼æ€§ã«åŸºã¥ãè‡ªå‹•ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

Usage:
    python curate.py              # é€šå¸¸å®Ÿè¡Œ
    python curate.py --dry-run    # æŠ•ç¨¿ã›ãšã«ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ç¢ºèª
    python curate.py --category science  # ç‰¹å®šã‚«ãƒ†ã‚´ãƒªã®ã¿
"""

import argparse
import json
import os
import re
import sys
import time
from collections import Counter
from datetime import datetime, timedelta
from urllib.parse import urlparse

import feedparser
import requests

import config


# =============================================================================
# STATE MANAGEMENT
# =============================================================================

def load_state(filepath="state.json"):
    """state.jsonã‚’èª­ã¿è¾¼ã‚€"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            state = json.load(f)
            # æ–°è¦ã‚«ãƒ†ã‚´ãƒªã®åˆæœŸåŒ–
            for category in config.CATEGORIES:
                if category not in state["posted"]:
                    state["posted"][category] = []
                if category not in state["pending"]:
                    state["pending"][category] = []
            return state
    except FileNotFoundError:
        return create_initial_state()


def save_state(state, filepath="state.json"):
    """state.jsonã‚’ä¿å­˜ã™ã‚‹"""
    state["meta"]["last_updated"] = datetime.utcnow().isoformat() + "Z"
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)


def create_initial_state():
    """åˆæœŸçŠ¶æ…‹ã‚’ä½œæˆ"""
    return {
        "meta": {
            "last_updated": None,
            "retention_days": config.POSTED_RETENTION_DAYS
        },
        "posted": {cat: [] for cat in config.CATEGORIES},
        "pending": {cat: [] for cat in config.CATEGORIES}
    }


def cleanup_old_entries(state):
    """å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤"""
    now = datetime.utcnow()

    # posted: 7æ—¥ä»¥ä¸Šå¤ã„ã‚‚ã®ã‚’å‰Šé™¤
    for category in state["posted"]:
        state["posted"][category] = [
            entry for entry in state["posted"][category]
            if _is_within_days(entry.get("posted_at"), config.POSTED_RETENTION_DAYS, now)
        ]

    # pending: 3æ—¥ä»¥ä¸Šå¤ã„ã‚‚ã®ã‚’å‰Šé™¤
    for category in state["pending"]:
        state["pending"][category] = [
            entry for entry in state["pending"][category]
            if _is_within_days(entry.get("fetched_at"), config.PENDING_RETENTION_DAYS, now)
        ]

    return state


def _is_within_days(date_str, days, now):
    """æ—¥ä»˜ãŒæŒ‡å®šæ—¥æ•°ä»¥å†…ã‹ãƒã‚§ãƒƒã‚¯"""
    if not date_str:
        return False
    try:
        date = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
        return (now - date.replace(tzinfo=None)).days <= days
    except (ValueError, AttributeError):
        return False


# =============================================================================
# RSS FETCHING
# =============================================================================

def fetch_rss(url):
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—"""
    try:
        headers = {"User-Agent": config.USER_AGENT}
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return feedparser.parse(response.content)
    except requests.RequestException as e:
        print(f"[ERROR] Failed to fetch {url}: {e}")
        return None


def fetch_all_feeds(category):
    """ã‚«ãƒ†ã‚´ãƒªã®ã™ã¹ã¦ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—"""
    articles = []
    sources = config.RSS_SOURCES.get(category, [])

    for url in sources:
        feed = fetch_rss(url)
        if feed and feed.entries:
            for entry in feed.entries:
                article = parse_feed_entry(entry, url)
                if article:
                    articles.append(article)
        time.sleep(config.REQUEST_INTERVAL_SECONDS)

    return articles


def parse_feed_entry(entry, source_url):
    """RSSã‚¨ãƒ³ãƒˆãƒªã‚’è¨˜äº‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
    try:
        return {
            "url": entry.get("link", ""),
            "title": entry.get("title", ""),
            "summary": entry.get("summary", entry.get("description", "")),
            "source": urlparse(source_url).netloc,
            "published": entry.get("published", ""),
            "fetched_at": datetime.utcnow().isoformat() + "Z",
            "tags": [],
            "structural_score": 0,
            "timeliness_score": 0,
            "final_score": 0,
        }
    except Exception as e:
        print(f"[ERROR] Failed to parse entry: {e}")
        return None


# =============================================================================
# TRANSLATION (DeepL API Free)
# =============================================================================

def translate_to_japanese(text):
    """DeepL APIã§æ—¥æœ¬èªã«ç¿»è¨³"""
    api_key = os.environ.get("DEEPL_API_KEY")
    if not api_key:
        return None

    try:
        response = requests.post(
            "https://api-free.deepl.com/v2/translate",
            data={
                "auth_key": api_key,
                "text": text,
                "target_lang": "JA"
            },
            timeout=10
        )
        response.raise_for_status()
        result = response.json()
        return result["translations"][0]["text"]
    except Exception as e:
        print(f"[WARN] Translation failed: {e}")
        return None


def translate_articles(articles):
    """è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã«ç¿»è¨³"""
    api_key = os.environ.get("DEEPL_API_KEY")
    if not api_key:
        print("[INFO] DEEPL_API_KEY not set, skipping translation")
        return articles

    print(f"Translating {len(articles)} article titles...")
    for article in articles:
        title_ja = translate_to_japanese(article["title"])
        article["title_ja"] = title_ja if title_ja else article["title"]
        time.sleep(0.1)  # Rate limitå¯¾ç­–

    return articles


def strip_html(text):
    """HTMLã‚¿ã‚°ã‚’é™¤å»"""
    return re.sub(r'<[^>]+>', '', text)


# =============================================================================
# TAG DETECTION
# =============================================================================

def detect_tags(article):
    """è¨˜äº‹ã«ã‚¿ã‚°ã‚’ä»˜ä¸"""
    # HTMLé™¤å»ã¨æ­£è¦åŒ–
    raw_text = f"{article['title']} {article['summary']}"
    text = strip_html(raw_text).lower()
    
    tags = []

    # transformation
    score = detect_transformation(text)
    if score > 0:
        tags.append({"name": "transformation", "score": score})

    # boundary_crossing
    score = detect_boundary_crossing(text)
    if score > 0:
        tags.append({"name": "boundary_crossing", "score": score})

    # visibility_gain
    score = detect_visibility_gain(text)
    if score > 0:
        tags.append({"name": "visibility_gain", "score": score})

    # value_redefinition
    score = detect_value_redefinition(text)
    if score > 0:
        tags.append({"name": "value_redefinition", "score": score})

    # scale_shift
    score = detect_scale_shift(text)
    if score > 0:
        tags.append({"name": "scale_shift", "score": score})

    # ontology_shift (ä»–ã‚¿ã‚°ã¨ã®çµ„ã¿åˆã‚ã›ã§ã®ã¿ä»˜ä¸)
    if tags:  # ä»–ã®ã‚¿ã‚°ãŒã‚ã‚‹å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯
        score = detect_ontology_shift(text, tags)
        if score > 0:
            tags.append({"name": "ontology_shift", "score": score})

    article["tags"] = tags
    return article


def count_matches(text, keywords):
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå˜èªå¢ƒç•Œã‚’è€ƒæ…®ï¼‰"""
    count = 0
    for keyword in keywords:
        # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«è¨˜å·ãŒå«ã¾ã‚Œã‚‹å ´åˆç”¨ï¼‰
        pattern = r'\b' + re.escape(keyword) + r'\b'
        if re.search(pattern, text):
            count += 1
    return count


def detect_transformation(text):
    """transformation ã‚¿ã‚°ã®æ¤œå‡º"""
    score = 0

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
    matches = count_matches(text, config.TRANSFORMATION_KEYWORDS)
    if matches > 0:
        score += config.TAG_SCORES["transformation_keyword"] * matches

    # å¯¾ç¾©èªãƒšã‚¢æ¤œå‡º
    for word1, word2 in config.ANTONYM_PAIRS:
        pattern1 = r'\b' + re.escape(word1) + r'\b'
        pattern2 = r'\b' + re.escape(word2) + r'\b'
        if re.search(pattern1, text) and re.search(pattern2, text):
            score += config.TAG_SCORES["transformation_antonym_pair"]

    return score


def detect_boundary_crossing(text):
    """boundary_crossing ã‚¿ã‚°ã®æ¤œå‡º"""
    score = 0
    detected_domains = []

    # ãƒ‰ãƒ¡ã‚¤ãƒ³æ¤œå‡º
    for domain, keywords in config.DOMAINS.items():
        if count_matches(text, keywords) > 0:
            detected_domains.append(domain)

    # 2ãƒ‰ãƒ¡ã‚¤ãƒ³ä»¥ä¸Šã§æ¤œå‡º
    if len(detected_domains) >= 3:
        score += config.TAG_SCORES["boundary_crossing_3_domains"]
    elif len(detected_domains) >= 2:
        score += config.TAG_SCORES["boundary_crossing_2_domains"]

    # å¢ƒç•Œèªæ¤œå‡º
    if count_matches(text, config.BOUNDARY_KEYWORDS) > 0:
        score += config.TAG_SCORES["boundary_crossing_keyword"]

    return score


def detect_visibility_gain(text):
    """visibility_gain ã‚¿ã‚°ã®æ¤œå‡º"""
    score = 0
    
    matches = count_matches(text, config.VISIBILITY_KEYWORDS)
    if matches > 0:
        score += config.TAG_SCORES["visibility_gain_keyword"] * matches

    return min(score, config.TAG_SCORES["visibility_gain_combo"])  # ä¸Šé™è¨­å®š


def detect_value_redefinition(text):
    """value_redefinition ã‚¿ã‚°ã®æ¤œå‡º"""
    score = 0

    matches = count_matches(text, config.VALUE_KEYWORDS)
    if matches > 0:
        score += config.TAG_SCORES["value_redefinition_keyword"] * matches

    for word1, word2 in config.CATEGORY_SHIFT_PAIRS:
        pattern1 = r'\b' + re.escape(word1) + r'\b'
        pattern2 = r'\b' + re.escape(word2) + r'\b'
        if re.search(pattern1, text) and re.search(pattern2, text):
            score += config.TAG_SCORES["value_redefinition_pair"]

    return score


def detect_scale_shift(text):
    """scale_shift ã‚¿ã‚°ã®æ¤œå‡º"""
    score = 0

    for word1, word2 in config.SCALE_PAIRS:
        pattern1 = r'\b' + re.escape(word1) + r'\b'
        pattern2 = r'\b' + re.escape(word2) + r'\b'
        if re.search(pattern1, text) and re.search(pattern2, text):
            score += config.TAG_SCORES["scale_shift_pair"]

    matches = count_matches(text, config.SCALE_KEYWORDS)
    if matches > 0:
        score += config.TAG_SCORES["scale_shift_paradox"]

    return score


def detect_ontology_shift(text, existing_tags):
    """ontology_shift ã‚¿ã‚°ã®æ¤œå‡ºï¼ˆä»–ã‚¿ã‚°ã¨ã®çµ„ã¿åˆã‚ã›ã§ã®ã¿ï¼‰"""
    has_ontology_keyword = count_matches(text, config.ONTOLOGY_KEYWORDS) > 0
    has_questioning = count_matches(text, config.QUESTIONING_KEYWORDS) > 0

    if not has_ontology_keyword:
        return 0

    # transformation ã¾ãŸã¯ boundary_crossing ã¨çµ„ã¿åˆã‚ã›
    relevant_tags = ["transformation", "boundary_crossing"]
    has_relevant = any(t["name"] in relevant_tags for t in existing_tags)

    if has_relevant and has_questioning:
        return config.TAG_SCORES["ontology_shift"]

    return 0


# =============================================================================
# SCORING
# =============================================================================

def calculate_base_scores(article):
    """è¨˜äº‹ã®åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼ˆæ§‹é€ ãƒ»è©±é¡Œæ€§ï¼‰ã‚’è¨ˆç®—"""
    # æ§‹é€ å¼·åº¦ã‚¹ã‚³ã‚¢
    structural_score = sum(tag["score"] for tag in article["tags"])

    # è©±é¡Œæ€§ã‚¹ã‚³ã‚¢
    timeliness_score = calculate_timeliness_score(article)

    article["structural_score"] = structural_score
    article["timeliness_score"] = timeliness_score
    return article

def calculate_weighted_score(article, weights):
    """æŒ‡å®šã•ã‚ŒãŸé‡ã¿ã§æœ€çµ‚ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    final_score = (
        article["structural_score"] * weights["structural"] +
        article["timeliness_score"] * weights["timeliness"]
    )
    return final_score

def calculate_timeliness_score(article):
    """è©±é¡Œæ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    score = 0

    # ã‚½ãƒ¼ã‚¹æ ¼ä»˜ã‘
    source = article.get("source", "")
    for domain, weight in config.SOURCE_WEIGHT.items():
        if domain in source:
            score += weight
            break
    else:
        score += config.SOURCE_WEIGHT["default"]

    # é®®åº¦ã‚¹ã‚³ã‚¢
    published = article.get("published", "")
    if published:
        try:
            # feedparserã®æ—¥ä»˜å½¢å¼ã«å¯¾å¿œ
            pub_date = feedparser._parse_date(published)
            if pub_date:
                pub_datetime = datetime(*pub_date[:6])
                age_hours = (datetime.utcnow() - pub_datetime).total_seconds() / 3600

                if age_hours <= 24:
                    score += 3
                elif age_hours <= 48:
                    score += 2
                elif age_hours <= 168:  # 7 days
                    score += 1
        except Exception:
            pass

    return score


# =============================================================================
# UTILS
# =============================================================================

def normalize_url(url):
    """URLã‚’æ­£è¦åŒ–ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰"""
    if not url:
        return ""
    try:
        parsed = urlparse(url.strip())
        # ã‚¹ã‚­ãƒ¼ãƒ ã¨ãƒãƒƒãƒˆãƒ­ãƒƒã‚¯ã‚’å°æ–‡å­—åŒ–
        scheme = parsed.scheme.lower()
        netloc = parsed.netloc.lower()
        path = parsed.path
        # æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’é™¤å»
        if path.endswith("/"):
            path = path[:-1]
        
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ãã®ã¾ã¾ï¼ˆè¨˜äº‹IDãªã©ãŒå«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ï¼‰
        # ãŸã ã—ã€ç‰¹å®šã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å‰Šé™¤ã—ã¦ã‚‚ã„ã„ã‹ã‚‚ã—ã‚Œãªã„ãŒã€ä»Šå›ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«
        
        return f"{scheme}://{netloc}{path}"
    except Exception:
        return url.strip()


# =============================================================================
# SELECTION LOGIC
# =============================================================================

def select_articles(articles, state, category):
    """ã‚«ãƒ†ã‚´ãƒªè¨­å®šã«åŸºã¥ã„ã¦è¨˜äº‹ã‚’é¸å‡º"""
    # 1. ã™ã¹ã¦ã®å€™è£œã‚’æ­£è¦åŒ–URLã§ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«ã™ã‚‹
    unique_candidates_map = {}
    for a in articles:
        norm_url = normalize_url(a["url"])
        if norm_url not in unique_candidates_map:
            unique_candidates_map[norm_url] = a
    
    # 2. æ—¢ã«æŠ•ç¨¿æ¸ˆã¿ã®URLã‚’é™¤å¤–ï¼ˆæ­£è¦åŒ–ã—ã¦æ¯”è¼ƒï¼‰
    posted_entries = state["posted"].get(category, [])
    posted_urls = {normalize_url(entry["url"]) for entry in posted_entries}
    
    new_articles = []
    for url, a in unique_candidates_map.items():
        if url not in posted_urls:
            new_articles.append(a)
    
    # 3. pendingè¨˜äº‹ã¨ãƒãƒ¼ã‚¸
    pending = state["pending"].get(category, [])
    # ã™ã§ã«new_articlesã«ã‚ã‚‹ã‚‚ã®ã¯pendingã‹ã‚‰é™¤å¤–
    new_urls = {normalize_url(a["url"]) for a in new_articles}
    
    # pendingå†…ã®é‡è¤‡ã‚‚æ’é™¤
    unique_pending = []
    seen_pending_urls = set()
    for p in pending:
        p_url = normalize_url(p["url"])
        if p_url not in new_urls and p_url not in posted_urls and p_url not in seen_pending_urls:
            unique_pending.append(p)
            seen_pending_urls.add(p_url)

    all_candidates = new_articles + unique_pending

    cat_config = config.CATEGORIES[category]
    mode = cat_config["selection_mode"]
    
    selected = []

    if mode == "trending_only":
        # è©±é¡Œæ€§é‡è¦–ã®ã¿
        weights = cat_config["weights"]
        for a in all_candidates:
            a["final_score"] = round(calculate_weighted_score(a, weights), 2)
        
        sorted_articles = sorted(all_candidates, key=lambda x: x["final_score"], reverse=True)
        # ã‚½ãƒ¼ã‚¹å¤šæ§˜æ€§ã‚’è€ƒæ…®ã—ã¦é¸å‡º
        selected = ensure_source_diversity(sorted_articles, cat_config["posts_per_day"])

    elif mode == "dual":
        # æ§‹é€ é‡è¦–1 + è©±é¡Œé‡è¦–1
        
        # 1. æ§‹é€ é‡è¦–
        weights_struct = cat_config["weights_structural"]
        for a in all_candidates:
            a["temp_score_struct"] = calculate_weighted_score(a, weights_struct)
        
        sorted_struct = sorted(all_candidates, key=lambda x: x["temp_score_struct"], reverse=True)
        struct_candidates = [a for a in sorted_struct if a["tags"]]
        
        top_structural = struct_candidates[:1]
        
        # 2. è©±é¡Œé‡è¦–
        weights_trend = cat_config["weights_trending"]
        # é‡è¤‡æ’é™¤ï¼ˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆIDãƒ™ãƒ¼ã‚¹ã ã¨å±é™ºãªã®ã§URLãƒ™ãƒ¼ã‚¹ã§ï¼‰
        selected_urls = {normalize_url(a["url"]) for a in top_structural}
        remaining = [a for a in all_candidates if normalize_url(a["url"]) not in selected_urls]
        
        for a in remaining:
            a["temp_score_trend"] = calculate_weighted_score(a, weights_trend)
            
        sorted_trend = sorted(remaining, key=lambda x: x["temp_score_trend"], reverse=True)
        
        # ã‚½ãƒ¼ã‚¹é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ§‹é€ æ ã¨åŒã˜ã‚½ãƒ¼ã‚¹ã°ã‹ã‚Šã«ãªã‚‰ãªã„ã‚ˆã†ã«ï¼‰
        top_trending = []
        if sorted_trend:
            # æ§‹é€ æ ã®ã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª
            struct_sources = [urlparse(a["url"]).netloc for a in top_structural]
            
            # å¯èƒ½ãªé™ã‚Šç•°ãªã‚‹ã‚½ãƒ¼ã‚¹ã‚’é¸ã¶
            for a in sorted_trend:
                if urlparse(a["url"]).netloc not in struct_sources:
                    top_trending.append(a)
                    break
            
            # è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã‚¹ã‚³ã‚¢1ä½ã‚’é¸ã¶
            if not top_trending and sorted_trend:
                top_trending.append(sorted_trend[0])
        
        # çµ±åˆ
        selected = top_structural + top_trending
        
        # final_scoreæ›´æ–°
        for a in top_structural:
            a["final_score"] = round(a.get("temp_score_struct", 0), 2)
        for a in top_trending:
            a["final_score"] = round(a.get("temp_score_trend", 0), 2)


    elif mode == "dual_enhanced":
        # æ§‹é€ é‡è¦–2 + è©±é¡Œé‡è¦–2 (AI)
        
        # 1. æ§‹é€ é‡è¦–
        weights_struct = cat_config["weights_structural"]
        for a in all_candidates:
            a["temp_score_struct"] = calculate_weighted_score(a, weights_struct)
        
        sorted_struct = sorted(all_candidates, key=lambda x: x["temp_score_struct"], reverse=True)
        struct_candidates = [a for a in sorted_struct if a["tags"]]
        
        # ã‚½ãƒ¼ã‚¹å¤šæ§˜æ€§ã‚’ç¢ºä¿ã—ã¤ã¤2ã¤é¸ã¶
        top_structural = ensure_source_diversity(struct_candidates, 2)
        
        # ã‚¿ã‚°å¤šæ§˜æ€§ãƒã‚§ãƒƒã‚¯
        top_structural = ensure_tag_diversity(top_structural, struct_candidates)

        # 2. è©±é¡Œé‡è¦–
        weights_trend = cat_config["weights_trending"]
        selected_urls = {normalize_url(a["url"]) for a in top_structural}
        remaining = [a for a in all_candidates if normalize_url(a["url"]) not in selected_urls]
        
        for a in remaining:
            a["temp_score_trend"] = calculate_weighted_score(a, weights_trend)
            
        sorted_trend = sorted(remaining, key=lambda x: x["temp_score_trend"], reverse=True)
        
        # ã“ã¡ã‚‰ã‚‚ã‚½ãƒ¼ã‚¹å¤šæ§˜æ€§ã‚’ç¢ºä¿ï¼ˆå…¨ä½“ã§ã®ãƒãƒ©ãƒ³ã‚¹ã‚‚è€ƒæ…®ã—ãŸã„ãŒã€ã¾ãšã¯ã“ã®æ å†…ã§ï¼‰
        # æ§‹é€ æ ã§ã™ã§ã«é¸ã°ã‚ŒãŸã‚½ãƒ¼ã‚¹ã¯å„ªå…ˆåº¦ã‚’ä¸‹ã’ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å…¥ã‚Œã‚‹ã¨ã‚ˆã‚Šè‰¯ã„
        struct_sources = Counter([urlparse(a["url"]).netloc for a in top_structural])
        
        top_trending = []
        for a in sorted_trend:
            if len(top_trending) >= 2:
                break
            
            domain = urlparse(a["url"]).netloc
            # å…¨ä½“ã§åŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯æœ€å¤§2ã¤ã¾ã§ï¼ˆæ§‹é€ æ ã§ã™ã§ã«2ã¤ãªã‚‰ã€è©±é¡Œæ ã§ã¯é¸ã°ãªã„ï¼‰
            if struct_sources.get(domain, 0) + 1 <= 2: # ã“ã“ã§ã®+1ã¯ä»Šå›é¸ã¶åˆ†
                 top_trending.append(a)
                 struct_sources[domain] = struct_sources.get(domain, 0) + 1
        
        # ã‚‚ã—å³ã—ã™ãã¦åŸ‹ã¾ã‚‰ãªã‹ã£ãŸå ´åˆã€åˆ¶é™ã‚’ç·©ã‚ã¦åŸ‹ã‚ã‚‹
        if len(top_trending) < 2:
            current_trending_urls = {normalize_url(a["url"]) for a in top_trending}
            remaining_trend = [a for a in sorted_trend if normalize_url(a["url"]) not in current_trending_urls]
            needed = 2 - len(top_trending)
            top_trending.extend(remaining_trend[:needed])

        # çµ±åˆ
        selected = top_structural + top_trending

        # final_scoreæ›´æ–°
        for a in top_structural:
            a["final_score"] = round(a.get("temp_score_struct", 0), 2)
        for a in top_trending:
            a["final_score"] = round(a.get("temp_score_trend", 0), 2)

    return selected


def ensure_source_diversity(candidates, limit, max_per_source=2):
    """åŒä¸€ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®é¸å‡ºã‚’åˆ¶é™ã—ã¦è¨˜äº‹ã‚’é¸ã¶"""
    selected = []
    source_counts = Counter()
    
    for article in candidates:
        if len(selected) >= limit:
            break
            
        domain = urlparse(article["url"]).netloc
        if source_counts[domain] < max_per_source:
            selected.append(article)
            source_counts[domain] += 1
            
    return selected



def ensure_tag_diversity(current_top, candidates):
    """ã‚¿ã‚°ã®å¤šæ§˜æ€§ã‚’ç¢ºä¿ï¼ˆæ§‹é€ é‡è¦–æ ç”¨ï¼‰"""
    if len(current_top) < 2:
        return current_top

    top1 = current_top[0]
    top2 = current_top[1]
    
    tags1 = {t["name"] for t in top1.get("tags", [])}
    tags2 = {t["name"] for t in top2.get("tags", [])}

    # åŒã˜ã‚¿ã‚°ã‚»ãƒƒãƒˆãªã‚‰ã€å€™è£œãƒªã‚¹ãƒˆã®3ç•ªç›®ä»¥é™ã‹ã‚‰ç•°ãªã‚‹ã‚¿ã‚°ã‚’æŒã¤ã‚‚ã®ã‚’æ¢ã™
    if tags1 == tags2 and len(candidates) > 2:
        for article in candidates[2:10]:  # ä¸Šä½10ä»¶ã¾ã§æ¢ç´¢
            article_tags = {t["name"] for t in article.get("tags", [])}
            if article_tags != tags1:
                return [top1, article]

    return current_top


def update_pending(articles, selected, state, category):
    """pendingè¨˜äº‹ã‚’æ›´æ–°"""
    selected_urls = {a["url"] for a in selected}
    
    # é¸æŠã•ã‚Œãªã‹ã£ãŸè¨˜äº‹
    remaining = [a for a in articles if a["url"] not in selected_urls]
    
    # ç°¡æ˜“ã‚¹ã‚³ã‚¢ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¦ã‚§ã‚¤ãƒˆï¼‰ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’ä¿æŒ
    # â€» æ¬¡å›å®Ÿè¡Œæ™‚ã«é©åˆ‡ãªãƒ¢ãƒ¼ãƒ‰ã§å†è¨ˆç®—ã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯æš«å®šçš„ã«ä¿æŒ
    default_weights = {"structural": 0.5, "timeliness": 0.5}
    for a in remaining:
        if "final_score" not in a or a["final_score"] == 0:
             a["final_score"] = calculate_weighted_score(a, default_weights)

    sorted_pending = sorted(remaining, key=lambda x: x.get("final_score", 0), reverse=True)
    
    state["pending"][category] = sorted_pending[:10]  # ä¸Šä½10ä»¶ä¿æŒ

    return state


# =============================================================================
# DISCORD POSTING
# =============================================================================

def post_to_discord(article, category, dry_run=False):
    """Discordã«æŠ•ç¨¿"""
    webhook_env = f"DISCORD_WEBHOOK_{category.upper()}"
    webhook_url = os.environ.get(webhook_env)

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆï¼ˆdry-runæ™‚ã‚‚è¡¨ç¤ºã™ã‚‹ãŸã‚å…ˆã«ä½œæˆï¼‰
    cat_info = config.CATEGORIES[category]
    tag_names = " Ã— ".join(t["name"] for t in article["tags"]) if article["tags"] else "no tags"
    date_str = datetime.utcnow().strftime("%Y-%m-%d")

    # æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚Œã°ä½¿ç”¨
    title = article.get('title_ja', article['title'])

    message = f"""{cat_info['emoji']} **{cat_info['name']}** | {date_str}

**[{tag_names}]**
{title}

ğŸ”— {article['url']}
ğŸ“° {article['source']} | Score: {article['final_score']}"""

    if dry_run:
        print(f"\n[DRY-RUN] Would post to {category}:")
        print(message)
        print("-" * 50)
        return True

    # Webhook URLãƒã‚§ãƒƒã‚¯ï¼ˆå®ŸæŠ•ç¨¿æ™‚ã®ã¿ï¼‰
    if not webhook_url:
        print(f"[WARN] {webhook_env} not set, skipping post")
        return False

    # å®Ÿéš›ã«æŠ•ç¨¿
    try:
        response = requests.post(
            webhook_url,
            json={"content": message},
            headers={"Content-Type": "application/json"},
            timeout=30
        )
        response.raise_for_status()
        print(f"[OK] Posted to {category}: {article['title'][:50]}...")
        return True
    except requests.RequestException as e:
        print(f"[ERROR] Failed to post to Discord: {e}")
        return False


# =============================================================================
# MAIN
# =============================================================================

def process_category(category, state, dry_run=False):
    """1ã‚«ãƒ†ã‚´ãƒªã‚’å‡¦ç†"""
    print(f"\n{'='*50}")
    print(f"Processing: {category}")
    print(f"{'='*50}")

    # RSSå–å¾—
    articles = fetch_all_feeds(category)
    print(f"Fetched {len(articles)} articles")

    if not articles:
        return state

    # ã‚¿ã‚°ä»˜ä¸ã¨åŸºæœ¬ã‚¹ã‚³ã‚¢è¨ˆç®—
    for article in articles:
        detect_tags(article)
        calculate_base_scores(article)

    # è¨˜äº‹é¸æŠï¼ˆã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ãƒ­ã‚¸ãƒƒã‚¯ã§ï¼‰
    selected = select_articles(articles, state, category)
    print(f"Selected {len(selected)} articles for posting")

    # é¸æŠã•ã‚ŒãŸè¨˜äº‹ã®ã¿ç¿»è¨³ï¼ˆAPIç¯€ç´„ï¼‰
    selected = translate_articles(selected)

    # æŠ•ç¨¿
    for article in selected:
        if post_to_discord(article, category, dry_run):
            # æŠ•ç¨¿æˆåŠŸã—ãŸã‚‰postedã«è¿½åŠ 
            if not dry_run:
                state["posted"][category].append({
                    "url": article["url"],
                    "posted_at": datetime.utcnow().strftime("%Y-%m-%d"),
                    "score": article["final_score"],
                    "tags": [t["name"] for t in article["tags"]]
                })

    # pendingæ›´æ–°
    # selectedã«å…¥ã‚‰ãªã‹ã£ãŸè¨˜äº‹ã®ä¸­ã‹ã‚‰pendingå€™è£œã‚’é¸ã¶
    state = update_pending(articles, selected, state, category)

    return state


def main():
    parser = argparse.ArgumentParser(description="News Curation Bot")
    parser.add_argument("--dry-run", action="store_true", help="Don't actually post")
    parser.add_argument("--category", type=str, help="Process specific category only")
    args = parser.parse_args()

    print("News Curation Bot starting...")
    print(f"Dry run: {args.dry_run}")

    # çŠ¶æ…‹èª­ã¿è¾¼ã¿
    state = load_state()
    state = cleanup_old_entries(state)

    # ã‚«ãƒ†ã‚´ãƒªå‡¦ç†
    categories = [args.category] if args.category else list(config.CATEGORIES.keys())

    for category in categories:
        if category not in config.CATEGORIES:
            print(f"[ERROR] Unknown category: {category}")
            continue
        state = process_category(category, state, args.dry_run)

    # çŠ¶æ…‹ä¿å­˜
    if not args.dry_run:
        save_state(state)
        print("\nState saved.")

    print("\nDone!")


if __name__ == "__main__":
    main()