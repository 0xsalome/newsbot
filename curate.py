#!/usr/bin/env python3
"""
News Curation Bot - Main Script
æ§‹é€ çš„é¡ä¼¼æ€§ã«åŸºã¥ãè‡ªå‹•ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

Usage:
    python curate.py              # é€šå¸¸å®Ÿè¡Œ
    python curate.py --dry-run    # æŠ•ç¨¿ã›ãšã«ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ç¢ºèª
    python curate.py --category science  # ç‰¹å®šã‚«ãƒ†ã‚´ãƒªã®ã¿
"""

import argparse
import json
import os
import re
import sys
import time
from collections import Counter
from datetime import datetime, timedelta
from urllib.parse import urlparse

import feedparser
import requests

import config


# =============================================================================
# STATE MANAGEMENT
# =============================================================================

def load_state(filepath="state.json"):
    """state.jsonã‚’èª­ã¿è¾¼ã‚€"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            state = json.load(f)
            # æ–°è¦ã‚«ãƒ†ã‚´ãƒªã®åˆæœŸåŒ–
            for category in config.CATEGORIES:
                if category not in state["posted"]:
                    state["posted"][category] = []
                if category not in state["pending"]:
                    state["pending"][category] = []
            return state
    except FileNotFoundError:
        return create_initial_state()


def save_state(state, filepath="state.json"):
    """state.jsonã‚’ä¿å­˜ã™ã‚‹"""
    state["meta"]["last_updated"] = datetime.utcnow().isoformat() + "Z"
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)


def create_initial_state():
    """åˆæœŸçŠ¶æ…‹ã‚’ä½œæˆ"""
    return {
        "meta": {
            "last_updated": None,
            "retention_days": config.POSTED_RETENTION_DAYS
        },
        "posted": {cat: [] for cat in config.CATEGORIES},
        "pending": {cat: [] for cat in config.CATEGORIES}
    }


def cleanup_old_entries(state):
    """å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤"""
    now = datetime.utcnow()

    # posted: 7æ—¥ä»¥ä¸Šå¤ã„ã‚‚ã®ã‚’å‰Šé™¤
    for category in state["posted"]:
        state["posted"][category] = [
            entry for entry in state["posted"][category]
            if _is_within_days(entry.get("posted_at"), config.POSTED_RETENTION_DAYS, now)
        ]

    # pending: 3æ—¥ä»¥ä¸Šå¤ã„ã‚‚ã®ã‚’å‰Šé™¤
    for category in state["pending"]:
        state["pending"][category] = [
            entry for entry in state["pending"][category]
            if _is_within_days(entry.get("fetched_at"), config.PENDING_RETENTION_DAYS, now)
        ]

    return state


def _is_within_days(date_str, days, now):
    """æ—¥ä»˜ãŒæŒ‡å®šæ—¥æ•°ä»¥å†…ã‹ãƒã‚§ãƒƒã‚¯"""
    if not date_str:
        return False
    try:
        date = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
        return (now - date.replace(tzinfo=None)).days <= days
    except (ValueError, AttributeError):
        return False


# =============================================================================
# RSS FETCHING
# =============================================================================

def fetch_rss(url):
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã¾ãŸã¯Reddit JSON APIã‚’å–å¾—"""
    try:
        headers = {"User-Agent": config.USER_AGENT}

        # Reddit URLã®å ´åˆã¯JSON APIã‚’ä½¿ç”¨
        if "reddit.com" in url:
            # .rss ã‚’ .json ã«ç½®æ›ï¼ˆãªã‘ã‚Œã°ãã®ã¾ã¾ .json è¿½åŠ ï¼‰
            json_url = url.replace(".rss", ".json") if ".rss" in url else url + ".json"
            response = requests.get(json_url, headers=headers, timeout=30)
            response.raise_for_status()
            return {"reddit_json": response.json(), "url": url}

        # é€šå¸¸ã®RSS
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return feedparser.parse(response.content)
    except requests.RequestException as e:
        print(f"[ERROR] Failed to fetch {url}: {e}")
        return None


def fetch_all_feeds(category):
    """ã‚«ãƒ†ã‚´ãƒªã®ã™ã¹ã¦ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—"""
    articles = []
    sources = config.RSS_SOURCES.get(category, [])

    for url in sources:
        feed = fetch_rss(url)
        if not feed:
            continue

        # Reddit JSON APIã®å ´åˆ
        if isinstance(feed, dict) and "reddit_json" in feed:
            reddit_data = feed["reddit_json"]
            if reddit_data.get("data", {}).get("children"):
                for child in reddit_data["data"]["children"]:
                    post_data = child.get("data", {})
                    article = parse_reddit_post(post_data, url)
                    if article:
                        articles.append(article)
        # é€šå¸¸ã®RSS
        elif hasattr(feed, 'entries') and feed.entries:
            for entry in feed.entries:
                article = parse_feed_entry(entry, url)
                if article:
                    articles.append(article)

        time.sleep(config.REQUEST_INTERVAL_SECONDS)

    return articles


def parse_feed_entry(entry, source_url):
    """RSSã‚¨ãƒ³ãƒˆãƒªã‚’è¨˜äº‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
    try:
        return {
            "url": entry.get("link", ""),
            "title": entry.get("title", ""),
            "summary": entry.get("summary", entry.get("description", "")),
            "source": urlparse(source_url).netloc,
            "published": entry.get("published", ""),
            "fetched_at": datetime.utcnow().isoformat() + "Z",
            "tags": [],
            "structural_score": 0,
            "timeliness_score": 0,
            "final_score": 0,
            # Redditä»¥å¤–ã¯äººæ°—åº¦æƒ…å ±ãªã—
            "reddit_score": None,
            "reddit_comments": None,
        }
    except Exception as e:
        print(f"[ERROR] Failed to parse entry: {e}")
        return None


def parse_reddit_post(post_data, source_url):
    """RedditæŠ•ç¨¿ã‚’è¨˜äº‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
    try:
        # äººæ°—åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼šæœ€ä½åŸºæº–ã‚’æº€ãŸã•ãªã„æŠ•ç¨¿ã‚’é™¤å¤–
        score = post_data.get("score", 0)
        num_comments = post_data.get("num_comments", 0)

        # æ¡ä»¶: upvotes < 10 ã‹ã¤ comments < 3 ã®å ´åˆã¯é™¤å¤–
        if score < config.REDDIT_MIN_UPVOTES and num_comments < config.REDDIT_MIN_COMMENTS:
            return None

        # upvote_ratioãŒä½ã™ãã‚‹ï¼ˆè³›å¦ãŒåˆ†ã‹ã‚Œã™ãï¼‰æŠ•ç¨¿ã‚‚é™¤å¤–
        upvote_ratio = post_data.get("upvote_ratio", 1.0)
        if upvote_ratio < config.REDDIT_MIN_UPVOTE_RATIO:
            return None

        # æŠ•ç¨¿æ™‚åˆ»ã‚’ISOå½¢å¼ã«å¤‰æ›
        created_utc = post_data.get("created_utc", 0)
        published_date = datetime.utcfromtimestamp(created_utc).isoformat() + "Z" if created_utc else ""

        # selftextï¼ˆæœ¬æ–‡ï¼‰ã¾ãŸã¯ãƒªãƒ³ã‚¯å…ˆURLã‚’å–å¾—
        url = post_data.get("url", "")
        # Redditå†…éƒ¨ãƒªãƒ³ã‚¯ã®å ´åˆã¯çµ¶å¯¾URLã«å¤‰æ›
        if url.startswith("/r/"):
            url = f"https://www.reddit.com{url}"

        return {
            "url": url,
            "title": post_data.get("title", ""),
            "summary": post_data.get("selftext", "")[:500],  # æœ¬æ–‡ã®æœ€åˆã®500æ–‡å­—
            "source": "reddit.com",
            "published": published_date,
            "fetched_at": datetime.utcnow().isoformat() + "Z",
            "tags": [],
            "structural_score": 0,
            "timeliness_score": 0,
            "final_score": 0,
            # Redditäººæ°—åº¦æƒ…å ±
            "reddit_score": score,
            "reddit_comments": num_comments,
            "reddit_upvote_ratio": upvote_ratio,
        }
    except Exception as e:
        print(f"[ERROR] Failed to parse Reddit post: {e}")
        return None


# =============================================================================
# TRANSLATION (DeepL API Free)
# =============================================================================

def translate_to_japanese(text):
    """DeepL APIã§æ—¥æœ¬èªã«ç¿»è¨³"""
    api_key = os.environ.get("DEEPL_API_KEY")
    if not api_key:
        return None

    try:
        response = requests.post(
            "https://api-free.deepl.com/v2/translate",
            data={
                "auth_key": api_key,
                "text": text,
                "target_lang": "JA"
            },
            timeout=10
        )
        response.raise_for_status()
        result = response.json()
        return result["translations"][0]["text"]
    except Exception as e:
        error_details = str(e)
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æœ¬æ–‡ãŒã‚ã‚Œã°è©³ç´°ã‚’è¿½åŠ ï¼ˆèªè¨¼ã‚¨ãƒ©ãƒ¼ã‚„åˆ¶é™è¶…éã®ç†ç”±ãŒã‚ã‹ã‚‹ï¼‰
        if 'response' in locals() and hasattr(response, 'text') and response.text:
            error_details += f" | Details: {response.text}"
        
        print(f"[WARN] Translation failed: {error_details}")
        return None


def translate_articles(articles):
    """è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã«ç¿»è¨³"""
    api_key = os.environ.get("DEEPL_API_KEY")
    if not api_key:
        print("[INFO] DEEPL_API_KEY not set, skipping translation")
        return articles

    print(f"Translating {len(articles)} article titles...")
    for article in articles:
        title_ja = translate_to_japanese(article["title"])
        article["title_ja"] = title_ja if title_ja else article["title"]
        time.sleep(0.1)  # Rate limitå¯¾ç­–

    return articles


def strip_html(text):
    """HTMLã‚¿ã‚°ã‚’é™¤å»"""
    return re.sub(r'<[^>]+>', '', text)


# =============================================================================
# TAG DETECTION
# =============================================================================

def detect_tags(article):
    """è¨˜äº‹ã«ã‚¿ã‚°ã‚’ä»˜ä¸"""
    # HTMLé™¤å»ã¨æ­£è¦åŒ–
    raw_text = f"{article['title']} {article['summary']}"
    text = strip_html(raw_text).lower()
    
    tags = []

    # transformation
    score = detect_transformation(text)
    if score > 0:
        tags.append({"name": "transformation", "score": score})

    # boundary_crossing
    score = detect_boundary_crossing(text)
    if score > 0:
        tags.append({"name": "boundary_crossing", "score": score})

    # visibility_gain
    score = detect_visibility_gain(text)
    if score > 0:
        tags.append({"name": "visibility_gain", "score": score})

    # value_redefinition
    score = detect_value_redefinition(text)
    if score > 0:
        tags.append({"name": "value_redefinition", "score": score})

    # scale_shift
    score = detect_scale_shift(text)
    if score > 0:
        tags.append({"name": "scale_shift", "score": score})

    # ontology_shift (ä»–ã‚¿ã‚°ã¨ã®çµ„ã¿åˆã‚ã›ã§ã®ã¿ä»˜ä¸)
    if tags:  # ä»–ã®ã‚¿ã‚°ãŒã‚ã‚‹å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯
        score = detect_ontology_shift(text, tags)
        if score > 0:
            tags.append({"name": "ontology_shift", "score": score})

    article["tags"] = tags
    return article


def count_matches(text, keywords):
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå˜èªå¢ƒç•Œã‚’è€ƒæ…®ï¼‰"""
    count = 0
    for keyword in keywords:
        # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«è¨˜å·ãŒå«ã¾ã‚Œã‚‹å ´åˆç”¨ï¼‰
        pattern = r'\b' + re.escape(keyword) + r'\b'
        if re.search(pattern, text):
            count += 1
    return count


def detect_transformation(text):
    """transformation ã‚¿ã‚°ã®æ¤œå‡º"""
    score = 0

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
    matches = count_matches(text, config.TRANSFORMATION_KEYWORDS)
    if matches > 0:
        score += config.TAG_SCORES["transformation_keyword"] * matches

    # å¯¾ç¾©èªãƒšã‚¢æ¤œå‡º
    for word1, word2 in config.ANTONYM_PAIRS:
        pattern1 = r'\b' + re.escape(word1) + r'\b'
        pattern2 = r'\b' + re.escape(word2) + r'\b'
        if re.search(pattern1, text) and re.search(pattern2, text):
            score += config.TAG_SCORES["transformation_antonym_pair"]

    return score


def detect_boundary_crossing(text):
    """boundary_crossing ã‚¿ã‚°ã®æ¤œå‡º"""
    score = 0
    detected_domains = []

    # ãƒ‰ãƒ¡ã‚¤ãƒ³æ¤œå‡º
    for domain, keywords in config.DOMAINS.items():
        if count_matches(text, keywords) > 0:
            detected_domains.append(domain)

    # 2ãƒ‰ãƒ¡ã‚¤ãƒ³ä»¥ä¸Šã§æ¤œå‡º
    if len(detected_domains) >= 3:
        score += config.TAG_SCORES["boundary_crossing_3_domains"]
    elif len(detected_domains) >= 2:
        score += config.TAG_SCORES["boundary_crossing_2_domains"]

    # å¢ƒç•Œèªæ¤œå‡º
    if count_matches(text, config.BOUNDARY_KEYWORDS) > 0:
        score += config.TAG_SCORES["boundary_crossing_keyword"]

    return score


def detect_visibility_gain(text):
    """visibility_gain ã‚¿ã‚°ã®æ¤œå‡º"""
    score = 0
    
    matches = count_matches(text, config.VISIBILITY_KEYWORDS)
    if matches > 0:
        score += config.TAG_SCORES["visibility_gain_keyword"] * matches

    return min(score, config.TAG_SCORES["visibility_gain_combo"])  # ä¸Šé™è¨­å®š


def detect_value_redefinition(text):
    """value_redefinition ã‚¿ã‚°ã®æ¤œå‡º"""
    score = 0

    matches = count_matches(text, config.VALUE_KEYWORDS)
    if matches > 0:
        score += config.TAG_SCORES["value_redefinition_keyword"] * matches

    for word1, word2 in config.CATEGORY_SHIFT_PAIRS:
        pattern1 = r'\b' + re.escape(word1) + r'\b'
        pattern2 = r'\b' + re.escape(word2) + r'\b'
        if re.search(pattern1, text) and re.search(pattern2, text):
            score += config.TAG_SCORES["value_redefinition_pair"]

    return score


def detect_scale_shift(text):
    """scale_shift ã‚¿ã‚°ã®æ¤œå‡º"""
    score = 0

    for word1, word2 in config.SCALE_PAIRS:
        pattern1 = r'\b' + re.escape(word1) + r'\b'
        pattern2 = r'\b' + re.escape(word2) + r'\b'
        if re.search(pattern1, text) and re.search(pattern2, text):
            score += config.TAG_SCORES["scale_shift_pair"]

    matches = count_matches(text, config.SCALE_KEYWORDS)
    if matches > 0:
        score += config.TAG_SCORES["scale_shift_paradox"]

    return score


def detect_ontology_shift(text, existing_tags):
    """ontology_shift ã‚¿ã‚°ã®æ¤œå‡ºï¼ˆä»–ã‚¿ã‚°ã¨ã®çµ„ã¿åˆã‚ã›ã§ã®ã¿ï¼‰"""
    has_ontology_keyword = count_matches(text, config.ONTOLOGY_KEYWORDS) > 0
    has_questioning = count_matches(text, config.QUESTIONING_KEYWORDS) > 0

    if not has_ontology_keyword:
        return 0

    # transformation ã¾ãŸã¯ boundary_crossing ã¨çµ„ã¿åˆã‚ã›
    relevant_tags = ["transformation", "boundary_crossing"]
    has_relevant = any(t["name"] in relevant_tags for t in existing_tags)

    if has_relevant and has_questioning:
        return config.TAG_SCORES["ontology_shift"]

    return 0


# =============================================================================
# SCORING
# =============================================================================

def calculate_base_scores(article):
    """è¨˜äº‹ã®åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼ˆæ§‹é€ ãƒ»è©±é¡Œæ€§ï¼‰ã‚’è¨ˆç®—"""
    # æ§‹é€ å¼·åº¦ã‚¹ã‚³ã‚¢
    structural_score = sum(tag["score"] for tag in article["tags"])

    # è©±é¡Œæ€§ã‚¹ã‚³ã‚¢
    timeliness_score = calculate_timeliness_score(article)

    article["structural_score"] = structural_score
    article["timeliness_score"] = timeliness_score
    return article

def calculate_weighted_score(article, weights):
    """æŒ‡å®šã•ã‚ŒãŸé‡ã¿ã§æœ€çµ‚ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    final_score = (
        article["structural_score"] * weights["structural"] +
        article["timeliness_score"] * weights["timeliness"]
    )
    return final_score

def calculate_timeliness_score(article):
    """è©±é¡Œæ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    import math
    score = 0

    # ã‚½ãƒ¼ã‚¹æ ¼ä»˜ã‘
    source = article.get("source", "")
    for domain, weight in config.SOURCE_WEIGHT.items():
        if domain in source:
            score += weight
            break
    else:
        score += config.SOURCE_WEIGHT["default"]

    # é®®åº¦ã‚¹ã‚³ã‚¢
    published = article.get("published", "")
    if published:
        try:
            # feedparserã®æ—¥ä»˜å½¢å¼ã«å¯¾å¿œ
            pub_date = feedparser._parse_date(published)
            if pub_date:
                pub_datetime = datetime(*pub_date[:6])
                age_hours = (datetime.utcnow() - pub_datetime).total_seconds() / 3600

                if age_hours <= 24:
                    score += 3
                elif age_hours <= 48:
                    score += 2
                elif age_hours <= 168:  # 7 days
                    score += 1
        except Exception:
            pass

    # Redditäººæ°—åº¦ã‚¹ã‚³ã‚¢ï¼ˆã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆï¼‰
    reddit_score = article.get("reddit_score")
    reddit_comments = article.get("reddit_comments")

    if reddit_score is not None and reddit_comments is not None:
        # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚’è©•ä¾¡ï¼ˆå¤§ããªæ•°å€¤å·®ã‚’ç·©å’Œï¼‰
        # log(upvotes + 1) + log(comments + 1) ã‚’æ­£è¦åŒ–
        engagement = math.log(reddit_score + 1) + math.log(reddit_comments + 1)
        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ã‚’0-5ã®ç¯„å›²ã«æ­£è¦åŒ–
        # å…¸å‹çš„ãªäººæ°—æŠ•ç¨¿: upvotes=100, comments=20 â†’ log(101)+log(21) â‰ˆ 4.6+3.0 = 7.6
        # ã‚¹ã‚³ã‚¢åŒ–: engagement / 2 (æœ€å¤§å€¤ã‚’5ç¨‹åº¦ã«æŠ‘ãˆã‚‹)
        engagement_score = min(engagement / 2, 5)
        score += engagement_score

    return score


# =============================================================================
# UTILS
# =============================================================================

def normalize_url(url):
    """URLã‚’æ­£è¦åŒ–ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰"""
    if not url:
        return ""
    try:
        parsed = urlparse(url.strip())
        # ã‚¹ã‚­ãƒ¼ãƒ ã¨ãƒãƒƒãƒˆãƒ­ãƒƒã‚¯ã‚’å°æ–‡å­—åŒ–
        scheme = parsed.scheme.lower()
        netloc = parsed.netloc.lower()
        path = parsed.path
        # æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’é™¤å»
        if path.endswith("/"):
            path = path[:-1]
        
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ãã®ã¾ã¾ï¼ˆè¨˜äº‹IDãªã©ãŒå«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚‹ãŸã‚ï¼‰
        # ãŸã ã—ã€ç‰¹å®šã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å‰Šé™¤ã—ã¦ã‚‚ã„ã„ã‹ã‚‚ã—ã‚Œãªã„ãŒã€ä»Šå›ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«
        
        return f"{scheme}://{netloc}{path}"
    except Exception:
        return url.strip()


# =============================================================================
# SELECTION LOGIC
# =============================================================================

def filter_bigtech_products(articles):
    """BigTechã‚«ãƒ†ã‚´ãƒªã§è£½å“è²©å£²æƒ…å ±ã‚’é™¤å¤–"""
    filtered = []
    for article in articles:
        text = f"{article.get('title', '')} {article.get('summary', '')}".lower()

        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
        has_product_keyword = False
        for keyword in config.BIGTECH_PRODUCT_EXCLUDE_KEYWORDS:
            if keyword.lower() in text:
                has_product_keyword = True
                break

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ãªã„è¨˜äº‹ã®ã¿è¿½åŠ 
        if not has_product_keyword:
            filtered.append(article)

    return filtered


def select_articles(articles, state, category):
    """ã‚«ãƒ†ã‚´ãƒªè¨­å®šã«åŸºã¥ã„ã¦è¨˜äº‹ã‚’é¸å‡º"""
    # 1. ã™ã¹ã¦ã®å€™è£œã‚’æ­£è¦åŒ–URLã§ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«ã™ã‚‹
    unique_candidates_map = {}
    for a in articles:
        norm_url = normalize_url(a["url"])
        if norm_url not in unique_candidates_map:
            unique_candidates_map[norm_url] = a
    
    # 2. æ—¢ã«æŠ•ç¨¿æ¸ˆã¿ã®URLã‚’é™¤å¤–ï¼ˆæ­£è¦åŒ–ã—ã¦æ¯”è¼ƒï¼‰
    posted_entries = state["posted"].get(category, [])
    posted_urls = {normalize_url(entry["url"]) for entry in posted_entries}
    
    new_articles = []
    for url, a in unique_candidates_map.items():
        if url not in posted_urls:
            new_articles.append(a)
    
    # 3. pendingè¨˜äº‹ã¨ãƒãƒ¼ã‚¸
    pending = state["pending"].get(category, [])
    # ã™ã§ã«new_articlesã«ã‚ã‚‹ã‚‚ã®ã¯pendingã‹ã‚‰é™¤å¤–
    new_urls = {normalize_url(a["url"]) for a in new_articles}
    
    # pendingå†…ã®é‡è¤‡ã‚‚æ’é™¤
    unique_pending = []
    seen_pending_urls = set()
    for p in pending:
        p_url = normalize_url(p["url"])
        if p_url not in new_urls and p_url not in posted_urls and p_url not in seen_pending_urls:
            unique_pending.append(p)
            seen_pending_urls.add(p_url)

    all_candidates = new_articles + unique_pending

    # BigTechã‚«ãƒ†ã‚´ãƒª: è£½å“è²©å£²æƒ…å ±ã‚’é™¤å¤–
    if category == "bigtech":
        all_candidates = filter_bigtech_products(all_candidates)

    cat_config = config.CATEGORIES[category]
    mode = cat_config["selection_mode"]

    selected = []

    if mode == "trending_only":
        # è©±é¡Œæ€§é‡è¦–ã®ã¿
        weights = cat_config["weights"]
        for a in all_candidates:
            a["final_score"] = round(calculate_weighted_score(a, weights), 2)

        sorted_articles = sorted(all_candidates, key=lambda x: x["final_score"], reverse=True)
        # ã‚½ãƒ¼ã‚¹å¤šæ§˜æ€§ã‚’è€ƒæ…®ã—ã¦é¸å‡º
        # BigTechã‚«ãƒ†ã‚´ãƒªã¯1ã‚½ãƒ¼ã‚¹1è¨˜äº‹ã®ã¿
        max_per_source = 1 if category in ["bigtech", "ai"] else 2
        selected = ensure_source_diversity(sorted_articles, cat_config["posts_per_day"], max_per_source=max_per_source)

    elif mode == "dual":
        # æ§‹é€ é‡è¦–1 + è©±é¡Œé‡è¦–1
        
        # 1. æ§‹é€ é‡è¦–
        weights_struct = cat_config["weights_structural"]
        for a in all_candidates:
            a["temp_score_struct"] = calculate_weighted_score(a, weights_struct)
        
        sorted_struct = sorted(all_candidates, key=lambda x: x["temp_score_struct"], reverse=True)
        struct_candidates = [a for a in sorted_struct if a["tags"]]
        
        top_structural = struct_candidates[:1]
        
        # 2. è©±é¡Œé‡è¦–
        weights_trend = cat_config["weights_trending"]
        # é‡è¤‡æ’é™¤ï¼ˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆIDãƒ™ãƒ¼ã‚¹ã ã¨å±é™ºãªã®ã§URLãƒ™ãƒ¼ã‚¹ã§ï¼‰
        selected_urls = {normalize_url(a["url"]) for a in top_structural}
        remaining = [a for a in all_candidates if normalize_url(a["url"]) not in selected_urls]
        
        for a in remaining:
            a["temp_score_trend"] = calculate_weighted_score(a, weights_trend)
            
        sorted_trend = sorted(remaining, key=lambda x: x["temp_score_trend"], reverse=True)
        
        # ã‚½ãƒ¼ã‚¹é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ§‹é€ æ ã¨åŒã˜ã‚½ãƒ¼ã‚¹ã°ã‹ã‚Šã«ãªã‚‰ãªã„ã‚ˆã†ã«ï¼‰
        top_trending = []
        if sorted_trend:
            # æ§‹é€ æ ã®ã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª
            struct_sources = [urlparse(a["url"]).netloc for a in top_structural]
            
            # å¯èƒ½ãªé™ã‚Šç•°ãªã‚‹ã‚½ãƒ¼ã‚¹ã‚’é¸ã¶
            for a in sorted_trend:
                if urlparse(a["url"]).netloc not in struct_sources:
                    top_trending.append(a)
                    break
            
            # è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã‚¹ã‚³ã‚¢1ä½ã‚’é¸ã¶
            if not top_trending and sorted_trend:
                top_trending.append(sorted_trend[0])
        
        # çµ±åˆ
        selected = top_structural + top_trending
        
        # final_scoreæ›´æ–°
        for a in top_structural:
            a["final_score"] = round(a.get("temp_score_struct", 0), 2)
        for a in top_trending:
            a["final_score"] = round(a.get("temp_score_trend", 0), 2)


    elif mode == "dual_enhanced":
        # æ§‹é€ é‡è¦–2 + è©±é¡Œé‡è¦–2 (AI)

        # RedditæŠ•ç¨¿æ•°ã®ä¸Šé™ã‚’å–å¾—
        reddit_max = config.REDDIT_MAX_POSTS.get(category, config.REDDIT_MAX_POSTS["default"])
        reddit_count = 0

        # 1. æ§‹é€ é‡è¦–
        weights_struct = cat_config["weights_structural"]
        for a in all_candidates:
            a["temp_score_struct"] = calculate_weighted_score(a, weights_struct)

        sorted_struct = sorted(all_candidates, key=lambda x: x["temp_score_struct"], reverse=True)
        struct_candidates = [a for a in sorted_struct if a["tags"]]

        # ã‚½ãƒ¼ã‚¹å¤šæ§˜æ€§ã‚’ç¢ºä¿ã—ã¤ã¤2ã¤é¸ã¶ï¼ˆAIã‚«ãƒ†ã‚´ãƒªã¯1ã‚½ãƒ¼ã‚¹1è¨˜äº‹ï¼‰
        max_per_source = 1 if category == "ai" else 2

        # Redditä¸Šé™ã‚’è€ƒæ…®ã—ãªãŒã‚‰é¸å‡º
        top_structural = []
        source_counts = Counter()
        for article in struct_candidates:
            if len(top_structural) >= 2:
                break

            domain = urlparse(article["url"]).netloc
            # RedditæŠ•ç¨¿ã¯reddit_scoreãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§åˆ¤å®šï¼ˆURLã§ã¯ãªãï¼‰
            is_reddit = article.get("reddit_score") is not None

            # Redditä¸Šé™ãƒã‚§ãƒƒã‚¯
            if is_reddit and reddit_count >= reddit_max:
                continue

            # ã‚½ãƒ¼ã‚¹å¤šæ§˜æ€§ãƒã‚§ãƒƒã‚¯
            if source_counts[domain] < max_per_source:
                top_structural.append(article)
                source_counts[domain] += 1
                if is_reddit:
                    reddit_count += 1

        # ã‚¿ã‚°å¤šæ§˜æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆRedditåˆ¶é™ã‚‚è€ƒæ…®ï¼‰
        if category == "ai":
            # Redditåˆ¶é™ã‚’è€ƒæ…®ã—ãŸã‚¿ã‚°å¤šæ§˜æ€§ãƒã‚§ãƒƒã‚¯
            top_structural = ensure_tag_diversity_with_reddit_limit(
                top_structural, struct_candidates, reddit_count, reddit_max
            )
            # reddit_countã‚’å†è¨ˆç®—ï¼ˆreddit_scoreãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§åˆ¤å®šï¼‰
            reddit_count = sum(1 for a in top_structural if a.get("reddit_score") is not None)
        else:
            top_structural = ensure_tag_diversity(top_structural, struct_candidates)

        # 2. è©±é¡Œé‡è¦–
        weights_trend = cat_config["weights_trending"]
        selected_urls = {normalize_url(a["url"]) for a in top_structural}
        remaining = [a for a in all_candidates if normalize_url(a["url"]) not in selected_urls]

        for a in remaining:
            a["temp_score_trend"] = calculate_weighted_score(a, weights_trend)

        sorted_trend = sorted(remaining, key=lambda x: x["temp_score_trend"], reverse=True)

        # ã“ã¡ã‚‰ã‚‚ã‚½ãƒ¼ã‚¹å¤šæ§˜æ€§ã‚’ç¢ºä¿ï¼ˆå…¨ä½“ã§ã®ãƒãƒ©ãƒ³ã‚¹ã‚‚è€ƒæ…®ã—ãŸã„ãŒã€ã¾ãšã¯ã“ã®æ å†…ã§ï¼‰
        # æ§‹é€ æ ã§ã™ã§ã«é¸ã°ã‚ŒãŸã‚½ãƒ¼ã‚¹ã¯å„ªå…ˆåº¦ã‚’ä¸‹ã’ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å…¥ã‚Œã‚‹ã¨ã‚ˆã‚Šè‰¯ã„
        struct_sources = Counter([urlparse(a["url"]).netloc for a in top_structural])

        top_trending = []
        for a in sorted_trend:
            if len(top_trending) >= 2:
                break

            domain = urlparse(a["url"]).netloc
            # RedditæŠ•ç¨¿ã¯reddit_scoreãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§åˆ¤å®š
            is_reddit = a.get("reddit_score") is not None

            # Redditä¸Šé™ãƒã‚§ãƒƒã‚¯
            if is_reddit and reddit_count >= reddit_max:
                continue

            # AIã‚«ãƒ†ã‚´ãƒªã¯å…¨ä½“ã§åŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯æœ€å¤§1ã¤ã¾ã§ï¼ˆæ§‹é€ æ ã§ã™ã§ã«é¸ã°ã‚ŒãŸã‚½ãƒ¼ã‚¹ã¯é™¤å¤–ï¼‰
            max_allowed = 1 if category == "ai" else 2
            if struct_sources.get(domain, 0) + 1 <= max_allowed: # ã“ã“ã§ã®+1ã¯ä»Šå›é¸ã¶åˆ†
                 top_trending.append(a)
                 struct_sources[domain] = struct_sources.get(domain, 0) + 1
                 if is_reddit:
                     reddit_count += 1
        
        # ã‚‚ã—å³ã—ã™ãã¦åŸ‹ã¾ã‚‰ãªã‹ã£ãŸå ´åˆã€åˆ¶é™ã‚’ç·©ã‚ã¦åŸ‹ã‚ã‚‹
        if len(top_trending) < 2:
            current_trending_urls = {normalize_url(a["url"]) for a in top_trending}
            remaining_trend = [a for a in sorted_trend if normalize_url(a["url"]) not in current_trending_urls]
            needed = 2 - len(top_trending)

            # AIã‚«ãƒ†ã‚´ãƒªã®å ´åˆã¯ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã‚‚ã‚½ãƒ¼ã‚¹é‡è¤‡ã‚’é¿ã‘ã‚‹
            if category == "ai":
                for candidate in remaining_trend:
                    if len(top_trending) >= 2:
                        break
                    candidate_domain = urlparse(candidate["url"]).netloc
                    # RedditæŠ•ç¨¿ã¯reddit_scoreãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§åˆ¤å®š
                    is_reddit_fallback = candidate.get("reddit_score") is not None

                    # Redditä¸Šé™ãƒã‚§ãƒƒã‚¯
                    if is_reddit_fallback and reddit_count >= reddit_max:
                        continue

                    if struct_sources.get(candidate_domain, 0) == 0:
                        top_trending.append(candidate)
                        struct_sources[candidate_domain] = 1
                        if is_reddit_fallback:
                            reddit_count += 1
            else:
                top_trending.extend(remaining_trend[:needed])

        # çµ±åˆ
        selected = top_structural + top_trending

        # final_scoreæ›´æ–°
        for a in top_structural:
            a["final_score"] = round(a.get("temp_score_struct", 0), 2)
        for a in top_trending:
            a["final_score"] = round(a.get("temp_score_trend", 0), 2)

    return selected


def ensure_source_diversity(candidates, limit, max_per_source=2):
    """åŒä¸€ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®é¸å‡ºã‚’åˆ¶é™ã—ã¦è¨˜äº‹ã‚’é¸ã¶"""
    selected = []
    source_counts = Counter()
    
    for article in candidates:
        if len(selected) >= limit:
            break
            
        domain = urlparse(article["url"]).netloc
        if source_counts[domain] < max_per_source:
            selected.append(article)
            source_counts[domain] += 1
            
    return selected



def ensure_tag_diversity(current_top, candidates):
    """ã‚¿ã‚°ã®å¤šæ§˜æ€§ã‚’ç¢ºä¿ï¼ˆæ§‹é€ é‡è¦–æ ç”¨ï¼‰"""
    if len(current_top) < 2:
        return current_top

    top1 = current_top[0]
    top2 = current_top[1]

    tags1 = {t["name"] for t in top1.get("tags", [])}
    tags2 = {t["name"] for t in top2.get("tags", [])}

    # åŒã˜ã‚¿ã‚°ã‚»ãƒƒãƒˆãªã‚‰ã€å€™è£œãƒªã‚¹ãƒˆã®3ç•ªç›®ä»¥é™ã‹ã‚‰ç•°ãªã‚‹ã‚¿ã‚°ã‚’æŒã¤ã‚‚ã®ã‚’æ¢ã™
    if tags1 == tags2 and len(candidates) > 2:
        for article in candidates[2:10]:  # ä¸Šä½10ä»¶ã¾ã§æ¢ç´¢
            article_tags = {t["name"] for t in article.get("tags", [])}
            if article_tags != tags1:
                return [top1, article]

    return current_top


def ensure_tag_diversity_with_reddit_limit(current_top, candidates, current_reddit_count, reddit_max):
    """ã‚¿ã‚°ã®å¤šæ§˜æ€§ã‚’ç¢ºä¿ï¼ˆRedditåˆ¶é™ã‚‚è€ƒæ…®ï¼‰"""
    if len(current_top) < 2:
        return current_top

    top1 = current_top[0]
    top2 = current_top[1]

    tags1 = {t["name"] for t in top1.get("tags", [])}
    tags2 = {t["name"] for t in top2.get("tags", [])}

    # åŒã˜ã‚¿ã‚°ã‚»ãƒƒãƒˆãªã‚‰ã€å€™è£œãƒªã‚¹ãƒˆã®3ç•ªç›®ä»¥é™ã‹ã‚‰ç•°ãªã‚‹ã‚¿ã‚°ã‚’æŒã¤ã‚‚ã®ã‚’æ¢ã™
    if tags1 == tags2 and len(candidates) > 2:
        for article in candidates[2:10]:  # ä¸Šä½10ä»¶ã¾ã§æ¢ç´¢
            article_tags = {t["name"] for t in article.get("tags", [])}
            if article_tags != tags1:
                # Redditåˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆreddit_scoreãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§åˆ¤å®šï¼‰
                is_reddit = article.get("reddit_score") is not None
                is_top2_reddit = top2.get("reddit_score") is not None

                # top2ãŒRedditã§ã€æ–°ã—ã„è¨˜äº‹ã‚‚Redditã®å ´åˆã€Redditæ•°ã¯å¤‰ã‚ã‚‰ãªã„
                # top2ãŒRedditã§ãªãã€æ–°ã—ã„è¨˜äº‹ãŒRedditã®å ´åˆã€Redditæ•°ãŒå¢—ãˆã‚‹
                if is_reddit and not is_top2_reddit:
                    # Redditä¸Šé™ã‚’ãƒã‚§ãƒƒã‚¯
                    if current_reddit_count >= reddit_max:
                        continue  # ã“ã®è¨˜äº‹ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ¬¡ã‚’æ¢ã™

                return [top1, article]

    return current_top


def update_pending(articles, selected, state, category):
    """pendingè¨˜äº‹ã‚’æ›´æ–°"""
    selected_urls = {a["url"] for a in selected}
    
    # é¸æŠã•ã‚Œãªã‹ã£ãŸè¨˜äº‹
    remaining = [a for a in articles if a["url"] not in selected_urls]
    
    # ç°¡æ˜“ã‚¹ã‚³ã‚¢ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¦ã‚§ã‚¤ãƒˆï¼‰ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’ä¿æŒ
    # â€» æ¬¡å›å®Ÿè¡Œæ™‚ã«é©åˆ‡ãªãƒ¢ãƒ¼ãƒ‰ã§å†è¨ˆç®—ã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯æš«å®šçš„ã«ä¿æŒ
    default_weights = {"structural": 0.5, "timeliness": 0.5}
    for a in remaining:
        if "final_score" not in a or a["final_score"] == 0:
             a["final_score"] = calculate_weighted_score(a, default_weights)

    sorted_pending = sorted(remaining, key=lambda x: x.get("final_score", 0), reverse=True)
    
    state["pending"][category] = sorted_pending[:10]  # ä¸Šä½10ä»¶ä¿æŒ

    return state


# =============================================================================
# DISCORD POSTING
# =============================================================================

def post_to_discord(article, category, dry_run=False):
    """Discordã«æŠ•ç¨¿"""
    webhook_env = f"DISCORD_WEBHOOK_{category.upper()}"
    webhook_url = os.environ.get(webhook_env)

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆï¼ˆdry-runæ™‚ã‚‚è¡¨ç¤ºã™ã‚‹ãŸã‚å…ˆã«ä½œæˆï¼‰
    cat_info = config.CATEGORIES[category]
    tag_names = " Ã— ".join(t["name"] for t in article["tags"]) if article["tags"] else "no tags"
    date_str = datetime.utcnow().strftime("%Y-%m-%d")

    # æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚Œã°ä½¿ç”¨
    title = article.get('title_ja', article['title'])

    message = f"""{cat_info['emoji']} **{cat_info['name']}** | {date_str}

**[{tag_names}]**
{title}

ğŸ”— {article['url']}
ğŸ“° {article['source']} | Score: {article['final_score']}"""

    if dry_run:
        print(f"\n[DRY-RUN] Would post to {category}:")
        print(message)
        print("-" * 50)
        return True

    # Webhook URLãƒã‚§ãƒƒã‚¯ï¼ˆå®ŸæŠ•ç¨¿æ™‚ã®ã¿ï¼‰
    if not webhook_url:
        print(f"[WARN] {webhook_env} not set, skipping post")
        return False

    # å®Ÿéš›ã«æŠ•ç¨¿
    try:
        response = requests.post(
            webhook_url,
            json={"content": message},
            headers={"Content-Type": "application/json"},
            timeout=30
        )
        response.raise_for_status()
        print(f"[OK] Posted to {category}: {article['title'][:50]}...")
        return True
    except requests.RequestException as e:
        print(f"[ERROR] Failed to post to Discord: {e}")
        return False


# =============================================================================
# MAIN
# =============================================================================

def process_category(category, state, dry_run=False):
    """1ã‚«ãƒ†ã‚´ãƒªã‚’å‡¦ç†"""
    print(f"\n{'='*50}")
    print(f"Processing: {category}")
    print(f"{'='*50}")

    # RSSå–å¾—
    articles = fetch_all_feeds(category)
    print(f"Fetched {len(articles)} articles")

    if not articles:
        return state

    # ã‚¿ã‚°ä»˜ä¸ã¨åŸºæœ¬ã‚¹ã‚³ã‚¢è¨ˆç®—
    for article in articles:
        detect_tags(article)
        calculate_base_scores(article)

    # è¨˜äº‹é¸æŠï¼ˆã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ãƒ­ã‚¸ãƒƒã‚¯ã§ï¼‰
    selected = select_articles(articles, state, category)
    print(f"Selected {len(selected)} articles for posting")

    # é¸æŠã•ã‚ŒãŸè¨˜äº‹ã®ã¿ç¿»è¨³ï¼ˆAPIç¯€ç´„ï¼‰
    selected = translate_articles(selected)

    # æŠ•ç¨¿
    for article in selected:
        if post_to_discord(article, category, dry_run):
            # æŠ•ç¨¿æˆåŠŸã—ãŸã‚‰postedã«è¿½åŠ 
            if not dry_run:
                state["posted"][category].append({
                    "url": article["url"],
                    "posted_at": datetime.utcnow().strftime("%Y-%m-%d"),
                    "score": article["final_score"],
                    "tags": [t["name"] for t in article["tags"]]
                })

    # pendingæ›´æ–°
    # selectedã«å…¥ã‚‰ãªã‹ã£ãŸè¨˜äº‹ã®ä¸­ã‹ã‚‰pendingå€™è£œã‚’é¸ã¶
    state = update_pending(articles, selected, state, category)

    return state


def main():
    parser = argparse.ArgumentParser(description="News Curation Bot")
    parser.add_argument("--dry-run", action="store_true", help="Don't actually post")
    parser.add_argument("--category", type=str, help="Process specific category only")
    args = parser.parse_args()

    print("News Curation Bot starting...")
    print(f"Dry run: {args.dry_run}")

    # çŠ¶æ…‹èª­ã¿è¾¼ã¿
    state = load_state()
    state = cleanup_old_entries(state)

    # ã‚«ãƒ†ã‚´ãƒªå‡¦ç†
    categories = [args.category] if args.category else list(config.CATEGORIES.keys())

    for category in categories:
        if category not in config.CATEGORIES:
            print(f"[ERROR] Unknown category: {category}")
            continue
        state = process_category(category, state, args.dry_run)

    # çŠ¶æ…‹ä¿å­˜
    if not args.dry_run:
        save_state(state)
        print("\nState saved.")

    print("\nDone!")


if __name__ == "__main__":
    main()