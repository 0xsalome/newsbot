#!/usr/bin/env python3
"""
News Curation Bot - Main Script
æ§‹é€ çš„é¡ä¼¼æ€§ã«åŸºã¥ãè‡ªå‹•ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

Usage:
    python curate.py              # é€šå¸¸å®Ÿè¡Œ
    python curate.py --dry-run    # æŠ•ç¨¿ã›ãšã«ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ç¢ºèª
    python curate.py --category science  # ç‰¹å®šã‚«ãƒ†ã‚´ãƒªã®ã¿
"""

import argparse
import json
import os
import re
import sys
import time
from datetime import datetime, timedelta
from urllib.parse import urlparse

import feedparser
import requests

import config


# =============================================================================
# STATE MANAGEMENT
# =============================================================================

def load_state(filepath="state.json"):
    """state.jsonã‚’èª­ã¿è¾¼ã‚€"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            state = json.load(f)
            # æ–°è¦ã‚«ãƒ†ã‚´ãƒªã®åˆæœŸåŒ–
            for category in config.CATEGORIES:
                if category not in state["posted"]:
                    state["posted"][category] = []
                if category not in state["pending"]:
                    state["pending"][category] = []
            return state
    except FileNotFoundError:
        return create_initial_state()


def save_state(state, filepath="state.json"):
    """state.jsonã‚’ä¿å­˜ã™ã‚‹"""
    state["meta"]["last_updated"] = datetime.utcnow().isoformat() + "Z"
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)


def create_initial_state():
    """åˆæœŸçŠ¶æ…‹ã‚’ä½œæˆ"""
    return {
        "meta": {
            "last_updated": None,
            "retention_days": config.POSTED_RETENTION_DAYS
        },
        "posted": {cat: [] for cat in config.CATEGORIES},
        "pending": {cat: [] for cat in config.CATEGORIES}
    }


def cleanup_old_entries(state):
    """å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤"""
    now = datetime.utcnow()

    # posted: 7æ—¥ä»¥ä¸Šå¤ã„ã‚‚ã®ã‚’å‰Šé™¤
    for category in state["posted"]:
        state["posted"][category] = [
            entry for entry in state["posted"][category]
            if _is_within_days(entry.get("posted_at"), config.POSTED_RETENTION_DAYS, now)
        ]

    # pending: 3æ—¥ä»¥ä¸Šå¤ã„ã‚‚ã®ã‚’å‰Šé™¤
    for category in state["pending"]:
        state["pending"][category] = [
            entry for entry in state["pending"][category]
            if _is_within_days(entry.get("fetched_at"), config.PENDING_RETENTION_DAYS, now)
        ]

    return state


def _is_within_days(date_str, days, now):
    """æ—¥ä»˜ãŒæŒ‡å®šæ—¥æ•°ä»¥å†…ã‹ãƒã‚§ãƒƒã‚¯"""
    if not date_str:
        return False
    try:
        date = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
        return (now - date.replace(tzinfo=None)).days <= days
    except (ValueError, AttributeError):
        return False


# =============================================================================
# RSS FETCHING
# =============================================================================

def fetch_rss(url):
    """RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—"""
    try:
        headers = {"User-Agent": config.USER_AGENT}
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        return feedparser.parse(response.content)
    except requests.RequestException as e:
        print(f"[ERROR] Failed to fetch {url}: {e}")
        return None


def fetch_all_feeds(category):
    """ã‚«ãƒ†ã‚´ãƒªã®ã™ã¹ã¦ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—"""
    articles = []
    sources = config.RSS_SOURCES.get(category, [])

    for url in sources:
        feed = fetch_rss(url)
        if feed and feed.entries:
            for entry in feed.entries:
                article = parse_feed_entry(entry, url)
                if article:
                    articles.append(article)
        time.sleep(config.REQUEST_INTERVAL_SECONDS)

    return articles


def parse_feed_entry(entry, source_url):
    """RSSã‚¨ãƒ³ãƒˆãƒªã‚’è¨˜äº‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
    try:
        return {
            "url": entry.get("link", ""),
            "title": entry.get("title", ""),
            "summary": entry.get("summary", entry.get("description", "")),
            "source": urlparse(source_url).netloc,
            "published": entry.get("published", ""),
            "fetched_at": datetime.utcnow().isoformat() + "Z",
            "tags": [],
            "structural_score": 0,
            "timeliness_score": 0,
            "final_score": 0,
        }
    except Exception as e:
        print(f"[ERROR] Failed to parse entry: {e}")
        return None


# =============================================================================
# TRANSLATION (DeepL API Free)
# =============================================================================

def translate_to_japanese(text):
    """DeepL APIã§æ—¥æœ¬èªã«ç¿»è¨³"""
    api_key = os.environ.get("DEEPL_API_KEY")
    if not api_key:
        return None

    try:
        response = requests.post(
            "https://api-free.deepl.com/v2/translate",
            data={
                "auth_key": api_key,
                "text": text,
                "target_lang": "JA"
            },
            timeout=10
        )
        response.raise_for_status()
        result = response.json()
        return result["translations"][0]["text"]
    except Exception as e:
        print(f"[WARN] Translation failed: {e}")
        return None


def translate_articles(articles):
    """è¨˜äº‹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ—¥æœ¬èªã«ç¿»è¨³"""
    api_key = os.environ.get("DEEPL_API_KEY")
    if not api_key:
        print("[INFO] DEEPL_API_KEY not set, skipping translation")
        return articles

    print(f"Translating {len(articles)} article titles...")
    for article in articles:
        title_ja = translate_to_japanese(article["title"])
        article["title_ja"] = title_ja if title_ja else article["title"]
        time.sleep(0.1)  # Rate limitå¯¾ç­–

    return articles


def strip_html(text):
    """HTMLã‚¿ã‚°ã‚’é™¤å»"""
    return re.sub(r'<[^>]+>', '', text)


# =============================================================================
# TAG DETECTION
# =============================================================================

def detect_tags(article):
    """è¨˜äº‹ã«ã‚¿ã‚°ã‚’ä»˜ä¸"""
    # HTMLé™¤å»ã¨æ­£è¦åŒ–
    raw_text = f"{article['title']} {article['summary']}"
    text = strip_html(raw_text).lower()
    
    tags = []

    # transformation
    score = detect_transformation(text)
    if score > 0:
        tags.append({"name": "transformation", "score": score})

    # boundary_crossing
    score = detect_boundary_crossing(text)
    if score > 0:
        tags.append({"name": "boundary_crossing", "score": score})

    # visibility_gain
    score = detect_visibility_gain(text)
    if score > 0:
        tags.append({"name": "visibility_gain", "score": score})

    # value_redefinition
    score = detect_value_redefinition(text)
    if score > 0:
        tags.append({"name": "value_redefinition", "score": score})

    # scale_shift
    score = detect_scale_shift(text)
    if score > 0:
        tags.append({"name": "scale_shift", "score": score})

    # ontology_shift (ä»–ã‚¿ã‚°ã¨ã®çµ„ã¿åˆã‚ã›ã§ã®ã¿ä»˜ä¸)
    if tags:  # ä»–ã®ã‚¿ã‚°ãŒã‚ã‚‹å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯
        score = detect_ontology_shift(text, tags)
        if score > 0:
            tags.append({"name": "ontology_shift", "score": score})

    article["tags"] = tags
    return article


def count_matches(text, keywords):
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆå˜èªå¢ƒç•Œã‚’è€ƒæ…®ï¼‰"""
    count = 0
    for keyword in keywords:
        # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«è¨˜å·ãŒå«ã¾ã‚Œã‚‹å ´åˆç”¨ï¼‰
        pattern = r'\b' + re.escape(keyword) + r'\b'
        if re.search(pattern, text):
            count += 1
    return count


def detect_transformation(text):
    """transformation ã‚¿ã‚°ã®æ¤œå‡º"""
    score = 0

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
    matches = count_matches(text, config.TRANSFORMATION_KEYWORDS)
    if matches > 0:
        score += config.TAG_SCORES["transformation_keyword"] * matches

    # å¯¾ç¾©èªãƒšã‚¢æ¤œå‡º
    for word1, word2 in config.ANTONYM_PAIRS:
        pattern1 = r'\b' + re.escape(word1) + r'\b'
        pattern2 = r'\b' + re.escape(word2) + r'\b'
        if re.search(pattern1, text) and re.search(pattern2, text):
            score += config.TAG_SCORES["transformation_antonym_pair"]

    return score


def detect_boundary_crossing(text):
    """boundary_crossing ã‚¿ã‚°ã®æ¤œå‡º"""
    score = 0
    detected_domains = []

    # ãƒ‰ãƒ¡ã‚¤ãƒ³æ¤œå‡º
    for domain, keywords in config.DOMAINS.items():
        if count_matches(text, keywords) > 0:
            detected_domains.append(domain)

    # 2ãƒ‰ãƒ¡ã‚¤ãƒ³ä»¥ä¸Šã§æ¤œå‡º
    if len(detected_domains) >= 3:
        score += config.TAG_SCORES["boundary_crossing_3_domains"]
    elif len(detected_domains) >= 2:
        score += config.TAG_SCORES["boundary_crossing_2_domains"]

    # å¢ƒç•Œèªæ¤œå‡º
    if count_matches(text, config.BOUNDARY_KEYWORDS) > 0:
        score += config.TAG_SCORES["boundary_crossing_keyword"]

    return score


def detect_visibility_gain(text):
    """visibility_gain ã‚¿ã‚°ã®æ¤œå‡º"""
    score = 0
    
    matches = count_matches(text, config.VISIBILITY_KEYWORDS)
    if matches > 0:
        score += config.TAG_SCORES["visibility_gain_keyword"] * matches

    return min(score, config.TAG_SCORES["visibility_gain_combo"])  # ä¸Šé™è¨­å®š


def detect_value_redefinition(text):
    """value_redefinition ã‚¿ã‚°ã®æ¤œå‡º"""
    score = 0

    matches = count_matches(text, config.VALUE_KEYWORDS)
    if matches > 0:
        score += config.TAG_SCORES["value_redefinition_keyword"] * matches

    for word1, word2 in config.CATEGORY_SHIFT_PAIRS:
        pattern1 = r'\b' + re.escape(word1) + r'\b'
        pattern2 = r'\b' + re.escape(word2) + r'\b'
        if re.search(pattern1, text) and re.search(pattern2, text):
            score += config.TAG_SCORES["value_redefinition_pair"]

    return score


def detect_scale_shift(text):
    """scale_shift ã‚¿ã‚°ã®æ¤œå‡º"""
    score = 0

    for word1, word2 in config.SCALE_PAIRS:
        pattern1 = r'\b' + re.escape(word1) + r'\b'
        pattern2 = r'\b' + re.escape(word2) + r'\b'
        if re.search(pattern1, text) and re.search(pattern2, text):
            score += config.TAG_SCORES["scale_shift_pair"]

    matches = count_matches(text, config.SCALE_KEYWORDS)
    if matches > 0:
        score += config.TAG_SCORES["scale_shift_paradox"]

    return score


def detect_ontology_shift(text, existing_tags):
    """ontology_shift ã‚¿ã‚°ã®æ¤œå‡ºï¼ˆä»–ã‚¿ã‚°ã¨ã®çµ„ã¿åˆã‚ã›ã§ã®ã¿ï¼‰"""
    has_ontology_keyword = count_matches(text, config.ONTOLOGY_KEYWORDS) > 0
    has_questioning = count_matches(text, config.QUESTIONING_KEYWORDS) > 0

    if not has_ontology_keyword:
        return 0

    # transformation ã¾ãŸã¯ boundary_crossing ã¨çµ„ã¿åˆã‚ã›
    relevant_tags = ["transformation", "boundary_crossing"]
    has_relevant = any(t["name"] in relevant_tags for t in existing_tags)

    if has_relevant and has_questioning:
        return config.TAG_SCORES["ontology_shift"]

    return 0


# =============================================================================
# SCORING
# =============================================================================

def calculate_base_scores(article):
    """è¨˜äº‹ã®åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼ˆæ§‹é€ ãƒ»è©±é¡Œæ€§ï¼‰ã‚’è¨ˆç®—"""
    # æ§‹é€ å¼·åº¦ã‚¹ã‚³ã‚¢
    structural_score = sum(tag["score"] for tag in article["tags"])

    # è©±é¡Œæ€§ã‚¹ã‚³ã‚¢
    timeliness_score = calculate_timeliness_score(article)

    article["structural_score"] = structural_score
    article["timeliness_score"] = timeliness_score
    return article

def calculate_weighted_score(article, weights):
    """æŒ‡å®šã•ã‚ŒãŸé‡ã¿ã§æœ€çµ‚ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    final_score = (
        article["structural_score"] * weights["structural"] +
        article["timeliness_score"] * weights["timeliness"]
    )
    return final_score

def calculate_timeliness_score(article):
    """è©±é¡Œæ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    score = 0

    # ã‚½ãƒ¼ã‚¹æ ¼ä»˜ã‘
    source = article.get("source", "")
    for domain, weight in config.SOURCE_WEIGHT.items():
        if domain in source:
            score += weight
            break
    else:
        score += config.SOURCE_WEIGHT["default"]

    # é®®åº¦ã‚¹ã‚³ã‚¢
    published = article.get("published", "")
    if published:
        try:
            # feedparserã®æ—¥ä»˜å½¢å¼ã«å¯¾å¿œ
            pub_date = feedparser._parse_date(published)
            if pub_date:
                pub_datetime = datetime(*pub_date[:6])
                age_hours = (datetime.utcnow() - pub_datetime).total_seconds() / 3600

                if age_hours <= 24:
                    score += 3
                elif age_hours <= 48:
                    score += 2
                elif age_hours <= 168:  # 7 days
                    score += 1
        except Exception:
            pass

    return score


# =============================================================================
# SELECTION LOGIC
# =============================================================================

def select_articles(articles, state, category):
    """ã‚«ãƒ†ã‚´ãƒªè¨­å®šã«åŸºã¥ã„ã¦è¨˜äº‹ã‚’é¸å‡º"""
    # é‡è¤‡æ’é™¤ï¼ˆæ—¢ã«æŠ•ç¨¿æ¸ˆã¿ã®URLï¼‰
    posted_urls = {entry["url"] for entry in state["posted"].get(category, [])}
    new_articles = [a for a in articles if a["url"] not in posted_urls]
    
    # pendingè¨˜äº‹ã¨ãƒãƒ¼ã‚¸
    pending = state["pending"].get(category, [])
    # pendingè¨˜äº‹ã‚‚å†åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã«ä¸€æ—¦ãƒªã‚¹ãƒˆã‚’çµ±åˆ
    all_candidates = new_articles + pending

    cat_config = config.CATEGORIES[category]
    mode = cat_config["selection_mode"]
    
    selected = []

    if mode == "trending_only":
        # è©±é¡Œæ€§é‡è¦–ã®ã¿ (BigTech, DevCommunity)
        weights = cat_config["weights"]
        for a in all_candidates:
            a["final_score"] = round(calculate_weighted_score(a, weights), 2)
        
        sorted_articles = sorted(all_candidates, key=lambda x: x["final_score"], reverse=True)
        selected = sorted_articles[:cat_config["posts_per_day"]]

    elif mode == "dual":
        # æ§‹é€ é‡è¦–1 + è©±é¡Œé‡è¦–1 (Science, Education, Mycotech, Curiosity)
        
        # 1. æ§‹é€ é‡è¦–ã§é¸å‡º
        weights_struct = cat_config["weights_structural"]
        for a in all_candidates:
            a["temp_score_struct"] = calculate_weighted_score(a, weights_struct)
        
        sorted_struct = sorted(all_candidates, key=lambda x: x["temp_score_struct"], reverse=True)
        # ã‚¿ã‚°ãŒã‚ã‚‹ã‚‚ã®ã®ã¿å¯¾è±¡
        struct_candidates = [a for a in sorted_struct if a["tags"]]
        top_structural = struct_candidates[:1]
        
        # 2. æ®‹ã‚Šã‹ã‚‰è©±é¡Œé‡è¦–ã§é¸å‡º
        weights_trend = cat_config["weights_trending"]
        remaining = [a for a in all_candidates if a not in top_structural]
        
        for a in remaining:
            a["temp_score_trend"] = calculate_weighted_score(a, weights_trend)
            
        sorted_trend = sorted(remaining, key=lambda x: x["temp_score_trend"], reverse=True)
        top_trending = sorted_trend[:1]
        
        # çµ±åˆ
        selected = top_structural + top_trending
        
        # final_scoreã‚’é¸æŠã•ã‚ŒãŸåŸºæº–ã«åˆã‚ã›ã¦æ›´æ–°ï¼ˆè¡¨ç¤ºç”¨ï¼‰
        for a in top_structural:
            a["final_score"] = round(a["temp_score_struct"], 2)
        for a in top_trending:
            a["final_score"] = round(a["temp_score_trend"], 2)


    elif mode == "dual_enhanced":
        # æ§‹é€ é‡è¦–2 + è©±é¡Œé‡è¦–2 (AI)
        
        # 1. æ§‹é€ é‡è¦–ã§é¸å‡º
        weights_struct = cat_config["weights_structural"]
        for a in all_candidates:
            a["temp_score_struct"] = calculate_weighted_score(a, weights_struct)
        
        sorted_struct = sorted(all_candidates, key=lambda x: x["temp_score_struct"], reverse=True)
        struct_candidates = [a for a in sorted_struct if a["tags"]]
        top_structural = struct_candidates[:2]
        
        # ã‚¿ã‚°å¤šæ§˜æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆæ§‹é€ é‡è¦–æ å†…ï¼‰
        top_structural = ensure_tag_diversity(top_structural, struct_candidates)

        # 2. æ®‹ã‚Šã‹ã‚‰è©±é¡Œé‡è¦–ã§é¸å‡º
        weights_trend = cat_config["weights_trending"]
        remaining = [a for a in all_candidates if a not in top_structural]
        
        for a in remaining:
            a["temp_score_trend"] = calculate_weighted_score(a, weights_trend)
            
        sorted_trend = sorted(remaining, key=lambda x: x["temp_score_trend"], reverse=True)
        top_trending = sorted_trend[:2]
        
        # çµ±åˆ
        selected = top_structural + top_trending

        # final_scoreæ›´æ–°
        for a in top_structural:
            a["final_score"] = round(a["temp_score_struct"], 2)
        for a in top_trending:
            a["final_score"] = round(a["temp_score_trend"], 2)

    return selected


def ensure_tag_diversity(current_top, candidates):
    """ã‚¿ã‚°ã®å¤šæ§˜æ€§ã‚’ç¢ºä¿ï¼ˆæ§‹é€ é‡è¦–æ ç”¨ï¼‰"""
    if len(current_top) < 2:
        return current_top

    top1 = current_top[0]
    top2 = current_top[1]
    
    tags1 = {t["name"] for t in top1.get("tags", [])}
    tags2 = {t["name"] for t in top2.get("tags", [])}

    # åŒã˜ã‚¿ã‚°ã‚»ãƒƒãƒˆãªã‚‰ã€å€™è£œãƒªã‚¹ãƒˆã®3ç•ªç›®ä»¥é™ã‹ã‚‰ç•°ãªã‚‹ã‚¿ã‚°ã‚’æŒã¤ã‚‚ã®ã‚’æ¢ã™
    if tags1 == tags2 and len(candidates) > 2:
        for article in candidates[2:10]:  # ä¸Šä½10ä»¶ã¾ã§æ¢ç´¢
            article_tags = {t["name"] for t in article.get("tags", [])}
            if article_tags != tags1:
                return [top1, article]

    return current_top


def update_pending(articles, selected, state, category):
    """pendingè¨˜äº‹ã‚’æ›´æ–°"""
    selected_urls = {a["url"] for a in selected}
    
    # é¸æŠã•ã‚Œãªã‹ã£ãŸè¨˜äº‹
    remaining = [a for a in articles if a["url"] not in selected_urls]
    
    # ç°¡æ˜“ã‚¹ã‚³ã‚¢ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¦ã‚§ã‚¤ãƒˆï¼‰ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’ä¿æŒ
    # â€» æ¬¡å›å®Ÿè¡Œæ™‚ã«é©åˆ‡ãªãƒ¢ãƒ¼ãƒ‰ã§å†è¨ˆç®—ã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯æš«å®šçš„ã«ä¿æŒ
    default_weights = {"structural": 0.5, "timeliness": 0.5}
    for a in remaining:
        if "final_score" not in a or a["final_score"] == 0:
             a["final_score"] = calculate_weighted_score(a, default_weights)

    sorted_pending = sorted(remaining, key=lambda x: x.get("final_score", 0), reverse=True)
    
    state["pending"][category] = sorted_pending[:10]  # ä¸Šä½10ä»¶ä¿æŒ

    return state


# =============================================================================
# DISCORD POSTING
# =============================================================================

def post_to_discord(article, category, dry_run=False):
    """Discordã«æŠ•ç¨¿"""
    webhook_env = f"DISCORD_WEBHOOK_{category.upper()}"
    webhook_url = os.environ.get(webhook_env)

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆï¼ˆdry-runæ™‚ã‚‚è¡¨ç¤ºã™ã‚‹ãŸã‚å…ˆã«ä½œæˆï¼‰
    cat_info = config.CATEGORIES[category]
    tag_names = " Ã— ".join(t["name"] for t in article["tags"]) if article["tags"] else "no tags"
    date_str = datetime.utcnow().strftime("%Y-%m-%d")

    # æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚Œã°ä½¿ç”¨
    title = article.get('title_ja', article['title'])

    message = f"""{cat_info['emoji']} **{cat_info['name']}** | {date_str}

**[{tag_names}]**
{title}

ğŸ”— {article['url']}
ğŸ“° {article['source']} | Score: {article['final_score']}"""

    if dry_run:
        print(f"\n[DRY-RUN] Would post to {category}:")
        print(message)
        print("-" * 50)
        return True

    # Webhook URLãƒã‚§ãƒƒã‚¯ï¼ˆå®ŸæŠ•ç¨¿æ™‚ã®ã¿ï¼‰
    if not webhook_url:
        print(f"[WARN] {webhook_env} not set, skipping post")
        return False

    # å®Ÿéš›ã«æŠ•ç¨¿
    try:
        response = requests.post(
            webhook_url,
            json={"content": message},
            headers={"Content-Type": "application/json"},
            timeout=30
        )
        response.raise_for_status()
        print(f"[OK] Posted to {category}: {article['title'][:50]}...")
        return True
    except requests.RequestException as e:
        print(f"[ERROR] Failed to post to Discord: {e}")
        return False


# =============================================================================
# MAIN
# =============================================================================

def process_category(category, state, dry_run=False):
    """1ã‚«ãƒ†ã‚´ãƒªã‚’å‡¦ç†"""
    print(f"\n{'='*50}")
    print(f"Processing: {category}")
    print(f"{'='*50}")

    # RSSå–å¾—
    articles = fetch_all_feeds(category)
    print(f"Fetched {len(articles)} articles")

    if not articles:
        return state

    # ã‚¿ã‚°ä»˜ä¸ã¨åŸºæœ¬ã‚¹ã‚³ã‚¢è¨ˆç®—
    for article in articles:
        detect_tags(article)
        calculate_base_scores(article)

    # è¨˜äº‹é¸æŠï¼ˆã‚«ãƒ†ã‚´ãƒªã”ã¨ã®ãƒ­ã‚¸ãƒƒã‚¯ã§ï¼‰
    selected = select_articles(articles, state, category)
    print(f"Selected {len(selected)} articles for posting")

    # é¸æŠã•ã‚ŒãŸè¨˜äº‹ã®ã¿ç¿»è¨³ï¼ˆAPIç¯€ç´„ï¼‰
    selected = translate_articles(selected)

    # æŠ•ç¨¿
    for article in selected:
        if post_to_discord(article, category, dry_run):
            # æŠ•ç¨¿æˆåŠŸã—ãŸã‚‰postedã«è¿½åŠ 
            if not dry_run:
                state["posted"][category].append({
                    "url": article["url"],
                    "posted_at": datetime.utcnow().strftime("%Y-%m-%d"),
                    "score": article["final_score"],
                    "tags": [t["name"] for t in article["tags"]]
                })

    # pendingæ›´æ–°
    # selectedã«å…¥ã‚‰ãªã‹ã£ãŸè¨˜äº‹ã®ä¸­ã‹ã‚‰pendingå€™è£œã‚’é¸ã¶
    state = update_pending(articles, selected, state, category)

    return state


def main():
    parser = argparse.ArgumentParser(description="News Curation Bot")
    parser.add_argument("--dry-run", action="store_true", help="Don't actually post")
    parser.add_argument("--category", type=str, help="Process specific category only")
    args = parser.parse_args()

    print("News Curation Bot starting...")
    print(f"Dry run: {args.dry_run}")

    # çŠ¶æ…‹èª­ã¿è¾¼ã¿
    state = load_state()
    state = cleanup_old_entries(state)

    # ã‚«ãƒ†ã‚´ãƒªå‡¦ç†
    categories = [args.category] if args.category else list(config.CATEGORIES.keys())

    for category in categories:
        if category not in config.CATEGORIES:
            print(f"[ERROR] Unknown category: {category}")
            continue
        state = process_category(category, state, args.dry_run)

    # çŠ¶æ…‹ä¿å­˜
    if not args.dry_run:
        save_state(state)
        print("\nState saved.")

    print("\nDone!")


if __name__ == "__main__":
    main()