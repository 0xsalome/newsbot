{
  "meta": {
    "last_updated": "2026-02-28T23:18:59.972934Z",
    "retention_days": 7
  },
  "posted": {
    "science": [
      {
        "url": "https://phys.org/news/2026-02-qa-gas-fermentation-game-changer.html",
        "posted_at": "2026-02-21",
        "score": 9.3,
        "tags": [
          "transformation",
          "boundary_crossing",
          "value_redefinition"
        ]
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260219040818.htm",
        "posted_at": "2026-02-21",
        "score": 5.8,
        "tags": [
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260221000307.htm",
        "posted_at": "2026-02-22",
        "score": 7.5,
        "tags": [
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://phys.org/news/2026-02-cosmic-curveball-distant-planet-formation.html",
        "posted_at": "2026-02-22",
        "score": 4.8,
        "tags": [
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://phys.org/news/2026-02-planet-doesnt-dry-scientists-global.html",
        "posted_at": "2026-02-23",
        "score": 8.6,
        "tags": [
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260222085209.htm",
        "posted_at": "2026-02-23",
        "score": 5.2,
        "tags": [
          "boundary_crossing",
          "visibility_gain"
        ]
      },
      {
        "url": "https://phys.org/news/2026-02-ai-powered-platform-discovery-mrna.html",
        "posted_at": "2026-02-24",
        "score": 8.6,
        "tags": [
          "boundary_crossing",
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260222092321.htm",
        "posted_at": "2026-02-24",
        "score": 5.5,
        "tags": [
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://phys.org/news/2026-02-bacterial-pathogens-antibiotic-resistant-bunkers.html",
        "posted_at": "2026-02-25",
        "score": 10.0,
        "tags": [
          "boundary_crossing",
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260225001250.htm",
        "posted_at": "2026-02-25",
        "score": 5.2,
        "tags": [
          "boundary_crossing",
          "visibility_gain"
        ]
      },
      {
        "url": "https://phys.org/news/2026-02-grasslands-faster-forests-global.html",
        "posted_at": "2026-02-26",
        "score": 7.2,
        "tags": [
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260219040745.htm",
        "posted_at": "2026-02-26",
        "score": 5.2,
        "tags": [
          "boundary_crossing",
          "visibility_gain"
        ]
      },
      {
        "url": "https://phys.org/news/2026-02-cooling-gases-molecular-solid-state.html",
        "posted_at": "2026-02-27",
        "score": 10.0,
        "tags": [
          "boundary_crossing",
          "value_redefinition",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260226042447.htm",
        "posted_at": "2026-02-27",
        "score": 4.9,
        "tags": [
          "boundary_crossing"
        ]
      },
      {
        "url": "https://phys.org/news/2026-02-tool-gene-dna-sequences-jobs.html",
        "posted_at": "2026-02-28",
        "score": 9.3,
        "tags": [
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260227071920.htm",
        "posted_at": "2026-02-28",
        "score": 5.2,
        "tags": [
          "boundary_crossing",
          "visibility_gain"
        ]
      }
    ],
    "ai": [
      {
        "url": "https://venturebeat.com/infrastructure/claude-code-costs-up-to-usd200-a-month-goose-does-the-same-thing-for-free",
        "posted_at": "2026-02-21",
        "score": 19.8,
        "tags": [
          "boundary_crossing",
          "value_redefinition",
          "scale_shift",
          "ontology_shift"
        ]
      },
      {
        "url": "https://arxiv.org/abs/2602.16745",
        "posted_at": "2026-02-21",
        "score": 15.4,
        "tags": [
          "boundary_crossing",
          "value_redefinition",
          "ontology_shift"
        ]
      },
      {
        "url": "https://techcrunch.com/2026/02/20/inscope-nabs-14-5m-to-solve-the-pain-of-financial-reporting/",
        "posted_at": "2026-02-21",
        "score": 4.2,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://www.technologyreview.com/2026/02/18/1132579/robots-predict-future-book-review/",
        "posted_at": "2026-02-21",
        "score": 4.0,
        "tags": []
      },
      {
        "url": "https://venturebeat.com/technology/anthropic-launches-cowork-a-claude-desktop-agent-that-works-in-your-files-no",
        "posted_at": "2026-02-22",
        "score": 32.6,
        "tags": [
          "transformation",
          "boundary_crossing",
          "visibility_gain",
          "value_redefinition",
          "scale_shift",
          "ontology_shift"
        ]
      },
      {
        "url": "https://www.anthropocenemagazine.org/2026/02/could-we-cool-the-planet-by-turning-crop-waste-into-building-materials/?utm_source=rss&utm_medium=rss&utm_campaign=could-we-cool-the-planet-by-turning-crop-waste-into-building-materials",
        "posted_at": "2026-02-22",
        "score": 7.2,
        "tags": [
          "transformation",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.technologyreview.com/2026/02/12/1132386/ai-already-making-online-swindles-easier/",
        "posted_at": "2026-02-22",
        "score": 4.0,
        "tags": []
      },
      {
        "url": "https://techcrunch.com/2026/02/20/openai-says-18-to-24-year-olds-account-for-nearly-50-of-chatgpt-usage-in-india/",
        "posted_at": "2026-02-22",
        "score": 3.6,
        "tags": [
          "boundary_crossing"
        ]
      },
      {
        "url": "https://arxiv.org/abs/2602.17689",
        "posted_at": "2026-02-23",
        "score": 18.6,
        "tags": [
          "boundary_crossing",
          "visibility_gain",
          "value_redefinition",
          "ontology_shift"
        ]
      },
      {
        "url": "https://magicalmushroom.com/index",
        "posted_at": "2026-02-23",
        "score": 3.8,
        "tags": [
          "boundary_crossing"
        ]
      },
      {
        "url": "https://www.technologyreview.com/2026/02/23/1133508/the-human-work-behind-humanoid-robots-is-being-hidden/",
        "posted_at": "2026-02-23",
        "score": 4.6,
        "tags": [
          "transformation"
        ]
      },
      {
        "url": "https://techcrunch.com/2026/02/23/anthropic-accuses-chinese-ai-labs-of-mining-claude-as-us-debates-ai-chip-exports/",
        "posted_at": "2026-02-23",
        "score": 3.2,
        "tags": []
      },
      {
        "url": "https://venturebeat.com/infrastructure/railway-secures-usd100-million-to-challenge-aws-with-ai-native-cloud",
        "posted_at": "2026-02-24",
        "score": 33.4,
        "tags": [
          "boundary_crossing",
          "visibility_gain",
          "value_redefinition",
          "scale_shift",
          "ontology_shift"
        ]
      },
      {
        "url": "https://arxiv.org/abs/2602.20133",
        "posted_at": "2026-02-24",
        "score": 18.6,
        "tags": [
          "transformation",
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.anthropocenemagazine.org/2026/02/less-air-pollution-means-more-warming-could-marine-cloud-brightening-offset-the-paradox/?utm_source=rss&utm_medium=rss&utm_campaign=making-ocean-clouds-brighter-can-help-with-global-warming-but-there-are-tradeoffs",
        "posted_at": "2026-02-24",
        "score": 4.2,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://www.technologyreview.com/2026/02/12/1132811/whats-next-for-chinese-open-source-ai/",
        "posted_at": "2026-02-24",
        "score": 4.0,
        "tags": []
      },
      {
        "url": "https://venturebeat.com/technology/listen-labs-raises-usd69m-after-viral-billboard-hiring-stunt-to-scale-ai",
        "posted_at": "2026-02-25",
        "score": 31.8,
        "tags": [
          "boundary_crossing",
          "visibility_gain",
          "value_redefinition",
          "scale_shift",
          "ontology_shift"
        ]
      },
      {
        "url": "https://arxiv.org/abs/2510.09469",
        "posted_at": "2026-02-25",
        "score": 18.6,
        "tags": [
          "boundary_crossing",
          "value_redefinition",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.technologyreview.com/2026/02/09/1132537/a-lesson-from-pokemon/",
        "posted_at": "2026-02-25",
        "score": 4.6,
        "tags": [
          "transformation"
        ]
      },
      {
        "url": "https://techcrunch.com/2026/02/25/nvidia-earnings-record-capex-spend-ai/",
        "posted_at": "2026-02-25",
        "score": 3.2,
        "tags": []
      },
      {
        "url": "https://venturebeat.com/technology/nous-researchs-nouscoder-14b-is-an-open-source-coding-model-landing-right-in",
        "posted_at": "2026-02-26",
        "score": 26.2,
        "tags": [
          "transformation",
          "boundary_crossing",
          "value_redefinition",
          "scale_shift",
          "ontology_shift"
        ]
      },
      {
        "url": "https://arxiv.org/abs/2602.21531",
        "posted_at": "2026-02-26",
        "score": 16.2,
        "tags": [
          "transformation",
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.technologyreview.com/2026/02/26/1133707/finding-value-with-ai-and-industry-5-0-transformation/",
        "posted_at": "2026-02-26",
        "score": 4.8,
        "tags": [
          "value_redefinition"
        ]
      },
      {
        "url": "https://www.anthropocenemagazine.org/2026/02/breadcrumbs-lay-path-away-from-fossil-fuels/?utm_source=rss&utm_medium=rss&utm_campaign=breadcrumbs-lay-path-away-from-fossil-fuels",
        "posted_at": "2026-02-26",
        "score": 4.6,
        "tags": [
          "transformation",
          "boundary_crossing"
        ]
      },
      {
        "url": "https://arxiv.org/abs/2602.23153",
        "posted_at": "2026-02-27",
        "score": 19.4,
        "tags": [
          "boundary_crossing",
          "value_redefinition",
          "scale_shift",
          "ontology_shift"
        ]
      },
      {
        "url": "https://venturebeat.com/technology/the-creator-of-claude-code-just-revealed-his-workflow-and-developers-are",
        "posted_at": "2026-02-27",
        "score": 19.0,
        "tags": [
          "transformation",
          "boundary_crossing",
          "visibility_gain",
          "value_redefinition"
        ]
      },
      {
        "url": "https://www.technologyreview.com/2026/02/27/1133624/ai-is-rewiring-how-the-worlds-best-go-players-think/",
        "posted_at": "2026-02-27",
        "score": 4.8,
        "tags": [
          "boundary_crossing"
        ]
      },
      {
        "url": "https://techcrunch.com/2026/02/26/mistral-ai-inks-a-deal-with-global-consulting-giant-accenture/",
        "posted_at": "2026-02-27",
        "score": 4.2,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://venturebeat.com/technology/salesforce-rolls-out-new-slackbot-ai-agent-as-it-battles-microsoft-and",
        "posted_at": "2026-02-28",
        "score": 18.2,
        "tags": [
          "boundary_crossing",
          "value_redefinition",
          "scale_shift",
          "ontology_shift"
        ]
      },
      {
        "url": "https://arxiv.org/abs/2602.22775",
        "posted_at": "2026-02-28",
        "score": 17.8,
        "tags": [
          "transformation",
          "boundary_crossing",
          "value_redefinition",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.anthropocenemagazine.org/2026/02/researchers-have-figured-out-how-to-make-airplanes-fly-on-landfill-gas/?utm_source=rss&utm_medium=rss&utm_campaign=researchers-have-figured-out-how-to-make-airplanes-fly-on-landfill-gas",
        "posted_at": "2026-02-28",
        "score": 4.8,
        "tags": [
          "transformation",
          "value_redefinition"
        ]
      },
      {
        "url": "https://www.technologyreview.com/2026/02/19/1133360/microsoft-has-a-new-plan-to-prove-whats-real-and-whats-ai-online/",
        "posted_at": "2026-02-28",
        "score": 4.0,
        "tags": []
      }
    ],
    "education": [
      {
        "url": "https://edsource.org/2026/technology-education-student-wellbeing/749262",
        "posted_at": "2026-02-21",
        "score": 5.1,
        "tags": [
          "boundary_crossing",
          "value_redefinition"
        ]
      },
      {
        "url": "https://www.openculture.com/2026/02/vivaldis-four-seasons-performed-on-original-baroque-instruments.html",
        "posted_at": "2026-02-21",
        "score": 3.3,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://edsource.org/2026/california-universal-prekindergarten-implementation/748208",
        "posted_at": "2026-02-22",
        "score": 4.4,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://www.citriniresearch.com/p/2028gic",
        "posted_at": "2026-02-22",
        "score": 3.6,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://edsource.org/2025/nixon-veto-childcare-lessons/747568",
        "posted_at": "2026-02-23",
        "score": 4.4,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://live.xweather.com/",
        "posted_at": "2026-02-23",
        "score": 3.6,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://theconversation.com/agroecology-rethinking-global-policy-efficiency-and-funding-priorities-to-overcome-the-blind-spot-in-climate-action-275839",
        "posted_at": "2026-02-24",
        "score": 6.6,
        "tags": [
          "value_redefinition",
          "scale_shift"
        ]
      },
      {
        "url": "https://edsource.org/2025/california-schools-to-use-reading-screening-test/733022",
        "posted_at": "2026-02-24",
        "score": 3.6,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://edsource.org/2024/as-we-expand-universal-preschool-access-lets-ensure-teachers-mirror-their-students-ethnicity/715393",
        "posted_at": "2026-02-25",
        "score": 4.4,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://www.openculture.com/2026/02/herbie-hancock-explains-the-lesson-he-learned-from-miles-davis.html",
        "posted_at": "2026-02-25",
        "score": 3.0,
        "tags": [
          "transformation"
        ]
      },
      {
        "url": "https://edsource.org/2024/survey-californians-are-worried-about-student-health-lukewarm-toward-a-state-school-bond/709604",
        "posted_at": "2026-02-26",
        "score": 4.4,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://www.combinatorprize.org/",
        "posted_at": "2026-02-26",
        "score": 3.3,
        "tags": [
          "value_redefinition"
        ]
      },
      {
        "url": "https://edsource.org/2025/how-one-california-school-came-together-to-pack-20000-meals-for-the-holidays/746481",
        "posted_at": "2026-02-27",
        "score": 7.2,
        "tags": [
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.openculture.com/2026/02/the-ancient-egyptian-book-of-the-dead-a-guidebook-for-surviving-the-afterlife.html",
        "posted_at": "2026-02-27",
        "score": 3.3,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://edsource.org/2025/fresno-unified-data-error-analysis/738872",
        "posted_at": "2026-02-28",
        "score": 6.5,
        "tags": [
          "transformation",
          "boundary_crossing"
        ]
      },
      {
        "url": "https://news.mit.edu/2026/turning-curiosity-about-engineering-into-careers-0227",
        "posted_at": "2026-02-28",
        "score": 2.8,
        "tags": [
          "boundary_crossing",
          "scale_shift"
        ]
      }
    ],
    "mycotech": [
      {
        "url": "https://phys.org/news/2026-02-body-cold-menthol-cool.html",
        "posted_at": "2026-02-21",
        "score": 7.2,
        "tags": [
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260213223904.htm",
        "posted_at": "2026-02-21",
        "score": 4.3,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://www.anthropocenemagazine.org/2026/02/could-we-cool-the-planet-by-turning-crop-waste-into-building-materials/?utm_source=rss&utm_medium=rss&utm_campaign=could-we-cool-the-planet-by-turning-crop-waste-into-building-materials",
        "posted_at": "2026-02-22",
        "score": 6.8,
        "tags": [
          "transformation",
          "scale_shift"
        ]
      },
      {
        "url": "https://phys.org/news/2026-02-bed-bugs-kryptonite-parasites-surfaces.html",
        "posted_at": "2026-02-22",
        "score": 4.5,
        "tags": [
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260206012210.htm",
        "posted_at": "2026-02-23",
        "score": 7.5,
        "tags": [
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://phys.org/news/2026-02-hormone-therapy-global-food-growth.html",
        "posted_at": "2026-02-23",
        "score": 4.8,
        "tags": [
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260206012213.htm",
        "posted_at": "2026-02-24",
        "score": 8.9,
        "tags": [
          "boundary_crossing",
          "visibility_gain"
        ]
      },
      {
        "url": "https://phys.org/news/2026-02-global-bee-hint-thousands-hidden.html",
        "posted_at": "2026-02-24",
        "score": 5.4,
        "tags": [
          "boundary_crossing",
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://phys.org/news/2026-02-bacterial-pathogens-antibiotic-resistant-bunkers.html",
        "posted_at": "2026-02-25",
        "score": 10.0,
        "tags": [
          "boundary_crossing",
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260207232242.htm",
        "posted_at": "2026-02-25",
        "score": 5.5,
        "tags": [
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://phys.org/news/2026-02-scientists-vitamin-enriched-tomato-global.html",
        "posted_at": "2026-02-26",
        "score": 10.0,
        "tags": [
          "boundary_crossing",
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260226042447.htm",
        "posted_at": "2026-02-26",
        "score": 4.9,
        "tags": [
          "boundary_crossing"
        ]
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260219040749.htm",
        "posted_at": "2026-02-27",
        "score": 9.6,
        "tags": [
          "boundary_crossing",
          "visibility_gain"
        ]
      },
      {
        "url": "https://phys.org/news/2026-02-tool-gene-dna-sequences-jobs.html",
        "posted_at": "2026-02-27",
        "score": 5.7,
        "tags": [
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://phys.org/news/2026-02-immune-cells-viral-rna-fast.html",
        "posted_at": "2026-02-28",
        "score": 7.2,
        "tags": [
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260227071920.htm",
        "posted_at": "2026-02-28",
        "score": 5.2,
        "tags": [
          "boundary_crossing",
          "visibility_gain"
        ]
      }
    ],
    "curiosity": [
      {
        "url": "https://www.atlasobscura.com/articles/odilia-alvarado-kissimmee",
        "posted_at": "2026-02-21",
        "score": 8.6,
        "tags": [
          "boundary_crossing",
          "visibility_gain"
        ]
      },
      {
        "url": "https://www.wired.com/story/ai-weiwei-gets-artsy-fartsy-about-surveillance/",
        "posted_at": "2026-02-21",
        "score": 4.0,
        "tags": [
          "boundary_crossing"
        ]
      },
      {
        "url": "https://www.atlasobscura.com/articles/centralia-pennsylvania-rebirth",
        "posted_at": "2026-02-22",
        "score": 14.2,
        "tags": [
          "transformation",
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.smithsonianmag.com/smart-news/louvre-hit-with-12-million-ticket-fraud-scheme-180988236/",
        "posted_at": "2026-02-22",
        "score": 4.0,
        "tags": [
          "visibility_gain"
        ]
      },
      {
        "url": "https://www.atlasobscura.com/articles/podcast-edison-ford-winter-estate",
        "posted_at": "2026-02-23",
        "score": 12.8,
        "tags": [
          "transformation",
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.quantamagazine.org/are-the-mysteries-of-quantum-mechanics-beginning-to-dissolve-20260213/",
        "posted_at": "2026-02-23",
        "score": 4.1,
        "tags": [
          "boundary_crossing"
        ]
      },
      {
        "url": "https://www.atlasobscura.com/articles/podcast-caroline-mazel-carlton-1000-places",
        "posted_at": "2026-02-24",
        "score": 11.4,
        "tags": [
          "boundary_crossing",
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://nautil.us/researchers-map-uranus-atmosphere-in-stunning-detail-1270213/",
        "posted_at": "2026-02-24",
        "score": 4.0,
        "tags": [
          "visibility_gain"
        ]
      },
      {
        "url": "https://www.atlasobscura.com/articles/pedro-rodriguez-kissimmee",
        "posted_at": "2026-02-25",
        "score": 10.0,
        "tags": [
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://nautil.us/imaging-the-most-far-out-jellyfish-galaxy-ever-observed-1270218/",
        "posted_at": "2026-02-25",
        "score": 4.0,
        "tags": [
          "visibility_gain"
        ]
      },
      {
        "url": "https://www.atlasobscura.com/articles/podcast-fordlandia",
        "posted_at": "2026-02-26",
        "score": 10.0,
        "tags": [
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.smithsonianmag.com/smart-news/woman-found-folder-drawer-when-she-opened-it-she-discovered-35-forgotten-rembrandt-etchings-180988261/",
        "posted_at": "2026-02-26",
        "score": 4.9,
        "tags": [
          "boundary_crossing",
          "visibility_gain"
        ]
      },
      {
        "url": "https://www.atlasobscura.com/articles/idaho-sun-valley-fascinating-places",
        "posted_at": "2026-02-27",
        "score": 12.1,
        "tags": [
          "transformation",
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.quantamagazine.org/the-biophysical-world-inside-a-jam-packed-cell-20260218/",
        "posted_at": "2026-02-27",
        "score": 4.7,
        "tags": [
          "visibility_gain"
        ]
      },
      {
        "url": "https://www.atlasobscura.com/foods/tiquira",
        "posted_at": "2026-02-28",
        "score": 9.3,
        "tags": [
          "boundary_crossing",
          "value_redefinition",
          "scale_shift"
        ]
      },
      {
        "url": "https://nautil.us/astronomers-capture-largest-image-of-milky-way-ever-1271018/",
        "posted_at": "2026-02-28",
        "score": 4.0,
        "tags": [
          "visibility_gain"
        ]
      }
    ],
    "bigtech": [
      {
        "url": "https://technode.com/2025/09/12/satellite-imaging-inclusive-ai-and-privacy-preserving-tech-win-at-ant-groups-global-competition/",
        "posted_at": "2026-02-21",
        "score": 4.8,
        "tags": [
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.wired.com/story/trump-imposes-new-tariffs-following-supreme-court-ruling/",
        "posted_at": "2026-02-21",
        "score": 4.3,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://technode.com/2025/06/05/behind-the-blind-box-boom-the-global-ascent-of-pop-marts-labubu/",
        "posted_at": "2026-02-22",
        "score": 4.5,
        "tags": [
          "transformation",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.scmp.com/news/world/united-states-canada/article/3344236/trump-sends-great-hospital-boat-treat-sick-people-greenland?utm_source=rss_feed",
        "posted_at": "2026-02-22",
        "score": 4.2,
        "tags": [
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://techcrunch.com/2026/02/23/uber-autonomous-solutions-av-robotaxi-delivery-robots/",
        "posted_at": "2026-02-23",
        "score": 4.0,
        "tags": [
          "boundary_crossing"
        ]
      },
      {
        "url": "https://www.scmp.com/news/china/politics/article/3344364/taiwan-seeks-clarity-after-us-supreme-court-upends-trumps-tariff-powers?utm_source=rss_feed",
        "posted_at": "2026-02-23",
        "score": 3.6,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://technode.com/2025/05/23/beyond-expo-2025-interview-with-zack-kass-ais-ultimate-challenge-will-be-crisis-of-purpose/",
        "posted_at": "2026-02-24",
        "score": 4.5,
        "tags": [
          "value_redefinition"
        ]
      },
      {
        "url": "https://techcrunch.com/2026/02/24/discord-delays-global-rollout-of-age-verification-after-backlash/",
        "posted_at": "2026-02-24",
        "score": 4.3,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://pandaily.com/jd-property-targets-hong-kong-ipo-reframing-its-valuation-story-through-global-expansion-and-capital-efficiency",
        "posted_at": "2026-02-25",
        "score": 4.8,
        "tags": [
          "value_redefinition",
          "scale_shift"
        ]
      },
      {
        "url": "https://techcrunch.com/2026/02/25/inside-the-story-of-the-us-defense-contractor-who-leaked-hacking-tools-to-russia/",
        "posted_at": "2026-02-25",
        "score": 4.0,
        "tags": [
          "boundary_crossing"
        ]
      },
      {
        "url": "https://technode.com/2025/11/04/eric-jing-ant-group-to-strengthen-support-for-hong-kongs-global-finance-and-tech-leadership-with-ai-goglobal-services/",
        "posted_at": "2026-02-26",
        "score": 4.2,
        "tags": [
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.scmp.com/news/world/middle-east/article/3344793/us-presses-syria-shift-chinese-telecoms-systems-cites-threat-national-security?utm_source=rss_feed",
        "posted_at": "2026-02-26",
        "score": 3.9,
        "tags": [
          "boundary_crossing",
          "value_redefinition"
        ]
      },
      {
        "url": "https://technode.com/2024/05/26/beyond-expo-2024-navigating-the-future-of-innovation-in-cross-border-e-commerce/",
        "posted_at": "2026-02-27",
        "score": 4.2,
        "tags": [
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://www.scmp.com/economy/global-economy/article/3344893/trumps-tariff-setback-could-spark-surge-chinese-imports-us-analysts?utm_source=rss_feed",
        "posted_at": "2026-02-27",
        "score": 3.6,
        "tags": [
          "scale_shift"
        ]
      },
      {
        "url": "https://technode.com/2025/06/25/alibaba-merges-ele-me-fliggy-into-e-commerce-unit-in-strategic-shift/",
        "posted_at": "2026-02-28",
        "score": 3.9,
        "tags": [
          "boundary_crossing",
          "value_redefinition"
        ]
      },
      {
        "url": "https://www.scmp.com/opinion/world-opinion/article/3344982/us-israeli-strike-iran-signals-new-phase-global-escalation?utm_source=rss_feed",
        "posted_at": "2026-02-28",
        "score": 3.6,
        "tags": [
          "scale_shift"
        ]
      }
    ],
    "devcommunity": [
      {
        "url": "https://github.com/vxcontrol/pentagi",
        "posted_at": "2026-02-21",
        "score": 15.7,
        "tags": [
          "transformation",
          "boundary_crossing",
          "scale_shift",
          "ontology_shift"
        ]
      },
      {
        "url": "https://dev.to/hypertextcoffeepot/activity-in-wonderland-distributed-tracing-with-opentelemetry-and-net-2ehe",
        "posted_at": "2026-02-21",
        "score": 11.3,
        "tags": [
          "transformation",
          "boundary_crossing",
          "scale_shift",
          "ontology_shift"
        ]
      },
      {
        "url": "https://dev.to/huckler/pcworkman-168-when-quick-fix-took-3-weeks-data-engine-ai-context-70-performance-2d9l",
        "posted_at": "2026-02-22",
        "score": 12.8,
        "tags": [
          "transformation",
          "boundary_crossing",
          "visibility_gain",
          "value_redefinition",
          "scale_shift"
        ]
      },
      {
        "url": "https://dev.to/qa-leaders/anatomy-of-a-schema-drift-incident-5-real-patterns-that-break-production-274l",
        "posted_at": "2026-02-22",
        "score": 10.7,
        "tags": [
          "transformation",
          "boundary_crossing",
          "value_redefinition",
          "scale_shift",
          "ontology_shift"
        ]
      },
      {
        "url": "https://dev.to/benjifisher/we-ran-180-ai-agent-shopping-sessions-across-11-models-and-20-stores-heres-what-we-found-2884",
        "posted_at": "2026-02-23",
        "score": 11.9,
        "tags": [
          "transformation",
          "boundary_crossing",
          "value_redefinition",
          "scale_shift"
        ]
      },
      {
        "url": "https://dev.to/sbwiley/strike-while-the-big-irons-hot-3mj",
        "posted_at": "2026-02-23",
        "score": 9.2,
        "tags": [
          "transformation",
          "boundary_crossing",
          "visibility_gain",
          "value_redefinition"
        ]
      },
      {
        "url": "https://dev.to/jerdog/developer-experience-is-more-than-just-productivity-metrics-4a2o",
        "posted_at": "2026-02-24",
        "score": 8.9,
        "tags": [
          "transformation",
          "boundary_crossing",
          "scale_shift"
        ]
      },
      {
        "url": "https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering",
        "posted_at": "2026-02-24",
        "score": 8.8,
        "tags": [
          "transformation",
          "boundary_crossing",
          "value_redefinition",
          "ontology_shift"
        ]
      },
      {
        "url": "https://github.com/ruvnet/ruvector",
        "posted_at": "2026-02-25",
        "score": 19.6,
        "tags": [
          "transformation",
          "boundary_crossing",
          "visibility_gain",
          "value_redefinition",
          "scale_shift",
          "ontology_shift"
        ]
      },
      {
        "url": "https://dev.to/fosres/week-7-scripting-challenge-jwt-token-validation-appsec-exercise-53ge",
        "posted_at": "2026-02-25",
        "score": 17.0,
        "tags": [
          "transformation",
          "boundary_crossing",
          "visibility_gain",
          "value_redefinition",
          "scale_shift",
          "ontology_shift"
        ]
      },
      {
        "url": "https://dev.to/oguzhanatalay/the-hard-way-to-learn-ai-agents-need-a-constitution-not-prompts-2hdm",
        "posted_at": "2026-02-26",
        "score": 10.1,
        "tags": [
          "transformation",
          "value_redefinition",
          "scale_shift",
          "ontology_shift"
        ]
      },
      {
        "url": "https://dev.to/amartyajha/how-to-test-llm-performance-on-real-code-instead-of-synthetic-benchmarks-40lk",
        "posted_at": "2026-02-26",
        "score": 8.9,
        "tags": [
          "transformation",
          "boundary_crossing",
          "value_redefinition",
          "scale_shift"
        ]
      },
      {
        "url": "https://github.com/ruvnet/claude-flow",
        "posted_at": "2026-02-27",
        "score": 18.4,
        "tags": [
          "transformation",
          "boundary_crossing",
          "visibility_gain",
          "value_redefinition",
          "scale_shift",
          "ontology_shift"
        ]
      },
      {
        "url": "https://github.com/moonshine-ai/moonshine",
        "posted_at": "2026-02-27",
        "score": 10.3,
        "tags": [
          "transformation",
          "boundary_crossing",
          "visibility_gain",
          "scale_shift"
        ]
      },
      {
        "url": "https://github.com/ruvnet/ruflo",
        "posted_at": "2026-02-28",
        "score": 18.4,
        "tags": [
          "transformation",
          "boundary_crossing",
          "visibility_gain",
          "value_redefinition",
          "scale_shift",
          "ontology_shift"
        ]
      },
      {
        "url": "https://dev.to/ccinaza/building-an-incremental-zoho-desk-to-bigquery-pipeline-lessons-from-the-trenches-op1",
        "posted_at": "2026-02-28",
        "score": 8.9,
        "tags": [
          "transformation",
          "boundary_crossing",
          "visibility_gain",
          "value_redefinition",
          "scale_shift"
        ]
      }
    ]
  },
  "pending": {
    "science": [
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260222092321.htm",
        "title": "Space lasers reveal oceans rising faster than ever",
        "summary": "A new 30-year analysis reveals that melting land ice is now the main force behind rising global sea levels. Researchers discovered that oceans rose about 90 millimeters since 1993, with most of the increase coming from added water mass rather than just warming expansion. Ice loss from Greenland and mountain glaciers accounts for the vast majority of this gain. Even more concerning, the rate of sea-level rise is accelerating.",
        "source": "www.sciencedaily.com",
        "published": "Tue, 24 Feb 2026 00:08:38 EST",
        "fetched_at": "2026-02-28T23:18:01.177628Z",
        "tags": [
          {
            "name": "visibility_gain",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 9,
        "timeliness_score": 4,
        "final_score": 6.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260225001250.htm",
        "title": "Microplastics found in 90% of prostate cancer tumors, study reveals",
        "summary": "Researchers have detected microplastics in nearly all prostate cancer tumors examined in a new study. Tumor tissue contained about 2.5 times more plastic than nearby healthy prostate tissue. Scientists say this is the first Western study to directly measure plastic particles in prostate tumors. More research is needed, but the findings suggest microplastic exposure could play a role in cancer development.",
        "source": "www.sciencedaily.com",
        "published": "Wed, 25 Feb 2026 01:28:19 EST",
        "fetched_at": "2026-02-28T23:18:01.177546Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 4
          },
          {
            "name": "visibility_gain",
            "score": 4
          }
        ],
        "structural_score": 8,
        "timeliness_score": 4,
        "final_score": 6.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://phys.org/news/2026-02-immune-cells-viral-rna-fast.html",
        "title": "How immune cells spot viral RNA fast: LGP2 helps MDA5 respond to short dsRNA",
        "summary": "A study reveals how two proteins cooperate in a key early step of antiviral detection, as reported by researchers at Science Tokyo. Using cryo-electron microscopy and high-speed atomic force microscopy, they found that LGP2 binds to viral RNA and recruits MDA5 molecules, as if threading beads on a string. This creates a scaffold that facilitates the formation of a large signaling complex, which ultimately triggers an innate immune response.",
        "source": "phys.org",
        "published": "Fri, 27 Feb 2026 19:20:01 EST",
        "fetched_at": "2026-02-28T23:18:02.391201Z",
        "tags": [
          {
            "name": "visibility_gain",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 9,
        "timeliness_score": 3,
        "final_score": 6.0,
        "reddit_score": null,
        "reddit_comments": null,
        "temp_score_struct": 7.199999999999999,
        "temp_score_trend": 4.799999999999999
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260227071922.htm",
        "title": "This plastic is made from milk and it vanishes in 13 weeks",
        "summary": "Scientists racing to tackle plastic pollution have created a surprising new contender: a biodegradable packaging film made partly from milk protein. Researchers at Flinders University blended calcium caseinate with starch and natural nanoclay to form a thin, durable material designed to mimic everyday plastic. In soil tests, the film fully broke down in about 13 weeks, pointing to a realistic alternative for single-use food packaging.",
        "source": "www.sciencedaily.com",
        "published": "Sat, 28 Feb 2026 08:23:21 EST",
        "fetched_at": "2026-02-28T23:18:01.177405Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 7
          }
        ],
        "structural_score": 7,
        "timeliness_score": 4,
        "final_score": 5.5,
        "reddit_score": null,
        "reddit_comments": null,
        "temp_score_struct": 6.1,
        "temp_score_trend": 4.9
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260226042447.htm",
        "title": "Hidden architecture inside cellular droplets opens new targets for cancer and ALS",
        "summary": "Biomolecular condensates were long believed to be simple liquid blobs inside cells. Researchers have now uncovered that some are actually supported by fine protein filaments forming an internal scaffold. When this structure is disrupted, cells fail to grow and divide properly. The discovery suggests scientists may one day design drugs that target condensate architecture to fight cancer and neurodegenerative disease.",
        "source": "www.sciencedaily.com",
        "published": "Thu, 26 Feb 2026 09:36:27 EST",
        "fetched_at": "2026-02-28T23:18:01.177482Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 7
          }
        ],
        "structural_score": 7,
        "timeliness_score": 4,
        "final_score": 5.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260228082717.htm",
        "title": "How the body really ages: 7 million cells mapped across 21 organs",
        "summary": "Scientists have built a massive cellular atlas showing how aging reshapes the body across 21 organs. Studying nearly 7 million cells, they found that aging starts earlier than expected and unfolds in a coordinated way throughout the body. About a quarter of cell types change in number over time, and many of these shifts differ between males and females. The research also highlights shared genetic “hotspots” that could become targets for anti-aging therapies.",
        "source": "www.sciencedaily.com",
        "published": "Sat, 28 Feb 2026 10:25:43 EST",
        "fetched_at": "2026-02-28T23:18:01.177367Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 2
          },
          {
            "name": "visibility_gain",
            "score": 4
          }
        ],
        "structural_score": 6,
        "timeliness_score": 4,
        "final_score": 5.0,
        "reddit_score": null,
        "reddit_comments": null,
        "temp_score_struct": 5.3999999999999995,
        "temp_score_trend": 4.6
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260228082723.htm",
        "title": "Scientists discover a bacterial kill switch and it could change the fight against superbugs",
        "summary": "Drug-resistant bacteria are becoming harder to treat, pushing scientists to look for new antibiotic targets. Researchers have now discovered that several unrelated viruses disable a key bacterial protein called MurJ, which is essential for building the bacterial cell wall. High-resolution imaging shows these viral proteins lock MurJ into a single position, stopping cell wall construction and leading to bacterial death.",
        "source": "www.sciencedaily.com",
        "published": "Sat, 28 Feb 2026 09:20:04 EST",
        "fetched_at": "2026-02-28T23:18:01.177354Z",
        "tags": [
          {
            "name": "visibility_gain",
            "score": 5
          }
        ],
        "structural_score": 5,
        "timeliness_score": 4,
        "final_score": 4.5,
        "reddit_score": null,
        "reddit_comments": null,
        "temp_score_struct": 4.7,
        "temp_score_trend": 4.3
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260227071945.htm",
        "title": "A lost moon may have created Titan and Saturn’s rings",
        "summary": "Saturn’s largest moon, Titan, may have been born in a colossal cosmic crash. New research suggests Titan formed when two older moons slammed together hundreds of millions of years ago—an event so violent it reshaped Saturn’s entire moon system and may have indirectly sparked the formation of its iconic rings. Clues come from Titan’s unusual orbit, its surprisingly smooth surface, and the strange behavior of the tumbling moon Hyperion.",
        "source": "www.sciencedaily.com",
        "published": "Fri, 27 Feb 2026 07:19:45 EST",
        "fetched_at": "2026-02-28T23:18:01.177380Z",
        "tags": [
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 5,
        "timeliness_score": 4,
        "final_score": 4.5,
        "reddit_score": null,
        "reddit_comments": null,
        "temp_score_struct": 4.7,
        "temp_score_trend": 4.3
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260227071931.htm",
        "title": "James Webb reveals a barred spiral galaxy shockingly early in the Universe",
        "summary": "Astronomers have spotted what may be one of the universe’s earliest barred spiral galaxies — a striking cosmic structure forming just 2 billion years after the Big Bang. The galaxy, COSMOS-74706, dates back about 11.5 billion years and contains a stellar bar, a bright, linear band of stars and gas stretching across its center, similar to the one in our own Milky Way.",
        "source": "www.sciencedaily.com",
        "published": "Fri, 27 Feb 2026 12:15:06 EST",
        "fetched_at": "2026-02-28T23:18:01.177390Z",
        "tags": [
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 5,
        "timeliness_score": 4,
        "final_score": 4.5,
        "reddit_score": null,
        "reddit_comments": null,
        "temp_score_struct": 4.7,
        "temp_score_trend": 4.3
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260227071918.htm",
        "title": "MIT study finds Earth’s first animals were likely ancient sea sponges",
        "summary": "Scientists at MIT have found compelling chemical evidence that Earth’s earliest animals were likely ancient sea sponges. Hidden inside rocks over 541 million years old are rare molecular “fingerprints” that match compounds made by modern demosponges. After testing rocks, living sponges, and lab-made molecules, researchers confirmed the signals came from life — not geology. The discovery suggests sponges were thriving in the oceans well before most other animal groups appeared.",
        "source": "www.sciencedaily.com",
        "published": "Fri, 27 Feb 2026 09:45:38 EST",
        "fetched_at": "2026-02-28T23:18:01.177414Z",
        "tags": [
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 5,
        "timeliness_score": 4,
        "final_score": 4.5,
        "reddit_score": null,
        "reddit_comments": null,
        "temp_score_struct": 4.7,
        "temp_score_trend": 4.3
      }
    ],
    "ai": [
      {
        "url": "https://venturebeat.com/infrastructure/railway-secures-usd100-million-to-challenge-aws-with-ai-native-cloud",
        "title": "Railway secures $100 million to challenge AWS with AI-native cloud infrastructure",
        "summary": "<p><a href=\"https://railway.com/\">Railway</a>, a San Francisco-based cloud platform that has quietly amassed two million developers without spending a dollar on marketing, announced Thursday that it raised $100 million in a Series B funding round, as surging demand for artificial intelligence applications exposes the limitations of legacy cloud infrastructure.</p><p><a href=\"https://tq.vc/\">TQ Ventures</a> led the round, with participation from <a href=\"https://fpvventures.com/\">FPV Ventures</a>, <a href=\"https://www.redpoint.com/\">Redpoint</a>, and <a href=\"https://www.unusual.vc/\">Unusual Ventures</a>. The investment values Railway as one of the most significant infrastructure startups to emerge during the AI boom, capitalizing on developer frustration with the complexity and cost of traditional platforms like <a href=\"https://aws.amazon.com/\">Amazon Web Services</a> and <a href=\"https://cloud.google.com/\">Google Cloud</a>.</p><p>&quot;As AI models get better at writing code, more and more people are asking the age-old question: where, and how, do I run my applications?&quot; said Jake Cooper, Railway&#x27;s 28-year-old founder and chief executive, in an exclusive interview with VentureBeat. &quot;The last generation of cloud primitives were slow and outdated, and now with AI moving everything faster, teams simply can&#x27;t keep up.&quot;</p><p>The funding is a dramatic acceleration for a company that has charted an unconventional path through the cloud computing industry. Railway raised just $24 million in total before this round, including a <a href=\"https://techcrunch.com/2022/05/31/railway-snags-20m-to-streamline-the-process-of-deploying-apps-and-services/\">$20 million Series A</a> from Redpoint in 2022. The company now processes more than 10 million deployments monthly and handles over one trillion requests through its edge network — metrics that rival far larger and better-funded competitors.</p><h2><b>Why three-minute deploy times have become unacceptable in the age of AI coding assistants</b></h2><p>Railway&#x27;s pitch rests on a simple observation: the tools developers use to deploy and manage software were designed for a slower era. A standard build-and-deploy cycle using <a href=\"https://station.railway.com/feedback/terraform-provider-954567d7\">Terraform</a>, the industry-standard infrastructure tool, takes two to three minutes. That delay, once tolerable, has become a critical bottleneck as AI coding assistants like <a href=\"https://claude.ai/login\">Claude</a>, <a href=\"https://chatgpt.com/\">ChatGPT</a>, and <a href=\"https://cursor.com/\">Cursor</a> can generate working code in seconds.</p><p>&quot;When godly intelligence is on tap and can solve any problem in three seconds, those amalgamations of systems become bottlenecks,&quot; Cooper told VentureBeat. &quot;What was really cool for humans to deploy in 10 seconds or less is now table stakes for agents.&quot;</p><p>The company claims its platform delivers deployments in under one second — fast enough to keep pace with AI-generated code. Customers report a tenfold increase in developer velocity and up to 65 percent cost savings compared to traditional cloud providers.</p><p>These numbers come directly from enterprise clients, not internal benchmarks. Daniel Lobaton, chief technology officer at G2X, a platform serving 100,000 federal contractors, measured deployment speed improvements of seven times faster and an 87 percent cost reduction after migrating to Railway. His infrastructure bill dropped from $15,000 per month to approximately $1,000.</p><p>&quot;The work that used to take me a week on our previous infrastructure, I can do in Railway in like a day,&quot; Lobaton said. &quot;If I want to spin up a new service and test different architectures, it would take so long on our old setup. In Railway I can launch six services in two minutes.&quot;</p><h2><b>Inside the controversial decision to abandon Google Cloud and build data centers from scratch</b></h2><p>What distinguishes <a href=\"https://railway.com/\">Railway</a> from competitors like <a href=\"https://render.com/\">Render</a> and <a href=\"http://fly.io\">Fly.io</a> is the depth of its vertical integration. In 2024, the company made the unusual decision to abandon Google Cloud entirely and build its own data centers, a move that echoes the famous Alan Kay maxim: &quot;People who are really serious about software should make their own hardware.&quot;</p><p>&quot;We wanted to design hardware in a way where we could build a differentiated experience,&quot; Cooper said. &quot;Having full control over the network, compute, and storage layers lets us do really fast build and deploy loops, the kind that allows us to move at &#x27;agentic speed&#x27; while staying 100 percent the smoothest ride in town.&quot;</p><p>The approach paid dividends during recent <a href=\"https://restofworld.org/2026/cloud-outages-2025-global-business-impact/\">widespread outages</a> that affected major cloud providers — Railway remained online throughout.</p><p>This soup-to-nuts control enables pricing that undercuts the hyperscalers by roughly 50 percent and newer cloud startups by three to four times. Railway charges by the second for actual compute usage: $0.00000386 per gigabyte-second of memory, $0.00000772 per vCPU-second, and $0.00000006 per gigabyte-second of storage. There are no charges for idle virtual machines — a stark contrast to the traditional cloud model where customers pay for provisioned capacity whether they use it or not.</p><p>&quot;The conventional wisdom is that the big guys have economies of scale to offer better pricing,&quot; Cooper noted. &quot;But when they&#x27;re charging for VMs that usually sit idle in the cloud, and we&#x27;ve purpose-built everything to fit much more density on these machines, you have a big opportunity.&quot;</p><h2><b>How 30 employees built a platform generating tens of millions in annual revenue</b></h2><p><a href=\"https://railway.com/\">Railway</a> has achieved its scale with a team of just 30 employees generating tens of millions in annual revenue — a ratio of revenue per employee that would be exceptional even for established software companies. The company grew revenue 3.5 times last year and continues to expand at 15 percent month-over-month.</p><p>Cooper emphasized that the fundraise was strategic rather than necessary. &quot;We&#x27;re default alive; there&#x27;s no reason for us to raise money,&quot; he said. &quot;We raised because we see a massive opportunity to accelerate, not because we needed to survive.&quot;</p><p>The company hired its first salesperson only last year and employs just two solutions engineers. Nearly all of Railway&#x27;s two million users discovered the platform through word of mouth — developers telling other developers about a tool that actually works.</p><p>&quot;We basically did the standard engineering thing: if you build it, they will come,&quot; Cooper recalled. &quot;And to some degree, they came.&quot;</p><h2><b>From side projects to Fortune 500 deployments: Railway&#x27;s unlikely corporate expansion</b></h2><p>Despite its grassroots developer community, Railway has made significant inroads into large organizations. The company claims that 31 percent of Fortune 500 companies now use its platform, though deployments range from company-wide infrastructure to individual team projects.</p><p>Notable customers include <a href=\"https://www.biltrewards.com/\">Bilt</a>, the loyalty program company; Intuit&#x27;s <a href=\"https://www.goco.io/\">GoCo</a> subsidiary; TripAdvisor&#x27;s <a href=\"https://www.cruisecritic.com/\">Cruise Critic</a>; and <a href=\"https://www.mgmresorts.com/en.html\">MGM Resorts</a>. <a href=\"https://www.ycombinator.com/companies/kernel\">Kernel</a>, a Y Combinator-backed startup providing AI infrastructure to over 1,000 companies, runs its entire customer-facing system on Railway for $444 per month.</p><p>&quot;At my previous company Clever, which sold for $500 million, I had six full-time engineers just managing AWS,&quot; said Rafael Garcia, Kernel&#x27;s chief technology officer. &quot;Now I have six engineers total, and they all focus on product. Railway is exactly the tool I wish I had in 2012.&quot;</p><p>For enterprise customers, <a href=\"https://railway.com/\">Railway</a> offers security certifications including SOC 2 Type 2 compliance and HIPAA readiness, with business associate agreements available upon request. The platform provides single sign-on authentication, comprehensive audit logs, and the option to deploy within a customer&#x27;s existing cloud environment through a &quot;bring your own cloud&quot; configuration.</p><p>Enterprise pricing starts at custom levels, with specific add-ons for extended log retention ($200 monthly), HIPAA BAAs ($1,000), enterprise support with SLOs ($2,000), and dedicated virtual machines ($10,000).</p><h2><b>The startup&#x27;s bold strategy to take on Amazon, Google, and a new generation of cloud rivals</b></h2><p>Railway enters a crowded market that includes not only the hyperscale cloud providers—Amazon Web Services, Microsoft Azure, and Google Cloud Platform—but also a growing cohort of developer-focused platforms like Vercel, Render, Fly.io, and Heroku.</p><p>Cooper argues that Railway&#x27;s competitors fall into two camps, neither of which has fully committed to the new infrastructure model that AI demands.</p><p>&quot;The hyperscalers have two competing systems, and they haven&#x27;t gone all-in on the new model because their legacy revenue stream is still printing money,&quot; he observed. &quot;They have this mammoth pool of cash coming from people who provision a VM, use maybe 10 percent of it, and still pay for the whole thing. To what end are they actually interested in going all the way in on a new experience if they don&#x27;t really need to?&quot;</p><p>Against startup competitors, Railway differentiates by covering the full infrastructure stack. &quot;We&#x27;re not just containers; we&#x27;ve got VM primitives, stateful storage, virtual private networking, automated load balancing,&quot; Cooper said. &quot;And we wrap all of this in an absurdly easy-to-use UI, with agentic primitives so agents can move 1,000 times faster.&quot;</p><p>The platform supports databases including PostgreSQL, MySQL, MongoDB, and Redis; provides up to 256 terabytes of persistent storage with over 100,000 input/output operations per second; and enables deployment to four global regions spanning the United States, Europe, and Southeast Asia. Enterprise customers can scale to 112 vCPUs and 2 terabytes of RAM per service.</p><h2><b>Why investors are betting that AI will create a thousand times more software than exists today</b></h2><p>Railway&#x27;s fundraise reflects broader investor enthusiasm for companies positioned to benefit from the AI coding revolution. As tools like <a href=\"https://github.com/features/copilot\">GitHub Copilot</a>, <a href=\"https://cursor.com/agents\">Cursor</a>, and <a href=\"https://claude.ai/login\">Claude</a> become standard fixtures in developer workflows, the volume of code being written — and the infrastructure needed to run it — is expanding dramatically.</p><p>&quot;The amount of software that&#x27;s going to come online over the next five years is unfathomable compared to what existed before — we&#x27;re talking a thousand times more software,&quot; Cooper predicted. &quot;All of that has to run somewhere.&quot;</p><p>The company has already integrated directly with AI systems, building what Cooper calls &quot;loops where Claude can hook in, call deployments, and analyze infrastructure automatically.&quot; Railway released a Model Context Protocol server in August 2025 that allows AI coding agents to deploy applications and manage infrastructure directly from code editors.</p><p>&quot;The notion of a developer is melting before our eyes,&quot; Cooper said. &quot;You don&#x27;t have to be an engineer to engineer things anymore — you just need critical thinking and the ability to analyze things in a systems capacity.&quot;</p><h2><b>What Railway plans to do with $100 million and zero marketing experience</b></h2><p><a href=\"https://railway.com/\">Railway</a> plans to use the new capital to expand its global data center footprint, grow its team beyond 30 employees, and build what Cooper described as a proper go-to-market operation for the first time in the company&#x27;s five-year history.</p><p>&quot;One of my mentors said you raise money when you can change the trajectory of the business,&quot; Cooper explained. &quot;We&#x27;ve built all the required substrate to scale indefinitely; what&#x27;s been holding us back is simply talking about it. 2026 is the year we play on the world stage.&quot;</p><p>The company&#x27;s investor roster reads like a who&#x27;s who of developer infrastructure. Angel investors include <a href=\"https://tom.preston-werner.com/\">Tom Preston-Werner,</a> co-founder of GitHub; <a href=\"https://rauchg.com/about\">Guillermo Rauch</a>, chief executive of Vercel; <a href=\"https://www.cockroachlabs.com/author/spencer-kimball/\">Spencer Kimball</a>, chief executive of Cockroach Labs; <a href=\"https://www.datadoghq.com/about/leadership/\">Olivier Pomel</a>, chief executive of Datadog; and <a href=\"https://sequoiacap.com/founder/jori-lallo/\">Jori Lallo</a>, co-founder of Linear.</p><p>The timing of Railway&#x27;s expansion coincides with what many in Silicon Valley view as a fundamental shift in how software gets made. Coding assistants are no longer experimental curiosities — they have become essential tools that millions of developers rely on daily. Each line of AI-generated code needs somewhere to run, and the incumbents, by Cooper&#x27;s telling, are too wedded to their existing business models to fully capitalize on the moment.</p><p>Whether <a href=\"https://railway.com/\">Railway</a> can translate developer enthusiasm into sustained enterprise adoption remains an open question. The cloud infrastructure market is littered with promising startups that failed to break the grip of Amazon, Microsoft, and Google. But Cooper, who previously worked as a software engineer at <a href=\"https://www.wolframalpha.com/\">Wolfram Alpha</a>, <a href=\"https://www.bloomberg.com/\">Bloomberg</a>, and <a href=\"https://www.uber.com/\">Uber</a> before founding Railway in 2020, seems unfazed by the scale of his ambition.</p><p>&quot;In five years, Railway [will be] the place where software gets created and evolved, period,&quot; he said. &quot;Deploy instantly, scale infinitely, with zero friction. That&#x27;s the prize worth playing for, and there&#x27;s no bigger one on offer.&quot;</p><p>For a company that built a $100 million business by doing the opposite of what conventional startup wisdom dictates — no marketing, no sales team, no venture hype—the real test begins now. Railway spent five years proving that developers would find a better mousetrap on their own. The next five will determine whether the rest of the world is ready to get on board.</p>",
        "source": "venturebeat.com",
        "published": "Thu, 22 Jan 2026 14:00:00 GMT",
        "fetched_at": "2026-02-28T23:17:50.451261Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 7
          },
          {
            "name": "visibility_gain",
            "score": 5
          },
          {
            "name": "value_redefinition",
            "score": 8
          },
          {
            "name": "scale_shift",
            "score": 13
          },
          {
            "name": "ontology_shift",
            "score": 8
          }
        ],
        "structural_score": 41,
        "timeliness_score": 3,
        "final_score": 22.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://venturebeat.com/technology/anthropic-launches-cowork-a-claude-desktop-agent-that-works-in-your-files-no",
        "title": "Anthropic launches Cowork, a Claude Desktop agent that works in your files — no coding required",
        "summary": "<p><a href=\"https://www.anthropic.com/\">Anthropic</a> released <a href=\"https://claude.com/blog/cowork-research-preview\">Cowork</a> on Monday, a new AI agent capability that extends the power of its wildly successful <a href=\"https://claude.com/product/claude-code\">Claude Code</a> tool to non-technical users — and according to company insiders, the team built the entire feature in approximately a week and a half, largely using Claude Code itself.</p><p>The launch marks a major inflection point in the race to deliver practical AI agents to mainstream users, positioning Anthropic to compete not just with <a href=\"https://openai.com/\">OpenAI</a> and <a href=\"https://gemini.google.com/app\">Google</a> in conversational AI, but with <a href=\"https://copilot.microsoft.com/\">Microsoft&#x27;s Copilot</a> in the burgeoning market for AI-powered productivity tools.</p><p>&quot;Cowork lets you complete non-technical tasks much like how developers use Claude Code,&quot; the <a href=\"https://x.com/claudeai/status/2010805682434666759?s=20\">company announced</a> via its official Claude account on X. The feature arrives as a research preview available exclusively to <a href=\"https://support.claude.com/en/articles/11014257-about-claude-s-max-plan-usage\">Claude Max subscribers</a> — Anthropic&#x27;s power-user tier priced between $100 and $200 per month — through the macOS desktop application.</p><p>For the past year, the industry narrative has focused on large language models that can write poetry or debug code. With <a href=\"https://claude.com/blog/cowork-research-preview\">Cowork</a>, Anthropic is betting that the real enterprise value lies in an AI that can open a folder, read a messy pile of receipts, and generate a structured expense report without human hand-holding.</p><div></div><h2><b>How developers using a coding tool for vacation research inspired Anthropic&#x27;s latest product</b></h2><p>The genesis of <a href=\"https://claude.com/blog/cowork-research-preview\">Cowork</a> lies in Anthropic&#x27;s recent success with the developer community. In late 2024, the company released <a href=\"https://www.anthropic.com/news/claude-3-7-sonnet\">Claude Code</a>, a terminal-based tool that allowed software engineers to automate rote programming tasks. The tool was a hit, but Anthropic noticed a peculiar trend: users were forcing the coding tool to perform non-coding labor.</p><p>According to <a href=\"https://x.com/bcherny/status/2010809450844831752\">Boris Cherny</a>, an engineer at Anthropic, the company observed users deploying the developer tool for an unexpectedly diverse array of tasks.</p><div></div><p>&quot;Since we launched Claude Code, we saw people using it for all sorts of non-coding work: doing vacation research, building slide decks, cleaning up your email, cancelling subscriptions, recovering wedding photos from a hard drive, monitoring plant growth, controlling your oven,&quot; Cherny wrote on X. &quot;These use cases are diverse and surprising — the reason is that the underlying Claude Agent is the best agent, and Opus 4.5 is the best model.&quot;</p><p>Recognizing this shadow usage, Anthropic effectively stripped the command-line complexity from their developer tool to create a consumer-friendly interface. In its blog post announcing the feature, <a href=\"https://claude.com/blog/cowork-research-preview\">Anthropic explained</a> that developers &quot;quickly began using it for almost everything else,&quot; which &quot;prompted us to build Cowork: a simpler way for anyone — not just developers — to work with Claude in the very same way.&quot;</p><h2><b>Inside the folder-based architecture that lets Claude read, edit, and create files on your computer</b></h2><p>Unlike a standard chat interface where a user pastes text for analysis, <a href=\"https://claude.com/blog/cowork-research-preview\">Cowork</a> requires a different level of trust and access. Users designate a specific folder on their local machine that Claude can access. Within that sandbox, the AI agent can read existing files, modify them, or create entirely new ones.</p><p>Anthropic offers several illustrative examples: reorganizing a cluttered downloads folder by sorting and intelligently renaming each file, generating a spreadsheet of expenses from a collection of receipt screenshots, or drafting a report from scattered notes across multiple documents.</p><p>&quot;In Cowork, you give Claude access to a folder on your computer. Claude can then read, edit, or create files in that folder,&quot; <a href=\"https://x.com/claudeai/status/2010805685530038351\">the company explained</a> on X. &quot;Try it to create a spreadsheet from a pile of screenshots, or produce a first draft from scattered notes.&quot;</p><div></div><p>The architecture relies on what is known as an &quot;agentic loop.&quot; When a user assigns a task, the AI does not merely generate a text response. Instead, it formulates a plan, executes steps in parallel, checks its own work, and asks for clarification if it hits a roadblock. Users can queue multiple tasks and let Claude process them simultaneously — a workflow Anthropic describes as feeling &quot;much less like a back-and-forth and much more like leaving messages for a coworker.&quot;</p><p>The system is built on Anthropic&#x27;s <a href=\"https://www.anthropic.com/engineering/building-agents-with-the-claude-agent-sdk\">Claude Agent SDK</a>, meaning it shares the same underlying architecture as Claude Code. Anthropic notes that Cowork &quot;can take on many of the same tasks that Claude Code can handle, but in a more approachable form for non-coding tasks.&quot;</p><h2><b>The recursive loop where AI builds AI: Claude Code reportedly wrote much of Claude Cowork</b></h2><p>Perhaps the most remarkable detail surrounding Cowork&#x27;s launch is the speed at which the tool was reportedly built — highlighting a recursive feedback loop where AI tools are being used to build better AI tools.</p><p>During a livestream hosted by Dan Shipper, Felix Rieseberg, an Anthropic employee, confirmed that <a href=\"https://x.com/blakeir/status/2010837251505205656\">t</a>he team <a href=\"https://x.com/blakeir/status/2010837251505205656\">built Cowork in approximately a week and a half</a>.</p><p>Alex Volkov, who covers AI developments, expressed surprise at the timeline: &quot;Holy shit Anthropic built &#x27;Cowork&#x27; in the last... week and a half?!&quot;</p><div></div><p>This prompted immediate speculation about how much of Cowork was itself built by Claude Code. <a href=\"https://x.com/_simonsmith\">Simon Smith</a>, EVP of Generative AI at Klick Health, put it bluntly on X: &quot;Claude Code wrote all of Claude Cowork. Can we all agree that we&#x27;re in at least somewhat of a recursive improvement loop here?&quot;</p><p>The implication is profound: Anthropic&#x27;s AI coding agent may have substantially contributed to building its own non-technical sibling product. If true, this is one of the most visible examples yet of AI systems being used to accelerate their own development and expansion — a strategy that could widen the gap between AI labs that successfully deploy their own agents internally and those that do not.</p><h2><b>Connectors, browser automation, and skills extend Cowork&#x27;s reach beyond the local file system</b></h2><p>Cowork doesn&#x27;t operate in isolation. The feature integrates with Anthropic&#x27;s existing ecosystem of connectors — tools that link <a href=\"https://claude.ai/login?returnTo=%2Fnew%3F\">Claude</a> to external information sources and services such as <a href=\"https://asana.com/\">Asana</a>, <a href=\"https://www.notion.com/\">Notion</a>, <a href=\"https://www.paypal.com/us/home\">PayPal</a>, and other supported partners. Users who have configured these connections in the standard Claude interface can leverage them within Cowork sessions.</p><p>Additionally, Cowork can pair with <a href=\"https://code.claude.com/docs/en/chrome\">Claude in Chrome</a>, Anthropic&#x27;s browser extension, to execute tasks requiring web access. This combination allows the agent to navigate websites, click buttons, fill forms, and extract information from the internet — all while operating from the desktop application.</p><p>&quot;Cowork includes a number of novel UX and safety features that we think make the product really special,&quot; <a href=\"https://x.com/bcherny/status/2010809450844831752\">Cherny explained</a>, highlighting &quot;a built-in VM [virtual machine] for isolation, out of the box support for browser automation, support for all your claude.ai data connectors, asking you for clarification when it&#x27;s unsure.&quot;</p><p><a href=\"https://www.anthropic.com/\">Anthropic</a> has also introduced an initial set of &quot;skills&quot; specifically designed for Cowork that enhance Claude&#x27;s ability to create documents, presentations, and other files. These build on the <a href=\"https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills\">Skills for Claude</a> framework the company announced in October, which provides specialized instruction sets Claude can load for particular types of tasks.</p><h2><b>Why Anthropic is warning users that its own AI agent could delete their files</b></h2><p>The transition from a chatbot that suggests edits to an agent that makes edits introduces significant risk. An AI that can organize files can, theoretically, delete them.</p><p>In a notable display of transparency, Anthropic devoted considerable space in its announcement to <a href=\"https://claude.com/blog/cowork-research-preview\">warning users about Cowork&#x27;s potential dangers</a> — an unusual approach for a product launch.</p><p>The company explicitly acknowledges that Claude &quot;can take potentially destructive actions (such as deleting local files) if it&#x27;s instructed to.&quot; Because Claude might occasionally misinterpret instructions, Anthropic urges users to provide &quot;very clear guidance&quot; about sensitive operations.</p><p>More concerning is the risk of prompt injection attacks — a technique where malicious actors embed hidden instructions in content Claude might encounter online, potentially causing the agent to bypass safeguards or take harmful actions.</p><p>&quot;We&#x27;ve built sophisticated defenses against prompt injections,&quot; Anthropic wrote, &quot;but agent safety — that is, the task of securing Claude&#x27;s real-world actions — is still an active area of development in the industry.&quot;</p><p>The company characterized these risks as inherent to the current state of AI agent technology rather than unique to Cowork. &quot;These risks aren&#x27;t new with Cowork, but it might be the first time you&#x27;re using a more advanced tool that moves beyond a simple conversation,&quot; the announcement notes.</p><h2><b>Anthropic&#x27;s desktop agent strategy sets up a direct challenge to Microsoft Copilot</b></h2><p>The launch of <a href=\"https://claude.com/blog/cowork-research-preview\">Cowork</a> places Anthropic in direct competition with <a href=\"https://www.microsoft.com/en-us/\">Microsoft</a>, which has spent years attempting to integrate its <a href=\"https://copilot.microsoft.com/\">Copilot AI</a> into the fabric of the Windows operating system with mixed adoption results.</p><p>However, Anthropic&#x27;s approach differs in its isolation. By confining the agent to specific folders and requiring explicit connectors, they are attempting to strike a balance between the utility of an OS-level agent and the security of a sandboxed application.</p><p>What distinguishes Anthropic&#x27;s approach is its bottom-up evolution. Rather than designing an AI assistant and retrofitting agent capabilities, Anthropic built a powerful coding agent first — <a href=\"https://code.claude.com/docs/en/overview\">Claude Code</a> — and is now abstracting its capabilities for broader audiences. This technical lineage may give Cowork more robust agentic behavior from the start.</p><p>Claude Code has generated significant enthusiasm among developers since its initial launch as <a href=\"https://www.anthropic.com/news/claude-3-7-sonnet\">a command-line tool in late 2024</a>. The company expanded access with a <a href=\"https://arstechnica.com/ai/2025/10/claude-code-gets-a-web-version-but-its-the-new-sandboxing-that-really-matters/\">web interface</a> in October 2025, followed by a <a href=\"https://venturebeat.com/ai/anthropics-claude-code-can-now-read-your-slack-messages-and-write-code-for\">Slack integration</a> in December. Cowork is the next logical step: bringing the same agentic architecture to users who may never touch a terminal.</p><h2><b>Who can access Cowork now, and what&#x27;s coming next for Windows and other platforms</b></h2><p>For now, Cowork remains exclusive to <a href=\"https://support.claude.com/en/articles/11014257-about-claude-s-max-plan-usage\">Claude Max subscribers</a> using the macOS desktop application. Users on other subscription tiers — Free, Pro, Team, or Enterprise — can join a waitlist for future access.</p><p>Anthropic has signaled clear intentions to expand the feature&#x27;s reach. The blog post explicitly mentions plans to add cross-device sync and bring Cowork to Windows as the company learns from the research preview.</p><p>Cherny set expectations appropriately, describing the product as &quot;early and raw, similar to what Claude Code felt like when it first launched.&quot;</p><p>To access <a href=\"https://claude.com/blog/cowork-research-preview\">Cowork</a>, Max subscribers can download or update the Claude macOS app and click on &quot;Cowork&quot; in the sidebar.</p><h2><b>The real question facing enterprise AI adoption</b></h2><p>For technical decision-makers, the implications of Cowork extend beyond any single product launch. The bottleneck for AI adoption is shifting — no longer is model intelligence the limiting factor, but rather workflow integration and user trust.</p><p>Anthropic&#x27;s goal, as the company puts it, is to make working with Claude feel less like operating a tool and more like delegating to a colleague. Whether mainstream users are ready to hand over folder access to an AI that might misinterpret their instructions remains an open question.</p><p>But the speed of Cowork&#x27;s development — a major feature built in ten days, possibly by the company&#x27;s own AI — previews a future where the capabilities of these systems compound faster than organizations can evaluate them. </p><p>The chatbot has learned to use a file manager. What it learns to use next is anyone&#x27;s guess.</p>",
        "source": "venturebeat.com",
        "published": "Mon, 12 Jan 2026 11:30:00 GMT",
        "fetched_at": "2026-02-28T23:17:50.451292Z",
        "tags": [
          {
            "name": "transformation",
            "score": 3
          },
          {
            "name": "boundary_crossing",
            "score": 9
          },
          {
            "name": "visibility_gain",
            "score": 4
          },
          {
            "name": "value_redefinition",
            "score": 8
          },
          {
            "name": "scale_shift",
            "score": 8
          },
          {
            "name": "ontology_shift",
            "score": 8
          }
        ],
        "structural_score": 40,
        "timeliness_score": 3,
        "final_score": 21.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://venturebeat.com/technology/listen-labs-raises-usd69m-after-viral-billboard-hiring-stunt-to-scale-ai",
        "title": "Listen Labs raises $69M after viral billboard hiring stunt to scale AI customer interviews",
        "summary": "<p>Alfred Wahlforss was running out of options. His startup, <a href=\"https://listenlabs.ai/\">Listen Labs</a>, needed to hire over 100 engineers, but competing against Mark Zuckerberg&#x27;s <a href=\"https://news.bloomberglaw.com/employee-benefits/zuckerbergs-100-million-ai-job-offers-pay-off-parmy-olson\">$100 million offers</a> seemed impossible. So he spent $5,000 — a fifth of his marketing budget — on a <a href=\"https://billboardinsider.com/ai-startup/\">billboard in San Francisco</a> displaying what looked like gibberish: five strings of random numbers.</p><p>The numbers were actually AI tokens. Decoded, they led to a coding challenge: build an algorithm to act as a digital bouncer at Berghain, the Berlin nightclub famous for rejecting nearly everyone at the door. Within days, thousands attempted the puzzle. 430 cracked it. Some got hired. The winner flew to Berlin, all expenses paid.</p><p>That unconventional approach has now attracted $69 million in Series B funding, led by <a href=\"https://www.ribbitcap.com/\">Ribbit Capital</a> with participation from <a href=\"https://www.evantic.ai/\">Evantic</a> and existing investors <a href=\"https://sequoiacap.com/\">Sequoia Capital</a>, <a href=\"https://www.conviction.com/\">Conviction</a>, and <a href=\"https://pear.vc/\">Pear VC</a>. The round values Listen Labs at $500 million and brings its total capital to $100 million. In nine months since launch, the company has grown annualized revenue by 15x to eight figures and conducted over one million AI-powered interviews.</p><div></div><p>&quot;When you obsess over customers, everything else follows,&quot; Wahlforss said in an interview with VentureBeat. &quot;Teams that use Listen bring the customer into every decision, from marketing to product, and when the customer is delighted, everyone is.&quot;</p><h2><b>Why traditional market research is broken, and what Listen Labs is building to fix it</b></h2><p>Listen&#x27;s <a href=\"https://listenlabs.ai/role/agencies\">AI researcher</a> finds participants, conducts in-depth interviews, and delivers actionable insights in hours, not weeks. The platform replaces the traditional choice between quantitative surveys — which provide statistical precision but miss nuance—and qualitative interviews, which deliver depth but cannot scale.</p><p>Wahlforss explained the limitation of existing approaches: &quot;Essentially surveys give you false precision because people end up answering the same question... You can&#x27;t get the outliers. People are actually not honest on surveys.&quot; The alternative, one-on-one human interviews, &quot;gives you a lot of depth. You can ask follow up questions. You can kind of double check if they actually know what they&#x27;re talking about. And the problem is you can&#x27;t scale that.&quot;</p><p>The platform works in four steps: users create a study with AI assistance, Listen recruits participants from its global network of 30 million people, an AI moderator conducts in-depth interviews with follow-up questions, and results are packaged into executive-ready reports including key themes, highlight reels, and slide decks.</p><p>What distinguishes Listen&#x27;s approach is its use of open-ended video conversations rather than multiple-choice forms. &quot;In a survey, you can kind of guess what you should answer, and you have four options,&quot; Wahlforss said. &quot;Oh, they probably want me to buy high income. Let me click on that button versus an open ended response. It just generates much more honesty.&quot;</p><h2><b>The dirty secret of the $140 billion market research industry: rampant fraud</b></h2><p><a href=\"https://listenlabs.ai/\">Listen</a> finds and qualifies the right participants in its global network of 30 million people. But building that panel required confronting what Wahlforss called &quot;one of the most shocking things that we&#x27;ve learned when we entered this industry&quot;—rampant fraud.</p><p>&quot;Essentially, there&#x27;s a financial transaction involved, which means there will be bad players,&quot; he explained. &quot;We actually had some of the largest companies, some of them have billions in revenue, send us people who claim to be kind of enterprise buyers to our platform and our system immediately detected, like, fraud, fraud, fraud, fraud, fraud.&quot;</p><p>The company built what it calls a &quot;quality guard&quot; that cross-references LinkedIn profiles with video responses to verify identity, checks consistency across how participants answer questions, and flags suspicious patterns. The result, according to Wahlforss: &quot;People talk three times more. They&#x27;re much more honest when they talk about sensitive topics like politics and mental health.&quot;</p><p><a href=\"https://listenlabs.ai/case-studies/emeritus\">Emeritus</a>, an online education company that uses Listen, reported that approximately 20% of survey responses previously fell into the fraudulent or low-quality category. With Listen, they reduced this to almost zero. &quot;We did not have to replace any responses because of fraud or gibberish information,&quot; said Gabrielli Tiburi, Assistant Manager of Customer Insights at Emeritus.</p><h2><b>How Microsoft, Sweetgreen, and Chubbies are using AI interviews to build better products</b></h2><p>The speed advantage has proven central to Listen&#x27;s pitch. Traditional customer research at <a href=\"https://listenlabs.ai/case-studies/microsoft\">Microsoft</a> could take four to six weeks to generate insights. &quot;By the time we get to them, either the decision has been made or we lose out on the opportunity to actually influence it,&quot; said Romani Patel, Senior Research Manager at Microsoft.</p><p>With Listen, Microsoft can now get insights in days, and in many cases, within hours.</p><p>The platform has already powered several high-profile initiatives. Microsoft used Listen Labs to collect global customer stories for its 50th anniversary celebration. &quot;We wanted users to share how Copilot is empowering them to bring their best self forward,&quot; Patel said, &quot;and we were able to collect those user video stories within a day.&quot; Traditionally, that kind of work would have taken six to eight weeks.</p><p><a href=\"https://listenlabs.ai/case-studies/simple-modern\">Simple Modern</a>, an Oklahoma-based drinkware company, used Listen to test a new product concept. The process took about an hour to write questions, an hour to launch the study, and 2.5 hours to receive feedback from 120 people across the country. &quot;We went from &#x27;Should we even have this product?&#x27; to &#x27;How should we launch it?&#x27;&quot; said Chris Hoyle, the company&#x27;s Chief Marketing Officer.</p><p><a href=\"https://listenlabs.ai/case-studies/chubbies\">Chubbies</a>, the shorts brand, achieved a 24x increase in youth research participation—growing from 5 to 120 participants — by using Listen to overcome the scheduling challenges of traditional focus groups with children. &quot;There&#x27;s school, sports, dinner, and homework,&quot; explained Lauren Neville, Director of Insights and Innovation. &quot;I had to find a way to hear from them that fit into their schedules.&quot;</p><p>The company also discovered product issues through AI interviews that might have gone undetected otherwise. Wahlforss described how the AI &quot;through conversations, realized there were like issues with the the kids short line, and decided to, like, interview hundreds of kids. And I understand that there were issues in the liner of the shorts and that they were, like, scratchy, quote, unquote, according to the people interviewed.&quot; The redesigned product became &quot;a blockbuster hit.&quot;</p><h2><b>The Jevons paradox explains why cheaper research creates more demand, not less</b></h2><p><a href=\"https://listenlabs.ai/\">Listen Labs</a> is entering a massive but fragmented market. Wahlforss cited research from Andreessen Horowitz estimating the market research industry at roughly <a href=\"https://a16z.com/ai-market-research/\">$140 billion annually</a>, populated by legacy players — some with more than a billion dollars in revenue — that he believes are vulnerable to disruption.</p><p>&quot;There are very much existing budget lines that we are replacing,&quot; Wahlforss said. &quot;Why we&#x27;re replacing them is that one, they&#x27;re super costly. Two, they&#x27;re kind of stuck in this old paradigm of choosing between a survey or interview, and they also take months to work with.&quot;</p><p>But the more intriguing dynamic may be that AI-powered research doesn&#x27;t just replace existing spending — it creates new demand. Wahlforss invoked the Jevons paradox, an economic principle that occurs when technological advancements make a resource more efficient to use, but increased efficiency leads to increased overall consumption rather than decreased consumption.</p><p>&quot;What I&#x27;ve noticed is that as something gets cheaper, you don&#x27;t need less of it. You want more of it,&quot; Wahlforss explained. &quot;There&#x27;s infinite demand for customer understanding. So the researchers on the team can do an order of magnitude more research, and also other people who weren&#x27;t researchers before can now do that as part of their job.&quot;</p><h2><b>Inside the elite engineering team that built Listen Labs before they had a working toilet</b></h2><p><a href=\"https://listenlabs.ai/\">Listen Labs</a> traces its origins to a consumer app that Wahlforss and his co-founder built after meeting at Harvard. &quot;We built this consumer app that got 20,000 downloads in one day,&quot; Wahlforss recalled. &quot;We had all these users, and we were thinking like, okay, what can we do to get to know them better? And we built this prototype of what Listen is today.&quot;</p><p>The founding team brings an unusual pedigree. Wahlforss&#x27;s co-founder &quot;was the national champion in competitive programming in Germany, and he worked at Tesla Autopilot.&quot; The company claims that 30% of its engineering team are medalists from the <a href=\"https://ioinformatics.org/\">International Olympiad in Informatics</a> — the same competition that produced the founders of <a href=\"https://cognition.ai/\">Cognition</a>, the AI coding startup.</p><p>The <a href=\"https://www.cbsnews.com/sanfrancisco/news/san-francisco-billboard-challenge-puts-ai-engineers-to-the-test/\">Berghain billboard stunt</a> generated approximately 5 million views across social media, according to Wahlforss. It reflected the intensity of the talent war in the Bay Area.</p><p>&quot;We had to do these things because some of our, like early employees, joined the company before we had a working toilet,&quot; he said. &quot;But now we fixed that situation.&quot;</p><p>The company grew from 5 to 40 employees in 2024 and plans to reach 150 this year. It hires engineers for non-engineering roles across marketing, growth, and operations — a bet that in the AI era, technical fluency matters everywhere.</p><h2><b>Synthetic customers and automated decisions: what Listen Labs is building next</b></h2><p>Wahlforss outlined an ambitious product roadmap that pushes into more speculative territory. The company is building &quot;the ability to simulate your customers, so you can take all of those interviews we&#x27;ve done, and then extrapolate based on that and create synthetic users or simulated user voices.&quot;</p><p>Beyond simulation, Listen aims to enable automated action based on research findings. &quot;Can you not just make recommendations, but also create spawn agents to either change things in code or some customer churns? Can you give them a discount and try to bring them back?&quot;</p><p>Wahlforss acknowledged the ethical implications. &quot;Obviously, as you said, there&#x27;s kind of ethical concerns there. Of like, automated decision making overall can be bad, but we will have considerable guardrails to make sure that the companies are always in the loop.&quot;</p><p>The company already handles sensitive data with care. &quot;We don&#x27;t train on any of the data,&quot; Wahlforss said. &quot;We will also scrub any sensitive PII automatically so the model can detect that. And there are times when, for example, you work with investors, where if you accidentally mention something that could be material, non public information, the AI can actually detect that and remove any information like that.&quot;</p><h2><b>How AI could reshape the future of product development</b></h2><p>Perhaps the most provocative implication of Listen&#x27;s model is how it could reshape product development itself. Wahlforss described a customer — an Australian startup — that has adopted what amounts to a continuous feedback loop.</p><p>&quot;They&#x27;re based in Australia, so they&#x27;re coding during the day, and then in their night, they&#x27;re releasing a Listen study with an American audience. Listen validates whatever they built during the day, and they get feedback on that. They can then plug that feedback directly into coding tools like Claude Code and iterate.&quot;</p><p>The vision extends Y Combinator&#x27;s famous dictum — &quot;<a href=\"https://www.ycombinator.com/library/4D-yc-s-essential-startup-advice\">write code, talk to users</a>&quot; — into an automated cycle. &quot;Write code is now getting automated. And I think like talk to users will be as well, and you&#x27;ll have this kind of infinite loop where you can start to ship this truly amazing product, almost kind of autonomously.&quot;</p><p>Whether that vision materializes depends on factors beyond Listen&#x27;s control — the continued improvement of AI models, enterprise willingness to trust automated research, and whether speed truly correlates with better products. A <a href=\"https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf\">2024 MIT study</a> found that 95% of AI pilots fail to move into production, a statistic Wahlforss cited as the reason he emphasizes quality over demos.</p><p>&quot;I&#x27;m constantly have to emphasize like, let&#x27;s make sure the quality is there and the details are right,&quot; he said.</p><p>But the company&#x27;s growth suggests appetite for the experiment. Microsoft&#x27;s Patel said Listen has &quot;removed the drudgery of research and brought the fun and joy back into my work.&quot; Chubbies is now pushing its founder to give everyone in the company a login. Sling Money, a stablecoin payments startup, can create a survey in ten minutes and receive results the same day.</p><p>&quot;It&#x27;s a total game changer,&quot; said Ali Romero, Sling Money&#x27;s marketing manager.</p><p>Wahlforss has a different phrase for what he&#x27;s building. When asked about the tension between speed and rigor — the long-held belief that moving fast means cutting corners — he cited Nat Friedman, the former GitHub CEO and Listen investor, who keeps a list of one-liners on his website.</p><p>One of them: &quot;Slow is fake.&quot;</p><p>It&#x27;s an aggressive claim for an industry built on methodological caution. But <a href=\"https://listenlabs.ai/\">Listen Labs</a> is betting that in the AI era, the companies that listen fastest will be the ones that win. The only question is whether customers will talk back.</p>",
        "source": "venturebeat.com",
        "published": "Fri, 16 Jan 2026 14:01:00 GMT",
        "fetched_at": "2026-02-28T23:17:50.451282Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 9
          },
          {
            "name": "visibility_gain",
            "score": 5
          },
          {
            "name": "value_redefinition",
            "score": 8
          },
          {
            "name": "scale_shift",
            "score": 9
          },
          {
            "name": "ontology_shift",
            "score": 8
          }
        ],
        "structural_score": 39,
        "timeliness_score": 3,
        "final_score": 21.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://venturebeat.com/technology/nous-researchs-nouscoder-14b-is-an-open-source-coding-model-landing-right-in",
        "title": "Nous Research's NousCoder-14B is an open-source coding model landing right in the Claude Code moment",
        "summary": "<p><a href=\"https://nousresearch.com/\">Nous Research</a>, the open-source artificial intelligence startup backed by crypto venture firm <a href=\"https://www.paradigm.xyz/\">Paradigm</a>, released a new competitive programming model on Monday that it says matches or exceeds several larger proprietary systems — trained in just four days using 48 of Nvidia&#x27;s latest <a href=\"https://www.nvidia.com/en-us/data-center/dgx-b200/\">B200 graphics processors</a>.</p><p>The model, called <a href=\"https://huggingface.co/NousResearch/NousCoder-14B\">NousCoder-14B</a>, is another entry in a crowded field of AI coding assistants, but arrives at a particularly charged moment: <a href=\"https://claude.com/product/claude-code\">Claude Code</a>, the agentic programming tool from rival Anthropic, has dominated social media discussion since New Year&#x27;s Day, with developers posting <a href=\"https://x.com/0xDesigner/status/2008202211738648767?s=20\">breathless</a> <a href=\"https://x.com/hayesdev_/status/2008043379805048948\">testimonials</a> <a href=\"https://x.com/0xDesigner/status/2008202211738648767?s=20\">about its capabilities</a>. The simultaneous developments underscore how quickly AI-assisted software development is evolving — and how fiercely companies large and small are competing to capture what many believe will become a foundational technology for how software gets written.</p><p><span>type: <!-- -->embedded-entry-inline<!-- --> id: <!-- -->74cSyrq6OUrp9SEQ5zOUSl</span></p><p><a href=\"https://nousresearch.com/nouscoder-14b-a-competitive-olympiad-programming-model/\">NousCoder-14B</a> achieves a 67.87 percent accuracy rate on <a href=\"https://livecodebench.github.io/\">LiveCodeBench v6</a>, a standardized evaluation that tests models on competitive programming problems published between August 2024 and May 2025. That figure represents a 7.08 percentage point improvement over the base model it was trained from, Alibaba&#x27;s <a href=\"https://huggingface.co/Qwen/Qwen3-14B\">Qwen3-14B</a>, according to Nous Research&#x27;s technical report published alongside the release.</p><p>&quot;I gave Claude Code a description of the problem, it generated what we built last year in an hour,&quot; <a href=\"https://www.reddit.com/r/OpenAI/comments/1q2uuil/google_engineer_im_not_joking_and_this_isnt_funny/\">wrote Jaana Dogan</a>, a principal engineer at Google responsible for the Gemini API, in a viral post on X last week that captured the prevailing mood around AI coding tools. Dogan was describing a distributed agent orchestration system her team had spent a year developing — a system Claude Code approximated from a three-paragraph prompt.</p><p>The juxtaposition is instructive: while Anthropic&#x27;s <a href=\"https://venturebeat.com/technology/the-creator-of-claude-code-just-revealed-his-workflow-and-developers-are\">Claude Code has captured imaginations</a> with demonstrations of end-to-end software development, Nous Research is betting that open-source alternatives trained on verifiable problems can close the gap — and that transparency in how these models are built matters as much as raw capability.</p><hr /><h2><b>How Nous Research built an AI coding model that anyone can replicate</b></h2><p>What distinguishes the <a href=\"https://huggingface.co/NousResearch/NousCoder-14B\">NousCoder-14B</a> release from many competitor announcements is its radical openness. Nous Research published not just the <a href=\"https://huggingface.co/NousResearch/NousCoder-14B\">model weights</a> but the <a href=\"https://github.com/NousResearch/atropos/pull/296\">complete reinforcement learning environment</a>, benchmark suite, and training harness — built on the company&#x27;s <a href=\"https://github.com/NousResearch/atropos/pull/296\">Atropos framework </a>— enabling any researcher with sufficient compute to <a href=\"https://wandb.ai/jli505/qwen14b/reports/HermesCoder-14B--VmlldzoxNTQ5Nzc0MQ?accessToken=4pt3stwyh4x83zqe2jgoo5j9b7j07jbe5omf2n40lray3tih17vfkavjootvnw8o\">reproduce or extend the work</a>.</p><p>&quot;Open-sourcing the Atropos stack provides the necessary infrastructure for reproducible olympiad-level reasoning research,&quot; <a href=\"https://x.com/o_mega___/status/2008907268700475450?s=20\">noted one observer on X</a>, summarizing the significance for the academic and open-source communities.</p><p>The model was trained by <a href=\"https://x.com/JoeLi5050\">Joe Li</a>, a researcher in residence at Nous Research and a former competitive programmer himself. Li&#x27;s <a href=\"https://nousresearch.com/nouscoder-14b-a-competitive-olympiad-programming-model/\">technical report </a>reveals an unexpectedly personal dimension: he compared the model&#x27;s improvement trajectory to his own journey on Codeforces, the competitive programming platform where participants earn ratings based on contest performance.</p><p>Based on rough estimates mapping LiveCodeBench scores to Codeforces ratings, Li calculated that NousCoder-14B&#x27;s improvemen t— from approximately the 1600-1750 rating range to 2100-2200 — mirrors a leap that took him nearly two years of sustained practice between ages 14 and 16. The model accomplished the equivalent in four days.</p><p>&quot;Watching that final training run unfold was quite a surreal experience,&quot; Li wrote in the technical report.</p><p>But Li was quick to note an important caveat that speaks to broader questions about AI efficiency: he solved roughly 1,000 problems during those two years, while the model required 24,000. Humans, at least for now, remain dramatically more sample-efficient learners.</p><hr /><h2><b>Inside the reinforcement learning system that trains on 24,000 competitive programming problems</b></h2><p><a href=\"https://huggingface.co/NousResearch/NousCoder-14B\">NousCoder-14B</a>&#x27;s training process offers a window into the increasingly sophisticated techniques researchers use to improve AI reasoning capabilities through reinforcement learning.</p><p>The approach relies on what researchers call &quot;verifiable rewards&quot; — a system where the model generates code solutions, those solutions are executed against test cases, and the model receives a simple binary signal: correct or incorrect. This feedback loop, while conceptually straightforward, requires significant infrastructure to execute at scale.</p><p>Nous Research used <a href=\"https://modal.com/\">Modal</a>, a cloud computing platform, to run sandboxed code execution in parallel. Each of the 24,000 training problems contains hundreds of test cases on average, and the system must verify that generated code produces correct outputs within time and memory constraints — 15 seconds and 4 gigabytes, respectively.</p><p>The training employed a technique called <a href=\"https://dapo-sia.github.io/\">DAPO (Dynamic Sampling Policy Optimization)</a>, which the researchers found performed slightly better than alternatives in their experiments. A key innovation involves &quot;dynamic sampling&quot; — discarding training examples where the model either solves all attempts or fails all attempts, since these provide no useful gradient signal for learning.</p><p>The researchers also adopted &quot;iterative context extension,&quot; first training the model with a 32,000-token context window before expanding to 40,000 tokens. During evaluation, extending the context further to approximately 80,000 tokens produced the best results, with accuracy reaching 67.87 percent.</p><p>Perhaps most significantly, the training pipeline overlaps inference and verification — as soon as the model generates a solution, it begins work on the next problem while the previous solution is being checked. This pipelining, combined with asynchronous training where multiple model instances work in parallel, maximizes hardware utilization on expensive GPU clusters.</p><hr /><h2><b>The looming data shortage that could slow AI coding model progress</b></h2><p>Buried in Li&#x27;s <a href=\"https://nousresearch.com/nouscoder-14b-a-competitive-olympiad-programming-model/\">technical report</a> is a finding with significant implications for the future of AI development: the training dataset for NousCoder-14B encompasses &quot;a significant portion of all readily available, verifiable competitive programming problems in a standardized dataset format.&quot;</p><p>In other words, for this particular domain, the researchers are approaching the limits of high-quality training data.</p><p>&quot;The total number of competitive programming problems on the Internet is roughly the same order of magnitude,&quot; Li wrote, referring to the 24,000 problems used for training. &quot;This suggests that within the competitive programming domain, we have approached the limits of high-quality data.&quot;</p><p>This observation echoes growing concern across the AI industry about data constraints. While compute continues to scale according to well-understood economic and engineering principles, training data is &quot;increasingly finite,&quot; as Li put it.</p><p>&quot;It appears that some of the most important research that needs to be done in the future will be in the areas of synthetic data generation and data efficient algorithms and architectures,&quot; he concluded.</p><p>The challenge is particularly acute for competitive programming because the domain requires problems with known correct solutions that can be verified automatically. Unlike natural language tasks where human evaluation or proxy metrics suffice, code either works or it doesn&#x27;t — making synthetic data generation considerably more difficult.</p><p>Li identified one potential avenue: training models not just to solve problems but to generate solvable problems, enabling a form of self-play similar to techniques that proved successful in game-playing AI systems. &quot;Once synthetic problem generation is solved, self-play becomes a very interesting direction,&quot; he wrote.</p><hr /><h2><b>A $65 million bet that open-source AI can compete with Big Tech</b></h2><p>Nous Research has carved out a distinctive position in the AI landscape: a company committed to <a href=\"https://nousresearch.com/\">open-source releases</a> that compete with — and sometimes exceed — proprietary alternatives.</p><p>The company raised<a href=\"https://fortune.com/crypto/2025/04/25/paradigm-nous-research-crypto-ai-venture-capital-deepseek-openai-blockchain/\"> $50 million in April 2025</a> in a round led by Paradigm, the cryptocurrency-focused venture firm founded by Coinbase co-founder Fred Ehrsam. Total funding reached $65 million, according to some reports. The investment reflected growing interest in decentralized approaches to AI training, an area where Nous Research has developed its <a href=\"https://psyche.network/\">Psyche platform</a>.</p><p>Previous releases include <a href=\"https://hermes4.nousresearch.com/\">Hermes 4</a>, a family of models that we reported &quot;<a href=\"https://venturebeat.com/ai/nous-research-drops-hermes-4-ai-models-that-outperform-chatgpt-without-content-restrictions\">outperform ChatGPT without content restrictions</a>,&quot; and DeepHermes-3, which the company described as the first &quot;<a href=\"https://venturebeat.com/ai/personalized-unrestricted-ai-lab-nous-research-launches-first-toggle-on-reasoning-model-deephermes-3\">toggle-on reasoning model</a>&quot; — allowing users to activate extended thinking capabilities on demand.</p><p>The company has cultivated a distinctive aesthetic and community, prompting some skepticism about whether style might overshadow substance. &quot;Ofc i&#x27;m gonna believe an anime pfp company. stop benchmarkmaxxing ffs,&quot; <a href=\"https://x.com/shydev69/status/2008654826356535510?s=20\">wrote one critic on X</a>, referring to Nous Research&#x27;s anime-style branding and the industry practice of optimizing for benchmark performance.</p><p>Others raised technical questions. &quot;<a href=\"https://x.com/yehor_smoliakov/status/2008659681489940757?s=20\">Based on the benchmark, Nemotron is better</a>,&quot; noted one commenter, referring to Nvidia&#x27;s family of language models. Another asked whether <a href=\"https://huggingface.co/NousResearch/NousCoder-14B\">NousCoder-14B</a> is &quot;agentic focused or just &#x27;one shot&#x27; coding&quot; — a distinction that matters for practical software development, where iterating on feedback typically produces better results than single attempts.</p><hr /><h2><b>What researchers say must happen next for AI coding tools to keep improving</b></h2><p>The release includes several directions for future work that hint at where AI coding research may be heading.</p><p>Multi-turn reinforcement learning tops the list. Currently, the model receives only a final binary reward — pass or fail — after generating a solution. But competitive programming problems typically include public test cases that provide intermediate feedback: compilation errors, incorrect outputs, time limit violations. Training models to incorporate this feedback across multiple attempts could significantly improve performance.</p><p>Controlling response length also remains a challenge. The researchers found that incorrect solutions tended to be longer than correct ones, and response lengths quickly saturated available context windows during training — a pattern that various algorithmic modifications failed to resolve.</p><p>Perhaps most ambitiously, Li proposed &quot;problem generation and self-play&quot; — training models to both solve and create programming problems. This would address the data scarcity problem directly by enabling models to generate their own training curricula.</p><p>&quot;Humans are great at generating interesting and useful problems for other competitive programmers, but it appears that there still exists a significant gap in LLM capabilities in creative problem generation,&quot; Li wrote.</p><p>The model is <a href=\"https://huggingface.co/NousResearch/NousCoder-14B\">available now on Hugging Face</a> under an Apache 2.0 license. For researchers and developers who want to build on the work, Nous Research has published the complete <a href=\"https://github.com/NousResearch/atropos/pull/296\">Atropos training stack</a> alongside it.</p><p>What took Li two years of adolescent dedication to achieve—climbing from a 1600-level novice to a 2100-rated competitor on Codeforces—an AI replicated in 96 hours. He needed 1,000 problems. The model needed 24,000. But soon enough, these systems may learn to write their own problems, teach themselves, and leave human benchmarks behind entirely.</p><p>The question is no longer whether machines can learn to code. It&#x27;s whether they&#x27;ll soon be better teachers than we ever were.</p><p>\n</p>",
        "source": "venturebeat.com",
        "published": "Wed, 07 Jan 2026 20:00:00 GMT",
        "fetched_at": "2026-02-28T23:17:50.451297Z",
        "tags": [
          {
            "name": "transformation",
            "score": 3
          },
          {
            "name": "boundary_crossing",
            "score": 9
          },
          {
            "name": "value_redefinition",
            "score": 8
          },
          {
            "name": "scale_shift",
            "score": 4
          },
          {
            "name": "ontology_shift",
            "score": 8
          }
        ],
        "structural_score": 32,
        "timeliness_score": 3,
        "final_score": 17.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://venturebeat.com/infrastructure/claude-code-costs-up-to-usd200-a-month-goose-does-the-same-thing-for-free",
        "title": "Claude Code costs up to $200 a month. Goose does the same thing for free.",
        "summary": "<p>The artificial intelligence coding revolution comes with a catch: it&#x27;s expensive.</p><p><a href=\"https://claude.com/product/claude-code\">Claude Code</a>, Anthropic&#x27;s terminal-based AI agent that can write, debug, and deploy code autonomously, has captured the imagination of software developers worldwide. But its <a href=\"https://claude.com/pricing\">pricing</a> — ranging from $20 to $200 per month depending on usage — has sparked a growing rebellion among the very programmers it aims to serve.</p><p>Now, a free alternative is gaining traction. <a href=\"https://block.github.io/goose/\">Goose</a>, an open-source AI agent developed by <a href=\"https://block.xyz/\">Block</a> (the financial technology company formerly known as Square), offers nearly identical functionality to <a href=\"https://claude.com/product/claude-code\">Claude Code</a> but runs entirely on a user&#x27;s local machine. No subscription fees. No cloud dependency. No rate limits that reset every five hours.</p><p>&quot;Your data stays with you, period,&quot; said Parth Sareen, a software engineer who demonstrated the tool during a <a href=\"https://www.youtube.com/watch?v=WG10r2N0IwM\">recent livestream</a>. The comment captures the core appeal: Goose gives developers complete control over their AI-powered workflow, including the ability to work offline — even on an airplane.</p><p>The project has exploded in popularity. Goose now boasts more than <a href=\"https://github.com/block/goose\">26,100 stars on GitHub</a>, the code-sharing platform, with 362 contributors and 102 releases since its launch. The latest version, <a href=\"https://block.github.io/goose/docs/getting-started/installation\">1.20.1</a>, shipped on January 19, 2026, reflecting a development pace that rivals commercial products.</p><p>For developers frustrated by Claude Code&#x27;s pricing structure and usage caps, Goose represents something increasingly rare in the AI industry: a genuinely free, no-strings-attached option for serious work.</p><div></div><h2><b>Anthropic&#x27;s new rate limits spark a developer revolt</b></h2><p>To understand why <a href=\"https://block.github.io/goose/\">Goose</a> matters, you need to understand the <a href=\"https://techcrunch.com/2025/07/17/anthropic-tightens-usage-limits-for-claude-code-without-telling-users/\">Claude Code pricing controversy</a>.</p><p>Anthropic, the San Francisco artificial intelligence company founded by former OpenAI executives, offers Claude Code as part of its subscription tiers. The free plan provides no access whatsoever. The <a href=\"https://www.anthropic.com/news/claude-pro\">Pro plan</a>, at $17 per month with annual billing (or $20 monthly), limits users to just 10 to 40 prompts every five hours — a constraint that serious developers exhaust within minutes of intensive work.</p><p>The <a href=\"https://support.claude.com/en/articles/11049741-what-is-the-max-plan\">Max plans</a>, at $100 and $200 per month, offer more headroom: 50 to 200 prompts and 200 to 800 prompts respectively, plus access to Anthropic&#x27;s most powerful model, <a href=\"https://www.anthropic.com/news/claude-opus-4-5\">Claude 4.5 Opus</a>. But even these premium tiers come with restrictions that have inflamed the developer community.</p><p>In late July, Anthropic announced new weekly rate limits. Under the system, Pro users receive 40 to 80 hours of Sonnet 4 usage per week. Max users at the $200 tier get 240 to 480 hours of Sonnet 4, plus 24 to 40 hours of Opus 4. Nearly five months later, the frustration has not subsided.</p><p>The problem? Those &quot;hours&quot; are not actual hours. They represent token-based limits that vary wildly depending on codebase size, conversation length, and the complexity of the code being processed. Independent analysis suggests the actual per-session limits translate to roughly 44,000 tokens for Pro users and 220,000 tokens for the $200 Max plan.</p><p>&quot;It&#x27;s confusing and vague,&quot; one developer wrote in a <a href=\"https://userjot.com/blog/claude-code-pricing-200-dollar-plan-worth-it\">widely shared analysis</a>. &quot;When they say &#x27;24-40 hours of Opus 4,&#x27; that doesn&#x27;t really tell you anything useful about what you&#x27;re actually getting.&quot;</p><p>The <a href=\"https://www.reddit.com/r/Anthropic/comments/1mbo4uw/claude_code_max_new_weekly_rate_limits/\">backlash on Reddit</a> and <a href=\"https://venturebeat.com/ai/anthropic-throttles-claude-rate-limits-devs-call-foul\">developer forums</a> has been fierce. Some users report hitting their daily limits within 30 minutes of intensive coding. Others have canceled their subscriptions entirely, calling the new restrictions &quot;a joke&quot; and &quot;unusable for real work.&quot;</p><p>Anthropic has defended the changes, stating that the limits affect fewer than five percent of users and target people running Claude Code &quot;<a href=\"https://techcrunch.com/2025/07/28/anthropic-unveils-new-rate-limits-to-curb-claude-code-power-users/\">continuously in the background, 24/7</a>.&quot; But the company has not clarified whether that figure refers to five percent of Max subscribers or five percent of all users — a distinction that matters enormously.</p><h2><b>How Block built a free AI coding agent that works offline</b></h2><p><a href=\"https://block.github.io/goose/\">Goose</a> takes a radically different approach to the same problem.</p><p>Built by <a href=\"https://block.xyz/\">Block</a>, the payments company led by Jack Dorsey, Goose is what engineers call an &quot;<a href=\"https://github.com/block/goose\">on-machine AI agent</a>.&quot; Unlike Claude Code, which sends your queries to Anthropic&#x27;s servers for processing, Goose can run entirely on your local computer using open-source language models that you download and control yourself.</p><p>The project&#x27;s documentation describes it as going &quot;<a href=\"https://github.com/block/goose\">beyond code suggestions</a>&quot; to &quot;install, execute, edit, and test with any LLM.&quot; That last phrase — &quot;any LLM&quot; — is the key differentiator. Goose is model-agnostic by design.</p><p>You can connect Goose to Anthropic&#x27;s <a href=\"https://platform.claude.com/docs/en/about-claude/models/overview\">Claude models</a> if you have <a href=\"https://claude.com/platform/api\">API access</a>. You can use OpenAI&#x27;s <a href=\"https://platform.openai.com/docs/models/gpt-5\">GPT-5</a> or Google&#x27;s <a href=\"https://ai.google.dev/gemini-api/docs\">Gemini</a>. You can route it through services like <a href=\"https://groq.com/\">Groq</a> or <a href=\"https://openrouter.ai/\">OpenRouter</a>. Or — and this is where things get interesting — you can run it entirely locally using tools like <a href=\"https://ollama.com/\">Ollama</a>, which let you download and execute open-source models on your own hardware.</p><p>The practical implications are significant. With a local setup, there are no subscription fees, no usage caps, no rate limits, and no concerns about your code being sent to external servers. Your conversations with the AI never leave your machine.</p><p>&quot;I use Ollama all the time on planes — it&#x27;s a lot of fun!&quot; <a href=\"https://www.youtube.com/watch?v=WG10r2N0IwM\">Sareen noted</a> during a demonstration, highlighting how local models free developers from the constraints of internet connectivity.</p><h2><b>What Goose can do that traditional code assistants can&#x27;t</b></h2><p><a href=\"https://block.github.io/goose/\">Goose</a> operates as a command-line tool or desktop application that can autonomously perform complex development tasks. It can build entire projects from scratch, write and execute code, debug failures, orchestrate workflows across multiple files, and interact with external APIs — all without constant human oversight.</p><p>The architecture relies on what the AI industry calls &quot;<a href=\"https://www.ibm.com/think/topics/tool-calling\">tool calling</a>&quot; or &quot;<a href=\"https://platform.openai.com/docs/guides/function-calling?api-mode=chat\">function calling</a>&quot; — the ability for a language model to request specific actions from external systems. When you ask <a href=\"https://block.github.io/goose/\">Goose</a> to create a new file, run a test suite, or check the status of a GitHub pull request, it doesn&#x27;t just generate text describing what should happen. It actually executes those operations.</p><p>This capability depends heavily on the underlying language model. <a href=\"https://platform.claude.com/docs/en/about-claude/models/overview\">Claude 4 models</a> from Anthropic currently perform best at tool calling, according to the <a href=\"https://gorilla.cs.berkeley.edu/leaderboard.html\">Berkeley Function-Calling Leaderboard</a>, which ranks models on their ability to translate natural language requests into executable code and system commands.</p><p>But newer open-source models are catching up quickly. Goose&#x27;s documentation highlights several options with strong tool-calling support: Meta&#x27;s <a href=\"https://www.llama.com/\">Llama series</a>, Alibaba&#x27;s <a href=\"https://qwen.ai/home\">Qwen models</a>, Google&#x27;s <a href=\"https://deepmind.google/models/gemma/\">Gemma variants</a>, and DeepSeek&#x27;s <a href=\"https://huggingface.co/deepseek-ai/DeepSeek-R1\">reasoning-focused architectures</a>.</p><p>The tool also integrates with the <a href=\"https://modelcontextprotocol.io/docs/getting-started/intro\">Model Context Protocol</a>, or MCP, an emerging standard for connecting AI agents to external services. Through MCP, Goose can access databases, search engines, file systems, and third-party APIs — extending its capabilities far beyond what the base language model provides.</p><h2><b>Setting Up Goose with a Local Model</b></h2><p>For developers interested in a completely free, privacy-preserving setup, the process involves three main components: <a href=\"https://block.github.io/goose/\">Goose</a> itself, <a href=\"https://ollama.com/\">Ollama</a> (a tool for running open-source models locally), and a compatible language model.</p><p><b>Step 1: Install Ollama</b></p><p><a href=\"https://ollama.com/\">Ollama</a> is an open-source project that dramatically simplifies the process of running large language models on personal hardware. It handles the complex work of downloading, optimizing, and serving models through a simple interface.</p><p>Download and install Ollama from <a href=\"http://ollama.com\">ollama.com</a>. Once installed, you can pull models with a single command. For coding tasks, <a href=\"https://qwen.ai/blog?id=qwen2.5-max\">Qwen 2.5</a> offers strong tool-calling support:</p><p>ollama run qwen2.5</p><p>The model downloads automatically and begins running on your machine.</p><p><b>Step 2: Install Goose</b></p><p><a href=\"https://block.github.io/goose/\">Goose</a> is available as both a desktop application and a command-line interface. The desktop version provides a more visual experience, while the CLI appeals to developers who prefer working entirely in the terminal.</p><p>Installation instructions vary by operating system but generally involve downloading from Goose&#x27;s <a href=\"https://github.com/block/goose\">GitHub releases page</a> or using a package manager. Block provides pre-built binaries for macOS (both Intel and Apple Silicon), Windows, and Linux.</p><p><b>Step 3: Configure the Connection</b></p><p>In Goose Desktop, navigate to Settings, then Configure Provider, and select Ollama. Confirm that the API Host is set to http://localhost:11434 (Ollama&#x27;s default port) and click Submit.</p><p>For the command-line version, run goose configure, select &quot;Configure Providers,&quot; choose Ollama, and enter the model name when prompted.</p><p>That&#x27;s it. Goose is now connected to a language model running entirely on your hardware, ready to execute complex coding tasks without any subscription fees or external dependencies.</p><h2><b>The RAM, processing power, and trade-offs you should know about</b></h2><p>The obvious question: what kind of computer do you need?</p><p>Running large language models locally requires substantially more computational resources than typical software. The key constraint is memory — specifically, RAM on most systems, or VRAM if using a dedicated graphics card for acceleration.</p><p>Block&#x27;s <a href=\"https://block.github.io/goose/docs/category/guides\">documentation</a> suggests that 32 gigabytes of RAM provides &quot;a solid baseline for larger models and outputs.&quot; For Mac users, this means the computer&#x27;s unified memory is the primary bottleneck. For Windows and Linux users with discrete NVIDIA graphics cards, GPU memory (VRAM) matters more for acceleration.</p><p>But you don&#x27;t necessarily need expensive hardware to get started. Smaller models with fewer parameters run on much more modest systems. <a href=\"https://qwen.ai/blog?id=qwen2.5-max\">Qwen 2.5</a>, for instance, comes in multiple sizes, and the smaller variants can operate effectively on machines with 16 gigabytes of RAM.</p><p>&quot;You don&#x27;t need to run the largest models to get excellent results,&quot; <a href=\"https://www.youtube.com/watch?v=WG10r2N0IwM\">Sareen emphasized</a>. The practical recommendation: start with a smaller model to test your workflow, then scale up as needed.</p><p>For context, Apple&#x27;s entry-level <a href=\"https://www.apple.com/macbook-air/\">MacBook Air</a> with 8 gigabytes of RAM would struggle with most capable coding models. But a <a href=\"https://www.apple.com/macbook-pro/\">MacBook Pro</a> with 32 gigabytes — increasingly common among professional developers — handles them comfortably.</p><h2><b>Why keeping your code off the cloud matters more than ever</b></h2><p><a href=\"https://block.github.io/goose/\">Goose</a> with a local LLM is not a perfect substitute for <a href=\"https://claude.com/product/claude-code\">Claude Code</a>. The comparison involves real trade-offs that developers should understand.</p><p><b>Model Quality</b>: <a href=\"https://www.anthropic.com/news/claude-opus-4-5\">Claude 4.5 Opus</a>, Anthropic&#x27;s flagship model, remains arguably the most capable AI for software engineering tasks. It excels at understanding complex codebases, following nuanced instructions, and producing high-quality code on the first attempt. Open-source models have improved dramatically, but a gap persists — particularly for the most challenging tasks.</p><p>One developer who switched to the $200 Claude Code plan <a href=\"https://userjot.com/blog/claude-code-pricing-200-dollar-plan-worth-it\">described the difference bluntly</a>: &quot;When I say &#x27;make this look modern,&#x27; Opus knows what I mean. Other models give me Bootstrap circa 2015.&quot;</p><p><b>Context Window</b>: <a href=\"https://www.anthropic.com/news/claude-sonnet-4-5\">Claude Sonnet 4.5</a>, accessible through the API, offers a massive one-million-token context window — enough to load entire large codebases without chunking or context management issues. Most local models are limited to 4,096 or 8,192 tokens by default, though many can be configured for longer contexts at the cost of increased memory usage and slower processing.</p><p><b>Speed</b>: Cloud-based services like <a href=\"https://claude.com/product/claude-code\">Claude Code</a> run on dedicated server hardware optimized for AI inference. Local models, running on consumer laptops, typically process requests more slowly. The difference matters for iterative workflows where you&#x27;re making rapid changes and waiting for AI feedback.</p><p><b>Tooling Maturity</b>: <a href=\"https://claude.com/product/claude-code\">Claude Code</a> benefits from Anthropic&#x27;s dedicated engineering resources. Features like prompt caching (which can reduce costs by up to 90 percent for repeated contexts) and structured outputs are polished and well-documented. <a href=\"https://block.github.io/goose/\">Goose</a>, while actively developed with 102 releases to date, relies on community contributions and may lack equivalent refinement in specific areas.</p><h2><b>How Goose stacks up against Cursor, GitHub Copilot, and the paid AI coding market</b></h2><p>Goose enters a crowded market of AI coding tools, but occupies a distinctive position.</p><p><a href=\"https://cursor.com/\">Cursor</a>, a popular AI-enhanced code editor, charges $20 per month for its <a href=\"https://cursor.com/pricing\">Pro tier</a> and $200 for <a href=\"https://cursor.com/pricing\">Ultra</a>—pricing that mirrors <a href=\"https://claude.com/pricing\">Claude Code&#x27;s Max plans</a>. Cursor provides approximately 4,500 Sonnet 4 requests per month at the Ultra level, a substantially different allocation model than Claude Code&#x27;s hourly resets.</p><p><a href=\"https://cline.bot/\">Cline</a>, <a href=\"https://roocode.com/\">Roo Code</a>, and similar open-source projects offer AI coding assistance but with varying levels of autonomy and tool integration. Many focus on code completion rather than the agentic task execution that defines Goose and Claude Code.</p><p>Amazon&#x27;s <a href=\"https://aws.amazon.com/blogs/aws/now-in-preview-amazon-codewhisperer-ml-powered-coding-companion/\">CodeWhisperer</a>, <a href=\"https://github.com/features/copilot\">GitHub Copilot</a>, and enterprise offerings from major cloud providers target large organizations with complex procurement processes and dedicated budgets. They are less relevant to individual developers and small teams seeking lightweight, flexible tools.</p><p>Goose&#x27;s combination of genuine autonomy, model agnosticism, local operation, and zero cost creates a unique value proposition. The tool is not trying to compete with commercial offerings on polish or model quality. It&#x27;s competing on freedom — both financial and architectural.</p><h2><b>The $200-a-month era for AI coding tools may be ending</b></h2><p>The AI coding tools market is evolving quickly. Open-source models are improving at a pace that continually narrows the gap with proprietary alternatives. Moonshot AI&#x27;s <a href=\"https://www.kimi.com/en\">Kimi K2</a> and z.ai&#x27;s <a href=\"https://z.ai/blog/glm-4.5\">GLM 4.5</a> now benchmark near <a href=\"https://www.anthropic.com/news/claude-4\">Claude Sonnet 4 levels</a> — and they&#x27;re freely available.</p><p>If this trajectory continues, the quality advantage that justifies Claude Code&#x27;s premium pricing may erode. Anthropic would then face pressure to compete on features, user experience, and integration rather than raw model capability.</p><p>For now, developers face a clear choice. Those who need the absolute best model quality, who can afford premium pricing, and who accept usage restrictions may prefer <a href=\"https://claude.com/product/claude-code\">Claude Code</a>. Those who prioritize cost, privacy, offline access, and flexibility have a genuine alternative in <a href=\"https://block.github.io/goose/\">Goose</a>.</p><p>The fact that a $200-per-month commercial product has a zero-dollar open-source competitor with comparable core functionality is itself remarkable. It reflects both the maturation of open-source AI infrastructure and the appetite among developers for tools that respect their autonomy.</p><p>Goose is not perfect. It requires more technical setup than commercial alternatives. It depends on hardware resources that not every developer possesses. Its model options, while improving rapidly, still trail the best proprietary offerings on complex tasks.</p><p>But for a growing community of developers, those limitations are acceptable trade-offs for something increasingly rare in the AI landscape: a tool that truly belongs to them.</p><hr /><p><i>Goose is available for download at </i><a href=\"http://github.com/block/goose\"><i>github.com/block/goose</i></a><i>. Ollama is available at </i><a href=\"http://ollama.com\"><i>ollama.com</i></a><i>. Both projects are free and open source.</i></p>",
        "source": "venturebeat.com",
        "published": "Mon, 19 Jan 2026 14:00:00 GMT",
        "fetched_at": "2026-02-28T23:17:50.451275Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 4
          },
          {
            "name": "value_redefinition",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 8
          },
          {
            "name": "ontology_shift",
            "score": 8
          }
        ],
        "structural_score": 24,
        "timeliness_score": 3,
        "final_score": 13.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://venturebeat.com/technology/the-creator-of-claude-code-just-revealed-his-workflow-and-developers-are",
        "title": "The creator of Claude Code just revealed his workflow, and developers are losing their minds",
        "summary": "<p>When the creator of the world&#x27;s most advanced coding agent speaks, Silicon Valley doesn&#x27;t just listen — it takes notes.</p><p>For the past week, the engineering community has been dissecting a <a href=\"https://x.com/bcherny/status/2007179832300581177\">thread on X</a> from <a href=\"https://x.com/bcherny\">Boris Cherny</a>, the creator and head of <a href=\"https://code.claude.com/docs/en/overview\">Claude Code</a> at <a href=\"https://www.anthropic.com/\">Anthropic</a>. What began as a casual sharing of his personal terminal setup has spiraled into a viral manifesto on the future of software development, with industry insiders calling it a watershed moment for the startup.</p><div></div><p>&quot;If you&#x27;re not reading the Claude Code best practices straight from its creator, you&#x27;re behind as a programmer,&quot; wrote <a href=\"https://x.com/jefftangx\">Jeff Tang</a>, a prominent voice in the developer community. <a href=\"https://x.com/KyleMcnease/status/2007555584724480338\">Kyle McNease</a>, another industry observer, went further, declaring that with Cherny&#x27;s &quot;game-changing updates,&quot; Anthropic is &quot;on fire,&quot; potentially facing &quot;their ChatGPT moment.&quot;</p><p>The excitement stems from a paradox: Cherny&#x27;s workflow is surprisingly simple, yet it allows a single human to operate with the output capacity of a small engineering department. As one user noted on X after implementing Cherny&#x27;s setup, the experience &quot;<a href=\"https://x.com/mtwichan\">feels more like Starcraft</a>&quot; than traditional coding — a shift from typing syntax to commanding autonomous units.</p><p>Here is an analysis of the workflow that is reshaping how software gets built, straight from the architect himself. </p><h2><b>How running five AI agents at once turns coding into a real-time strategy game</b></h2><p>The most striking revelation from Cherny&#x27;s disclosure is that he does not code in a linear fashion. In the traditional &quot;<a href=\"https://notes.paulswail.com/public/The+inner+and+outer+loops+of+software+development+workflow\">inner loop</a>&quot; of development, a programmer writes a function, tests it, and moves to the next. Cherny, however, acts as a fleet commander.</p><p>&quot;I run 5 Claudes in parallel in my terminal,&quot; Cherny wrote. &quot;I number my tabs 1-5, and use system notifications to know when a Claude needs input.&quot;</p><p>By utilizing iTerm2 system notifications, Cherny effectively manages five simultaneous work streams. While one agent runs a test suite, another refactors a legacy module, and a third drafts documentation. He also runs &quot;5-10 Claudes on <a href=\"https://claude.ai/\">claude.ai</a>&quot; in his browser, using a &quot;teleport&quot; command to hand off sessions between the web and his local machine.</p><p>This validates the &quot;<a href=\"https://www.cnbc.com/2026/01/03/anthropic-daniela-amodei-do-more-with-less-bet.html\">do more with less</a>&quot; strategy articulated by Anthropic President Daniela Amodei earlier this week. While competitors like OpenAI pursue trillion-dollar infrastructure build-outs, Anthropic is proving that superior orchestration of existing models can yield exponential productivity gains.</p><h2><b>The counterintuitive case for choosing the slowest, smartest model</b></h2><p>In a surprising move for an industry obsessed with latency, Cherny revealed that he exclusively uses Anthropic&#x27;s heaviest, slowest model: <a href=\"https://www.anthropic.com/news/claude-opus-4-5\">Opus 4.5</a>.</p><p>&quot;I use Opus 4.5 with thinking for everything,&quot; Cherny <a href=\"https://x.com/bcherny/status/2007179838864666847\">explained</a>. &quot;It&#x27;s the best coding model I&#x27;ve ever used, and even though it&#x27;s bigger &amp; slower than Sonnet, since you have to steer it less and it&#x27;s better at tool use, it is almost always faster than using a smaller model in the end.&quot;</p><p>For enterprise technology leaders, this is a critical insight. The bottleneck in modern AI development isn&#x27;t the generation speed of the token; it is the human time spent correcting the AI&#x27;s mistakes. Cherny&#x27;s workflow suggests that paying the &quot;compute tax&quot; for a smarter model upfront eliminates the &quot;correction tax&quot; later.</p><h2><b>One shared file turns every AI mistake into a permanent lesson</b></h2><p>Cherny also detailed how his team solves the problem of AI amnesia. Standard large language models do not &quot;remember&quot; a company&#x27;s specific coding style or architectural decisions from one session to the next.</p><p>To address this, Cherny&#x27;s team maintains a single file named <a href=\"https://x.com/bcherny/status/2007179842928947333\">CLAUDE.md</a> in their git repository. &quot;Anytime we see Claude do something incorrectly we add it to the CLAUDE.md, so Claude knows not to do it next time,&quot; he wrote.</p><p>This practice transforms the codebase into a self-correcting organism. When a human developer reviews a pull request and spots an error, they don&#x27;t just fix the code; they tag the AI to update its own instructions. &quot;<a href=\"https://x.com/aakashgupta/status/2007347705945944153\">Every mistake becomes a rule</a>,&quot; noted <a href=\"https://x.com/aakashgupta\">Aakash Gupta</a>, a product leader analyzing the thread. The longer the team works together, the smarter the agent becomes.</p><h2><b>Slash commands and subagents automate the most tedious parts of development</b></h2><p>The &quot;vanilla&quot; workflow one observer praised is powered by rigorous automation of repetitive tasks. Cherny uses slash commands — custom shortcuts checked into the project&#x27;s repository — to handle complex operations with a single keystroke.</p><p>He highlighted a command called <i><b>/commit-push-pr</b></i>, which he invokes dozens of times daily. Instead of manually typing git commands, writing a commit message, and opening a pull request, the agent handles the bureaucracy of version control autonomously.</p><p>Cherny also deploys subagents — specialized AI personas — to handle specific phases of the development lifecycle. He uses a code-simplifier to clean up architecture after the main work is done and a verify-app agent to run end-to-end tests before anything ships.</p><h2><b>Why verification loops are the real unlock for AI-generated code</b></h2><p>If there is a single reason Claude Code has reportedly hit <a href=\"https://www.anthropic.com/news/anthropic-acquires-bun-as-claude-code-reaches-usd1b-milestone\">$1 billion in annual recurring revenue</a> so quickly, it is likely the verification loop. The AI is not just a text generator; it is a tester.</p><p>&quot;Claude tests every single change I land to claude.ai/code using the Claude Chrome extension,&quot; Cherny wrote. &quot;It opens a browser, tests the UI, and iterates until the code works and the UX feels good.&quot;</p><p>He argues that giving the AI a way to verify its own work — whether through browser automation, running bash commands, or executing test suites — improves the quality of the final result by &quot;2-3x.&quot; The agent doesn&#x27;t just write code; it proves the code works.</p><h2><b>What Cherny&#x27;s workflow signals about the future of software engineering</b></h2><p>The reaction to Cherny&#x27;s thread suggests a pivotal shift in how developers think about their craft. For years, &quot;AI coding&quot; meant an autocomplete function in a text editor — a faster way to type. Cherny has demonstrated that it can now function as an operating system for labor itself.</p><p>&quot;Read this if you&#x27;re already an engineer... and want more power,&quot; <a href=\"https://x.com/jefftangx/status/2008246873275215890\">Jeff Tang</a> summarized on X.</p><p>The tools to multiply human output by a factor of five are already here. They require only a willingness to stop thinking of AI as an assistant and start treating it as a workforce. The programmers who make that mental leap first won&#x27;t just be more productive. They&#x27;ll be playing an entirely different game — and everyone else will still be typing.</p>",
        "source": "venturebeat.com",
        "published": "Mon, 05 Jan 2026 07:45:00 GMT",
        "fetched_at": "2026-02-28T23:17:50.451302Z",
        "tags": [
          {
            "name": "transformation",
            "score": 6
          },
          {
            "name": "boundary_crossing",
            "score": 9
          },
          {
            "name": "visibility_gain",
            "score": 4
          },
          {
            "name": "value_redefinition",
            "score": 4
          }
        ],
        "structural_score": 23,
        "timeliness_score": 3,
        "final_score": 13.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.anthropocenemagazine.org/2026/02/breadcrumbs-lay-path-away-from-fossil-fuels/?utm_source=rss&utm_medium=rss&utm_campaign=breadcrumbs-lay-path-away-from-fossil-fuels",
        "title": "Breadcrumbs (literally) lay path away from fossil fuels",
        "summary": "Bacteria munching on waste bread release hydrogen that could run chemical reactions, providing a carbon-negative way to produce drugs and food products.",
        "source": "www.anthropocenemagazine.org",
        "published": "Thu, 26 Feb 2026 13:00:12 +0000",
        "fetched_at": "2026-02-28T23:17:54.512025Z",
        "tags": [
          {
            "name": "transformation",
            "score": 3
          },
          {
            "name": "boundary_crossing",
            "score": 4
          }
        ],
        "structural_score": 7,
        "timeliness_score": 4,
        "final_score": 5.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.technologyreview.com/2026/02/27/1133624/ai-is-rewiring-how-the-worlds-best-go-players-think/",
        "title": "AI is rewiring how the world’s best Go players think",
        "summary": "Burrowed in the alleys of Hongik-dong, a hushed residential neighborhood in eastern Seoul, is a faded stone-tiled building stamped “Korea Baduk Association,” the governing body for professional Go. The game is an ancient one, with sacred stature in South Korea.&#160; But inside the building, rooms once filled with the soft clatter of hands dipping into&#8230;",
        "source": "www.technologyreview.com",
        "published": "Fri, 27 Feb 2026 10:00:00 +0000",
        "fetched_at": "2026-02-28T23:17:49.259087Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 4
          }
        ],
        "structural_score": 4,
        "timeliness_score": 5,
        "final_score": 4.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.technologyreview.com/2026/02/26/1133707/finding-value-with-ai-and-industry-5-0-transformation/",
        "title": "Finding value with AI and Industry 5.0 transformation",
        "summary": "For years, Industry 4.0 transformation has centered on the convergence of intelligent technologies like AI, cloud, the internet of things, robotics, and digital twins. Industry 5.0 marks a pivotal shift from integrating emerging technologies to orchestrating them at scale. With Industry 5.0, the purpose of this interconnected web of technologies is more nuanced: to augment&#8230;",
        "source": "www.technologyreview.com",
        "published": "Thu, 26 Feb 2026 15:00:59 +0000",
        "fetched_at": "2026-02-28T23:17:49.259099Z",
        "tags": [
          {
            "name": "value_redefinition",
            "score": 4
          }
        ],
        "structural_score": 4,
        "timeliness_score": 5,
        "final_score": 4.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://techcrunch.com/2026/02/26/mistral-ai-inks-a-deal-with-global-consulting-giant-accenture/",
        "title": "Mistral AI inks a deal with global consulting giant Accenture",
        "summary": "Mistral AI lands a partnership with Accenture, the consultant that has also recently announced partnerships with rivals OpenAI and Anthropic.",
        "source": "techcrunch.com",
        "published": "Thu, 26 Feb 2026 19:17:27 +0000",
        "fetched_at": "2026-02-28T23:17:51.515204Z",
        "tags": [
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 5,
        "timeliness_score": 4,
        "final_score": 4.5,
        "reddit_score": null,
        "reddit_comments": null
      }
    ],
    "education": [
      {
        "url": "https://edsource.org/2025/how-one-california-school-came-together-to-pack-20000-meals-for-the-holidays/746481",
        "title": "How one California school came together to pack 20,000 meals for the holidays",
        "summary": "At an Elk Grove high school in Sacramento County, students worked a night in the cafeteria to combat global food insecurity.",
        "source": "edsource.org",
        "published": "Mon, 08 Dec 2025 08:03:00 +0000",
        "fetched_at": "2026-02-28T23:18:20.828571Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 9,
        "timeliness_score": 3,
        "final_score": 6.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://edsource.org/2026/technology-education-student-wellbeing/749262",
        "title": "Rethinking screen time in California classrooms",
        "summary": "Effective instruction requires a balance between traditional methods and digital engagement. Here's what school districts, families and the state must do.",
        "source": "edsource.org",
        "published": "Tue, 20 Jan 2026 02:58:44 +0000",
        "fetched_at": "2026-02-28T23:18:20.827914Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 2
          },
          {
            "name": "value_redefinition",
            "score": 4
          }
        ],
        "structural_score": 6,
        "timeliness_score": 3,
        "final_score": 4.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://edsource.org/2026/california-universal-prekindergarten-implementation/748208",
        "title": "Universal prekindergarten has arrived; now we must sustain it",
        "summary": "County offices of education across the state are calling on the governor and the Legislature to support universal prekindergarten with sustained funding.",
        "source": "edsource.org",
        "published": "Tue, 06 Jan 2026 03:38:57 +0000",
        "fetched_at": "2026-02-28T23:18:20.828049Z",
        "tags": [
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 5,
        "timeliness_score": 3,
        "final_score": 4.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://edsource.org/2025/nixon-veto-childcare-lessons/747568",
        "title": "The path to universal preschool in California: Avoiding past mistakes",
        "summary": "California is expanding its transitional kindergarten (TK) to a universal prekindergarten (UPK) system, and must learn from the mistakes of the 1971 federal effort to create a universal early care and education system, which was vetoed by President Nixon.",
        "source": "edsource.org",
        "published": "Tue, 23 Dec 2025 07:03:30 +0000",
        "fetched_at": "2026-02-28T23:18:20.828435Z",
        "tags": [
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 5,
        "timeliness_score": 3,
        "final_score": 4.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://edsource.org/2025/california-schools-to-use-reading-screening-test/733022",
        "title": "California schools prepare to introduce universal reading screening",
        "summary": "A quick screening test will be administered to all students in kindergarten through second grade to detect possible reading difficulties, but it is not intended to be a final diagnosis.",
        "source": "edsource.org",
        "published": "Tue, 20 May 2025 07:05:00 +0000",
        "fetched_at": "2026-02-28T23:18:20.830292Z",
        "tags": [
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 5,
        "timeliness_score": 3,
        "final_score": 4.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://edsource.org/2024/as-we-expand-universal-preschool-access-lets-ensure-teachers-mirror-their-students-ethnicity/715393",
        "title": "As we expand universal preschool access, let’s ensure teachers mirror their students’ ethnicity",
        "summary": "Author&#8217;s original hed: As Universal Preschool Access Expands to Reach More Families of Color, So Do Inequitable Practices Such as Racial Bias, Exclusionary Discipline and Lack of Cultural Representation, Leading to a Crisis for Black Boys As California progresses toward universal preschool access, the need increases for training, hiring and retaining early childhood male educators who are racially and ethnically representative of the children... <span class=\"read-more\"><a href=\"https://edsource.org/2024/as-we-expand-universal-preschool-access-lets-ensure-teachers-mirror-their-students-ethnicity/715393\">read more</a></span>",
        "source": "edsource.org",
        "published": "Tue, 09 Jul 2024 15:53:36 +0000",
        "fetched_at": "2026-02-28T23:18:20.832793Z",
        "tags": [
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 5,
        "timeliness_score": 3,
        "final_score": 4.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://edsource.org/2024/survey-californians-are-worried-about-student-health-lukewarm-toward-a-state-school-bond/709604",
        "title": "Survey: Californians are worried about student health, lukewarm toward a state school bond",
        "summary": "The annual Public Policy Institute of California survey on education issues found wide support for universal TK and teaching about slavery but divisions on transgender issues.",
        "source": "edsource.org",
        "published": "Thu, 11 Apr 2024 05:11:37 +0000",
        "fetched_at": "2026-02-28T23:18:20.833454Z",
        "tags": [
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 5,
        "timeliness_score": 3,
        "final_score": 4.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://theconversation.com/tiny-recording-backpacks-reveal-bats-surprising-hunting-strategy-271996",
        "title": "Tiny recording backpacks reveal bats’ surprising hunting strategy",
        "summary": "By listening in on their nightly hunts, scientists discovered that small, fringe-lipped bats are unexpectedly able to efficiently take down prey nearly their own size.",
        "source": "theconversation.com",
        "published": "2026-02-27T13:22:11Z",
        "fetched_at": "2026-02-28T23:18:28.334213Z",
        "tags": [
          {
            "name": "transformation",
            "score": 3
          },
          {
            "name": "visibility_gain",
            "score": 4
          }
        ],
        "structural_score": 7,
        "timeliness_score": 1,
        "final_score": 4.0,
        "reddit_score": null,
        "reddit_comments": null,
        "temp_score_struct": 5.199999999999999,
        "temp_score_trend": 2.8
      },
      {
        "url": "https://edsource.org/2026/supporting-new-teachers-retention/750763",
        "title": "How districts can fix the teacher ‘support shortage’",
        "summary": "California's teacher workforce is recovering, but retention is still a challenge, and districts need to invest in comprehensive support systems to ensure teachers stay in the profession and thrive.",
        "source": "edsource.org",
        "published": "Mon, 09 Feb 2026 23:38:55 +0000",
        "fetched_at": "2026-02-28T23:18:20.827690Z",
        "tags": [
          {
            "name": "value_redefinition",
            "score": 4
          }
        ],
        "structural_score": 4,
        "timeliness_score": 3,
        "final_score": 3.5,
        "reddit_score": null,
        "reddit_comments": null,
        "temp_score_struct": 3.6999999999999997,
        "temp_score_trend": 3.3
      },
      {
        "url": "https://edsource.org/2026/appeals-court-pauses-california-gender-law/748472",
        "title": "Federal appeals court pauses ruling on student gender identity disclosure in California",
        "summary": "An appeals court panel wrote that it is “skeptical” of the lower court’s decision, which would challenge policies adopted by 598 of the state’s nearly 1,000 local school districts.",
        "source": "edsource.org",
        "published": "Thu, 08 Jan 2026 00:04:46 +0000",
        "fetched_at": "2026-02-28T23:18:20.828023Z",
        "tags": [
          {
            "name": "value_redefinition",
            "score": 4
          }
        ],
        "structural_score": 4,
        "timeliness_score": 3,
        "final_score": 3.5,
        "reddit_score": null,
        "reddit_comments": null,
        "temp_score_struct": 3.6999999999999997,
        "temp_score_trend": 3.3
      }
    ],
    "mycotech": [
      {
        "url": "https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3003638",
        "title": "Metabolic modeling reveals determinants of prebiotic and probiotic treatment efficacy across multiple human intervention trials",
        "summary": "<p>by Nick Quinn-Bohmann, Alex V. Carr, Sean M. Gibbons</p>\n\nPrebiotic, probiotic, and combined (synbiotic) interventions often show variable outcomes across individuals, driven by complex interactions between introduced biotics, the endogenous microbiota, and the host diet. Predicting individual-specific success or failure of probiotic and prebiotic therapies remains a major challenge. Here, we leverage microbial community-scale metabolic models (MCMMs) to predict probiotic engraftment and microbiota-mediated short-chain fatty acid (SCFA) production in response to probiotic and prebiotic interventions. Using data from two human clinical trial cohorts, testing a five-strain probiotic combined with the prebiotic inulin designed to improve metabolic health and an eight-strain probiotic designed to treat recurrent <i>Clostridioides difficile</i> infections, respectively, we show that MCMM-predicted engraftment largely agrees with measurements, achieving 75%–80% accuracy. Engraftment probabilities varied across taxa. MCMMs captured treatment-driven shifts in predicted SCFA production, and higher model-predicted growth rates of <i>Akkermansia muciniphila</i> were negatively associated with glucose area under the curve (AUC) in the first trial, providing clues about the mechanisms underlying treatment efficacy. Extending these models to a third human cohort undergoing a healthy diet and lifestyle intervention revealed substantial inter-individual variability in predicted responses to increasing dietary fiber, which were significantly associated with baseline-to-follow-up changes in cardiometabolic health markers. Finally, our simulation results suggested that personalized prebiotic selection may further enhance probiotic efficacy. Together, these findings demonstrate the potential of metabolic modeling to guide personalized microbiome-mediated interventions.",
        "source": "journals.plos.org",
        "published": "2026-02-19T14:00:00Z",
        "fetched_at": "2026-02-28T23:18:35.788834Z",
        "tags": [
          {
            "name": "transformation",
            "score": 8
          },
          {
            "name": "boundary_crossing",
            "score": 6
          },
          {
            "name": "visibility_gain",
            "score": 4
          },
          {
            "name": "value_redefinition",
            "score": 4
          }
        ],
        "structural_score": 22,
        "timeliness_score": 1,
        "final_score": 11.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260219040749.htm",
        "title": "Scientists discover gene that could save bananas from deadly Panama disease",
        "summary": "A major breakthrough could help save the world’s bananas from a devastating disease. Scientists have discovered the exact genetic region in a wild banana that provides resistance to Fusarium wilt Subtropical Race 4 — a destructive strain that threatens Cavendish bananas worldwide. While this wild banana isn’t edible, the discovery gives breeders a powerful genetic roadmap to develop future bananas that are both delicious and naturally protected from this deadly pathogen.",
        "source": "www.sciencedaily.com",
        "published": "Thu, 19 Feb 2026 09:43:15 EST",
        "fetched_at": "2026-02-28T23:18:34.601492Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 7
          },
          {
            "name": "visibility_gain",
            "score": 5
          }
        ],
        "structural_score": 12,
        "timeliness_score": 4,
        "final_score": 8.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://phys.org/news/2026-02-tool-gene-dna-sequences-jobs.html",
        "title": "Promoters and enhancers: Tool catches gene-controlling DNA sequences doing each other's jobs",
        "summary": "Researchers at the Weill Institute for Cell and Molecular Biology have uncovered new evidence that two major types of gene-controlling DNA sequences, promoters and enhancers, operate with a shared logic and often perform the same jobs. The finding, made possible through a high-throughput assay they developed called QUASARR-seq, could reshape how scientists design gene therapies, interpret disease-related mutations, and understand cancer genetics.",
        "source": "phys.org",
        "published": "Fri, 27 Feb 2026 16:40:01 EST",
        "fetched_at": "2026-02-28T23:18:33.370936Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 7
          },
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 12,
        "timeliness_score": 3,
        "final_score": 7.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260206012213.htm",
        "title": "A hidden Aloe vera compound takes aim at Alzheimer’s",
        "summary": "Scientists have uncovered promising clues that compounds found in Aloe vera could play a role in fighting Alzheimer’s disease. Using advanced computer modeling, researchers discovered that beta-sitosterol—a natural plant compound—strongly interacts with two key enzymes involved in memory loss and cognitive decline. The compound showed stability, strong binding, and favorable safety indicators, making it a standout candidate for future drug development.",
        "source": "www.sciencedaily.com",
        "published": "Sun, 08 Feb 2026 07:57:41 EST",
        "fetched_at": "2026-02-28T23:18:34.601647Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 7
          },
          {
            "name": "visibility_gain",
            "score": 4
          }
        ],
        "structural_score": 11,
        "timeliness_score": 4,
        "final_score": 7.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260207232242.htm",
        "title": "This weird deep-sea creature was named by thousands of people online",
        "summary": "A newly discovered deep-sea creature has become an unlikely Internet star. After appearing in a popular YouTube video, a rare chiton found nearly three miles beneath the ocean surface sparked a global naming effort, drawing more than 8,000 suggestions from people around the world. Scientists ultimately chose the name Ferreiraella populi, meaning “of the people,” honoring the public that helped bring it into the scientific record.",
        "source": "www.sciencedaily.com",
        "published": "Sat, 07 Feb 2026 23:32:36 EST",
        "fetched_at": "2026-02-28T23:18:34.601637Z",
        "tags": [
          {
            "name": "visibility_gain",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 9,
        "timeliness_score": 4,
        "final_score": 6.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260206012210.htm",
        "title": "This tiny molecular trick makes spider silk almost unbreakable",
        "summary": "Scientists have cracked a key mystery behind spider silk’s legendary strength and flexibility. They discovered that tiny molecular interactions act like natural glue, holding silk proteins together as they transform from liquid into incredibly tough fibers. This same process helps create silk that’s stronger than steel by weight and tougher than Kevlar.",
        "source": "www.sciencedaily.com",
        "published": "Fri, 06 Feb 2026 01:22:10 EST",
        "fetched_at": "2026-02-28T23:18:34.601652Z",
        "tags": [
          {
            "name": "visibility_gain",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 9,
        "timeliness_score": 4,
        "final_score": 6.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3003668",
        "title": "Vasopressin and angiotensin II pathways differentially modulate human fear response dynamics to looming threats",
        "summary": "<p>by Mengfan Han, Wenyi Dong, Kun Fu, Junjie Wang, Yuanhang Xu, Yueyuan Zheng, Keith Kendrick, Ferraro Stefania, Ting Xu, Dezhong Yao, Benjamin Becker</p>\n\nWhile basal threat processing dynamics (e.g., visual looming) are well characterized in animals, the underlying mechanisms and their modulation by neuropeptide systems with different modulatory roles in threat processing (vasopressin, angiotensin II) remain poorly understood in humans. In a randomized, placebo-controlled eye-tracking study (<i>N</i> = 111), we administered vasopressin (AVP) or an angiotensin II receptor blocker (via Losartan, LT) during a time-to-collision threat paradigm. This study was prospectively registered at ClinicalTrials.gov (NCT06329076, NCT06329063) on April 11, 2024, prior to participant enrollment. Behaviorally, AVP induced a systematic time overestimation while LT induced temporal compression and reduced state anxiety. Pupillometry revealed distinguishable profiles: AVP induced sustained constriction during stimulus approach followed by post-stimulus threat-specific dilation, LT maintained sustained pupillary constriction throughout both approach and occlusion phases yet preserving threat-specificity, while placebo (PLC) showed no threat-specific modulation. A computational framework (combining Functional Principal Component Analysis, clustering, and Markov chain analysis) underscored the distinct modulations: AVP stabilized a high-arousal state characterized by the co-activation of vigilance, threat-proactive preparation and a shift from perception to internal simulation. LT suppressed transitions to high-arousal states and exhibited maximal sequence entropy, reflecting flexible response patterns—contrasting with placebo’s lowest entropy dynamics. These results demonstrate that AVP and LT differentially regulate basal threat processing via separable neuropeptide pathways: AVP sustains hypervigilance while LT promotes anxiolysis and adaptive flexibility. Our findings suggest neuropeptide pathway-specific targets maladaptive threat processing in trauma- or anxiety-related disorders.",
        "source": "journals.plos.org",
        "published": "2026-02-24T14:00:00Z",
        "fetched_at": "2026-02-28T23:18:35.788796Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 4
          },
          {
            "name": "visibility_gain",
            "score": 4
          },
          {
            "name": "value_redefinition",
            "score": 4
          }
        ],
        "structural_score": 12,
        "timeliness_score": 1,
        "final_score": 6.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260218044628.htm",
        "title": "New map reveals where lethal scorpions are most likely to strike",
        "summary": "Scientists have developed a powerful new way to forecast where some of the world’s most dangerous scorpions are likely to be found. By combining fieldwork in Africa with advanced computer modeling, the team discovered that soil type is the strongest factor shaping where many lethal species live, while temperature patterns also play a key role.",
        "source": "www.sciencedaily.com",
        "published": "Wed, 18 Feb 2026 23:36:03 EST",
        "fetched_at": "2026-02-28T23:18:34.601497Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 4
          },
          {
            "name": "visibility_gain",
            "score": 4
          }
        ],
        "structural_score": 8,
        "timeliness_score": 4,
        "final_score": 6.0,
        "reddit_score": null,
        "reddit_comments": null,
        "temp_score_struct": 6.8,
        "temp_score_trend": 5.199999999999999
      },
      {
        "url": "https://www.anthropocenemagazine.org/2026/02/researchers-have-figured-out-how-to-make-airplanes-fly-on-landfill-gas/?utm_source=rss&utm_medium=rss&utm_campaign=researchers-have-figured-out-how-to-make-airplanes-fly-on-landfill-gas",
        "title": "Researchers have figured out how to make airplanes fly on landfill gas",
        "summary": "Specially designed efficient catalysts are at the heart of a reactor that makes sustainable aviation fuels from methane-rich gases created when waste decomposes",
        "source": "www.anthropocenemagazine.org",
        "published": "Thu, 12 Feb 2026 13:00:16 +0000",
        "fetched_at": "2026-02-28T23:18:37.264919Z",
        "tags": [
          {
            "name": "transformation",
            "score": 3
          },
          {
            "name": "value_redefinition",
            "score": 5
          }
        ],
        "structural_score": 8,
        "timeliness_score": 4,
        "final_score": 6.0,
        "reddit_score": null,
        "reddit_comments": null,
        "temp_score_struct": 6.8,
        "temp_score_trend": 5.199999999999999
      },
      {
        "url": "https://www.sciencedaily.com/releases/2026/02/260226042447.htm",
        "title": "Hidden architecture inside cellular droplets opens new targets for cancer and ALS",
        "summary": "Biomolecular condensates were long believed to be simple liquid blobs inside cells. Researchers have now uncovered that some are actually supported by fine protein filaments forming an internal scaffold. When this structure is disrupted, cells fail to grow and divide properly. The discovery suggests scientists may one day design drugs that target condensate architecture to fight cancer and neurodegenerative disease.",
        "source": "www.sciencedaily.com",
        "published": "Thu, 26 Feb 2026 09:36:27 EST",
        "fetched_at": "2026-02-28T23:18:34.601425Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 7
          }
        ],
        "structural_score": 7,
        "timeliness_score": 4,
        "final_score": 5.5,
        "reddit_score": null,
        "reddit_comments": null
      }
    ],
    "curiosity": [
      {
        "url": "https://www.atlasobscura.com/articles/centralia-pennsylvania-rebirth",
        "title": "The Rebirth of Pennsylvania’s Infamous Burning Town",
        "summary": "<p>“There’s not much there anymore, it’s pretty much just a crossroads.”</p>\n<p>I read the posts online telling me not to bother, but I wanted to go anyway. Certainly I could feel something as we got close: the sense of desperation, of ruin and abandon. So I drove with a small group of friends deep into eastern Pennsylvania—coal country—through towns with names like Frackville, Pottsville, Ashland. Many downtowns had at least one house that had burned to ruin and been left abandoned. It was early June, but clouds covered the sky and we drove through a slight but persistent rain.</p>\n<p>We were on our way to Centralia, Pennsylvania. The Burning Town.</p>\n<p>The coal that made this valley famous accreted in layers over tens of thousands of years, organic swamp matter turning first to peat, and then compressed over millennia into billions of tons of anthracite—the densest and most pure form of coal—the stuff that made this region of Pennsylvania famous. Mines first opened here in 1856 and Centralia was incorporated as a town a decade later. Through the years bitter labor disputes broke out over exploitative treatment of the (largely Irish immigrant) miners, leading to regular outbreaks of violence. Add to that the boom and bust cycle of the coal industry—and the environmental desolation and impoverishment of the region—and you end up with a town that is deeply scarred, both literally and metaphorically.</p>\n<p>But the story that made Centralia famous began in May 1962, when officials set fire to the trash in a local landfill in an open strip-mine pit. This wasn’t the first year they’d done this, and there were firefighters stationed to ensure the blaze didn’t get out of control. After two days, the trash fire seemed to have burned itself out. But this time, for whatever reason (the actual cause was never fully determined), something went wrong. The landfill burn had lit the coal mines beneath the town.</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106279/image.jpg\" width=\"auto\" /></figure>\n<p>Over the years, numerous attempts were made to put out the fire. Nothing worked. In all, federal, state, and local governments spent over $3.3 million on the blaze, which raged on, uncontrollably. Over time, residents reported that their basements were strangely hot, and in 1979, the mayor John Coddington lowered a thermometer into an underground fuel tank at the gas station he owned, only to discover that the gasoline was 172 degrees Fahrenheit. And then on Valentine’s Day, 1981, a twelve-year old boy fell into a four-foot-wide sinkhole that opened up in his grandmother’s backyard, barely rescued by his fourteen year-old cousin. A plume of lethal carbon monoxide bellowed out from the hole.</p>\n<p>Realizing that topsoil was the only thing separating the town from a massive, raging inferno, the federal government finally decided to clear the town. The United States Congress allocated money for a buyout, which nearly all of the town’s 1,000 or so residents took. By 1990, 63 people remained in the town. Two years later, governor Bob Casey invoked eminent domain and condemned all the remaining buildings. By 2021, only five homes were still left standing.</p>\n<p>I had come here expecting that we would find ruin and neglect, toxicity and destitution. I expected Centralia to be an exemplar of the <em>eerie: </em>A place where once there had been a town, place of thriving life, and instead now was only absence, an emptiness, a void.</p>\n<p>What we found instead, strangely, was beauty. Centralia, despite everything I’d been led to expect, was thriving.</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106274/image.jpg\" width=\"auto\" /></figure>\n<hr class=\"baseline-grid-hr\" />\n<p>The Burning Town has come to stand in as a kind of exemplar of a post-industrial wasteland, a place where human folly reached its apex, scorching the land. All but abandoned, it became known primarily for the vents that poured smoke from the fire below, and for Graffiti Highway—a closed stretch of Route 61 covered in tags, doodles of genitalia, and declarations of love.</p>\n<p>When adapting the video game franchise <em>Silent Hill </em>for film, screenwriter Roger Avary used Centralia as a model for both the town’s backstory and its look. For years it drew curious onlookers and legend trippers, while the name “Centralia” itself became an almost byword for late capitalism: a term for that mixture of rapacious profit-seeking and thoughtless stewardship that created America’s own Chernobyl.</p>\n<p>Locals see the story a little differently, though their version borrows from similar themes. Phil, a tour guide at Pioneer Tunnel in neighboring Ashland, pointed out that while the grim toil of the mines claimed many human lives, their closure left the valley with little else to offer. He explained how the families that didn’t leave Centralia were harassed, as government forces tried to drive them off their land. Those that stayed had to go to court to defend their right to live on this abandoned land, all because they wanted to keep the mineral rights to their property. So now, people like Phil assume that the government is just waiting them out. Once they’re gone, putting out the fire will be easy enough. “They’ll take all that red hot coals, but also they’re going to get that rich anthracite coal,” he told us. “And I’m sure they’ll sell that. But are the people or the relatives going to get anything? It’s very doubtful. It’ll probably go to the federal government. Or the coal baron, maybe?”</p>\n<p>His voice, I noticed after a while, has a peculiar kind of nostalgia for the worst times in the world. Like so many others in these towns, he seems to long for a return, another chance for Pennsylvanians to throw their children back into the maw of the mine. Anything for a chance to get the coal jobs will come back. Anything in service of waking the Mountain once more.</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106256/image.jpg\" width=\"auto\" /></figure>\n<p>When we finally got to Centralia, we were met not with destruction or despair, but with what seemed at first simply like nothing. The streets are still laid out, and there are still a handful of houses left, but the graffiti highway has been covered over. Any abandoned buildings have long been torn down.</p>\n<p>It’s why, if you ask around these days, folks will tell you there’s nothing to see in Centralia. “I drove through Centralia 2 weeks ago,” one local commented on a <a href=\"https://www.reddit.com/r/Pennsylvania/comments/1cw0xqc/looking_to_visit_centralia_is_it_still_legal_to_go/\">Reddit thread</a>. “I didn’t realize till after I had already passed it. That should tell you everything you need to know.” In another thread a different local <a href=\"https://www.reddit.com/r/Pennsylvania/comments/1ikd2rs/i_have_some_questions_regarding_traveling_through/\">commented</a>, “What is the draw? It’s just empty ground now.”</p>\n<p>But emptiness can tell its own story. Standing on the empty streets of Centralia, I thought mainly of Cal Flynn’s <em>Islands of Abandonment: Nature Rebounding in the Post-Human Landscape. </em>Flynn travels the world to places that have been forsworn by humanity: not the pristine, untouched wilderness, but places abandoned, like Chernobyl and the exclusion zone that divides the island of Cyprus between its Greek and Turkish halves. Places where, Flynn writes, “nature has been allowed to work unfettered.” Such places are often thriving with plant and animal life. Abandonment, she writes, “<em>is </em>rewilding, in a very pure sense, as humans draw back and nature reclaims what once was hers.”</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106273/image.jpg\" width=\"auto\" /></figure>\n<p>What Flynn makes clear is that while we tend to think of human activity on the landscape as not only damaging but <em>irreversible</em>, this may not always be the case. We believe, in our hubris, that we have the power to wreck nature for good. And while it’s true that places like the Bikini Atoll and Chernobyl will be radioactive for unimaginable human lifetimes, that doesn’t mean that other species haven’t moved in and, left unmolested by human activity, found ways to flourish.</p>\n<p>Flynn’s book catalogs a variety of ways in which nature has reclaimed places that we’ve left behind, often with surprising speed. When Estonia, for example, became independent of the Soviet Union, some 245 million square miles of collectivist farmlands were simply abandoned. They weren’t plowed over, repurposed, or re-seeded. They simply were left alone. Flora immediately went to work: soon these fields were covered in wildflowers and weeds, and then thorn bushes and brambles, and then the skinny shoots of young spruce trees. Now, thirty-five years later, Estonia is now one of the most forested countries in Europe, having nearly doubled the size of its forests by doing … nothing. Half the country is now a forest, and over 90 percent of those forests have naturally regenerated.</p>\n<p>When I say that Centralia is <em>thriving, </em>this is what I mean. It is a landscape pulsing with life, overflowing with lush greenery. The old grid of streets is still visible, and there are still a handful of houses with carefully mowed lawns sitting in defiance. But everything else is the wild and vital province of nature. Turkeyfoot, broom-sedge, and switchgrass and silky dogwood. Young white oaks and linden trees push their way through this cacophony of life. Everywhere that’s not asphalt is a riot of green in every possible shade. And all of this is possible, at least in part, because the state and federal governments have forbidden any new human settlement, giving the wild and the lush and untrammeled room to grow.</p>\n<p>Not all of this is just nature. In 2021, the Eastern Pennsylvania Coalition for Abandoned Mine Reclamation planted 250 apple trees in the hope of attracting butterflies. EPCAMR has hosted annual trash clean-ups in the town, but a few years ago turned to planting and furthering the former town’s potential as an unofficial wildlife sanctuary. “We’re trying to get that area designated as a monarch way station eventually,” Robert “Bobby” Hughes, executive director of EPCAMR said at the time. But as vital as this work is, it seems primarily that the rewilding of Centralia is simply the work of leaving it alone.</p>\n<p>Standing in what was once a small, otherwise forgettable town, I came to understand how folly, mistake, calamitous hubris, neglect, and plain stupidity—could all be weapons in an arsenal to rewild and reforest the Earth, a future waiting in places we mistakenly believe we have irredeemably scarred.</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106280/image.jpg\" width=\"auto\" /></figure>\n<hr class=\"baseline-grid-hr\" />\n<p>Beyond the town itself, the thing people have come to mourn here is the Graffiti Highway, which for years was a strange destination before it was covered over in 2020. It began, as these things often do, as spontaneous tagging and defacement. But over time, more taggers added their names, their designs, their art, and their stories, until it had become a makeshift historical record of the people who live here.</p>\n<p>Over time, it had begun to encroach on the natural history that was also unfolding, spilling out beyond the asphalt and into the forest, as trees and plants started to get defaced. It became an attractive nuisance, repeated bonfires and ATV crashes straining local resources, so when coal company Pagnotti Enterprises bought the land in 2018, they chose to bury the road in dirt and erased it for good. There is now, in the words of many Redditors, no reason to go to Centralia. But the company’s decision also obliterated what some saw as a vital piece in the region’s history. Pagnotti’s<a href=\"https://www.google.com/search?q=pagnotti+enterprises&amp;oq=pagnotti+enterprises&amp;gs_lcrp=EgZjaHJvbWUyBggAEEUYOdIBCDM4MjBqMGo3qAIAsAIA&amp;sourceid=chrome&amp;ie=UTF-8#lrd=0x89c51a61c01ed687:0x1b1a2cd6c4d6b514,1,,,,\"> reviews</a> on Google are uniformly one-star ratings alongside comments like “You ruined graffiti highway,” “ruined a landmark, nice piles of dirt, go die,” and so on.</p>\n<p>For those who contributed to the Graffiti Highway, it had marked loves and losses, honored the dead and celebrated the living, all in a hundred different colors. (Park Street in Centralia has since begun to take the place of the old Graffiti Highway, decorated with a variety of tags, but at the moment it has nowhere near the density of the original Graffiti Highway. Some monuments take time to rebuild.)</p>\n<p>Kutztown University professor Deryl Johnson has called the story of Graffiti Highway an “epilogue” to the story of Centralia itself, but I’m not sure I agree. The story of Centralia is still very much unfolding—it did not end in 1982, and it did not end in 2020. Now that the highway is gone, the tourist attraction draw of this place has waned, leaving even more space for the natural world to reclaim the land. A new chapter has begun, and there may be other chapters in the story yet to come—chapters whose shape and direction we can only guess at.</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106277/image.jpg\" width=\"auto\" /></figure>\n<hr class=\"baseline-grid-hr\" />\n<p>If you think of Centralia in terms of human habitation, it’s a ghost town, a few stubborn holdouts fighting against entropy and inertia. If you think of Centralia in terms of legend tripping and ruin porn, it’s nothing at all, barely a wide spot in the road. But if you think of Centralia as an unintended nature preserve, it is absolutely bursting with life and potential and possibility.</p>\n<p>Yet still the ground burns. Just out of the grid of streets that was once the town, down Big Mine Run Road, are the vents themselves: small holes in the sides of the hills like something out of Tolkien that lead down to inferno below. These days, the smoke itself is rarely visible, but when rain filters down to the fires, it comes back out as steam. So on the rainy day of our visit, we watched as these vents let out a small, steady stream of white steam, proof of the heat somewhere beneath our feet.</p>\n<p>It was an odd sensation. The wisps seemed peaceful, laconic, almost soothing. And at the same time, it seemed as though at any moment the entire valley would explode. Somehow it felt like both of these things at once.</p>\n<p>Looking at these gentle wisps of smoke, it is difficult to picture the smoldering inferno they emerged from. A fire that has raged out of control for sixty years, unending and older than most people you know. You try and you fail every time.</p>\n<p>Which is to say, Centralia’s mine fire is a thing that should not be. I can describe to you its history, the actions of the people involved. I can describe to you what the surface looks like, the species of plants, the words etched into the tombstones at the Odd Fellows Cemetery. But the secret, raging, burning heart of the Valley remains elusive.</p>\n<p>The plumes are a subtle reminder, easy to miss, that there is a reason for this pristine, thriving wildness all around us. That the coal mines underground are a price that has to be paid, paid to an underworld god that must be forever fed.</p>",
        "source": "www.atlasobscura.com",
        "published": "Tue, 13 Jan 2026 17:18:00 -0500",
        "fetched_at": "2026-02-28T23:18:43.753493Z",
        "tags": [
          {
            "name": "transformation",
            "score": 9
          },
          {
            "name": "boundary_crossing",
            "score": 6
          },
          {
            "name": "scale_shift",
            "score": 4
          }
        ],
        "structural_score": 19,
        "timeliness_score": 3,
        "final_score": 11.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.atlasobscura.com/articles/podcast-edison-ford-winter-estate",
        "title": "Inside Thomas Edison’s Botanical Laboratory",
        "summary": "<div>\n<p class=\"item-body-text-graf\"><strong>Listen and subscribe on <a href=\"https://podcasts.apple.com/us/podcast/the-atlas-obscura-podcast/id1555769970\">Apple Podcasts</a>, <a href=\"https://open.spotify.com/show/0s0c4Z99PwbW8efTmHckyT\">Spotify</a>, and all major podcast apps.</strong></p>\n</div>\n<hr class=\"baseline-grid-hr\" />\n<p><strong>Kelly McEvers: </strong>Thomas Edison and his family had a ritual. Every winter, they would leave freezing cold New Jersey and head down to Fort Myers, Florida. Back then, Fort Myers was out there. Think swamps and mosquitoes. It was actually easier to get around by boat than over land.</p>\n<p>The Edisons would do vacation stuff: go fishing, go on boat rides, collect interesting plants. And in 1914, they invited a different branch of American inventing royalty to join them. That year, Henry Ford, of the Model T Ford, came down to Florida with his wife, Clara.</p>\n<p>Ford must have been psyched because Edison was actually his hero. They’d met briefly years before at a conference when Ford was still a low-level employee at an Edison company. Now they were meeting on something like equal terms.</p>\n<p>So to celebrate the occasion, Ford had some Model Ts shipped down to Fort Myers. Everyone went out joyriding around the swamps. The cars flooded, their campsite got soaked. Clara Ford was really afraid of snakes, and there were snakes everywhere. Henry tried to scare them away by shooting off a pistol. Needless to say, it was a trip.</p>\n<p>But soon, once the smoke from Ford’s pistol had cleared and the Model Ts had dried out, Edison and Ford would become more than just travel buddies. They were actually about to embark on an enormous inventing project, a project that would turn Edison’s Florida house into a full-fledged botanical laboratory and would become the last great obsession of Edison’s life.</p>\n<p>I’m Kelly McEvers, and this is <em>Atlas Obscura</em>, a celebration of the world’s strange, incredible, and wondrous places. Today’s episode is brought to you in partnership with Fort Myers – Islands, Beaches and Neighborhoods. Maybe when you think of Henry Ford and Thomas Edison, you think technology, cars, light bulbs, electricity. But the success of both of their inventions depended on plants. That is why they had come to Florida: to experiment.</p>\n<p><em>This is an edited transcript of the </em><a href=\"https://www.atlasobscura.com/podcast\"><em>Atlas Obscura Podcast</em></a><em>: a celebration of the world’s strange, incredible, and wondrous places. Find the show on </em><a href=\"https://go.skimresources.com/?id=89027X1542228&amp;isjs=1&amp;jv=15.7.1&amp;sref=https%3A%2F%2Fwww.atlasobscura.com%2Farticles%2Fpodcast-montezuma-well&amp;url=https%3A%2F%2Fpodcasts.apple.com%2Fus%2Fpodcast%2Fthe-atlas-obscura-podcast%2Fid1555769970&amp;xs=1&amp;xtz=300&amp;xuuid=f238828fc9c8f1386593b6f8b1d81e7b&amp;xjsf=other_click__contextmenu%20%5B2%5D\"><em>Apple Podcasts</em></a><em>, </em><a href=\"https://open.spotify.com/show/0s0c4Z99PwbW8efTmHckyT\"><em>Spotify</em></a><em>, and all major podcast apps.</em></p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106299/image.jpg\" width=\"auto\" /></figure>\n<p><strong>Kelly: </strong>Plants were actually the reason Thomas Edison had fallen in love with Fort Myers in the first place. Around 30 years before that camping trip with Ford, Edison was working away in his Menlo Park lab on one of his most famous projects.</p>\n<p><strong>Karen Maxwell:</strong> Many people are under the misimpression he invented the light bulb. He actually perfected it.</p>\n<p><strong>Kelly: </strong>This is Karen Maxwell. She’s the horticulture director at the Edison and Ford Winter Estates.</p>\n<p><strong>Karen: </strong>So, at this time, there are about 20 different varieties of incandescent light bulbs, but none of them burned for very long.</p>\n<p><strong>Kelly:</strong> The problem was this teeny tiny piece inside the bulb called a filament. When electricity passes through, the filament heats up and glows and we get light. But none of these early filaments could glow long enough to make a practical light bulb.</p>\n<p>So Edison set out to change that, testing thousands and thousands of different materials. Cotton, platinum, cedar, and finally, bamboo.</p>\n<p><strong>Karen: </strong>And he had his team—I’m glad I wasn’t one of them then—they stayed up and did shifts to record how long it burned. That filament burned for 1,200 hours. And that made the incandescent light bulb a national product.</p>\n<p><strong>Kelly:</strong> Edison, already a famous inventor, was now a legend. But by the end of the project, his personal life was a mess.</p>\n<p><strong>Karen:</strong> He was 38 years old, burned out, and had lost his first wife, Mary. Three children. His doctor says, Thomas, you need to go south, take a vacation, and take a break. He ends up arriving in St. Augustine during the winter and finds that is really too cold. It didn’t meet what his doctor had prescribed. So one of his friends takes him further down the river and they end up going by the property, which is currently today what we know as the Edison and Ford Winter Estates. What does he see but stands of bamboo growing along the riverside? He bought it on the spot.</p>\n<p><strong>Kelly:</strong> Edison remarried, and soon he and his second wife, Mina, started transforming the Florida property and its stand of bamboo into their wintertime home away from home. Edison even had an old laboratory shipped down from New Jersey in case inspiration struck while he was on vacation. You know, his lab away from lab.</p>\n<p>At first, he did some experimenting with bamboo, but then in 1905, the invention of the tungsten filament for the light bulb made the bamboo one obsolete. Soon enough, though, he would have another project to focus on.</p>\n<p>After the Fords joined the Edison family vacation in 1914, it was time for Ford to invite Edison on a trip. They went to San Francisco, and Ford introduced Edison to some friends: a botanist named Luther Burbank, who was interested in plant hybridization, and the tire magnate, Harvey Firestone, of Firestone Tires. It wasn’t long before their conversation turned to rubber.</p>\n<p>And the thing was, in order to make cars, you needed tires, and in order to make tires, you needed rubber. Back then, there was no such thing as synthetic rubber. All of it came from plants. Most natural rubber was grown in Southeast Asia, in British and Dutch colonies, and that meant the British and Dutch set rubber prices. The crew became convinced that America needed its own domestic rubber supply. Edison got to work right away.</p>\n<p><strong>Karen:</strong> So he starts looking for a product that can grow quickly, produce latex. Latex is what makes rubber. Latex is a milky white substance. If you break open the stem, out comes a sticky white milky product. That is latex and that is the basis of all natural rubber.</p>\n<p>Over 17,000 plants are brought in and studied. There were botanists, volunteers, they even engaged the Union Pacific Railroad, who instructed every section chief to collect any plants growing along their extensive miles of right-of-way and forward them to Edison’s laboratory.</p>\n<p><strong>Kelly:</strong> The Florida House essentially became a latex distilling factory. Today, if you visit, you can still see a lot of these plants that Edison was experimenting on. There’s a spiny vine called crown of thorns, which looks like a cactus; a scrubby desert shrub called guayule, which is native to Mexico; and the most spectacular specimen, or at least the biggest, was the banyan tree.</p>\n<p><strong>Karen:</strong> It’s been in place for 100 years. And over the years, it’s grown extensively. We’ve had to maintain trimming so it doesn’t just eat up the buildings. The first impression people have is they’re looking at a forest of trees.</p>\n<p><strong>Kelly</strong>: Today, the tree covers nearly an entire acre of land. It’s the largest banyan tree in the continental U.S. But unfortunately for Edison, it just did not produce enough latex.</p>\n<p><strong>Karen:</strong> In 1928, he discovers, right here in his backyard, the plant that produces the most latex is goldenrod.</p>\n<p><strong>Kelly: </strong>Goldenrod is a very fast-growing weed with yellow flowers. Looks a lot like ragweed. So Edison ripped out rows and rows of his wife Mina’s citrus trees to plant goldenrod, which I’m sure she wasn’t thrilled about.</p>\n<p><strong>Karen:</strong> He mows them all down and he transforms their estate-like atmosphere to just a conglomeration of disorderly beds with markers and irrigation ditches all around, 500 plots of yellow goldenrod. And as you can imagine, that did little to kindle her enthusiasm for his work.</p>\n<p><strong>Kelly:</strong> Speaking of Mina’s view of his work, she was annoyed about the citrus trees, yes, but she was also worried about her husband’s health. Edison was in his 80s now and still keeping pretty long hours.</p>\n<p>Mina wrote, “He thinks of nothing else now. He has no time for anything else, no recreation,” and, “Everything turned to rubber in the family. We talked rubber, thought rubber, dreamed rubber.”</p>\n<p>There was also some tension between her and Henry Ford. For one thing, Ford had bought the house right next door. That’s why the museum today is known as the Edison and Ford Estates. And another thing: Ford had convinced Edison to let him dismantle his Florida lab and ship it up to Michigan. Because Ford wanted to start a museum dedicated to American innovation, and he said he simply needed his hero’s lab. Mina was not too happy about this. Though, with the help of Ford and Firestone, Edison did end up building a brand new botanical lab.</p>\n<p>Still, by the end of the 1920s, Edison’s health got worse. He came down with pneumonia and by the fall of 1931 was bedridden in New Jersey. At one point on his deathbed, as he was slipping in and out of consciousness, someone came in with a package sent from the Florida house.</p>\n<p>Inside was a small piece of rubber made from Edison’s goldenrod plants. According to biographer Michele Albion, he had a moment of lucidity, and then sunk into a coma. Just a few days later, he died on October 18th, 1931. The Edison family kept the botanical research lab going until 1934, when it was transferred over to the Department of Agriculture.</p>\n<p><strong>Karen:</strong> But it turned out his vision of the importance became true because when World War II came about, Japan captured Malaysia, Singapore, and most of the Pacific Rim rubber plantations.</p>\n<p><strong>Kelly: </strong>During the war, there were serious rubber shortages in the U.S. The government rationed gasoline and lowered speed limits just to make tires last longer.</p>\n<p><strong>Karen:</strong> But it was shortly after that that synthetic rubber ended the goldenrod destiny. That was in 1944. And It was pretty much what Tungsten did for his carbonized bamboo filament, the synthetic rubber did to his goldenrod rubber research. But he was right. I mean, he kept people going in the right direction. Without that foundation, we probably wouldn’t have been here today.</p>\n<p><strong>Kelly: </strong>Today, the Ford and Edison Winter Estates are combined into one big museum property. You can spend hours wandering around the grounds and seeing many of the plants that we talked about in this episode. The bamboo, the goldenrod, the banyan tree, and of course, the botanical laboratory itself.</p>\n<p><strong>Karen: </strong>It’s a 21-acre paradise of discovery for people that enjoy gardens and enjoy the different textures, the structures, the colors. There’s something blooming every single day. Many, many things.</p>\n<p><strong>Kelly:</strong> In our episode description, we will post a link to more info about visiting the Edison and Ford winter estates. And if you enjoyed today’s show, check out another episode of ours called <a href=\"https://www.atlasobscura.com/articles/podcast-fordlandia\">Fordlandia</a>. It’s all about Henry Ford’s very unsuccessful attempt to start an industrial rubber town in Brazil.</p>\n<p><strong><em>Listen and subscribe on</em></strong><a href=\"https://podcasts.apple.com/us/podcast/the-atlas-obscura-podcast/id1555769970\"> <strong><em>Apple Podcasts</em></strong></a><strong><em>,</em></strong><a href=\"https://open.spotify.com/show/0s0c4Z99PwbW8efTmHckyT\"> <strong><em>Spotify</em></strong></a><strong><em>, and all major podcast apps.</em></strong></p>\n<p><em>Our podcast is a co-production of Atlas Obscura and Sirius XM Podcasts. This episode was produced by Amanda McGowan. The production team for this episode includes Dylan Thuras, Doug Baldinger, Kameel Stanley, Johanna Mayer, Manolo Morales, Jerome Campbell, Amanda McGowan, Alexa Lim, Casey Holford, and Luz Fleming. Our theme music is by Sam Tyndall.</em></p>",
        "source": "www.atlasobscura.com",
        "published": "Wed, 28 Jan 2026 17:15:00 -0500",
        "fetched_at": "2026-02-28T23:18:43.753474Z",
        "tags": [
          {
            "name": "transformation",
            "score": 3
          },
          {
            "name": "boundary_crossing",
            "score": 6
          },
          {
            "name": "scale_shift",
            "score": 8
          }
        ],
        "structural_score": 17,
        "timeliness_score": 3,
        "final_score": 10.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.atlasobscura.com/articles/idaho-sun-valley-fascinating-places",
        "title": "Atlas Obscura’s Guide to Sun Valley, Idaho’s Most Fascinating Places",
        "summary": "<p>From top to bottom, Sun Valley is full of surprises. Only in this fascinating pocket of central Idaho can you experience an annual heritage festival that parades thousands of sheep from the mountains to Main Street by day, then discover some of the darkest night skies in the world for mind-blowing star gazing.</p>\n<p>In between, you’ll relax in a botanical garden’s meditative nook, and visit the gravesite of one of the world’s most notable writers and explore a moon-like national park full of caves and lava flows. Enjoy this guide to 10 wonderful ways to start your Sun Valley adventure.</p>\n<h2 class=\"article-subheading-pre-rd\" style=\"text-align: left;\">The Roundhouse</h2>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106296/image.jpg\" width=\"auto\" /></figure>\n<p>The Roundhouse, a staple of Sun Valley Resort since 1939, elevates any dining experience—literally. Located 7,700 feet above sea level on Bald Mountain, the restaurant has been a featured fine dining spot since 1939, and is open seasonally, December through March. The octagonal restaurant, featuring 46 windows, is only accessible only by gondola, and the sweeping views of the entire valley make the views as impressive as the menu. Inside oozes with a ski chalet-style, cozy ambiance, especially the four-sided fireplace. A popular starter, the Fondue For Two, comes with artisan bread, Granny Smith apples, grapes, and gherkins. You can also add specialty meats and vegetables for an extra charge. A Wagyu burger, lobster rolls, scallops, and elk Swedish meatballs all make the menu here.</p>\n<h2 class=\"article-subheading-pre-rd\" style=\"text-align: left;\"><strong>Central Idaho Dark Sky Reserve</strong></h2>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106289/image.jpg\" width=\"auto\" /></figure>\n<p>Grab your tent and experience the awe-inspiring wonder of Central Idaho’s starry, night sky in the <a href=\"https://visitsunvalley.com/searching-for-sun-valley/the-dark-skies-of-sun-valley-id/\">Central Idaho Dark Sky Reserve</a>. One of the last remaining areas of this level of nighttime natural darkness in the world, the reserve encompasses just under 1,500 miles of public lands inside the Sawtooth National Forest. Certified by the International Dark Sky Association in 2017, and given its highest “gold tier” status, the reserve features an ultra-dark core, plus dark periphery that helps protect the central dark area. Meteor showers, lunar eclipses, spring equinox and the summer solstice are just a few of the many public viewing events held at the reserve annually. The protected wilderness areas under these dark skies are also home to a stunning array of wildlife, including bears, wolverines, elk, wolves, and sandhill cranes.</p>\n<h2 class=\"article-subheading-pre-rd\" style=\"text-align: left;\"><strong>Trailing of the Sheep</strong> <strong>Festival</strong></h2>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106286/image.jpg\" width=\"auto\" /></figure>\n<p>Each fall, a woolly throng of sheep, roughly 1,200 in all, parade down the main street of Ketchum, Idaho, for the <a href=\"https://visitsunvalley.com/events/annual-trailing-of-the-sheep-festival/\">Trailing of the Sheep Festival</a>. The treasured annual event commemorates the time-honored migration of sheep from Idaho’s high mountain summer pastures to the warmer, grazing and lambing grounds found farther south. For five days, the community celebrates the history, culture, and traditions of the region’s longstanding sheep ranchers, which include Basques, Peruvians, and Scots. Signature events include lamb-centered culinary classes, woolmaking workshops, a heritage fair, and national sheepdog trials. The 2026 festival is October 7-11.</p>\n<h2 class=\"article-subheading-pre-rd\" style=\"text-align: left;\"><strong>Craters of the Moon National Monument and Preserve</strong></h2>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106287/image.jpg\" width=\"auto\" /></figure>\n<p>A trip to Central Idaho’s Snake River Plain is just about as close to the moon as most of us will ever get. Aptly described as “a weird and scenic landscape” by President Calvin Coolidge when he established the 750,000-acre federally protected site in 1924, the <a href=\"https://www.atlasobscura.com/places/craters-of-the-moon-national-monument-and-preserve\">Craters of the Moon National Monument and Preserve</a> features a vast, lunar-like landscape of lava flows, cinder cones, and sagebrush. The unique environment was created thousands of years ago by a series of major eruptions along the 52-mile stretch of deep cracks in the Earth’s crust called the Great Rift. For generations, the park has garnered attention and profound fascination, and the wild terrain even served as a training ground for Apollo astronauts in the 1960s. Today, explorers enjoy discovering the park’s many lava tube caves and trails, and viewing the impressive overlooks while driving along the 7-mile Loop Road. Nature lovers and photographers also flock to the park for its surprising diversity of birds and other wildlife, plus it’s a designated dark sky park.</p>\n<h2 class=\"article-subheading-pre-rd\"><strong>Sun Valley Museum of Art</strong></h2>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106293/image.jpg\" width=\"auto\" /></figure>\n<p>In downtown Ketchum, the <a href=\"https://visitsunvalley.com/to-do/sun-valley-museum-of-art/\">Sun Valley Museum of Art</a> is just one of the many ways to explore the rich culture of the region—off the slopes. Now an integral part of Sun Valley’s arts and culture community, this free museum opened in 1971 and has grown to feature works from greats like Andy Warhol to important pieces from local and regional artists. Equal parts museum and educational hub, the center also features interesting lecture series, live music, films, and hands-on art classes and workshops throughout the year. The exhibit, \"Hidden Gems: Idaho Collects,\" brings art held in private collections in the region into public view through February 28, 2026. The exhibit aims to illuminate the region's community through the art they make and collect</p>\n<h2 class=\"article-subheading-pre-rd\" style=\"text-align: left;\">Pioneer Saloon</h2>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106285/image.jpg\" width=\"auto\" /></figure>\n<p>One part time capsule, one part fine dining, the Pioneer Saloon is a beloved go-to for Ketchum locals and visitors alike. Located on Main Street, and affectionately called “the Pio,” the <a href=\"https://visitsunvalley.com/dining-shopping/the-pioneer-saloon/\">Pioneer Saloon</a> opened in the 1940s as a casino, despite gambling being outlawed in Idaho. Originally called the Commercial Club, the gambling hub closed its doors after just a few years, and the American Legion turned it into a meeting hall. For a short time, the facility also served as a dry goods store until, in 1950, a man named Whitey Hirschman, turned it back into a casino. Containing decades of local lore and history, the saloon won a 2025 James Beard America's Classics Award. Today, the menu consists of hearty steaks, prime rib, ribs, and seafood, including Idaho trout. Order the signature “Jim Spud,” and you’ll get a hot baked potato with teriyaki beef, cheese, and other toppings. There’s even a “Hemingway Margarita” that pays homage to the famed author whose final resting place is in Sun Valley. Amid the rustic décor inside, you’ll find antiques and artifacts, including Hemingway’s hunting rifle, Western posters and artwork, a Native American canoe and arrowheads, and more.</p>\n<h2 class=\"article-subheading-pre-rd\" style=\"text-align: left;\"><strong>Ernest Hemingway’s Grave</strong></h2>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106288/image.jpg\" width=\"auto\" /></figure>\n<p>Despite Ernest Hemingway’s flamboyant, hard-living nature, the <a href=\"https://www.atlasobscura.com/places/ernest-hemingway-s-grave\">famed writer’s final resting place</a> is a simple slab in a Sun Valley cemetery. Known for his heavy drinking, hunting, and womanizing lifestyle, Hemingway lived all over, from Spain and Cuba to Florida, penning works like, “The Sun Also Rises,” “For Whom the Bell Tolls,” and the Pulitzer Prize-awarded “The Old Man and the Sea.” He visited central Idaho many times before moving to the area prior to his death in 1961. Placed alongside his wife, Mary, under two towering spruce trees, the grave is a modest rectangular marker including just the writer’s name and dates of birth and death. In addition to the expected flowers, fans also pay respects by leaving behind booze bottles, coins, matches, and pens.</p>\n<h2 class=\"article-subheading-pre-rd\" style=\"text-align: left;\"><strong>Sawtooth Botanical Garden</strong></h2>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106297/image.jpg\" width=\"auto\" /></figure>\n<p>For a serene escape, head to the <a href=\"https://visitsunvalley.com/services/sawtooth-botanical-garden\">Sawtooth Botanical Garden</a> in Ketchum. Located on five acres, the garden, which is also an educational non profit, centers on five major display gardens that represent the varied biomes in central Idaho. One must-see feature is the colorful Tibetan prayer wheel in the Garden of Infinite Compassion. It’s the only such wheel commissioned and blessed by the Dalai Lama in North America and the only one powered by flowing water. The 1,100-pound wheel is said to symbolize peace, healing and the dissemination prayers when turned.</p>\n<h2 class=\"article-subheading-pre-rd\" style=\"text-align: left;\"><strong>Wood River Museum of History &amp; Culture</strong></h2>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106283/image.jpg\" width=\"auto\" /></figure>\n<p>This free cultural museum in downtown Ketchum celebrates the rich and varied history of central Idaho, from its native people and immigrants to the iconic Bald Mountain and its effect on the local landscape. One exhibit at the <a href=\"https://visitsunvalley.com/to-do/wood-river-museum-of-history-and-culture/\">Wood River Museum</a>, “A Writer in the New Country: Hemingway in 1939,” highlights Ernest Hemingway’s first trip to Sun Valley, a place that was dear to the writer up until his death in 1961. Sheep shears, a telegraph key, and vintage skis are all part of the interactive Cabinet of Wonders, which houses important regional artifacts. At the museum’s entrance, another exhibit honors the Shoshone-Bannock native peoples, who first inhabited central Idaho.</p>\n<h2 class=\"article-subheading-pre-rd\" style=\"text-align: left;\"><strong>Ore Wagon Museum</strong></h2>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106298/image.jpg\" width=\"auto\" /></figure>\n<p>This <a href=\"https://visitsunvalley.com/events/ore-wagon-museum/\">history museum in Ketchum</a> highlights the importance of ore wagons during the region’s rich mining boom of the 1880s. These sturdy wagons, donated to the museum by the Lewis family, whose Fast Freight Line was integral in transporting silver ore from remote mines to in-town railheads, are reportedly the only of their kind in existence. In honor of its mining roots, the city hosts a heritage festival, Wagon Days, every Labor Day weekend. The beloved event features live music, food vendors, cultural presentations, and culminates with the Big Hitch, a parade of these historic, non-motorized vehicles that served as the backbone of the region’s economy before the development of the railroads.</p>",
        "source": "www.atlasobscura.com",
        "published": "Mon, 26 Jan 2026 14:00:00 -0500",
        "fetched_at": "2026-02-28T23:18:43.753483Z",
        "tags": [
          {
            "name": "transformation",
            "score": 3
          },
          {
            "name": "boundary_crossing",
            "score": 9
          },
          {
            "name": "scale_shift",
            "score": 4
          }
        ],
        "structural_score": 16,
        "timeliness_score": 3,
        "final_score": 9.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.atlasobscura.com/articles/podcast-caroline-mazel-carlton-1000-places",
        "title": "The Quest to Visit 1,000 Places",
        "summary": "<div>\n<p class=\"item-body-text-graf\"><strong>Listen and subscribe on <a href=\"https://podcasts.apple.com/us/podcast/the-atlas-obscura-podcast/id1555769970\">Apple Podcasts</a>, <a href=\"https://open.spotify.com/show/0s0c4Z99PwbW8efTmHckyT\">Spotify</a>, and all major podcast apps.</strong></p>\n</div>\n<hr class=\"baseline-grid-hr\" />\n<p>I’m Kelly McEvers, and this is Atlas Obscura, a celebration of the world’s strange, incredible, and wondrous places.</p>\n<p>So I don’t know about you, but I like to keep track of all the places that I have visited, say, in the past year. I have lists of all the countries that I visit in a given region. Each year I go back to my handwritten calendar planner book because, yes, I still write everything down.</p>\n<p>I have kept track of all my trips, and that helps me remember all the places I’ve visited and the people I saw. Most people I know are, of course, more advanced than this. They actually keep digital records like lists of restaurants where they want to go or Google Maps with pins on places.</p>\n<p>In case you have somehow stumbled upon this podcast and you don’t know too much about Atlas Obscura, we actually have a map, an Atlas, filled with thousands upon thousands of unusual places across the globe. Each place is submitted by a person, and it is a fun tool to use whether you are on vacation or you want to get to know your own hometown better.</p>\n<p>My guest today has visited over 1,000 of these places. Her name is Caroline Mazel-Carlton, and she has been working toward that goal for more than 10 years. This project, Visiting 1,000 places, was about more than just taking items off the list. She says it helped save her life.</p>\n<p>Caroline, welcome.</p>\n<p><em>This is an edited transcript of the </em><a href=\"https://www.atlasobscura.com/podcast\"><em>Atlas Obscura Podcast</em></a><em>: a celebration of the world’s strange, incredible, and wondrous places. Find the show on </em><a href=\"https://go.skimresources.com/?id=89027X1542228&amp;isjs=1&amp;jv=15.7.1&amp;sref=https%3A%2F%2Fwww.atlasobscura.com%2Farticles%2Fpodcast-montezuma-well&amp;url=https%3A%2F%2Fpodcasts.apple.com%2Fus%2Fpodcast%2Fthe-atlas-obscura-podcast%2Fid1555769970&amp;xs=1&amp;xtz=300&amp;xuuid=f238828fc9c8f1386593b6f8b1d81e7b&amp;xjsf=other_click__contextmenu%20%5B2%5D\"><em>Apple Podcasts</em></a><em>, </em><a href=\"https://open.spotify.com/show/0s0c4Z99PwbW8efTmHckyT\"><em>Spotify</em></a><em>, and all major podcast apps. </em><em>This episode contains discussions of suicidal thoughts. If you or someone you know is struggling, contact the Suicide Crisis Hotline by calling or texting 988.</em></p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106271/image.jpg\" width=\"auto\" /></figure>\n<p><strong>Caroline Mazel-Carlton: </strong>Oh, I’m getting teary already. It’s so good to be here. Thank you, Kelly.</p>\n<p><strong>Kelly McEvers: </strong>Yeah, welcome. So talk about your first ever visit to an Atlas Obscura place.</p>\n<p><strong>Caroline Mazel-Carlton: </strong>Yeah. So one of the first times that I remember using the Atlas Obscura was when I wanted to take my now-husband on a romantic interlude, like a nice weekend away. And so I was looking for spots—bed and breakfasts—and the Atlas Obscura was so helpful because it showed me that not too far away in Fall River, Massachusetts, you can find <a href=\"https://www.atlasobscura.com/places/lizzie-borden-bed-and-breakfast-and-museum\">Lizzie Borden’s house</a>.</p>\n<p><strong>Kelly: </strong>In case you’re not familiar, in 1892, Lizzie Borden allegedly murdered her parents, Abby and Andrew Borden, in their house with an axe. Lizzie was acquitted. And Caroline believes she was innocent. But the whole thing has become a bit of a folk story.</p>\n<p>And the house where the murders took place still stands now as this untraditional bed and breakfast.</p>\n<p><strong>Caroline: </strong>They had this whole getaway that you could have and sleep in Lizzie Borden’s house. They had dummies set up, sort of positioned where, Andrew Borden, what he would have looked like after the crime had been committed. So it was this beautiful Victorian house full of wonderful <a href=\"https://www.atlasobscura.com/places/leilas-hair-museum\">Victorian hair art</a>, which I’m a big fan of Victorian hair art as well—some great specimens of that there. So it was just an amazing experience.</p>\n<p><strong>Kelly: </strong>And I would imagine that your now husband was into it?</p>\n<p><strong>Caroline: </strong>Oh, yeah, yeah. It was sort of like a litmus test in a way.</p>\n<p><strong>Kelly: </strong>I was going to say, if he passed that, then he knew he was a keeper.</p>\n<p><strong>Caroline: </strong>There’s a beautiful picture of us taken where we were sitting on this like Victorian couch and we have the dummy representing Andrew Borden’s bloody corpse splayed out across our laps. And we’re just brimming with young love. And it’s such a beautiful photograph.</p>\n<p><strong>Kelly: </strong>Yeah. I love it. You’re like, this is the one for me.</p>\n<p><strong>Caroline: </strong>Absolutely. And I did try, when we got married, I tried to convince my mom to let me use that photo for our save the date. But she said, “No, I’m not into the idea of this bloody corpse photo.” So we ended up using a picture from another trip we took to Paris.</p>\n<p><strong>Kelly: </strong>Nice. And I would love to just know where your urge to go places started. What was one of your most memorable trips you took as a kid?</p>\n<p><strong>Caroline: </strong>So my family growing up, we weren’t the type of family that went to the same beach or the same lake house every year for vacation. One of my family mottos was, “We’ll go anywhere once.”</p>\n<p><strong>Kelly: </strong>Oh, I love that.</p>\n<p><strong>Caroline: </strong>And so my dad has always been a history buff, but he’s never shied away from the weirder and grittier parts of American history. Some of my early memories are definitely wandering around graveyards.</p>\n<p>I remember seeing the <a href=\"https://www.atlasobscura.com/places/the-skin-of-little-sorrel-lexington-virginia\">taxidermied horse</a> of Stonewall Jackson in some weird museum in Virginia. One place we went, and sadly, you can’t go here anymore. My dad has sort of, like, a dark streak, like, dark humor.</p>\n<p>And he became obsessed with the <a href=\"https://www.atlasobscura.com/articles/31-days-of-halloween-floyd-collins\">story of this guy named Floyd Collins</a>, who was a cave explorer that actually got trapped and died in the Mammoth Cave system. So my dad and I actually did some caving together and visited the museum that honors this man. A tribute to explorers everywhere, but sadly he did not make it out of the cave.</p>\n<p><strong>Kelly: </strong>Mm-hmm. You actually set this goal of trying to visit 1,000 Atlas Obscura places over a decade ago in 2012. And for so many people, you know, travel and seeing the world, there’s all these reasons we do it, but a lot of it is like: I want a change in perspective, or I want to learn more about this culture. I want to be wowed.</p>\n<p>For you, it sounds like there was a really kind of specific reason that you did this. Can you take us back to that time and talk about what was going on in your life?</p>\n<p><strong>Caroline: </strong>So for me, I grew up experiencing a lot of bullying over how I looked or the way that I acted. And I started to struggle a lot with thoughts of suicide. And in fact, for certain parts of my life I was hospitalized and was in treatment programs where you’re not allowed to leave places like that. So it’s kind of a smaller existence.</p>\n<p>For me, it was always trying to figure out, how do I survive? How do I find a way to exist in this world? And what I realized is, for a lot of us that grapple with suicidal thoughts, it’s not truly that we want to literally die, but that the life that we’re living needs to end. It’s sort of this desire to be transformed in a way.</p>\n<p>For me, trying to figure out how to exist in the world has always been a bit of a battle in and of itself. And I remember one time seeing a book on my uncle. My uncle Doug also loved to travel the world. And he had a book called <em>1,000 Places to See Before You Die.</em></p>\n<p><strong>Kelly: </strong>Okay.</p>\n<p><strong>Caroline: </strong>And I thought about that. And I thought about the power of saying to myself, you know what? You can’t die today because there’s still places that you haven’t seen yet. So I used that book for a while, but then when I discovered Atlas Obscura, I was like, these sites are actually more interesting to me.</p>\n<p>They’re more accessible. They’re weirder. As I visit Atlas Obscura sites, I often learn about weird people like myself. I’ve seen amazing outsider art. So reaching a thousand Atlas Obscura sites before I died became really, really important to me.</p>\n<p><strong>Kelly: </strong>Since then, Caroline has visited Atlas Obscura places around the world, from the <a href=\"https://www.atlasobscura.com/places/grave-of-johnny-appleseed\">grave of Johnny Appleseed</a> in Fort Wayne, Indiana, to a <a href=\"https://www.atlasobscura.com/places/shree-ganesh-darshan-museum\">temple complex</a> in Pune, India, with 500 statues of Lord Ganesh. Once, on a 16-hour layover in Hong Kong, she left the airport and took a tram over the mountains to see the world's <a href=\"https://www.atlasobscura.com/places/tian-tan-buddha\">largest-seated bronze Buddha.</a></p>\n<p>She’s been to the <a href=\"https://www.atlasobscura.com/places/icelandic-phallological-museum\">Icelandic Phallological Museum</a> in Reykjavik and the <a href=\"https://www.atlasobscura.com/places/worlds-largest-czech-egg\">world’s largest Czech egg</a> in Wilson, Kansas, and <a href=\"https://www.atlasobscura.com/places/deyrolle-taxidermy\">a taxidermy shop in Paris</a> that Pablo Picasso and Salvador Dali would visit for inspiration. Taxidermy holds a special place in Caroline’s heart.</p>\n<p><strong>Caroline: </strong>There’s one Atlas Obscura site I’m going to give a shout out to, <a href=\"https://www.atlasobscura.com/places/oles-big-game-steakhouse-and-lounge\">Ole’s Big Game Steakhouse in Nebraska</a>, where you can be surrounded by taxidermy and also you can eat at the same time.</p>\n<p><strong>Kelly: </strong>Which, not going to lie, doesn’t sound great to some people, but I love it.</p>\n<p>Today, Caroline works in suicide prevention. with an organization that does peer support, advocacy, and training for harm reduction. And she brought her 1,000 places goal into that work.</p>\n<p>Caroline has led trainings around the world, and sometimes on these trips, she and her colleagues will visit Atlas Obscura sites together. Caroline says it is really hard to choose a favorite memory.</p>\n<p><strong>Caroline: </strong>Oh, there are so many. I remember one time we were doing an alternatives to suicide training and we were in Tacoma, Washington, and we actually found on Atlas Obscura the grave of Kurt Cobain, who was someone that I looked up to when I was younger, one of my favorite musicians, and who did die by suicide.</p>\n<p>But we went there together and it felt like such a special place to be there and honor him and his role in our lives and the way he could give voice to pain in a way that other people could connect with. I also remember a time where I was giving a talk at The Hague in the Netherlands and we visited a museum.</p>\n<p>I think it’s called Museum of the Mind, which had been a psychiatric hospital. But then they filled it with art, beautiful art made from former psychiatric patients. So going there and to some of the Van Gogh sites. And it’s just been incredible to do that with some of my colleagues who’ve also struggled with thoughts of suicide.</p>\n<p>And I really look at this achievement of reaching a thousand sites as something that we did together. And it felt really special because it was all connected to the journey of healing and embracing our weirdness and our desire to live in a world that’s not always, you know, normative.</p>\n<p><strong>Kelly: </strong>So, I mean, you hit the goal, right? You’re over 1,000. You’re at 1,048, to be exact. So what’s next? I mean, how do you, you know, where do you go from there? Do you set a new goal? Are you just going to keep on keeping on at this point? Do you feel like you’re going to travel differently now?</p>\n<p><strong>Caroline: </strong>Yeah. Well, after meeting the goal, I was like, I can rest a little bit because I honestly thought I’m 43. So I thought I would be at least 50 before I hit 1,000. but I hit it much more quickly than I thought I would. But the thing about Atlas Obscura is there’s always more you can do.</p>\n<p>And one of the things that I really encourage everyone listening to do is to add sites to the Atlas yourself. It’s a thrill for me to do that. I remember one time I was working in Brazil and we were just in this little town that had no Atlas Obscura sites, but I’m like, I’m going to find something.</p>\n<p>And I found this guy with a little, he had a cell phone store, but then he had sort of in the back rooms, all these historical communication devices. Even one of the first Morse code devices and a phonograph. And we got to, through broken English and broken Portuguese, I wrote an article and posted that on the Atlas, and I checked it today, and now eight people have been there.</p>\n<p>When you add a site to the Atlas, you really do change people’s lives. You know, I don’t struggle as much in my life anymore as when I started because the world just seems more weird and welcoming.</p>\n<p><strong>Kelly: </strong>Caroline Mazel-Carlton, thank you so much for sharing your story and thank you for the work that you do helping other people too.</p>\n<p><strong>Caroline: </strong>Absolutely. I just seek to make this place more welcoming and, you know, people are struggling. My organization, we have alternatives to suicide support groups. There are places you can go to talk where people will listen and not shame you or judge you and where we acknowledge that there’s many paths to healing.</p>\n<p>And sometimes that path to healing means walking around a really weird taxidermy store and that’s okay.</p>\n<p><strong>Kelly: </strong>While eating a steak.</p>\n<p><strong>Caroline: </strong>Yes. I’m here for it.</p>\n<p><strong>Kelly: </strong>That was Caroline Mazel-Carlton. She has visited 1,048 Atlas Obscura places. No doubt many more to come. We will put a link to the Atlas in our show notes, so maybe you can start ticking off your own list of 1,000 places. Also, if you or someone you know is struggling, you can contact the 988 Suicide and Crisis Lifeline.</p>",
        "source": "www.atlasobscura.com",
        "published": "Tue, 13 Jan 2026 11:00:00 -0500",
        "fetched_at": "2026-02-28T23:18:43.753498Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 7
          },
          {
            "name": "visibility_gain",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 4
          }
        ],
        "structural_score": 15,
        "timeliness_score": 3,
        "final_score": 9.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.atlasobscura.com/articles/pedro-rodriguez-kissimmee",
        "title": "Pedro Rodriguez Is on a Quest for Freshness",
        "summary": "<p>When Pedro Rodriguez is in his Kissimmee, Florida restaurant, Sajoma Latin Fusion, he makes sure to check in on the kitchen. And when he does, there’s a rule that all of his cooks must follow.</p>\n<p>“I better not catch you with anything that’s artificial,” he says. Sajoma’s sancocho, for example, is made from scratch, not with bouillon, which many cooks use to build flavor quickly.</p>\n<p>The approach has paid off. Sajoma has developed an avid following in Central Florida for its approach to Latin cuisine, rooted in good ingredients and creative cooking. Pedro, gregarious and perceptive with a quick smile and a salt and pepper beard, is proud of his brainchild. He’s a grocery supplier by trade; the restaurant business is relatively new for him.</p>\n<p>Sajoma is Pedro’s most personal project yet, the capstone of a lifelong obsession with good food and good produce. And it all started on his family’s farm.</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106304/image.jpg\" width=\"auto\" /></figure>\n<h3 class=\"article-second-subheading-pre-rd\">Feeding Off the Land</h3>\n<p>Until the age of 12, Pedro grew up in the town of San Jose de las Matas in the Dominican Republic. The municipality is known for its natural beauty and mineral water. “It’s almost like one of the greenest towns there,” he says. Sajoma, as the town is called for short, boasts dramatic hills, lush vegetation, and rolling rivers.</p>\n<p>And even in a beautiful town, Pedro lived a particularly idyllic life. His family owned a 120-acre farm with animals like cows, chickens, and goats, and crops including rice, beans, coffee, and yams. “We pretty much used to feed off the land,” he says. Beef was one of the only basic foodstuffs that he recalls leaving their property to obtain.</p>\n<p>The family home sat on the top of a hill. From there, Pedro could see a 360-degree view of mountains, greenery, and livestock grazing in the meadow. After school, he would hang around the house and play with the animals on their property.</p>\n<p>The men who worked for his family would hunt for crabs in caves. Pedro would go with them on their hunts, but he would watch from the side, apprehensive, as they stuck their bare hands into the darkness for huge, snapping crabs. He enjoyed the result, though: a dish called locrio where stewed crab meat releases its flavors into brown rice.</p>\n<p>Pedro grew up loving food, and it’s easy to see why. His mother was—and still is—a great cook who can turn any ingredient into a special meal. And she had the pick of ingredients in their family home. Milk from their own cows, yams dug up from their own soil. Pedro remembers his mother cooking cerdo guisado, or stewed pork, with onions and cubanelle peppers; and pasta with cooked green bananas.</p>\n<p>“The food was, like, unexplainably good, because everything was natural,” Pedro says.</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106305/image.jpg\" width=\"auto\" /></figure>\n<p>Twenty years ago in New York City, Pedro met his wife, Marisol, who was born in the U.S. to Dominican parents. When they were dating, she cooked him a meal that was, somehow, even better than his mother’s cooking. Pedro went home and told his mother; she was thrilled that her son had found a worthy match. And Marisol shares her in-laws’ dedication to natural cooking. “She does not use anything artificial,” Pedro says. “She’s very big on that.” That means no bouillon, and no pre-made seasonings, like the dried adobo mix that supermarkets sell.</p>\n<p>With Sajoma, Pedro’s goal was to let good ingredients sing without any additives. Customers have taken notice. Pedro says that when he walks the floor of the restaurant, diners tell him, “I literally feel like I’m eating this at home.”</p>\n<p>He believes this is testament to the power of simple cooking with no shortcuts. “Sometimes people think that you could force flavor. You don’t force flavor,” Pedro insists. With natural ingredients, “Flavor is very easy to accomplish.”</p>\n<h3 class=\"article-second-subheading-pre-rd\">From the Dominican Republic to the World</h3>\n<p>If the Rodriguez family farm was Pedro’s first culinary education, the multicultural restaurants of New York were his second. When Pedro was 12, his parents moved to New York and sent Pedro, his brother, and his sister to the city of Santiago to live with his grandparents. When Pedro was 14, his parents brought their children to the Big Apple.</p>\n<p>One might think moving from verdant island to concrete jungle would be difficult. For Pedro, it wasn’t.</p>\n<p>He received a warm welcome from his extended family, most of whom had settled in New York by the time he and his siblings got there. His first summer in New York, relatives toured him and his siblings around to the city’s parks and botanic garden. He loved the communal culture of 1980s Brooklyn, where he would wile away the day outdoors, playing ball on the streets and hanging out with his cousins. When Pedro’s mother offered to send him back to the Dominican Republic the following winter, he declined.</p>\n<p>Chief among these new experiences were the city’s food offerings. A family member blew Pedro’s mind when he took him for his first glazed donut. “I was like, ‘Holy shit!’” He remembers. “Where has this been all my life?”</p>\n<p>Pedro had a similar reaction to his first Chinese meal. Before he learned to speak English, his cousin took him to a restaurant where the staff spoke fluent Spanish with customers before calling out orders to the kitchen in Chinese. Pedro and his cousin bought fried rice with a half chicken and tostones, or fried plantains, and ate it outside on one of their stoops. “I fell in love with that,” he says.</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106306/image.jpg\" width=\"auto\" /></figure>\n<h3 class=\"article-second-subheading-pre-rd\">Starting Small and Expanding Slowly</h3>\n<p>The excited, food-loving child is very much alive in 53-year-old Pedro. He describes with equal relish his recent meal at a Peruvian restaurant as well as the locrio he ate on his family’s farm growing up. But food is also his business. In addition to Sajoma Latin Fusion in Kissimmee, Pedro owns four restaurants in New York and runs a fleet of trucks that he says supply most of New York City’s independent grocers. When asked about his secret to success in business, he uses a distinctly Dominican analogy: “I compare it to baseball players.”</p>\n<p>Many baseball players grow up playing on poorly kept fields. A ball might hit a rock, and smack you in the face. “It’s harder when you’re in the minor leagues,” he says. But, “You got to make sure that you could do that. Because once you go to the majors, the field is perfect now.”</p>\n<p>The message: “Start small,” he says, master your craft, and expand slowly.</p>\n<p>For Pedro, starting small meant working at his uncle’s grocery stores in Far Rockaway, Queens during high school. On Saturdays, he traveled with him to produce markets to stock the store. When Pedro graduated high school, he decided that he would rather spend the next few years growing a business. “What do I know at the time and what do I like at the time? Produce,” he says.</p>\n<p>So Pedro bought a van, and started delivering groceries to supermarkets, drawing on the connections he had built while working for his uncle. Soon, he bought a large truck, then two trucks. Today, he runs a fleet of 20 trucks.</p>\n<p>The road has not been easy. His equivalent of errant baseballs that threaten to hit you in the face were snowstorms that he had to fight through to deliver groceries. For years, he worked 18-hour shifts, rain, shine or snow. “I’d come home and eat, sleep for three or four hours, and go right back out there,” he remembers. He has since stepped back from physically driving trucks and delivering produce, but still helms the business.</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106307/image.jpg\" width=\"auto\" /></figure>\n<h3 class=\"article-second-subheading-pre-rd\">A Foothold in Florida</h3>\n<p>Over the years, many family members of Pedro’s have moved to Kissimmee. A friend told him about an open lot, wondering whether Pedro would be interested in opening a restaurant there. When Pedro saw the place, disparate threads of his life knit together: his childhood spent eating fresh produce on a Dominican farm; his exposure to cuisines from every corner of the world in New York; the New York hustle that had become his way of being.</p>\n<p>“Oh my god, this is perfect,” he remembers thinking after laying eyes on the space. He wanted to build a restaurant that combined fresh ingredients, Latin American cuisine, international influences, and New York service. And he would name it “Sajoma,” after the town that started his journey.</p>\n<p>After a period of renovation and menu-tweaking, Pedro opened Sajoma Latin Fusion in August of 2022. The restaurant’s interior is sleek and spacious, with an outdoor patio and plush couches. The team makes sure the produce is fresh, hand-picking it themselves from local independent supermarkets rather than large suppliers. Sajoma’s menu dances between Latin America—especially the Caribbean—and other parts of the world, like Europe, Asia, and North America. Their tuna tartare comes on a bed of guacamole and corn chips; their burger is topped with sweet plantains; and their sancocho is made from scratch with no additives.</p>\n<p>A pair of elderly Puerto Rican ladies recently visited the restaurant and made a point of telling Pedro how much they appreciated the sancocho. “We’ve had something like this at a house,” they told him. But “we have never tried anything like this at a restaurant.” They would spread the word to their family, they said.</p>\n<p>The word, it seems, has already gotten out. The restaurant has a loyal and growing following, and it becomes a party on weekends, when DJs and bands play salsa, bachata, merengue, and more.</p>\n<p>Much of Pedro’s work has been helping the team emulate the type of prompt, attentive service that one finds at a restaurant in New York. Achieving that has taken a lot of repetition, but they’ve pulled it off. “I’m just so proud, you know?” he says.</p>\n<p>Pedro says he approaches restaurant ownership as an eater, not a cook. He is actually not much of a chef, having been blessed with great cooking in his mother’s and wife’s kitchens, and in restaurants around the world.</p>\n<p>He constantly tries new restaurants, and he acts as the president of a group of around 40 New York supermarket industry professionals that call themselves the “Friday club” because they meet up at restaurants for food and wine every Friday. It’s easy to see why he would be named president: He knows good food and has the gift of gab.</p>\n<p>Pedro’s love of conversation and a good time is part of what draws him to the restaurant business, and when he is not checking on the kitchen at Sajoma, he is walking the floor, entertaining guests. He knows what it is to work hard all week and turn to a restaurant to provide delicious food and a space to connect with friends.</p>\n<p>“I don’t have to know how to cook,” in order to run a good restaurant, he says. “I have to know how to eat.”</p>",
        "source": "www.atlasobscura.com",
        "published": "Fri, 30 Jan 2026 13:15:00 -0500",
        "fetched_at": "2026-02-28T23:18:43.753469Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 9
          },
          {
            "name": "scale_shift",
            "score": 4
          }
        ],
        "structural_score": 13,
        "timeliness_score": 3,
        "final_score": 8.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.atlasobscura.com/articles/podcast-fordlandia",
        "title": "Why Did Henry Ford Build a Midwestern Town in the Amazon Rainforest?",
        "summary": "<div>\n<p class=\"item-body-text-graf\"><strong>Listen and subscribe on <a href=\"https://podcasts.apple.com/us/podcast/the-atlas-obscura-podcast/id1555769970\">Apple Podcasts</a>, <a href=\"https://open.spotify.com/show/0s0c4Z99PwbW8efTmHckyT\">Spotify</a>, and all major podcast apps.</strong></p>\n</div>\n<hr class=\"baseline-grid-hr\" />\n<p><strong>Elah Feder: </strong>Johanna, do you ever buy lottery tickets?</p>\n<p><strong>Johanna Mayer:</strong> No, never. Not a lottery ticket kind of gal.</p>\n<p><strong>Elah:</strong> I actually just got shamed by the man selling me lottery tickets for wasting my money.</p>\n<p><strong>Johanna: </strong>You buy lottery tickets?</p>\n<p><strong>Elah: </strong>I do buy lottery tickets. And I think what I really like about it is fantasizing that, you know, if I have enough money, I will finally be able to do whatever I want.</p>\n<p><strong>Johanna: </strong>And this is the appeal of being a multimillionaire, Elah.</p>\n<p><strong>Elah:</strong> Right, right.</p>\n<p><strong>Johanna:</strong> You’re not the first one to have this impulse.</p>\n<p><strong>Elah: </strong>I have this crazy, wild notion that money will give me power. And the story that we’re going to talk about today is about a lot of things. But one of them is a lesson about how even with unlimited money, from time to time, the world refuses to do your bidding. So I want to take you back to the 1920s and tell you about Henry Ford. The 1920s was a time when Henry Ford was incredibly wealthy. Classic story. He started off as a simple Michigan farm boy, started tinkering. And then in 1908, he created the Model T, the first ever affordable mass-produced car, which made him incredibly rich. But it also reshaped America in the process. He decided that well-paid workers weren’t going to quit, so he brought in higher wages. He also brought in the eight-hour workday.</p>\n<p><strong>Johanna: </strong>It’s funny, I was just talking last weekend with my partner about Ford a little bit, where we were like, he is the reason that we have a car-centric society. But he was surprisingly good to his workers. Complicated figure.</p>\n<p><strong>Elah: </strong>He started off good to his workers. We’ll get there. But in the late 1920s, Ford, despite all of his wealth, he was forced to cave on a couple of pretty big things. He was forced to finally update his cars after years of resisting even a simple color change. Even more humiliating, a defamation suit forced him to apologize to Jewish people, which was very difficult for him because he loved talking about Jews before that. So in the late ’20s, Ford was realizing he was not all-powerful. But then in 1927, an incredible opportunity presented itself. A real chance to enact his vision of society, maybe without having to compromise this time. It was a place called Fordlândia in Brazil. And it didn’t quite make the biography on the Ford website for reasons that I think will soon become clear.</p>\n<p>I’m Johanna Mayer, and this is <em>Atlas Obscura</em>.</p>\n<p>And I’m Elah Feder. And today, the story of Fordlândia, Henry Ford’s attempt to build a wholesome Midwestern town in the Amazon rainforest.</p>\n<p><em>This is an edited transcript of the </em><a href=\"https://www.atlasobscura.com/podcast\"><em>Atlas Obscura Podcast</em></a><em>: a celebration of the world’s strange, incredible, and wondrous places. Find the show on </em><a href=\"https://go.skimresources.com/?id=89027X1542228&amp;isjs=1&amp;jv=15.7.1&amp;sref=https%3A%2F%2Fwww.atlasobscura.com%2Farticles%2Fpodcast-montezuma-well&amp;url=https%3A%2F%2Fpodcasts.apple.com%2Fus%2Fpodcast%2Fthe-atlas-obscura-podcast%2Fid1555769970&amp;xs=1&amp;xtz=300&amp;xuuid=f238828fc9c8f1386593b6f8b1d81e7b&amp;xjsf=other_click__contextmenu%20%5B2%5D\"><em>Apple Podcasts</em></a><em>, </em><a href=\"https://open.spotify.com/show/0s0c4Z99PwbW8efTmHckyT\"><em>Spotify</em></a><em>, and all major podcast apps.</em></p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/105971/image.jpg\" width=\"auto\" /></figure>\n<p><strong>Johanna: </strong>Okay, I am intrigued. Why the rainforest? Why did Ford decide to build his vision of utopia in the Amazon rainforest?</p>\n<p><strong>Elah: </strong>So, it didn’t start out with a town. It started with rubber. So, as you know, cars need rubber for tires, for hoses. Today, most rubber is synthetic. The 1920s, it pretty much all came from rubber trees.</p>\n<p><strong>Johanna: </strong>So I did know this, and I can picture a rubber tree, which I think has a lot of big roots and like a wide trunk and stuff.</p>\n<p><strong>Elah:</strong> Massive.</p>\n<p><strong>Johanna:</strong> But I have never understood exactly how you get rubber from these trees.</p>\n<p><strong>Elah:</strong> It’s not too complicated. Ancient Mesoamericans figured this out. All you need to do is injure the tree.</p>\n<p><strong>Johanna:</strong> It saps it out?</p>\n<p><strong>Elah:</strong> It’s not technically sap. It’s another substance that oozes out of the tree. It kind of looks like coconut milk. It’s sticky and white and full of defense compounds. And that substance is called latex. So if you peel the bark of a rubber tree and let the latex drip out into a bucket, and then you dry it out, you get this bendy, bouncy material that we call rubber. So Ford decides he’s going to grow these rubber trees where they came from: the Amazon rainforest in Brazil.</p>\n<p><strong>Johanna: </strong>Seems like a solid plan.</p>\n<p><strong>Elah:</strong> It does seem that way. I should say it wasn’t actually Ford’s idea. He was actually being courted pretty aggressively by Brazilians. There was a Brazilian diplomat who really wanted to bring Ford to Brazil. There was a wealthy Brazilian businessman. And the idea was that bringing Ford, this wealthy industrialist, could potentially revive a really impoverished region, the northeast of Brazil.</p>\n<p>Ford very quickly agreed, and the company acquired 2.5 million acres of land, which they called Fordlândia. So, Fordlândia was on the east side of the Tapajos River, which is a tributary of the Amazon. This land is really deep in the rainforest. There were no roads, no railways. It took about 18 hours by boat to get there from the nearest city.</p>\n<p>So just imagine your classic kind of jungle. Towering trees, thick vines, tons of insects, birds, thousands of species, and, of course, rubber trees.</p>\n<p><strong>Johanna:</strong> Okay, goal is to create a rubber plantation. Makes sense to go to the Amazon. The part that I’m snagging on is the Midwestern town aspect.</p>\n<p><strong>Elah:</strong> Right.</p>\n<p><strong>Johanna:</strong> How does that come in?</p>\n<p><strong>Elah:</strong> So, a plantation obviously doesn’t run itself. It needs people. You need people to tap the trees, harvest the rubber. And then you need other people to feed those people, provide medical care. If you have families coming with the workers, then you’re going to need schools. You might need entertainment. You really need a whole town.</p>\n<p>And at Fordlândia, that’s what Ford created. Although not Ford himself, Ford didn’t go to Brazil. He had a crew of Ford company men who were dedicated to making this place according to Henry Ford’s vision.</p>\n<p><strong>Johanna:</strong> It’s how it usually goes.</p>\n<p><strong>Elah:</strong> So the town itself, it took a little bit of time to build. People started showing up well before there was a town. People who needed work came, and they brought their families. So they needed a place to live. They slapped together temporary shelters using planks from packing crates for walls and palm leaves for roofs.</p>\n<p>But within a couple of years, there was the start of a recognizable American-style town. They had a power plant, a hospital, a neighborhood with wooden houses with sidewalks and street lamps. A little later would come tennis courts, a dance hall, a movie theater, a golf course.</p>\n<p>But this was not just a lovely oasis in the Amazon. Because Henry Ford was a man with very particular ideas about how a society should be run. So increasingly, as he got older, he had this nostalgia for his old pastoral life. But at the same time, he hated cows.</p>\n<p><strong>Johanna:</strong> What’s wrong with cows?</p>\n<p><strong>Elah:</strong> Well, he thought they were very crude and inefficient machines. And he thought—</p>\n<p><strong>Johanna: </strong>Was—</p>\n<p><strong>Elah:</strong> Sorry, go ahead. I don’t think he was vegetarian.</p>\n<p><strong>Johanna:</strong> That’s what I was going to ask, yeah.</p>\n<p><strong>Elah:</strong> But he was a big fan of soy.</p>\n<p><strong>Johanna:</strong> Okay.</p>\n<p><strong>Elah: </strong>One time he built a full soy body. He had a suit made out of soy fibers.</p>\n<p><strong>Johanna:</strong> This is a whole other podcast episode.</p>\n<p><strong>Elah:</strong> The cow thing kind of threw me for a loop. But some of his ideas were actually really good. Like we mentioned, he thought people should be well paid, shouldn’t work super long hours. He also thought it was important that people be healthy. So he didn’t think they should drink or smoke. But he took this wholesome lifestyle thing a little far. He thought, for example, that dancing was good, but should not involve too much touching.</p>\n<p><strong>Johanna:</strong> No sexy dancing allowed.</p>\n<p><strong>Elah:</strong> Yes. Too many people were sexy dancing, which he blamed on Jewish people. So …</p>\n<p><strong>Johanna: </strong>What?</p>\n<p><strong>Elah:</strong> You’re welcome for that. I’m sure a lot of us have our own idiosyncratic spin on what makes a good life. The difference between Henry Ford and most of us is that he actually had the power to make his vision happen, to fashion a world in his image. This is not necessarily a good power for everyone to have.</p>\n<p>Henry Ford didn’t just encourage good habits and provide healthy food to his workers. He forced these things on them, not just in Fordlândia, but in all of his facilities. But as you can imagine, workers in the Amazon did not get the royal treatment.</p>\n<p>They were supposed to eat Henry Ford prescribed healthy meals at the company mess hall. They had to report any sexually transmitted infections to the company or risk getting caught at random STI inspections. They were not allowed to drink. A team of men would actually do spot searches of people’s homes and confiscate any alcohol that they found.</p>\n<p><strong>Johanna:</strong> It strikes me that this may not be the best route to creating the utopian society that you desire. The difference between Ford’s utopian society, Fordlândia, and a lot of other ones that come up throughout history is that in other utopian societies, people are signing up. They’re actively joining them of their own volition because they supposedly believe in some sort of common vision. Not the case here.</p>\n<p><strong>Elah: </strong>People just came to make rubber and get a paycheck. They did not come to have every aspect of their lives controlled. There were also unique challenges in the Amazon that Ford’s men did not anticipate. It turns out that you cannot just build an American town exactly as it is in America, wherever you want.</p>\n<p><strong>Johanna:</strong> Wait, you can’t?</p>\n<p><strong>Elah: </strong>Yeah. Revise life plan. For example, the houses that they had built. People were used to these houses with dirt floors and thatched roofs. These new houses had concrete floors and metal roofs. It impressed the journalists that visited, but they were unbearably hot in this climate. You do not want to be cooking under a metal roof, and you want good airflow. The Ford company provided free medical care for the workers, at least.</p>\n<p><strong>Johanna: </strong>Sounds good.</p>\n<p><strong>Elah:</strong> Despite that, a lot of people died. It is hard going in the Amazon. Both the American families and the Brazilian workers, a lot of people died of tropical diseases. People were being bitten by vipers when they were trying to clear jungle. This one guy whose job was to saw timber, he ended up preparing a lot of the wood they needed for coffins. He estimated they were averaging a death a day.</p>\n<p>In 1930, so just two years into the project, frustrations were at an all-time high. Ford’s men were also realizing that they weren’t really doing a good job of keeping people in line. In December of that year, 1930, one of Ford’s officials decides they need to make a change. Ford, as you know, wanted people to eat healthy. Apparently, he prescribed that people eat oatmeal and canned peaches for breakfast.</p>\n<p><strong>Johanna: </strong>That sounds good.</p>\n<p><strong>Elah: </strong>And rice and whole wheat bread for dinner. But—</p>\n<p><strong>Johanna:</strong> Sounds less good.</p>\n<p><strong>Elah: </strong>People wanted to eat whatever they wanted. And so they were getting food elsewhere. And this Ford employee decided that the solution was to feed them food from the cafeteria and deduct it from their wages.</p>\n<p>And that is when people snapped. It started when a guy named Manuel Caetano de Jesus, who was a brick mason, he decided to confront a payroll worker in the dining hall. And Manuel was yelling at him in Portuguese, which apparently this guy did not understand. But then Manuel hands him his badge, which he did understand. And this payroll worker’s reaction is to laugh.</p>\n<p>And that’s when the whole place erupts. People are suddenly smashing plates, pots, sinks, and they go and find all the Ford cars and smash them up. According to one person who was there, people started chanting “Brazil for Brazilians, kill all the Americans.” This was a massive riot across Fordlândia. And by the time that things calm down, the place is basically in ruins.</p>\n<p><strong>Johanna:</strong> Is that it? Is that the end of Fordlândia?</p>\n<p><strong>Elah:</strong> Weirdly not. Somehow.</p>\n<p><strong>Johanna:</strong> Incredible.</p>\n<p><strong>Elah:</strong> Yeah. So they end up firing most of the workers, but keep a skeleton crew and start to rebuild. And a few years later, they end up acquiring another plot of land nearby and building a second town and more plantations. And Fordlândia chugs along. The bigger problem, at least for the Ford company, is not that the workers hate them. It’s that Fordlândia isn’t actually doing the one thing it’s supposed to do, which is produce rubber.</p>\n<p><strong>Johanna: </strong>God, this has been such a journey, I forgot that they were supposed to be producing rubber this whole time.</p>\n<p><strong>Elah: </strong>That was the point of all of this. So it does take time, right? And they’d had many false starts. You know, they planted trees in the dry season. That didn’t work well. But eventually they get it together. And by 1940, they have three million trees planted across 30,000 acres of land.</p>\n<p><strong>Johanna:</strong> Whoa.</p>\n<p><strong>Elah:</strong> But here’s the thing. It turns out Brazil is not actually the best place to grow Brazilian rubber trees.</p>\n<p><strong>Johanna: </strong>What?</p>\n<p><strong>Elah: </strong>Because Brazil, the place the trees are native to, also has all of the trees’ natural enemies.</p>\n<p><strong>Johanna:</strong> Ah, interesting.</p>\n<p><strong>Elah: </strong>When trees are scattered throughout a forest, the trees manage to grow okay. But then imagine you are a rubber tree-eating bug or fungus, and you come upon all of these rubber trees jam-packed together in one place. You are going to come out and feast. You’re going to reproduce. You’re going to hop from tree to tree. It’s a massive buffet.</p>\n<p><strong>Johanna:</strong> Like, here we are!</p>\n<p><strong>Elah:</strong> Yeah. So by 1940, 70 percent of Fordlândia’s rubber trees were infected with a fungal blight. They get through that. But then in 1942, they’re hit with caterpillars.</p>\n<p><strong>Johanna:</strong> Dun, dun, dun.</p>\n<p><strong>Elah:</strong> I mean, caterpillars had always been a problem. But for a few years, the workers managed to keep them at bay. But in 1942, there is a total caterpillar explosion that they just can’t keep up with. And just as the situation was starting to get under control, they were hit with a second wave of fungal blight. And combined, it’s a pretty fatal blow. And just a few years later, in November of 1945, the company decides it is time to abandon this project. Apparently, they did not give the local workers much notice. Many Brazilians didn’t even know the Americans were leaving until the day they got on the ships. And that was how they found out they were unemployed.</p>\n<p><strong>Johanna: </strong>Oh, my God.</p>\n<p><strong>Elah: </strong>Yeah. By this point, Ford himself was over 80. He wasn’t doing well. And two years later, he died.</p>\n<p><strong>Johanna: </strong>You said that they just picked up and left and got on ships. What happened to the town? Are the buildings still there? Does anyone still live there? What happened to Fordlândia? <strong>Elah:</strong> So a lot of the story I’ve told you is based on a book by Greg Grandin called <a href=\"https://us.macmillan.com/books/9780312429621/fordlandia/\"><em>Fordlandia</em></a>, which came out in 2009. When he visited, a lot of the old structures were there. The old factory buildings, the sawmill, the warehouse, they’re kind of falling apart but standing. And a few of the old houses were there, too, apparently full of bats and just covered in guano.</p>\n<p>And back when Greg Grandin visited, one of the main sources of income was cattle ranching. Apparently, there were cows grazing on the old golf course. The old tennis courts had been turned into cattle stalls. And the hillsides that used to be planted with rubber trees were turned into pasture land for cows.</p>\n<p><strong>Johanna: </strong>Yes, justice for the cows. This was a totally fascinating story, Elah. Thank you.</p>\n<p><strong>Elah: </strong>Thanks for having me, Johanna. The town of Fordlândia is still around. And since Greg Grandin’s visit, it’s had a bit of a resurgence. An estimated 3,000 people live there. There’s now a tall Catholic church, a guest house, a bar, a restaurant. And scattered throughout, crumbling remains of Henry Ford’s failed American town.</p>\n<p><strong><em>Listen and subscribe on</em></strong><a href=\"https://podcasts.apple.com/us/podcast/the-atlas-obscura-podcast/id1555769970\"> <strong><em>Apple Podcasts</em></strong></a><strong><em>,</em></strong><a href=\"https://open.spotify.com/show/0s0c4Z99PwbW8efTmHckyT\"> <strong><em>Spotify</em></strong></a><strong><em>, and all major podcast apps.</em></strong></p>\n<p><em>Our podcast is a co-production of Atlas Obscura and Stitcher Studios. The people who make our show include Dylan Thuras, Doug Baldinger, Kameel Stanley, Johanna Mayer, Manolo Morales, Amanda McGowan, Alexa Lim, Casey Holford, and Luz Fleming. Our theme music is by Sam Tyndall.</em></p>",
        "source": "www.atlasobscura.com",
        "published": "Tue, 27 Jan 2026 17:15:00 -0500",
        "fetched_at": "2026-02-28T23:18:43.753478Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 9
          },
          {
            "name": "scale_shift",
            "score": 4
          }
        ],
        "structural_score": 13,
        "timeliness_score": 3,
        "final_score": 8.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.atlasobscura.com/articles/odilia-alvarado-kissimmee",
        "title": "How La Mexicana Became a Kissimmee Institution",
        "summary": "<p>Though Odilia Alvarado is responsible for 80 employees, the first people she attends to in the mornings are her children. Every day by 8:00 a.m., she drops her 8-year-old daughter and 10-year-old son off at school. Then it’s off to La Mexicana Restaurant, or the nearby affiliated bakery, for breakfast service.</p>\n<p>In the last three decades, Odilia has helped her mother, father, siblings, aunts, and uncles, build a series of Mexican food businesses that have taken Central Florida by storm, usually under the moniker “La Mexicana.” In 2011, she and her husband struck out on their own and opened the first Kissimmee outpost of La Mexicana. Today, she runs a restaurant, supermarket, tortilleria, bakery, and ice cream shop in Kissimmee that can barely keep up with demand for their delicious treats.</p>\n<p>If you ask Odilia, she’ll attribute her success to her faith in God, and her tight family that has supported her every step of the way. Her dedication to perfecting dishes inspired by the southwestern region of Mexico hasn’t hurt, either.</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106355/image.jpg\" width=\"auto\" /></figure>\n<h3 class=\"article-second-subheading-pre-rd\">From the Mountains of Mexico to Central Florida</h3>\n<p>Odilia Alvarado spent her early childhood in the town of Tenanguillo de las Cañas in the mountainous state of Guerrero. To buy groceries or clothing in the larger town nearby, her family would travel by car down a dirt road that took 20 minutes to traverse. But the rural setting came with upsides too, like the widespread practice of home-growing fresh herbs and vegetables, which Odilia believes is a big part of what makes the region’s cuisine so special.</p>\n<p>Odilia also draws inspiration from her grandmother, Angela Guadarrama Millan: a prodigious cook who supplied many of the recipes that made La Mexicana locally famous.</p>\n<p>She remembers hiking up rocky mountains with her grandmother to reach her vegetable patch, where she cultivated beans. Angela would harvest the beans, clean them, cook them, and grind them down in a molino, a mortar and pestle. She would then stuff the ground beans into homemade corn dough that she would toast on a comal, a traditional Mexican griddle, to make gorditas. The gorditas, plus a homemade salsa picante made from tomatillos and dried chiles de arbol would make up many of their meals.</p>\n<p>“We would eat really good,” said Odilia. “That’s all we’d eat, mainly.” They would also have the occasional bean soup, flavored with the medicinal-tasting epazote herb and lapped up with tortillas.</p>\n<p>When Odilia was around six years old, her mother, Paulina Cervantes, and father, Alejandrino Honorato Guadarrama, left their hometown to stake out a home for the family in the United States. Odilia, the second-oldest and the only girl among eight children, spent a year living with her grandmother and her older brother. A year later, Odilia’s parents brought Odilia and her older brother to Apopka, Florida. Odilia remembers being happy to be reunited with her parents, and the world taking on a sheen of novelty.</p>\n<p>Odilia was seven when she arrived, and she initially struggled in her new school, where there was limited support for Spanish-speaking students. But she soon transferred to a school with a bilingual education program. “My brain just started to pop up,” she remembers. She started soaking up English and getting good grades. In her first year in the new school, she made honor roll and won a trip to Disney World, an experience that she describes as “magical.”</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106300/image.jpg\" width=\"auto\" /></figure>\n<h3 class=\"article-second-subheading-pre-rd\">The Birth of a Restaurant Family</h3>\n<p>When Odilia was twelve, she and Paulina started cooking tacos de barbacoa, a slow cooked, richly-spiced shredded beef, on weekends. Odilia was in charge of making the tortillas by hand with a mechanical stamp. They would sell them to her Alejandrino’s colleagues at his job at a greenhouse. The tacos were a hit, and customers started asking for Paulina to bring the tacos to their soccer and basketball games.</p>\n<p>When Odilia was 14, the Alvarados opened a brick-and-mortar taqueria called La Mexicana in a plaza in Apopka. It was a family business: Odilia’s mother and grandmother prepared the meat, Alejandrino and Odilia’s uncle made the tortillas, and Odilia would chop the garnishes before preparing the tacos with her cousins.</p>\n<p>People clamored for their carnitas, carne asada, pollo, and, most of all, Odilia’s grandmother’s adobada, pork chunks marinated in a complex, spicy red sauce. “We had lines and lines of people waiting for the food, for the tacos,” Odilia says.</p>\n<p>From that first taquería, the Alvarado family sprang a bunch of other iterations of La Mexicana across Central Florida. Different branches were operated by different family members who would work closely together, and it expanded to encompass tortilla-making, baking, ice cream, and Mexican groceries. Odilia worked hard alongside her parents and brothers. Along the way, she discovered that she loved cooking. “Even when I was making the tortillas,” Odilia says, she was in her happy place. Today, she cooks dinner for her husband and kids after work, often inspired by videos on Facebook and Instagram that advertise the dishes in restaurants and Mexican pueblos. She says it’s worth it to cook for her family, even though she owns a restaurant that could easily supply them with cooked meals. “When you see them eat and they like your food,” she says, “I feel more happy.”</p>\n<p>In 2011, Odilia and her husband were working together with Odilia’s mother, father, and two brothers in the family’s Orlando location. “We didn’t fit there anymore,” Odilia says. Her younger brother came across a space for rent in a shopping plaza in Kissimmee, but he didn’t yet have the money for it. “You go—you try over there,” he told his sister.</p>\n<p>Odilia and her husband opened up the Kissimmee branch of La Mexicana in December 2011. Their original plan was to open a taquería, but Odilia’s father Alejandrino said that they should take a shot at opening a supermarket and a restaurant, like they had opened in Orlando. She was intimidated, but he encouraged them. “If you’re going to go for it, go for something big. You don’t go for something small,” Odilia remembers Alejandrino telling her.</p>\n<p>So Odilia went for it, opening a supermarket with a small restaurant in a 2,000-square-foot space.</p>\n<p>“We were scared at the beginning, because we were starting to struggle,” she remembers. The first two years were rough. Odilia and her husband would do much of the cooking themselves, and would often spend their entire days in the restaurant.</p>\n<p>Odilia emerged from the first hard years, and eventually was able to expand the supermarket, and open a tortilleria and bakery.</p>\n<p>Odilia took her creative leap with the opening of a large, colorful, full-service sit-down restaurant a few years ago. This time, she didn’t need any convincing from her father: she and her husband spearheaded the process from start to finish. She drew a sketch of what she wanted the restaurant to look like, and handed it to an architect. She and her husband sourced decorative animals and hand-carved tables from Mexico. The space is playful and colorful with an emphasis on the natural world, because “it brings you back to Mexico” and evokes fresh, natural food.</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106301/image.jpg\" width=\"auto\" /></figure>\n<h3 class=\"article-second-subheading-pre-rd\">Secrets to Success</h3>\n<p>Over the years, Odilia’s father has developed a few rules to success. “Clean area, good service, and good food. The three main things that we always keep in mind,” Odilia says. She and her staff remember customers’ names and make sure to always be friendly.</p>\n<p>As for the food, she uses recipes from the matriarchs that form the backbone of the La Mexicana empire. She helps create the restaurant’s menu, but she isn’t usually in the kitchen cooking for customers. Her kitchen prepares nopales, or prickly pear cactus, according to her grandmother’s method; and a healthy green juice according to her mother’s recipe that also includes its fair share of nopal.</p>\n<p>The restaurant serves a wide range of Mexican dishes, from rich soups to crispy tacos. Many of them have their roots in Guerrero, such as their golden-fried quesadillas and their green and red salsas. The tacos de birria, a choice of goat or beef stewed in a rich consummé, are a customer favorite.</p>\n<p>Odilia says that when it comes to her success, faith is a major factor. For as long as Odilia can remember, her family has believed that “if you have God in your life, you’re good,” she says. She keeps an image of the Virgin Mary in each of her businesses, to protect her family and bring them blessings.</p>\n<p>Odilia thanks her family for helping her achieve her goals. Her husband, whom she met when she served him at La Mexicana in Orlando, has been a constant support as well. He jokes that he picked the right wife—someone who could make his belly happy.</p>\n<p>But the truth is that he helps her, too. He takes initiative and is constantly strategic and ambitious about the restaurant. At the same time, he encourages Odilia’s ideas. If she and her partner did not have such good teamwork, “we would not have what we have,” she said.</p>\n<p>She is also thankful for the mentorship of her father and other family members. “I have learned a lot from my dad and my family,” she says. These lessons are “something that you want to pass on to your kids.”</p>\n<p>Odilia has seven children, and it seems as if her third child may follow in her footsteps and become an entrepreneur. “She looks like she wants to open her own business,” Odilia says. “It makes me very proud.”</p>",
        "source": "www.atlasobscura.com",
        "published": "Fri, 20 Feb 2026 12:36:00 -0500",
        "fetched_at": "2026-02-28T23:18:43.753444Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 7
          },
          {
            "name": "visibility_gain",
            "score": 4
          }
        ],
        "structural_score": 11,
        "timeliness_score": 3,
        "final_score": 7.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.atlasobscura.com/foods/nectar-soda",
        "title": "Nectar Soda",
        "summary": "<p><img alt=\"An Aglamesis nectar soda.\" height=\"200\" src=\"https://img.atlasobscura.com/gLqA8RaTQNIL0MupnRjPCWB4QRxXZdJs1eCFvMqaXY8/rs:fill:300:200:1/g:ce/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL3RoaW5n/X2ltYWdlcy80YTQw/MzA1NC04MjBhLTQw/MmEtYmU5My1iYWZi/YWU5ZGViNDc5Y2Rk/YjY1YjA4NGY1MmFm/YzRfQWdsYW1lc2lz/IG5lY3RhciBzb2Rh/IG9uIHRhYmxlIDIu/anBn.jpg\" width=\"300\" /></p> <p><span style=\"font-weight: 400;\">Though Cincinnati is best known for breweries, another effervescent beverage has a long history in the Queen City: the nectar soda.</span></p>\n<p><span style=\"font-weight: 400;\">Home to the oldest pharmacy college in the U.S. west of the Alleghenies, the</span><a href=\"https://lloydlibrary.org/research/archives/eclectic-medicine/\"><span style=\"font-weight: 400;\"> Eclectic Medical Institute</span></a><span style=\"font-weight: 400;\"> (1845-1952), and</span><a href=\"https://lloydlibrary.org/about/a-brief-history-of-the-lloyd-library-and-museum/\"><span style=\"font-weight: 400;\"> Lloyd Brothers Pharmacists</span></a><span style=\"font-weight: 400;\">, Cincinnati was long on the forefront of the pharmaceutical industry. The city had a number of apothecaries with soda fountains, as well as confectioners serving countless carbonated concoctions—some claiming to cure a variety of ailments, and others simply providing customers with something sweet and refreshing to drink.</span></p>\n<p><span style=\"font-weight: 400;\">Enter the nectar soda. The flavor is a combination of vanilla and bitter almond, and the drink is pastel pink in color—a nod to the hue of almond flowers, according to </span><a href=\"https://dannwoellertthefoodetymologist.wordpress.com/\"><span style=\"font-weight: 400;\">Dann Woellert</span></a><span style=\"font-weight: 400;\">, a Cincinnati food historian, etymologist, and the author of </span><a href=\"https://www.amazon.com/Cincinnati-Candy-History-American-Palate/dp/1467137952\"><em><span style=\"font-weight: 400;\">Cincinnati Candy: A Sweet History</span></em></a><span style=\"font-weight: 400;\">. Nicknamed the “</span><a href=\"https://www.proquest.com/hnpcincinnatienquirershell/historical-newspapers/august-2-1942-page-55-108/docview/1882746511/sem-2?accountid=39387\"><span style=\"font-weight: 400;\">drink of the gods</span></a><span style=\"font-weight: 400;\">,” the bitter almond flavor of nectar soda balances out what would otherwise be overly sweet vanilla, creating an addictive taste that grows on you with each sip. </span></p>\n<p><span style=\"font-weight: 400;\">Nectar sodas have been served in Cincinnati since at least the late 1870s, though, like many iconic foods and beverages, its precise origins are murky. The only other U.S. city to embrace nectar sodas was New Orleans, but unlike Cincinnati, the tradition fizzled out in the Big Easy in the mid-20th century. Plus, Woellert says that the Queen City popularized them first. “They were served in Cincinnati nearly a decade before New Orleans,” he says.</span></p>\n<p><span style=\"font-weight: 400;\">While the Cincinnati nectar soda has multiple origin stories, each crediting a different pharmacist or confectioner, Woellert has concluded that </span><a href=\"https://www.proquest.com/hnpcincinnatienquirershell/historical-newspapers/april-13-1947-page-98-151/docview/1882885311/sem-2?accountid=39387\"><span style=\"font-weight: 400;\">John Mullane</span></a><span style=\"font-weight: 400;\"> created the flavor after traveling to Quebec City to learn the art of confectionery from a prominent Canadian candymaker. He began serving nectar sodas in his confectionery shop in downtown Cincinnati in the late 1870s.</span></p>\n<p><span style=\"font-weight: 400;\">So, why did the nectar soda end up in Cincinnati and New Orleans, of all places? Wollert suspects that the bitter almond and vanilla flavor was used by the French Acadians who settled in both Quebec City and New Orleans.</span></p>\n<p><span style=\"font-weight: 400;\">Though nectar sodas aren’t as common as they were in the early 20th century, when they could be found at countless confectioneries and pharmacy soda fountains across Cincinnati, they’re still served at establishments throughout the city and the surrounding area. Nectar sodas have been on the menu at ice cream and chocolate shop </span><a href=\"https://www.aglamesis.com/\"><span style=\"font-weight: 400;\">Aglamesis Brothers</span></a><span style=\"font-weight: 400;\"> since it opened in Cincinnati in 1908, if not shortly thereafter. That’s according to company president and CEO Randy Young, who is also a third-generation family member. </span></p>\n<p><span style=\"font-weight: 400;\">It’s unclear when nectar sodas were added to the </span><a href=\"https://digital.cincinnatilibrary.org/digital/collection/p16998coll32/id/2220/rec/19\"><span style=\"font-weight: 400;\">menu</span></a><span style=\"font-weight: 400;\"> at </span><a href=\"https://www.graeters.com/\"><span style=\"font-weight: 400;\">Graeter’s</span></a><span style=\"font-weight: 400;\">, a Cincinnati ice cream and chocolate shop that opened in 1870 and now has locations throughout the city and the Midwest, but Chip Graeter, chief of retail operations and a fourth-generation family member, says that they were especially popular throughout the 1940s, 1950s and 1960s.</span></p>\n<p><span style=\"font-weight: 400;\">In a </span><a href=\"https://www.proquest.com/hnpcincinnatienquirershell/historical-newspapers/january-28-1947-page-2-26/docview/1882876222/sem-2?accountid=39387\"><span style=\"font-weight: 400;\">January 28, 1947 article</span></a><span style=\"font-weight: 400;\"> in the </span><em><span style=\"font-weight: 400;\">Cincinnati Enquirer</span></em><span style=\"font-weight: 400;\">, Tom Moore, the head of the soda department at Dow Drug Store—which operated 32 soda fountains throughout the metropolitan area at that time—said that “nectar is one of the most popular flavors in all of their stores, and has been for many years.” Five years prior, </span><a href=\"https://www.proquest.com/hnpcincinnatienquirershell/historical-newspapers/august-16-1942-page-63-99/docview/1882739776/sem-2?accountid=39387\"><span style=\"font-weight: 400;\">Dow ran an ad</span></a><span style=\"font-weight: 400;\"> in the same newspaper which read: “Be glad you live in Cincinnati, the only place in the country where you can enjoy a Dow double-dip nectar soda.”</span></p>\n<p><span style=\"font-weight: 400;\">Originally, nectar syrup was made by combining half-and-half or milk with water, bitter almond extract, vanilla extract and red food coloring. While Aglamesis eventually switched to a dairy-free shelf-stable syrup, Graeter's recipe has never changed—it still contains milk and needs to be refrigerated. </span></p>\n<p><span style=\"font-weight: 400;\">Both Aglamesis and Graeter’s make nectar soda by mixing nectar syrup with a dollop of whipped cream, adding a scoop or two of vanilla ice cream, then topping it off with some soda water and more whipped cream.</span></p>\n<p><span style=\"font-weight: 400;\">Though Young says that nectar sodas are most popular with older adults, they’re also a hit with members of younger generations who try them. “People who grew up with them still love them today,” Graeter says. “We still make them in all of our stores, but they're not nearly as popular today as they once were, simply because milkshakes and smoothies have taken over.”  </span></p>\n<p><span style=\"font-weight: 400;\">According to Young, there is a commercially available descendant of </span><a href=\"https://www.coca-cola.com/us/en/brands/barq-s\"><span style=\"font-weight: 400;\">the nectar soda</span></a><span style=\"font-weight: 400;\">. “Commercial soda companies like Barqs and others came out with their version of cream soda—a bright pink soda—which got its flavoring from nectar soda,” he explains.</span></p>",
        "source": "www.atlasobscura.com",
        "published": "Tue, 03 Dec 2024 11:00:00 -0500",
        "fetched_at": "2026-02-28T23:18:43.753503Z",
        "tags": [
          {
            "name": "transformation",
            "score": 3
          },
          {
            "name": "boundary_crossing",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 4
          }
        ],
        "structural_score": 11,
        "timeliness_score": 3,
        "final_score": 7.0,
        "reddit_score": null,
        "reddit_comments": null,
        "temp_score_struct": 8.6,
        "temp_score_trend": 5.3999999999999995
      },
      {
        "url": "https://www.atlasobscura.com/places/london-2012-olympic-truce-wall",
        "title": "London 2012 Olympic Truce Wall in Lausanne, Switzerland",
        "summary": "<p><img alt=\"Spanish artist Rosa Serra's bronze dedicated to the Olympic Truce, found in the park outside the Museum.\" height=\"200\" src=\"https://img.atlasobscura.com/QvWycTgqMGYOBLW5aDMtIZcspKESMtd1BSa7-EyVnaA/rs:fill:300:200:1/g:ce/c:896:597:nowe:1085:2426/q:81/sm:1/scp:1/ar:1/aHR0cHM6Ly9hdGxh/cy1kZXYuczMuYW1h/em9uYXdzLmNvbS91/cGxvYWRzL3BsYWNl/X2ltYWdlcy8yYzE5/YTllOC1kMzEzLTQ0/ODYtYTJiZi1iOWFh/ZjUyOTdhMDFjNmVk/NWM2ZjQ2NTI1ZmY4/MWVfb2x5ICg2KS5q/cGc.jpg\" width=\"300\" /></p> <p>Watching the Olympic Games, it may often feel like they are as much about pomp and ceremony as they are about sports. The International Olymic Committee (IOC) has a long list of protocols that need to take place during opening and closing ceremonies, the playing of the Olympic and National Anthems and lighting of the flame, for example; and medal ceremonies have similar requirements. Looking at the Games of the 21st century, it becomes clear that many of these traditions have their origins in the birth of the modern Games in 1896 Europe, a context of chivalrous ideals marred by racist notions of superiority; while other traditions can be traced further back to the original games in Ancient Greece, where religion and ritual were key components of the celebrations.</p>\n<p>In 1992, a previously-abandoned tradition of the Games was revived, at least in theory: the Olympic Truce. Originally known as ekecheiria, it was established by rulers of Greek city-states in the ninth century BCE to try and guarantee safety from conflict for participants and spectators to the Olympiads. The following year, the United Nations supported this revival, asking that it would take place from one week before the start of the Olympic and Paralympic Games to one week after their conclusion. The 1994 Lillehammer Winter Games were the first for which the President of the UN General Assembly requested the observance of an Olympic Truce. Since Nagano 1998, the UN's Secretary-General has also joined the call for this Truce.</p>\n<p>With the \"homecoming\" Athens 2004 Summer Games, the Truce started being represented by a wall or mural, a physical installation in the Olympic Village, on which athletes, volunteers and occasionally the general public could leave messages and dedications aspiring to the Truce's ideal of peace. These Truce Walls have featured various materials, from tiles in Rio 2016 to wood for Tokyo 2020. After the respective Games and Truce, the wall/mural is left to the host city, some of which have repurposed the materials, kept the objects in storage or displayed them in local museums.</p>\n<p>A partial exception can be found in the Olympic Museum in Lausanne. Some of the panels for the London 2012 edition, in which the Truce Wall consisted of translucent acrylic \"totems\", are on display in the Swiss city known as the Olympic Capital due to its hosting of the IOC. Heavily-signed, these panels center a section of the Museum dedicated to the Olympic Truce, which is also represented in the Museum's gardens with a sculpture by artist Rosa Serra from Spain. The revived Olympic Truce is non-binding, and has therefore been broken a few times, specially by Russia in the context of the Crimea conflict. The Russian invasion of Ukraine in 2022 coincided with the Beijing Winter Olympics, representing the clearest violation of the Truce as of the time of writing.</p>",
        "source": "www.atlasobscura.com",
        "published": "Wed, 25 Feb 2026 10:00:00 -0500",
        "fetched_at": "2026-02-28T23:18:43.753439Z",
        "tags": [
          {
            "name": "transformation",
            "score": 6
          },
          {
            "name": "boundary_crossing",
            "score": 4
          }
        ],
        "structural_score": 10,
        "timeliness_score": 3,
        "final_score": 6.5,
        "reddit_score": null,
        "reddit_comments": null,
        "temp_score_struct": 7.9,
        "temp_score_trend": 5.1
      },
      {
        "url": "https://www.atlasobscura.com/articles/visiting-every-museum-in-new-york-city-a-q-a-with-jane-august",
        "title": "Visiting every museum in New York City",
        "summary": "<p>Jane August has made it her mission to visit every museum in New York City and five years in, she’s still discovering new ones. What began as a pandemic-era way to leave the house has turned into a sprawling, spreadsheet-powered project that’s connected her to hidden institutions, museum professionals, and a growing community of fellow culture lovers. Known as \"the museum girl\" among her fans, August documents her explorations across multiple <a href=\"https://www.janeaugust.co/every-museum-in-nyc\" rel=\"noopener noreferrer\" target=\"_blank\">social channels,</a> where she has amassed thousands of followers, and has even launched a <a href=\"https://podcasts.apple.com/us/podcast/the-next-stop-is-with-jane-august/id1740787173\" rel=\"noopener noreferrer\" target=\"_blank\">podcast.</a></p>\n<p>Atlas Obscura Executive Editor Emma Patti spoke with August about how the quest began, what’s surprised her most, and how to explore New York like a museum insider.</p>\n<p><strong>Atlas Obscura: </strong>How did this quest to visit every museum in New York City even begin?</p>\n<p><strong>Jane August:</strong> I was furloughed during the pandemic. I work in live music, bars, and venues, and suddenly all of that stopped. In the fall of 2020, some friends and I went to the Brooklyn Museum, because museums were really the only cultural spaces that had reopened.</p>\n<p>By that winter, I was like, I need to leave my house. I need to do <em>something</em> this year. All the things I usually did—shows, parties, places where people gather—weren’t options. Museums were one of the only places you could go alone and still feel like you were doing something meaningful.</p>\n<p>I thought, “There can’t be that many museums. Maybe I’ll visit them all and be done in a year or two.” That was five years ago.</p>\n<p><strong>AO:</strong> Were you surprised by how long it’s taken?</p>\n<p><strong>August:</strong> Completely. I originally thought there were maybe 150 or 160 museums in the city. I’m at about 150 visited now, so I <em>should</em> be done.</p>\n<p>But museums keep appearing. Some come out of the woodwork and say, “We don’t really post online—we’re kind of a secret museum.” Others reopen, or I’m still trying to figure out if they even exist. I’m emailing board members and stalking LinkedIn trying to confirm whether a place is real or permanently closed. The spreadsheet keeps growing.</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106345/image.jpg\" width=\"auto\" /></figure>\n<p><strong>AO:</strong> When you started, did you imagine this would turn into such a public project?</p>\n<p><strong>August:</strong> Not at all. Like everyone else in 2020, I was playing around on TikTok. I realized people liked New York City content, and I thought maybe some people would be interested in this project.</p>\n<p>I didn’t expect it to become my identity. I didn’t expect to be introduced as “the museum girl,” or for museum-going to become part of my brand. That part really surprised me.</p>\n<p><strong>AO:</strong> Do you visit museums outside New York the same way?</p>\n<p><strong>August:</strong> Not on this scale. When I travel, I go to museums I <em>want</em> to see. I don’t feel obligated. That’s actually when I enjoy museums the most—when I’m not thinking about how I’ll document it or explain it to other people.</p>\n<p><strong>AO:</strong> After visiting so many museums, do you have favorites?</p>\n<p><strong>August:</strong> Picking favorites is hard when you’ve been to so many. But the ones I return to a lot include Poster House—it wasn’t even on my radar at first, and now I take everyone there.</p>\n<p>I love the Museum of the City of New York and New-York Historical Society. I realized early on that I like history museums more than art museums. I just love learning things.</p>\n<p>The Museum of the Moving Image is a favorite, especially for film and TV. I also love the Nicholas Roerich Museum, the Transit Museum, the Red Hook Pinball Museum, and the Brooklyn Seltzer Museum.</p>\n<p>And then there are the big ones—the Guggenheim, the Whitney—where I now sometimes get to experience them when they’re empty or after hours. That still feels surreal.</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106341/image.jpg\" width=\"auto\" /></figure>\n<p><strong>AO:</strong> Have any museums totally surprised you?</p>\n<p><strong>August:</strong> Definitely. The Maritime Industry Museum at Fort Schuyler was a big one. There were no photos online, and it took me over two hours to get there. I thought, “If this is one small room, I’m going to be devastated.”</p>\n<p>But it was huge. We got lost inside. It’s in a fort and covers every nautical thing you can imagine. My parents work in the maritime industry, so it was especially meaningful.</p>\n<p>I was also surprised by the New York Sign Museum, which is inside an operating sign shop, and by the Salvador Mundi Museum in Brooklyn. That one really made me think about what <em>counts</em> as a museum—it has a gift shop, a café, rotating exhibits, and events, just scaled way down. It’s almost conceptual art about museums themselves.</p>\n<p><strong>AO:</strong> How do you keep track of all this?</p>\n<p><strong>August:</strong> I have a very intense spreadsheet. I studied stage management in college, so spreadsheets are my love language.</p>\n<p>It tracks every museum, when it’s open, the neighborhood, whether I’ve contacted them, when I visited, who I went with, whether I’ve posted the video yet. Some entries are marked in red because they’re still a mystery: “Do they exist? Find out.”</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106342/image.jpg\" width=\"auto\" /></figure>\n<p><strong>AO:</strong> Do people send you tips now?</p>\n<p><strong>August:</strong> All the time. That’s how a lot of this has grown. Museum founders DM me, followers tell me about new openings, and organizations reach out when they start doing exhibitions.</p>\n<p>Sometimes I also just find museums by dragging around Google Maps. I’ll be walking to work and realize, “Wait—that’s a museum I didn’t know existed.” Then it goes on the list.</p>\n<p><strong>AO:</strong> Has this connected you to the museum world in unexpected ways?</p>\n<p><strong>August:</strong> Absolutely. I’ve met so many people in museum marketing, social media, and public engagement, and they all seem to move between institutions. Suddenly I’m being invited to places because I know someone from somewhere else.</p>\n<p>A lot of these people also have their own art practices or side projects, and I love being able to highlight that through my platform or my podcast.</p>\n<p><strong>AO:</strong> Speaking of which—how did your podcast come about?</p>\n<p><strong>August:</strong> I had a radio show in college, and I missed interviewing people. Through this museum project, I kept meeting fascinating people, but I only had a short window to tell their stories.</p>\n<p>The podcast lets me expand beyond museums. I’ve had theater people, musicians, authors—people whose stories don’t fit neatly into one niche.</p>\n<figure class=\" contains-caption \"><img alt=\"article-image\" class=\"article-image with-structured-caption \" src=\"https://assets.atlasobscura.com/article_images/106344/image.jpg\" width=\"auto\" /></figure>\n<p><strong>AO:</strong> Any tips for visiting museums?</p>\n<p><strong>August:</strong> I go in completely blind. I don’t research much beforehand, and I like being surprised. I wander.</p>\n<p>My one consistent rule is: always go to the gift shop. I buy a postcard at every museum. I send one to my mom, and whoever I go with has to send one to me. If I go alone, I’ll mail one to myself.</p>\n<p>Postcards are my way of documenting what I’ve seen. I have a giant box full of them.</p>\n<p><strong>AO:</strong> If someone had one day to explore museums in a single New York neighborhood, where should they go?</p>\n<p><strong>August:</strong> Prospect Park and Crown Heights are great—you’ve got the Brooklyn Museum, the Botanic Garden, and Lefferts Historic House.</p>\n<p>The Lower East Side is another favorite. You can do the Tenement Museum, the International Center of Photography, and the new Automatic Photo Booth Museum, plus a bunch of smaller institutions nearby.</p>\n<p>Lower Manhattan is underrated for museums, especially National Park Service sites—and you can get Junior Ranger badges at any age, which I love.</p>\n<p>And Staten Island’s Snug Harbor is basically a museum campus with multiple institutions in one beautiful area.</p>\n<p>Honestly, museums are everywhere in New York. Even after five years, I’m still finding new ones.</p>\n<hr style=\"border: 1px solid black;\" />\n<p>Jane also appeared on the Atlas Obscura podcast. Listen to her episode here.</p>\n<p></p>",
        "source": "www.atlasobscura.com",
        "published": "Tue, 10 Feb 2026 08:00:00 -0500",
        "fetched_at": "2026-02-28T23:18:43.753459Z",
        "tags": [
          {
            "name": "transformation",
            "score": 3
          },
          {
            "name": "boundary_crossing",
            "score": 2
          },
          {
            "name": "scale_shift",
            "score": 4
          }
        ],
        "structural_score": 9,
        "timeliness_score": 3,
        "final_score": 6.0,
        "reddit_score": null,
        "reddit_comments": null,
        "temp_score_struct": 7.199999999999999,
        "temp_score_trend": 4.799999999999999
      }
    ],
    "bigtech": [
      {
        "url": "https://technode.com/2025/11/26/over-5000-global-attendees-celebrate-the-successful-debut-of-the-xin-summit-showcasing-the-next-generation-of-innovation-from-the-greater-bay-area-to-the-world/",
        "title": "Over 5,000 Global Attendees Celebrate the Successful Debut of the XIN Summit, Showcasing the Next Generation of Innovation From the Greater Bay Area to the World",
        "summary": "<figure><img alt=\"\" class=\"attachment-rss-image-size size-rss-image-size wp-post-image\" height=\"312\" src=\"https://i0.wp.com/technode.com/wp-content/uploads/2025/11/3.png?fit=556%2C312&amp;ssl=1\" width=\"556\" /></figure>The inaugural&#160;XIN Summit&#160;concluded on 16 November with a powerful debut presented by&#160;BEYOND Expo — Asia’s largest technology innovation and ecosystem event. Focused on&#160;AI Hardware Ecosystems and Frontier Technologies, the Summit connected&#160;Media Day, the 2025 “Next Star” Global Innovation Challenge Awards Ceremony, a two-day Innovation Summit, curated Innovation Exhibition, and high-efficiency investment matchmaking&#160;to demonstrate how technology, [&#8230;]",
        "source": "technode.com",
        "published": "Wed, 26 Nov 2025 01:51:46 +0000",
        "fetched_at": "2026-02-28T23:17:15.295091Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 7
          },
          {
            "name": "value_redefinition",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 16,
        "timeliness_score": 3,
        "final_score": 9.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://technode.com/2025/10/30/funflys-last-war-tops-global-mobile-game-revenue-chart-in-september-with-180-million-in-earnings/",
        "title": "Funfly’s Last War tops global mobile game revenue chart in September with $180 million in earnings",
        "summary": "<figure><img alt=\"\" class=\"attachment-rss-image-size size-rss-image-size wp-post-image\" height=\"491\" src=\"https://i0.wp.com/technode.com/wp-content/uploads/2025/10/last-war.png?fit=1024%2C491&amp;ssl=1\" width=\"1024\" /></figure>According to Sensor Tower, FUNFLY’s mobile title Last War topped the global mobile game revenue chart in September, earning an estimated RMB 1.3 billion ($180 million) in in-app purchases across iOS and Google Play. Last War: Survival Game is a SLG (Simulation and Strategy Game), featuring a chibi-style 3D art design, the game blends runner-shooter [&#8230;]",
        "source": "technode.com",
        "published": "Thu, 30 Oct 2025 02:08:57 +0000",
        "fetched_at": "2026-02-28T23:17:15.295539Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 7
          },
          {
            "name": "visibility_gain",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 16,
        "timeliness_score": 3,
        "final_score": 9.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://technode.com/2025/12/17/french-studio-drama-secures-tencent-investment-for-tactical-shooter-unrecord/",
        "title": "French studio Drama secures Tencent investment for tactical shooter Unrecord",
        "summary": "<figure><img alt=\"\" class=\"attachment-rss-image-size size-rss-image-size wp-post-image\" height=\"576\" src=\"https://i0.wp.com/technode.com/wp-content/uploads/2025/12/unrecord.jpg?fit=1024%2C576&amp;ssl=1\" width=\"1024\" /></figure>French independent game studio Drama Studios said its Unreal Engine 5–powered tactical shooter Unrecord has received a strategic investment from Tencent. The game, presented from the perspective of a police body camera, has drawn global attention for its cinematic visual quality and immersive narrative style. Unrecord previously surpassed 600,000 at its peak on Steam’s wishlist [&#8230;]",
        "source": "technode.com",
        "published": "Wed, 17 Dec 2025 10:03:37 +0000",
        "fetched_at": "2026-02-28T23:17:15.294778Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 4
          },
          {
            "name": "visibility_gain",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 13,
        "timeliness_score": 3,
        "final_score": 8.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://technode.com/2025/09/15/mit-technology-review-releases-2025-50-smartest-companies-list-recognizes-deepseek-game-science-and-unitree-robotics/",
        "title": "MIT Technology Review releases 2025 ’50 Smartest Companies’ list, recognizes Deepseek, Game Science and Unitree Robotics",
        "summary": "<figure><img alt=\"\" class=\"attachment-rss-image-size size-rss-image-size wp-post-image\" height=\"567\" src=\"https://i0.wp.com/technode.com/wp-content/uploads/2023/08/Beijing-forbids-generative-AI-in-online-medical-prescriptions-e1694161793934.jpg?fit=1024%2C567&amp;ssl=1\" width=\"1024\" /></figure>At the EmTech China 2025 Global Technology Summit last Friday, MIT Technology Review unveiled its annual list of the “50 Smartest Companies,” with Deepseek, Game Science, and Unitree Robotics earning spots in the ranking. Deepseek was recognized for achieving world-class model performance at low training costs — a breakthrough in algorithm optimization and resource efficiency [&#8230;]",
        "source": "technode.com",
        "published": "Mon, 15 Sep 2025 07:38:25 +0000",
        "fetched_at": "2026-02-28T23:17:15.296699Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 4
          },
          {
            "name": "visibility_gain",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 13,
        "timeliness_score": 3,
        "final_score": 8.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://technode.com/2025/10/09/vivo-x300-pro-to-debut-sony-lyt-828-gimbal-camera-with-enhanced-hdr-and-stabilization/",
        "title": "Vivo X300 Pro to debut Sony LYT-828 gimbal camera with enhanced HDR and stabilization",
        "summary": "<figure><img alt=\"\" class=\"attachment-rss-image-size size-rss-image-size wp-post-image\" height=\"596\" src=\"https://i0.wp.com/technode.com/wp-content/uploads/2025/10/vivo-x300.png?fit=1024%2C596&amp;ssl=1\" width=\"1024\" /></figure>Vivo announced on Wednesday that its upcoming X300 Pro will make the global debut of Sony’s LYT-828, a gimbal-level main camera sensor. The 50MP sensor features a large 1/1.28-inch size and an f/1.57 aperture, offering CIPA 5.5-level stabilization. With Hybrid Frame-HDR fusion technology, it offers a 100dB dynamic range for improved backlit and low-light performance. [&#8230;]",
        "source": "technode.com",
        "published": "Thu, 09 Oct 2025 09:43:32 +0000",
        "fetched_at": "2026-02-28T23:17:15.296118Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 2
          },
          {
            "name": "visibility_gain",
            "score": 5
          },
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 12,
        "timeliness_score": 3,
        "final_score": 7.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://www.scmp.com/tech/article/3344817/breakthrough-or-hype-how-weride-aims-steer-past-rivals-crowded-robotaxi-field?utm_source=rss_feed",
        "title": "Breakthrough or hype? How WeRide aims to steer past rivals in crowded robotaxi field",
        "summary": "WeRide, one of China’s big three robotaxi companies, has cut research and development (R&amp;D) costs by “millions” of US dollars by using artificial intelligence to train its fleet in virtual worlds, its CEO said.\nWhile rivals had also developed AI models simulating the physical world, WeRide’s efforts stood out as it was using its world model Genesis to support its global expansion strategy, said Tony Han.\n“It’s the first real marriage between physical AI and generative AI,” Han told the South...",
        "source": "www.scmp.com",
        "published": "Sat, 28 Feb 2026 02:00:38 +0000",
        "fetched_at": "2026-02-28T23:17:09.320538Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 2
          },
          {
            "name": "visibility_gain",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 11,
        "timeliness_score": 3,
        "final_score": 7.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://technode.com/2025/08/19/preview-of-chinese-game-developers-at-gamescom-2025%ef%bc%9ablack-myth-wukong-wuxia-rpgs-and-more/",
        "title": "Preview of Chinese game developers at Gamescom 2025：Black Myth Wukong, wuxia, RPGs and more",
        "summary": "<figure><img alt=\"\" class=\"attachment-rss-image-size size-rss-image-size wp-post-image\" height=\"607\" src=\"https://i0.wp.com/technode.com/wp-content/uploads/2025/08/blade-2.png?fit=1024%2C607&amp;ssl=1\" width=\"1024\" /></figure>As one of the world’s largest gaming events, Gamescom has become a key bridge between Europe and the global industry. This year, several Chinese games will debut new trailers or offer hands-on demos to overseas players for the very first time, signaling both confidence in their products and a deeper commitment to engaging with international [&#8230;]",
        "source": "technode.com",
        "published": "Tue, 19 Aug 2025 09:58:32 +0000",
        "fetched_at": "2026-02-28T23:17:15.297010Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 2
          },
          {
            "name": "visibility_gain",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 11,
        "timeliness_score": 3,
        "final_score": 7.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://technode.com/2025/08/12/renault-and-geely-collaborate-to-make-electric-suv-for-overseas-markets-report/",
        "title": "Renault and Geely collaborate to make electric SUV for overseas markets: report",
        "summary": "<figure><img alt=\"\" class=\"attachment-rss-image-size size-rss-image-size wp-post-image\" height=\"350\" src=\"https://i0.wp.com/technode.com/wp-content/uploads/2024/09/1-1.png?fit=700%2C350&amp;ssl=1\" width=\"700\" /></figure>Renault is developing an electric sports utility vehicle built on the newest platform from Geely called the Global Intelligent New Energy Architecture (GEA), one of the company’s core technologies that has underpinned the success of its Galaxy lineup, as reported by Chinese media publication AutoPix. The new SUV will have both all-electric and plug-in hybrid [&#8230;]",
        "source": "technode.com",
        "published": "Tue, 12 Aug 2025 09:10:21 +0000",
        "fetched_at": "2026-02-28T23:17:15.297153Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 2
          },
          {
            "name": "scale_shift",
            "score": 9
          }
        ],
        "structural_score": 11,
        "timeliness_score": 3,
        "final_score": 7.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://technode.com/2025/04/12/huawei-patent-reinvents-periscope-camera-with-retractable-design-reducing-camera-bump/",
        "title": "Huawei patent reinvents periscope camera with retractable design reducing camera bump",
        "summary": "<figure><img alt=\"\" class=\"attachment-rss-image-size size-rss-image-size wp-post-image\" height=\"683\" src=\"https://i0.wp.com/technode.com/wp-content/uploads/2023/09/151451493_l_normal_none-scaled.jpg?fit=1024%2C683&amp;ssl=1\" width=\"1024\" /></figure>Source @xleaks7 revealed on platform X that the United States Patent and Trademark Office (USPTO) approved a Huawei patent last month. According to the patent, Huawei proposes using a drive motor to adjust the distance between the camera module and the image sensor, aiming to enhance the zoom performance of telephoto lenses while maintaining a [&#8230;]",
        "source": "technode.com",
        "published": "Sat, 12 Apr 2025 12:50:52 +0000",
        "fetched_at": "2026-02-28T23:17:15.300713Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 6
          },
          {
            "name": "visibility_gain",
            "score": 5
          }
        ],
        "structural_score": 11,
        "timeliness_score": 3,
        "final_score": 7.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://technode.com/2025/10/27/huawei-vivo-and-oppo-help-establish-first-global-fast-charging-standard-under-itu/",
        "title": "Huawei, vivo, and OPPO help establish first global fast-charging standard under ITU",
        "summary": "<figure><img alt=\"\" class=\"attachment-rss-image-size size-rss-image-size wp-post-image\" height=\"683\" src=\"https://i0.wp.com/technode.com/wp-content/uploads/2025/10/charger-marcus-urbenz-4xMAiJZPQXI-unsplash.jpg?fit=1024%2C683&amp;ssl=1\" width=\"1024\" /></figure>The International Telecommunication Union (ITU) has approved and released L.1004, a universal fast-charging standard for mobile terminals co-authored by China’s CAICT with Huawei, vivo, and OPPO. The standard enables cross-brand and cross-device fast charging and is intended to reduce charger duplication and electronic waste. [TechNode reporting]",
        "source": "technode.com",
        "published": "Mon, 27 Oct 2025 10:51:44 +0000",
        "fetched_at": "2026-02-28T23:17:15.295627Z",
        "tags": [
          {
            "name": "transformation",
            "score": 3
          },
          {
            "name": "boundary_crossing",
            "score": 2
          },
          {
            "name": "scale_shift",
            "score": 5
          }
        ],
        "structural_score": 10,
        "timeliness_score": 3,
        "final_score": 6.5,
        "reddit_score": null,
        "reddit_comments": null
      }
    ],
    "devcommunity": [
      {
        "url": "https://github.com/ruvnet/ruvector",
        "title": "ruvnet/ruvector",
        "summary": "<p>RuVector is a High Performance, Real-Time, Self-Learning, Vector Graph Neural Network, and Database built in Rust.</p><hr /><h1>RuVector — A Self-Learning, Agentic Operating System</h1> \n<p><a href=\"https://cognitum.one\"><img alt=\"CES 2026 Innovation Award\" src=\"https://img.shields.io/badge/%F0%9F%8F%85_CES_2026-Innovation_Award-gold.svg?sanitize=true\" /></a> <a href=\"https://github.com/ruvnet/ruvector\"><img alt=\"GitHub Trending\" src=\"https://img.shields.io/badge/%F0%9F%94%A5_GitHub-Trending-orange.svg?sanitize=true\" /></a></p> \n<p><a href=\"https://crates.io/crates/ruvector-core\"><img alt=\"Crates.io\" src=\"https://img.shields.io/crates/v/ruvector-core.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/ruvector\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/ruvector.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/ruvector\"><img alt=\"Downloads\" src=\"https://img.shields.io/npm/dt/ruvector.svg?label=Downloads\" /></a> <a href=\"https://www.npmjs.com/package/ruvector\"><img alt=\"Monthly Downloads\" src=\"https://img.shields.io/npm/dm/ruvector.svg?label=Monthly%20Downloads\" /></a> <a href=\"https://ruv.io\"><img alt=\"ruv.io\" src=\"https://img.shields.io/badge/ruv.io-website-purple.svg?sanitize=true\" /></a> <a href=\"https://opensource.org/licenses/MIT\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true\" /></a></p> \n<h3><strong>The self-learning, self-optimizing vector database — with graph intelligence, local AI, and PostgreSQL built in.</strong></h3> \n<blockquote> \n <p>Created by <a href=\"https://ruv.io\">rUv</a> and powering <a href=\"https://cognitum.one\">Cognitum</a>, a 🏅 <strong>CES 2026 Innovation Awards Honoree</strong> — the world's first Agentic Chip designed to be always running for AI agents. Tens of thousands of agents, near-zero power, learns from every signal. <a href=\"https://cognitum.one\">Learn more →</a></p> \n</blockquote> \n<pre><code class=\"language-bash\">npx ruvector\n</code></pre> \n<h4>Most vector databases store your data and search it — the same way, every time.</h4> \n<h4><strong>RuVector</strong> is fundamentally different. It watches how you use it and gets smarter: search results improve automatically, the system tunes itself to your workload, and it runs AI models right on your hardware — no cloud APIs, no per-query bills, GPUs optional, CPUs preferred. It drops into PostgreSQL, runs in browsers, and ships as a single file.</h4> \n<p>Open source. ❤️ Free forever.</p> \n<pre><code>User Query → [SONA Engine] → Model Response → User Feedback\n                  ↑                                 │\n                  └─────── Learning Signal ─────────┘\n                         (&lt; 1ms adaptation)\n</code></pre> \n<details> \n 🔍 RuVector vs Typical Vector Databases (20 differences) \n <table> \n  <thead> \n   <tr> \n    <th></th> \n    <th>RuVector</th> \n    <th>Typical Vector DB</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Self-Learning &amp; Optimization</strong></td> \n    <td></td> \n    <td></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn\">Search quality</a></td> \n    <td>🧠 GNN learns from every query — results improve over time</td> \n    <td>Static — same results every time</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/sona\">Self-optimizing</a></td> \n    <td>⚡ SONA auto-tunes routing, ranking, and compression to your workload</td> \n    <td>Manual tuning required</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention\">46 attention mechanisms</a></td> \n    <td>🎯 Flash, linear, graph, hyperbolic, <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attn-mincut\">mincut-gated</a> (cuts compute 50%)</td> \n    <td>Basic similarity only</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-domain-expansion\">Transfer learning</a></td> \n    <td>🔄 Knowledge transfers across domains — new tasks bootstrap from past learning</td> \n    <td>Start from scratch each time</td> \n   </tr> \n   <tr> \n    <td><strong>Graph &amp; Relationships</strong></td> \n    <td></td> \n    <td></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph\">Graph queries</a></td> \n    <td>🔗 Full Cypher engine — <code>MATCH (a)-[:KNOWS]-&gt;(b)</code> like Neo4j</td> \n    <td>Flat list of results</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-transformer\">Graph transformers</a></td> \n    <td>🔬 8 verified modules: physics, bio, manifold, temporal, economic</td> \n    <td>No graph support</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph\">Hyperedges</a></td> \n    <td>🕸️ Connect 3+ nodes at once — model group relationships natively</td> \n    <td>Pairwise only</td> \n   </tr> \n   <tr> \n    <td><strong>AI &amp; Compute</strong></td> \n    <td></td> \n    <td></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm\">Local LLMs</a></td> \n    <td>🤖 Run models on your hardware — Metal, CUDA, WebGPU, no API costs</td> \n    <td>Cloud API required (pay per call)</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-solver\">Sublinear solvers</a></td> \n    <td>📐 O(log n) PageRank, spectral methods, sparse linear systems</td> \n    <td>Not available</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/dna\">Genomics</a></td> \n    <td>🧬 Variant calling, protein translation, HNSW k-mer search in 12 ms</td> \n    <td>Not available</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruqu\">Quantum coherence</a></td> \n    <td>⚛️ Error correction via dynamic min-cut optimization</td> \n    <td>Not available</td> \n   </tr> \n   <tr> \n    <td><strong>Database &amp; Platform</strong></td> \n    <td></td> \n    <td></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-postgres\">PostgreSQL</a></td> \n    <td>🐘 230+ SQL functions — drop into your existing database, <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/postgres/\">pgvector replacement</a></td> \n    <td>Separate service to manage</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">Deploy anywhere</a></td> \n    <td>🌐 One file — servers, browsers, phones, IoT, bare metal, WASM (58 KB)</td> \n    <td>Cloud server required</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">Cognitive containers</a></td> \n    <td>🚀 Single <code>.rvf</code> file boots as a service in 125 ms — includes vectors, models, kernel</td> \n    <td>Configure a cluster</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-core\">Live updates</a></td> \n    <td>⚡ Update vectors and graph connections instantly, no downtime</td> \n    <td>Rebuild index or wait</td> \n   </tr> \n   <tr> \n    <td><strong>Operations</strong></td> \n    <td></td> \n    <td></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto\">Tamper-proof audit</a></td> \n    <td>🔐 Cryptographic witness chain records every operation automatically</td> \n    <td>Manual logging</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-cow\">Branch your data</a></td> \n    <td>🌿 Git-like COW branching — 1M vectors, 100 edits = ~2.5 MB branch</td> \n    <td>Copy everything</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-replication\">Scale out</a></td> \n    <td>📈 Raft consensus, multi-master replication, auto-sharding</td> \n    <td>Paid tiers, per-vector pricing</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto\">Post-quantum crypto</a></td> \n    <td>🛡️ ML-DSA-65 and Ed25519 signatures on every segment</td> \n    <td>Not available</td> \n   </tr> \n   <tr> \n    <td>Cost</td> \n    <td>💰 Free forever — open source (MIT)</td> \n    <td>Per-query or per-vector pricing</td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<details> \n 📋 See Full Capabilities (75 features across 10 categories) \n <p><strong>Core Vector Database</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>1</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-core\"><strong>Store vectors</strong></a></td> \n    <td>Embeddings from OpenAI, Cohere, local ONNX with HNSW indexing and SIMD acceleration</td> \n   </tr> \n   <tr> \n    <td>2</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph\"><strong>Query with Cypher</strong></a></td> \n    <td>Graph queries like Neo4j — <code>MATCH (a)-[:SIMILAR]-&gt;(b)</code> with hyperedges</td> \n   </tr> \n   <tr> \n    <td>3</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn\"><strong>The index learns</strong></a></td> \n    <td>GNN layers make search results improve over time — every query teaches the system</td> \n   </tr> \n   <tr> \n    <td>4</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-hyperbolic-hnsw\"><strong>Hyperbolic HNSW</strong></a></td> \n    <td>Hierarchy-aware search in Poincare ball space — better for trees and taxonomies</td> \n   </tr> \n   <tr> \n    <td>5</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-temporal-tensor\"><strong>Compress automatically</strong></a></td> \n    <td>2-32x memory reduction with adaptive tiered compression and temporal tensor reuse</td> \n   </tr> \n   <tr> \n    <td>6</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-filter\"><strong>Metadata filtering</strong></a></td> \n    <td>Filter search results by any field before scanning vectors — fast hybrid queries</td> \n   </tr> \n   <tr> \n    <td>7</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-collections\"><strong>Collections</strong></a></td> \n    <td>Multi-tenant, schema-managed collections — isolate data per customer or project</td> \n   </tr> \n   <tr> \n    <td>8</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-snapshot\"><strong>Snapshots</strong></a></td> \n    <td>Point-in-time backups — restore your database to any previous state</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Distributed Systems</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>9</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-raft\"><strong>Raft consensus</strong></a></td> \n    <td>Leader election and log replication — nodes agree on state even when some fail</td> \n   </tr> \n   <tr> \n    <td>10</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-replication\"><strong>Multi-master replication</strong></a></td> \n    <td>Vector clocks, conflict resolution, geo-distributed sync across data centers</td> \n   </tr> \n   <tr> \n    <td>11</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-cluster\"><strong>Cluster management</strong></a></td> \n    <td>Horizontal scaling with consistent hashing — add nodes without rebalancing everything</td> \n   </tr> \n   <tr> \n    <td>12</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-delta-consensus\"><strong>Delta consensus</strong></a></td> \n    <td>Track behavioral changes across distributed nodes with CRDTs and causal ordering</td> \n   </tr> \n   <tr> \n    <td>13</td> \n    <td><strong>Burst scaling</strong></td> \n    <td>10-50x capacity scaling for traffic spikes — absorb load then scale back down</td> \n   </tr> \n   <tr> \n    <td>14</td> \n    <td><strong>Auto-sharding</strong></td> \n    <td>Automatic data partitioning across nodes based on access patterns</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>AI &amp; Machine Learning</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>15</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm\"><strong>Run LLMs locally</strong></a></td> \n    <td>Load GGUF models and run inference on your hardware — Metal, CUDA, ANE, WebGPU</td> \n   </tr> \n   <tr> \n    <td>16</td> \n    <td><a href=\"https://huggingface.co/ruv/ruvltra\"><strong>RuvLTRA models</strong></a></td> \n    <td>Pre-trained GGUF for routing and embeddings in under 10 ms</td> \n   </tr> \n   <tr> \n    <td>17</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/sona\"><strong>SONA learning</strong></a></td> \n    <td>Self-Optimizing Neural Architecture — LoRA fine-tuning + EWC++ memory preservation</td> \n   </tr> \n   <tr> \n    <td>18</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention\"><strong>46 attention mechanisms</strong></a></td> \n    <td>Flash, linear, graph, hyperbolic, mincut-gated (cuts compute 50%)</td> \n   </tr> \n   <tr> \n    <td>19</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-router-core\"><strong>Semantic routing</strong></a></td> \n    <td>Route AI requests to the right model or handler using FastGRNN neural inference</td> \n   </tr> \n   <tr> \n    <td>20</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-sparse-inference\"><strong>Sparse inference</strong></a></td> \n    <td>PowerInfer-style engine — only activate the neurons you need, 2-10x faster on edge</td> \n   </tr> \n   <tr> \n    <td>21</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-tiny-dancer-core\"><strong>Tiny Dancer</strong></a></td> \n    <td>Production-grade agent routing with FastGRNN — lightweight alternative to full LLM</td> \n   </tr> \n   <tr> \n    <td>22</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-domain-expansion\"><strong>Domain expansion</strong></a></td> \n    <td>Cross-domain transfer learning — new tasks bootstrap from past learning automatically</td> \n   </tr> \n   <tr> \n    <td>23</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-math\"><strong>Advanced math</strong></a></td> \n    <td>Optimal transport, Sinkhorn distances, KL divergence, spectral clustering</td> \n   </tr> \n   <tr> \n    <td>24</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-coherence\"><strong>Coherence measurement</strong></a></td> \n    <td>Measure signal quality and compare attention mechanisms objectively</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Graph Transformers</strong> (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-transformer\">8 verified modules</a>)</p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>25</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-verified\"><strong>Proof-gated mutation</strong></a></td> \n    <td>Every write to graph state requires a formal proof — bugs cannot corrupt data</td> \n   </tr> \n   <tr> \n    <td>26</td> \n    <td><strong>Sublinear attention</strong></td> \n    <td>O(n log n) via LSH bucketing, PPR sampling, and spectral sparsification</td> \n   </tr> \n   <tr> \n    <td>27</td> \n    <td><strong>Physics-informed layers</strong></td> \n    <td>Hamiltonian dynamics, gauge equivariant message passing — energy conserved by construction</td> \n   </tr> \n   <tr> \n    <td>28</td> \n    <td><strong>Biological layers</strong></td> \n    <td>Spiking attention, Hebbian/STDP learning, dendritic branching</td> \n   </tr> \n   <tr> \n    <td>29</td> \n    <td><strong>Self-organizing layers</strong></td> \n    <td>Morphogenetic fields, reaction-diffusion growth — graphs that restructure themselves</td> \n   </tr> \n   <tr> \n    <td>30</td> \n    <td><strong>Verified training</strong></td> \n    <td>Training certificates, delta-apply rollback — bad gradient steps auto-reversed</td> \n   </tr> \n   <tr> \n    <td>31</td> \n    <td><strong>Manifold geometry</strong></td> \n    <td>Product manifolds S^n x H^m x R^k — work in curved spaces, not just flat</td> \n   </tr> \n   <tr> \n    <td>32</td> \n    <td><strong>Temporal-causal layers</strong></td> \n    <td>Causal masking, Granger causality extraction, continuous-time ODE integration</td> \n   </tr> \n   <tr> \n    <td>33</td> \n    <td><strong>Economic layers</strong></td> \n    <td>Nash equilibrium attention, Shapley attribution — fair value assignment in multi-agent graphs</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Cognitive Containers</strong> (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">RVF format</a>)</p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>34</td> \n    <td><strong>Self-boot as a microservice</strong></td> \n    <td>A single <code>.rvf</code> file contains vectors, models, and a Linux kernel — boots in 125 ms</td> \n   </tr> \n   <tr> \n    <td>35</td> \n    <td><strong>eBPF acceleration</strong></td> \n    <td>Hot vectors served in kernel data path via XDP, socket filter, and TC programs</td> \n   </tr> \n   <tr> \n    <td>36</td> \n    <td><strong>5.5 KB WASM runtime</strong></td> \n    <td>Same <code>.rvf</code> file runs queries in a browser tab with zero backend</td> \n   </tr> \n   <tr> \n    <td>37</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf\"><strong>COW branching</strong></a></td> \n    <td>Git-like copy-on-write — 1M vectors, 100 edits = ~2.5 MB branch</td> \n   </tr> \n   <tr> \n    <td>38</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto\"><strong>Witness chains</strong></a></td> \n    <td>Tamper-evident hash-linked audit trail records every operation automatically</td> \n   </tr> \n   <tr> \n    <td>39</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto\"><strong>Post-quantum signatures</strong></a></td> \n    <td>ML-DSA-65 and SLH-DSA-128s alongside Ed25519 — future-proof cryptography</td> \n   </tr> \n   <tr> \n    <td>40</td> \n    <td><strong>DNA-style lineage</strong></td> \n    <td>Track parent/child derivation chains with cryptographic hashes</td> \n   </tr> \n   <tr> \n    <td>41</td> \n    <td><strong>25 segment types</strong></td> \n    <td>VEC, INDEX, KERNEL, EBPF, WASM, COW_MAP, WITNESS, CRYPTO, and 17 more</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>PostgreSQL Extension</strong> (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-postgres\">230+ SQL functions</a>)</p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>42</td> \n    <td><strong>Drop-in pgvector replacement</strong></td> \n    <td>Same SQL interface but with self-learning search — no app changes needed</td> \n   </tr> \n   <tr> \n    <td>43</td> \n    <td><strong>Sublinear solvers in SQL</strong></td> \n    <td>PageRank, conjugate gradient, Laplacian solver — O(log n) to O(sqrt(n))</td> \n   </tr> \n   <tr> \n    <td>44</td> \n    <td><strong>Math distances in SQL</strong></td> \n    <td>Wasserstein, Sinkhorn, KL divergence, spectral clustering — all from SQL</td> \n   </tr> \n   <tr> \n    <td>45</td> \n    <td><strong>Topological data analysis</strong></td> \n    <td>Persistent homology, Betti numbers, embedding drift detection</td> \n   </tr> \n   <tr> \n    <td>46</td> \n    <td><strong>SONA learning in SQL</strong></td> \n    <td>Micro-LoRA trajectory learning with EWC++ forgetting prevention</td> \n   </tr> \n   <tr> \n    <td>47</td> \n    <td><strong>Extended attention in SQL</strong></td> \n    <td>O(n) linear, MoE, hyperbolic, sliding window attention — all callable from SQL</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Specialized Processing</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>48</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/scipix\"><strong>SciPix OCR</strong></a></td> \n    <td>Extract LaTeX and MathML from scientific documents and PDFs</td> \n   </tr> \n   <tr> \n    <td>49</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-dag\"><strong>DAG workflows</strong></a></td> \n    <td>Self-learning directed acyclic graph execution for multi-step pipelines</td> \n   </tr> \n   <tr> \n    <td>50</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/cognitum-gate-kernel\"><strong>Cognitum Gate</strong></a></td> \n    <td>Cognitive AI gateway with TileZero acceleration for fast routing</td> \n   </tr> \n   <tr> \n    <td>51</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-fpga-transformer\"><strong>FPGA transformer</strong></a></td> \n    <td>Hardware-accelerated transformer inference on programmable chips</td> \n   </tr> \n   <tr> \n    <td>52</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruQu\"><strong>Quantum coherence</strong></a></td> \n    <td>Error correction via dynamic min-cut optimization for quantum circuits</td> \n   </tr> \n   <tr> \n    <td>53</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-solver\"><strong>Sublinear solvers</strong></a></td> \n    <td>8 algorithms (Neumann, CG, Forward Push, TRUE, BMSSP) — O(log n) to O(sqrt(n))</td> \n   </tr> \n   <tr> \n    <td>54</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-mincut-gated-transformer\"><strong>Mincut-gated transformer</strong></a></td> \n    <td>Dynamic attention that prunes irrelevant connections using graph min-cut</td> \n   </tr> \n   <tr> \n    <td>55</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-nervous-system\"><strong>Nervous system</strong></a></td> \n    <td>5-layer bio-inspired adaptive system with spiking networks and BTSP learning</td> \n   </tr> \n   <tr> \n    <td>56</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/prime-radiant\"><strong>Prime Radiant</strong></a></td> \n    <td>Coherence engine using sheaf Laplacian math for AI safety and hallucination detection</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Genomics &amp; Health</strong> (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/dna\">rvDNA</a>)</p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>57</td> \n    <td><strong>rvDNA genomic analysis</strong></td> \n    <td>Variant calling, protein translation, HNSW k-mer search in 12 ms</td> \n   </tr> \n   <tr> \n    <td>58</td> \n    <td><strong><code>.rvdna</code> file format</strong></td> \n    <td>AI-native binary with pre-computed vectors, tensors, and embeddings</td> \n   </tr> \n   <tr> \n    <td>59</td> \n    <td><strong>Instant diagnostics</strong></td> \n    <td>Sickle cell, cancer mutations, drug dosing — runs on any device</td> \n   </tr> \n   <tr> \n    <td>60</td> \n    <td><strong>Privacy-first WASM</strong></td> \n    <td>Browser-based genomics — your DNA data never leaves the device</td> \n   </tr> \n   <tr> \n    <td>61</td> \n    <td><strong>Biomarker engine</strong></td> \n    <td>Composite polygenic risk scoring (20 SNPs, 6 gene-gene interactions, 2 us)</td> \n   </tr> \n   <tr> \n    <td>62</td> \n    <td><strong>Streaming biomarkers</strong></td> \n    <td>Real-time anomaly detection, CUSUM changepoints, trend analysis (&gt;100k readings/sec)</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Platform &amp; Integration</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>63</td> \n    <td><strong>Run anywhere</strong></td> \n    <td>Rust, Node.js, browser (WASM), edge (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvlite\">rvLite</a>), HTTP server, bare metal</td> \n   </tr> \n   <tr> \n    <td>64</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/mcp-gate\"><strong>MCP server</strong></a></td> \n    <td>Model Context Protocol for AI assistants — Claude, GPT, and other agents can use RuVector as a tool</td> \n   </tr> \n   <tr> \n    <td>65</td> \n    <td><strong>Cloud deployment</strong></td> \n    <td>One-click deploy to <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/google-cloud\">Google Cloud Run</a>, Kubernetes</td> \n   </tr> \n   <tr> \n    <td>66</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/app-clip\"><strong>iOS App Clip</strong></a></td> \n    <td>Scan a QR code to load an RVF cognitive seed on your phone — under 15 MB</td> \n   </tr> \n   <tr> \n    <td>67</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-metrics\"><strong>Prometheus metrics</strong></a></td> \n    <td>Built-in monitoring — export latency, throughput, and memory stats to Grafana</td> \n   </tr> \n   <tr> \n    <td>68</td> \n    <td><strong>90+ Rust crates + npm packages</strong></td> \n    <td>Published on <a href=\"https://crates.io/crates/rvf-runtime\">crates.io</a> and <a href=\"https://www.npmjs.com/package/@ruvector/rvf\">npm</a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Examples &amp; Applications</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>69</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/neural-trader\"><strong>Neural Trader</strong></a></td> \n    <td>Algorithmic trading with Kelly Criterion position sizing and LSTM-Transformer prediction</td> \n   </tr> \n   <tr> \n    <td>70</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/meta-cognition-spiking-neural-network\"><strong>Spiking Neural Network</strong></a></td> \n    <td>Hybrid AI combining spiking networks, SIMD vector ops, and 5 attention types</td> \n   </tr> \n   <tr> \n    <td>71</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/refrag-pipeline\"><strong>ReFrag Pipeline</strong></a></td> \n    <td>Compress-Sense-Expand architecture — ~30x RAG latency reduction</td> \n   </tr> \n   <tr> \n    <td>72</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge-net\"><strong>Edge Network</strong></a></td> \n    <td>Distributed collective AI — share idle compute across devices</td> \n   </tr> \n   <tr> \n    <td>73</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/vibecast-7sense\"><strong>Vibecast 7Sense</strong></a></td> \n    <td>Transform bird calls into navigable geometric space using vector search</td> \n   </tr> \n   <tr> \n    <td>74</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/ultra-low-latency-sim\"><strong>Ultra-Low Latency Sim</strong></a></td> \n    <td>Meta-simulation achieving quadrillion simulations per second on CPU with SIMD</td> \n   </tr> \n   <tr> \n    <td>75</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/verified-applications\"><strong>Verified Applications</strong></a></td> \n    <td>10 exotic proof-carrying apps: weapons filters, legal forensics, medical diagnostics</td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<h3>Built by rUv, powered by <a href=\"https://cognitum.one\">Cognitum.one</a></h3> \n<details> \n <strong>Cognitum Hardware — The Agentic Appliance &amp; Chip</strong> \n <p><strong>Cognitum v0 — The Agentic Appliance</strong>: Run tens of thousands of always-on agents at no incremental cost beyond the box. Learns in proximity to any signal — sensors, networks, machines — at near-zero power (~5 uW/inference, &lt;15W total). Sub-millisecond response, 500x cheaper than cloud AI. No cloud bills, no per-agent fees. Like a nervous system, not a brain.</p> \n <p><strong>Cognitum v1 — The Agentic Chip</strong>: Same architecture on a single 257-core custom chip. Runs on less than 2W — a AA battery. Idle-to-8 GHz burst on demand, 2 TB/s interconnect, built-in encryption per core.</p> \n</details> \n<h3>A Complete Agentic AI Operating System</h3> \n<p>RuVector isn't a database you add to your stack — it's the entire stack. Self-learning, self-optimizing, and self-deploying. Everything an AI application needs to run, from bare metal hardware up to the application layer, in one package:</p> \n<p><strong>Intelligence</strong></p> \n<table> \n <thead> \n  <tr> \n   <th></th> \n   <th>Layer</th> \n   <th>Replaces</th> \n   <th>What It Does</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>🔄</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/sona/README.md\"><strong>Self-Learning</strong></a></td> \n   <td>Manual retraining, MLOps</td> \n   <td>SONA adapts in &lt;1 ms — LoRA fine-tuning + EWC++ memory on every request</td> \n  </tr> \n  <tr> \n   <td>⚡</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn/README.md\"><strong>Self-Optimizing</strong></a></td> \n   <td>Manual tuning, config files</td> \n   <td>Auto-tunes routing, ranking, compression, and index parameters</td> \n  </tr> \n  <tr> \n   <td>🎯</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm/README.md\"><strong>Embeddings</strong></a></td> \n   <td>OpenAI API, Cohere, static models</td> \n   <td>Contrastive training, triplet loss, real-time fine-tuning — embeddings improve as you use them</td> \n  </tr> \n  <tr> \n   <td>✅</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-verified/README.md\"><strong>Verified Training</strong></a></td> \n   <td>Manual validation</td> \n   <td>Formal proofs + statistical tests on every training step — gradients only apply if invariants pass</td> \n  </tr> \n </tbody> \n</table> \n<p><strong>Data &amp; Search</strong></p> \n<table> \n <thead> \n  <tr> \n   <th></th> \n   <th>Layer</th> \n   <th>Replaces</th> \n   <th>What It Does</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>🔍</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-core/README.md\"><strong>Search</strong></a></td> \n   <td>Pinecone, Weaviate, Qdrant</td> \n   <td>Self-learning HNSW — GNN improves results from every query</td> \n  </tr> \n  <tr> \n   <td>🗄️</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-core/README.md\"><strong>Storage</strong></a></td> \n   <td>Separate database + cache</td> \n   <td>Vector store, graph DB, key-value cache — unified engine</td> \n  </tr> \n  <tr> \n   <td>🐘</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-postgres/README.md\"><strong>PostgreSQL</strong></a></td> \n   <td>pgvector, pg_embedding</td> \n   <td>Drop-in replacement — 230+ SQL functions, same interface but search gets smarter over time</td> \n  </tr> \n  <tr> \n   <td>🔗</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph/README.md\"><strong>Graph</strong></a></td> \n   <td>Neo4j, Amazon Neptune</td> \n   <td>Cypher, W3C SPARQL 1.1, hyperedges — all built in</td> \n  </tr> \n </tbody> \n</table> \n<p><strong>AI &amp; ML</strong></p> \n<table> \n <thead> \n  <tr> \n   <th></th> \n   <th>Layer</th> \n   <th>Replaces</th> \n   <th>What It Does</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>🤖</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm/README.md\"><strong>AI Runtime</strong></a></td> \n   <td>llama.cpp, vLLM, Ollama</td> \n   <td>ruvllm — GGUF models, MicroLoRA (&lt;1 ms), speculative decoding, continuous batching, WASM</td> \n  </tr> \n  <tr> \n   <td>🧠</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention/README.md\"><strong>ML Framework</strong></a></td> \n   <td>PyTorch, TensorFlow</td> \n   <td>46 attention types, 8 graph transformers, spiking networks, sparse inference, sublinear solvers</td> \n  </tr> \n  <tr> \n   <td>🔬</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-mincut/README.md\"><strong>Coherence</strong></a></td> \n   <td>Manual testing, guardrails</td> \n   <td>Min-cut finds the weakest links in any network — detects AI drift, prunes wasted compute (50% reduction), keeps agents in sync</td> \n  </tr> \n  <tr> \n   <td>🧬</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-domain-expansion/README.md\"><strong>Domain Models</strong></a></td> \n   <td>Custom ML pipelines</td> \n   <td>Genomics (DNA variant calling), physics simulation, economic modeling, biological networks</td> \n  </tr> \n </tbody> \n</table> \n<p><strong>Infrastructure</strong></p> \n<table> \n <thead> \n  <tr> \n   <th></th> \n   <th>Layer</th> \n   <th>Replaces</th> \n   <th>What It Does</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>🔧</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-fpga-transformer/README.md\"><strong>Hardware</strong></a></td> \n   <td>CUDA toolkit, driver configs</td> \n   <td>Sparse/spiking CPU (AVX-512, NEON) — GPU for bursts (Metal, CUDA, ANE, WebGPU, FPGA)</td> \n  </tr> \n  <tr> \n   <td>🐧</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\"><strong>Kernel</strong></a></td> \n   <td>Linux + Docker + eBPF</td> \n   <td><code>.rvf</code> file boots its own kernel in 125 ms — eBPF accelerates hot paths</td> \n  </tr> \n  <tr> \n   <td>🌐</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-raft/README.md\"><strong>Coordination</strong></a></td> \n   <td>etcd, ZooKeeper, Consul</td> \n   <td>Raft consensus, multi-master replication, CRDT delta sync, auto-sharding</td> \n  </tr> \n  <tr> \n   <td>📦</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\"><strong>Packaging</strong></a></td> \n   <td>Docker, Kubernetes</td> \n   <td>One <code>.rvf</code> file = your entire service — servers, browsers, phones, IoT, bare metal</td> \n  </tr> \n </tbody> \n</table> \n<p><strong>Routing &amp; Observability</strong></p> \n<table> \n <thead> \n  <tr> \n   <th></th> \n   <th>Layer</th> \n   <th>Replaces</th> \n   <th>What It Does</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>🚦</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-tiny-dancer-core/README.md\"><strong>Routing</strong></a></td> \n   <td>API gateways, LLM routers</td> \n   <td>Semantic routing (Tiny Dancer), MCP protocol gateway, agent-to-agent discovery</td> \n  </tr> \n  <tr> \n   <td>📊</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-profiler/README.md\"><strong>Observability</strong></a></td> \n   <td>Datadog, Prometheus</td> \n   <td>Latency/power/memory profiling, coherence scoring, real-time metrics</td> \n  </tr> \n  <tr> \n   <td>🛡️</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/cognitum-gate-tilezero/README.md\"><strong>Safety</strong></a></td> \n   <td>Manual review, guardrails</td> \n   <td>Cognitum Gate — 256-tile WASM fabric, Permit/Defer/Deny in &lt;1 ms, witness receipts</td> \n  </tr> \n </tbody> \n</table> \n<p><strong>Security &amp; Trust</strong></p> \n<table> \n <thead> \n  <tr> \n   <th></th> \n   <th>Layer</th> \n   <th>Replaces</th> \n   <th>What It Does</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>🔐</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto/README.md\"><strong>Crypto</strong></a></td> \n   <td>Vault, manual audit logs</td> \n   <td>Post-quantum (ML-DSA-65, Ed25519), SHAKE-256, witness chains, hardware attestation</td> \n  </tr> \n  <tr> \n   <td>📜</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto/README.md\"><strong>Lineage</strong></a></td> \n   <td>No equivalent</td> \n   <td>Every operation recorded in a tamper-proof chain — full provenance from creation to deployment</td> \n  </tr> \n </tbody> \n</table> \n<p>The <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">RVF cognitive container</a> ties it all together: a single file that packages your vectors, models, data, and a bootable kernel. Drop it on any machine and it starts serving in 125 ms — no install, no dependencies. It branches like Git (only changes are copied), logs every operation in a tamper-proof chain, and runs anywhere from a browser to bare metal.</p> \n<hr /> \n<h2>How the GNN Works</h2> \n<p>Traditional vector search:</p> \n<pre><code>Query → HNSW Index → Top K Results\n</code></pre> \n<p>RuVector with GNN:</p> \n<pre><code>Query → HNSW Index → GNN Layer → Enhanced Results\n                ↑                      │\n                └──── learns from ─────┘\n</code></pre> \n<p>The GNN layer:</p> \n<ol> \n <li>Takes your query and its nearest neighbors</li> \n <li>Applies multi-head attention to weigh which neighbors matter</li> \n <li>Updates representations based on graph structure</li> \n <li>Returns better-ranked results — all in under 1ms</li> \n</ol> \n<p>This is <strong>temporal learning</strong> — the system learns from the sequence and timing of queries, not just their content. A query asked right after another carries context. Patterns that repeat get reinforced. Paths that lead to good results get stronger over time. The result: search gets faster and more accurate the more you use it, adapting in real time without retraining.</p> \n<details> \n <strong>Deep Dive: How Self-Learning Search Actually Works</strong> \n <h3>The Problem with Normal Search</h3> \n <p>Every vector database does the same thing: you give it a query, it finds the closest matches by distance, and returns them. The results never change. Search the same thing a thousand times and you get the same answer a thousand times — even if the first result was wrong and you always clicked the third one instead.</p> \n <p>RuVector is different. It watches what happens <em>after</em> the search and uses that to make the next search better.</p> \n <h3>What the GNN Actually Does</h3> \n <p>Think of your data as a city map. Each vector is a building, and the HNSW index creates roads between similar buildings. A normal search just walks the shortest road to find nearby buildings.</p> \n <p>The GNN is like a local who knows the shortcuts. It looks at the neighborhood around your destination and says: \"Yes, that building is close, but <em>this</em> one over here is what you actually want.\" It learns these shortcuts by watching which results people actually use.</p> \n <p><strong>Technically, it works in three steps:</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Step</th> \n    <th>What Happens</th> \n    <th>Plain English</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>1. Message Passing</strong></td> \n    <td>Each node collects information from its HNSW neighbors</td> \n    <td>\"Ask the neighborhood what they know\"</td> \n   </tr> \n   <tr> \n    <td><strong>2. Attention Weighting</strong></td> \n    <td>Multi-head attention scores which neighbors matter most for this specific query</td> \n    <td>\"Some neighbors are more helpful than others — figure out which ones\"</td> \n   </tr> \n   <tr> \n    <td><strong>3. Representation Update</strong></td> \n    <td>Node representations shift based on what the neighborhood says</td> \n    <td>\"Update your understanding based on what you learned\"</td> \n   </tr> \n  </tbody> \n </table> \n <p>This entire process takes <strong>under 1ms</strong> thanks to SIMD acceleration (processing 4-8 numbers at once instead of one at a time).</p> \n <h3>Temporal Learning: Time Matters</h3> \n <p>Most AI systems treat every input as independent — they don't know or care what happened 5 seconds ago. RuVector tracks the <em>sequence</em> and <em>timing</em> of queries, which reveals patterns that individual queries can't:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Pattern</th> \n    <th>What It Reveals</th> \n    <th>How RuVector Adapts</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Same user searches A then B within seconds</td> \n    <td>A and B are related, even if they're far apart in vector space</td> \n    <td>Strengthens the path between A and B</td> \n   </tr> \n   <tr> \n    <td>Many users skip result #1 and click result #3</td> \n    <td>Result #3 is actually more relevant</td> \n    <td>GNN learns to rank #3 higher next time</td> \n   </tr> \n   <tr> \n    <td>Query bursts around a topic at certain times</td> \n    <td>Temporal relevance — some things matter more at certain times</td> \n    <td>Boosts recently-active paths</td> \n   </tr> \n   <tr> \n    <td>A query that follows a specific sequence</td> \n    <td>Context from previous queries changes what \"good results\" means</td> \n    <td>Attention weights shift based on session context</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Three Types of Learning</h3> \n <p>RuVector learns at three different speeds simultaneously:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Speed</th> \n    <th>Mechanism</th> \n    <th>What It Does</th> \n    <th>Latency</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Instant</strong></td> \n    <td>MicroLoRA adaptation</td> \n    <td>Adjusts weights for this specific request based on immediate feedback</td> \n    <td>&lt;1ms</td> \n   </tr> \n   <tr> \n    <td><strong>Session</strong></td> \n    <td>GNN attention updates</td> \n    <td>Reinforces paths that led to good results during this session</td> \n    <td>~10ms (background)</td> \n   </tr> \n   <tr> \n    <td><strong>Long-term</strong></td> \n    <td>EWC++ consolidation</td> \n    <td>Permanently strengthens important patterns without forgetting old ones</td> \n    <td>~100ms (background)</td> \n   </tr> \n  </tbody> \n </table> \n <p>The key innovation is <strong>EWC++ (Elastic Weight Consolidation)</strong> — it solves the \"catastrophic forgetting\" problem. Without it, learning new patterns would erase old ones. EWC++ identifies which weights are important for existing knowledge and protects them while still allowing new learning.</p> \n <h3>Why It's Fast: The HNSW Shortcut</h3> \n <p>The GNN doesn't run on your entire dataset. It only runs on the small subgraph of HNSW neighbors that are relevant to the current query — typically 10-50 nodes out of millions. This is why it adds under 1ms of latency instead of seconds:</p> \n <pre><code>1M vectors in your database\n    → HNSW finds ~50 candidate neighbors        (0.3ms)\n    → GNN re-ranks those 50 with attention       (0.4ms)\n    → Return top K results                       (0.1ms)\n    ──────────────────────────────────────────\n    Total: &lt;1ms, and results improve over time\n</code></pre> \n <h3>What Improves Over Time</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Metric</th> \n    <th>Day 1</th> \n    <th>After 1K Queries</th> \n    <th>After 100K Queries</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Recall@10</strong></td> \n    <td>Baseline (HNSW only)</td> \n    <td>+5-8%</td> \n    <td>+12.4%</td> \n   </tr> \n   <tr> \n    <td><strong>Query latency</strong></td> \n    <td>~0.8ms</td> \n    <td>~0.7ms (hot paths cached)</td> \n    <td>~0.5ms (optimized routing)</td> \n   </tr> \n   <tr> \n    <td><strong>Relevance</strong></td> \n    <td>Distance-based only</td> \n    <td>Learns user preferences</td> \n    <td>Personalized per query pattern</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Three GNN Architectures (Pick One or Stack Them)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Architecture</th> \n    <th>Best For</th> \n    <th>How It Works</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>GCN</strong> (Graph Convolutional Network)</td> \n    <td>General-purpose re-ranking</td> \n    <td>Averages neighbor information — simple, fast, effective</td> \n   </tr> \n   <tr> \n    <td><strong>GAT</strong> (Graph Attention Network)</td> \n    <td>Queries where some neighbors matter more than others</td> \n    <td>Learns <em>which</em> neighbors to pay attention to per query</td> \n   </tr> \n   <tr> \n    <td><strong>GraphSAGE</strong></td> \n    <td>Datasets that change frequently (new vectors added often)</td> \n    <td>Can score new vectors it's never seen before, without retraining</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Runs Everywhere</h3> \n <p>The same GNN code runs natively in Rust, in Node.js via NAPI-RS bindings, and in the browser via WebAssembly. Models trained on the server can be exported and run client-side — a user's browser can do personalized re-ranking without sending queries to a server.</p> \n</details> \n<h2>Quick Start</h2> \n<h3>One-Line Install</h3> \n<pre><code class=\"language-bash\"># Interactive installer - lists all packages\nnpx ruvector install\n\n# Or install directly\nnpm install ruvector\nnpx ruvector\n\n# Self-learning hooks for Claude Code\nnpx @ruvector/cli hooks init\nnpx @ruvector/cli hooks install\n\n# LLM runtime (SONA learning, HNSW memory)\nnpm install @ruvector/ruvllm\n</code></pre> \n<h3>Node.js / Browser</h3> \n<pre><code class=\"language-bash\"># Install\nnpm install ruvector\n\n# Or try instantly\nnpx ruvector\n</code></pre> \n<hr /> \n<h3>Ecosystem: AI Agent Orchestration</h3> \n<p>RuVector powers two major AI orchestration platforms:</p> \n<table> \n <thead> \n  <tr> \n   <th>Platform</th> \n   <th>Purpose</th> \n   <th>Install</th> \n   <th>Downloads</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><a href=\"https://github.com/ruvnet/claude-flow\"><strong>ruFlo</strong></a></td> \n   <td>Enterprise multi-agent orchestration for Claude Code</td> \n   <td><code>npx ruvflo@latest</code></td> \n   <td><a href=\"https://www.npmjs.com/package/claude-flow\"><img alt=\"npm downloads\" src=\"https://img.shields.io/npm/dt/claude-flow.svg?sanitize=true\" /></a></td> \n  </tr> \n  <tr> \n   <td><a href=\"https://github.com/ruvnet/agentic-flow\"><strong>Agentic-Flow</strong></a></td> \n   <td>Run AI agents on any cloud with any model — Claude, GPT, Gemini, or local</td> \n   <td><code>npx agentic-flow@latest</code></td> \n   <td><a href=\"https://www.npmjs.com/package/agentic-flow\"><img alt=\"npm downloads\" src=\"https://img.shields.io/npm/dt/agentic-flow.svg?sanitize=true\" /></a></td> \n  </tr> \n  <tr> \n   <td><a href=\"https://github.com/ruvnet/agentdb\"><strong>AgentDB</strong></a></td> \n   <td>Give AI agents long-term memory that gets smarter over time</td> \n   <td><code>npm install agentdb@alpha</code></td> \n   <td><a href=\"https://www.npmjs.com/package/agentdb\"><img alt=\"npm downloads\" src=\"https://img.shields.io/npm/dt/agentdb.svg?sanitize=true\" /></a></td> \n  </tr> \n </tbody> \n</table> \n<details> \n <strong>Claude-Flow v3</strong> — Turn Claude Code into a collaborative AI team \n <p><strong>54+ specialized agents</strong> working together on complex software engineering tasks:</p> \n <pre><code class=\"language-bash\"># Install\nnpx ruvflo@latest init --wizard\n\n# Spawn a swarm\nnpx ruvflo@latest swarm init --topology hierarchical --max-agents 8\n</code></pre> \n <p><strong>Key Features:</strong></p> \n <ul> \n  <li><strong>SONA Learning</strong>: Sub-50ms adaptive routing, learns optimal patterns over time</li> \n  <li><strong>Queen-led Swarms</strong>: Byzantine fault-tolerant consensus with 5 protocols (Raft, Gossip, CRDT)</li> \n  <li><strong>HNSW Memory</strong>: 150x-12,500x faster pattern retrieval via RuVector</li> \n  <li><strong>175+ MCP Tools</strong>: Native Model Context Protocol integration</li> \n  <li><strong>Cost Optimization</strong>: 3-tier routing extends Claude Code quota by 2.5x</li> \n  <li><strong>Security</strong>: AIDefence threat detection (&lt;10ms), prompt injection blocking</li> \n </ul> \n</details> \n<details> \n <strong>Agentic-Flow v2</strong> — Production AI agents for any cloud \n <p><strong>66 self-learning agents</strong> with Claude Agent SDK, deployable to any cloud:</p> \n <pre><code class=\"language-bash\"># Install\nnpx agentic-flow@latest\n\n# Or with npm\nnpm install agentic-flow\n</code></pre> \n <p><strong>Key Features:</strong></p> \n <ul> \n  <li><strong>SONA Architecture</strong>: &lt;1ms adaptive learning, +55% quality improvement</li> \n  <li><strong>Flash Attention</strong>: 2.49x JS speedup, 7.47x with NAPI bindings</li> \n  <li><strong>213 MCP Tools</strong>: Swarm management, memory, GitHub integration</li> \n  <li><strong>Agent Booster</strong>: 352x faster code editing for simple transforms</li> \n  <li><strong>Multi-Provider</strong>: Claude, GPT, Gemini, Cohere, local models with failover</li> \n  <li><strong>Graph Reasoning</strong>: GNN query refinement with +12.4% recall improvement</li> \n </ul> \n</details> \n<details> \n <strong>rvDNA</strong> — AI-native genomic diagnostics, instant and available to everyone \n <p><strong>Using AI to make the world a healthier place.</strong> rvDNA puts genomic diagnostics on any device — a phone, a laptop, a browser tab — in 12 milliseconds. No cloud, no GPU, no subscription. Private by default.</p> \n <pre><code class=\"language-bash\">cargo add rvdna              # Rust\nnpm install @ruvector/rvdna  # JavaScript / TypeScript\n</code></pre> \n <table> \n  <thead> \n   <tr> \n    <th>What It Does</th> \n    <th>How</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Find mutations (sickle cell, cancer)</td> \n    <td>Bayesian variant calling, 155 ns/SNP</td> \n   </tr> \n   <tr> \n    <td>Translate DNA to protein</td> \n    <td>Full codon table + GNN contact graphs</td> \n   </tr> \n   <tr> \n    <td>Predict biological age</td> \n    <td>Horvath clock, 353 CpG sites</td> \n   </tr> \n   <tr> \n    <td>Recommend drug doses</td> \n    <td>CYP2D6 star alleles + CPIC guidelines</td> \n   </tr> \n   <tr> \n    <td>Score health risks</td> \n    <td>20 SNPs, 6 gene-gene interactions, composite risk scoring in 2 us</td> \n   </tr> \n   <tr> \n    <td>Stream biomarker data</td> \n    <td>Real-time anomaly detection, CUSUM changepoints, &gt;100k readings/sec</td> \n   </tr> \n   <tr> \n    <td>Search genomes by similarity</td> \n    <td>HNSW k-mer vectors, O(log N)</td> \n   </tr> \n   <tr> \n    <td>Store pre-computed AI features</td> \n    <td><code>.rvdna</code> binary format — open and instant</td> \n   </tr> \n  </tbody> \n </table> \n <ul> \n  <li><strong>Rust crate</strong>: <a href=\"https://crates.io/crates/rvdna\">crates.io/crates/rvdna</a></li> \n  <li><strong>npm package</strong>: <a href=\"https://www.npmjs.com/package/@ruvector/rvdna\">@ruvector/rvdna</a> (NAPI-RS native + JS fallback)</li> \n  <li><strong>Source</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/dna\">examples/dna</a></li> \n </ul> \n</details> \n<details> \n <strong>RVF Cognitive Containers</strong> — One file that stores, boots, and proves everything \n <p><strong><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">RVF (RuVector Format)</a></strong> is a universal binary substrate that merges database, model, graph engine, kernel, and attestation into a single deployable file. A <code>.rvf</code> file can store vector embeddings, carry LoRA adapter deltas, embed GNN graph state, include a bootable Linux microkernel, run queries in a 5.5 KB WASM runtime, and prove every operation through a cryptographic witness chain — all in one file that runs anywhere from a browser to bare metal.</p> \n <p>This is not a database format. It is an <strong>executable knowledge unit</strong>.</p> \n <pre><code class=\"language-bash\">cargo install rvf-cli                          # CLI tool\ncargo add rvf-runtime                          # Rust library\nnpm install @ruvector/rvf                      # TypeScript SDK\nnpx @ruvector/rvf-mcp-server --transport stdio # MCP server for AI agents\n</code></pre> \n <table> \n  <thead> \n   <tr> \n    <th>What It Does</th> \n    <th>How</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Self-boot as a microservice</td> \n    <td>Real Linux kernel in the file, boots in 125 ms on QEMU/KVM</td> \n   </tr> \n   <tr> \n    <td>Hardware-speed lookups</td> \n    <td>eBPF programs (XDP, TC, socket filter) bypass userspace entirely</td> \n   </tr> \n   <tr> \n    <td>Run in any browser</td> \n    <td>5.5 KB WASM runtime, zero backend</td> \n   </tr> \n   <tr> \n    <td>Git-like branching</td> \n    <td>COW at cluster granularity — 1M vectors, 100 edits = ~2.5 MB child</td> \n   </tr> \n   <tr> \n    <td>Tamper-evident audit</td> \n    <td>Hash-linked witness chain for every insert, query, and deletion</td> \n   </tr> \n   <tr> \n    <td>Post-quantum signatures</td> \n    <td>ML-DSA-65 and Ed25519 signing on every segment</td> \n   </tr> \n   <tr> \n    <td>DNA-style lineage</td> \n    <td>Parent/child derivation chains with cryptographic verification</td> \n   </tr> \n   <tr> \n    <td>28 segment types</td> \n    <td>VEC, INDEX, KERNEL, EBPF, WASM, COW_MAP, WITNESS, CRYPTO, FEDERATED_MANIFEST, and 19 more</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Rust crates</strong> (23): <a href=\"https://crates.io/crates/rvf-types\"><code>rvf-types</code></a> <code>rvf-wire</code> <code>rvf-manifest</code> <code>rvf-quant</code> <code>rvf-index</code> <code>rvf-crypto</code> <a href=\"https://crates.io/crates/rvf-runtime\"><code>rvf-runtime</code></a> <code>rvf-kernel</code> <code>rvf-ebpf</code> <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-federation\"><code>rvf-federation</code></a> <code>rvf-launch</code> <code>rvf-server</code> <code>rvf-import</code> <a href=\"https://crates.io/crates/rvf-cli\"><code>rvf-cli</code></a> <code>rvf-wasm</code> <code>rvf-solver-wasm</code> <code>rvf-node</code> + 6 adapters (claude-flow, agentdb, ospipe, agentic-flow, rvlite, sona)</p> \n <p><strong>npm packages</strong> (4): <a href=\"https://www.npmjs.com/package/@ruvector/rvf\"><code>@ruvector/rvf</code></a> <a href=\"https://www.npmjs.com/package/@ruvector/rvf-node\"><code>@ruvector/rvf-node</code></a> <a href=\"https://www.npmjs.com/package/@ruvector/rvf-wasm\"><code>@ruvector/rvf-wasm</code></a> <a href=\"https://www.npmjs.com/package/@ruvector/rvf-mcp-server\"><code>@ruvector/rvf-mcp-server</code></a></p> \n <ul> \n  <li><strong>Security Hardened RVF</strong> (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/security_hardened.rvf\"><code>examples/security_hardened.rvf</code></a>) — 2.1 MB sealed artifact with 22 verified capabilities: TEE attestation (SGX/SEV-SNP/TDX/ARM CCA), AIDefence (injection/jailbreak/PII/exfil), hardened Linux microkernel, eBPF firewall, Ed25519 signing, 6-role RBAC, Coherence Gate, 30-entry witness chain, Paranoid policy, COW branching, audited k-NN. See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-042-Security-RVF-AIDefence-TEE.md\">ADR-042</a>.</li> \n  <li><strong>Full documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">crates/rvf/README.md</a></li> \n  <li><strong>ADR-030</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-030-rvf-cognitive-container.md\">Cognitive Container Architecture</a></li> \n  <li><strong>ADR-031</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-031-rvcow-branching-and-real-cognitive-containers.md\">COW Branching &amp; Real Containers</a></li> \n  <li><strong>ADR-042</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-042-Security-RVF-AIDefence-TEE.md\">Security RVF — AIDefence + TEE</a></li> \n  <li><strong>56 runnable examples</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/rvf/examples/\">examples/rvf/examples/</a></li> \n </ul> \n</details> \n<details> \n <strong>Sublinear-Time Solver</strong> — math that gets faster as your data gets bigger \n <p><strong><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-solver/README.md\">ruvector-solver</a></strong> solves large math problems (like ranking pages, finding connections in graphs, or computing AI attention) in a fraction of the time traditional solvers need. Where standard approaches slow down dramatically with scale (doubling data = 8x slower), RuVector's 8 specialized algorithms barely notice the increase (doubling data = barely any slower). This is what powers the self-learning engine — fast graph math is what lets search improve in real time instead of waiting minutes to retrain.</p> \n <pre><code class=\"language-bash\">cargo add ruvector-solver --features all-algorithms\n</code></pre> \n <table> \n  <thead> \n   <tr> \n    <th>Algorithm</th> \n    <th>Complexity</th> \n    <th>Best For</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Neumann Series</strong></td> \n    <td>O(k · nnz)</td> \n    <td>Diagonally dominant, fast convergence</td> \n   </tr> \n   <tr> \n    <td><strong>Conjugate Gradient</strong></td> \n    <td>O(√κ · log(1/ε) · nnz)</td> \n    <td>Gold-standard SPD solver</td> \n   </tr> \n   <tr> \n    <td><strong>Forward Push</strong></td> \n    <td>O(1/ε)</td> \n    <td>Single-source PageRank</td> \n   </tr> \n   <tr> \n    <td><strong>Backward Push</strong></td> \n    <td>O(1/ε)</td> \n    <td>Reverse relevance computation</td> \n   </tr> \n   <tr> \n    <td><strong>Hybrid Random Walk</strong></td> \n    <td>O(√n/ε)</td> \n    <td>Pairwise relevance, Monte Carlo</td> \n   </tr> \n   <tr> \n    <td><strong>TRUE</strong></td> \n    <td>O(log n) amortized</td> \n    <td>Large-scale Laplacian systems</td> \n   </tr> \n   <tr> \n    <td><strong>BMSSP</strong></td> \n    <td>O(nnz · log n)</td> \n    <td>Multigrid hierarchical solve</td> \n   </tr> \n   <tr> \n    <td><strong>Auto Router</strong></td> \n    <td>Automatic</td> \n    <td>Selects optimal algorithm</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Key optimizations</strong>: AVX2 SIMD SpMV, fused residual kernels, bounds-check elimination, arena allocator</p> \n <p><strong>Supporting crates</strong>:</p> \n <ul> \n  <li> <p><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attn-mincut/README.md\"><code>ruvector-attn-mincut</code></a> — Min-cut gating as alternative to softmax attention</p> </li> \n  <li> <p><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-coherence/README.md\"><code>ruvector-coherence</code></a> — Coherence measurement for attention comparison</p> </li> \n  <li> <p><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-profiler/README.md\"><code>ruvector-profiler</code></a> — Memory, power, and latency benchmarking</p> </li> \n  <li> <p><strong>177 tests</strong> | 5 Criterion benchmarks | WASM + NAPI bindings</p> </li> \n  <li> <p><strong>ADR documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/research/sublinear-time-solver/\">docs/research/sublinear-time-solver/</a></p> </li> \n </ul> \n</details> \n<hr /> \n<h2>How RuVector Compares</h2> \n<p>See how RuVector stacks up against popular vector databases across 40+ features — from latency and graph queries to self-learning, cognitive containers, and PostgreSQL integration.</p> \n<details> \n 📊 Comparison with Other Vector Databases \n <p>Grouped comparison across 10 categories. RuVector is the only vector database that learns from usage, runs AI locally, and ships as a single self-booting file.</p> \n <p><strong>Performance &amp; Storage</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>Pinecone</th> \n    <th>Qdrant</th> \n    <th>Milvus</th> \n    <th>ChromaDB</th> \n    <th>Weaviate</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Latency (p50)</td> \n    <td><strong>61 us</strong></td> \n    <td>~2 ms</td> \n    <td>~1 ms</td> \n    <td>~5 ms</td> \n    <td>~50 ms</td> \n    <td>~5 ms</td> \n   </tr> \n   <tr> \n    <td>Memory (1M vectors)</td> \n    <td><strong>200 MB</strong>*</td> \n    <td>2 GB</td> \n    <td>1.5 GB</td> \n    <td>1 GB</td> \n    <td>3 GB</td> \n    <td>1.5 GB</td> \n   </tr> \n   <tr> \n    <td>SIMD acceleration</td> \n    <td>AVX-512, NEON</td> \n    <td>Partial</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>Partial</td> \n   </tr> \n   <tr> \n    <td>Auto-compression</td> \n    <td>2-32x adaptive</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>PQ only</td> \n   </tr> \n   <tr> \n    <td>Temporal tensor compression</td> \n    <td>4-10x reuse</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Sparse vectors (BM25/TF-IDF)</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>✅</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Search &amp; Query</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>Pinecone</th> \n    <th>Qdrant</th> \n    <th>Milvus</th> \n    <th>ChromaDB</th> \n    <th>Weaviate</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Vector similarity search</td> \n    <td>✅ HNSW</td> \n    <td>✅</td> \n    <td>✅ HNSW</td> \n    <td>✅ HNSW</td> \n    <td>✅</td> \n    <td>✅ HNSW</td> \n   </tr> \n   <tr> \n    <td>Metadata filtering</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n   </tr> \n   <tr> \n    <td>Graph queries (Cypher)</td> \n    <td>✅ full engine</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>SPARQL/RDF (W3C 1.1)</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Hyperedges (3+ node)</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Hyperbolic embeddings</td> \n    <td>Poincare + Lorentz</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Multi-tenancy</td> \n    <td>✅ collections</td> \n    <td>✅ namespaces</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Self-Learning &amp; AI</strong> — features unique to RuVector</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>All Others</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>GNN on HNSW — search improves from usage</td> \n    <td>✅ every query teaches the index</td> \n    <td>❌ static index</td> \n   </tr> \n   <tr> \n    <td>SONA runtime adaptation</td> \n    <td>✅ LoRA + EWC++ auto-tuning</td> \n    <td>❌ manual tuning</td> \n   </tr> \n   <tr> \n    <td>46 attention mechanisms</td> \n    <td>Flash, linear, graph, hyperbolic, mincut-gated</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Semantic routing (Tiny Dancer)</td> \n    <td>FastGRNN neural agent routing</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Sparse inference (PowerInfer-style)</td> \n    <td>2-10x faster on edge devices</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Domain expansion</td> \n    <td>Cross-domain transfer learning with bandits</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Self-learning hooks</td> \n    <td>Q-learning, neural patterns, HNSW memory</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>ReasoningBank</td> \n    <td>Trajectory learning with verdict judgment</td> \n    <td>❌</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Local AI — no cloud APIs needed</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>Pinecone</th> \n    <th>Qdrant</th> \n    <th>Milvus</th> \n    <th>ChromaDB</th> \n    <th>Weaviate</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Built-in LLM runtime</td> \n    <td>✅ ruvllm (GGUF)</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Hardware acceleration</td> \n    <td>Metal, CUDA, ANE, WebGPU</td> \n    <td>N/A</td> \n    <td>N/A</td> \n    <td>GPU indexing</td> \n    <td>N/A</td> \n    <td>N/A</td> \n   </tr> \n   <tr> \n    <td>Pre-trained models</td> \n    <td><a href=\"https://huggingface.co/ruv/ruvltra\">RuvLTRA</a> (&lt;10 ms)</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Local ONNX embeddings</td> \n    <td>8+ models, no API calls</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>text2vec modules</td> \n   </tr> \n   <tr> \n    <td>MCP server for AI agents</td> \n    <td>✅ mcp-gate</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Graph Transformers</strong> — verified graph neural network modules</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>All Others</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Proof-gated mutation</td> \n    <td>Every write requires a formal proof — bugs cannot corrupt</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Sublinear attention</td> \n    <td>O(n log n) via LSH, PPR, spectral sparsification</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Physics-informed layers</td> \n    <td>Hamiltonian dynamics, energy conserved by construction</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Biological layers</td> \n    <td>Spiking, Hebbian/STDP, dendritic branching</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Manifold geometry</td> \n    <td>Product manifolds S^n x H^m x R^k</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Temporal-causal layers</td> \n    <td>Granger causality, continuous-time ODE</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Economic layers</td> \n    <td>Nash equilibrium, Shapley attribution</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Verified training</td> \n    <td>Certificates, delta-apply rollback, fail-closed</td> \n    <td>❌</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Math &amp; Solvers</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>Pinecone</th> \n    <th>Qdrant</th> \n    <th>Milvus</th> \n    <th>ChromaDB</th> \n    <th>Weaviate</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Sublinear solvers (8 algorithms)</td> \n    <td>O(log n) to O(sqrt(n))</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Dynamic min-cut</td> \n    <td>n^0.12 complexity</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Optimal transport distances</td> \n    <td>Wasserstein, Sinkhorn, KL</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Topological data analysis</td> \n    <td>Persistent homology, Betti numbers</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Coherence measurement</td> \n    <td>Prime Radiant sheaf Laplacian</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Quantum error correction</td> \n    <td>ruQu dynamic min-cut</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Distributed Systems</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>Pinecone</th> \n    <th>Qdrant</th> \n    <th>Milvus</th> \n    <th>ChromaDB</th> \n    <th>Weaviate</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Raft consensus</td> \n    <td>✅</td> \n    <td>❌ managed</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>✅</td> \n   </tr> \n   <tr> \n    <td>Multi-master replication</td> \n    <td>✅ vector clocks</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>✅</td> \n   </tr> \n   <tr> \n    <td>Auto-sharding</td> \n    <td>✅ consistent hashing</td> \n    <td>✅ managed</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>✅</td> \n   </tr> \n   <tr> \n    <td>Delta consensus (CRDT)</td> \n    <td>✅ causal ordering</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Burst scaling (10-50x)</td> \n    <td>✅</td> \n    <td>✅ managed</td> \n    <td>❌</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Snapshots / backups</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>✅</td> \n   </tr> \n   <tr> \n    <td>Streaming API</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>✅</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Cognitive Containers (RVF)</strong> — single-file deployment unique to RuVector</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>All Others</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Self-booting microservice</td> \n    <td><code>.rvf</code> file boots in 125 ms with Linux kernel</td> \n    <td>❌ requires server setup</td> \n   </tr> \n   <tr> \n    <td>eBPF acceleration</td> \n    <td>XDP, socket filter, TC kernel data path</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>COW branching</td> \n    <td>Git-like — 1M vectors, 100 edits = ~2.5 MB branch</td> \n    <td>❌ copy everything</td> \n   </tr> \n   <tr> \n    <td>Witness chains</td> \n    <td>Tamper-evident hash-linked audit trail</td> \n    <td>❌ manual logging</td> \n   </tr> \n   <tr> \n    <td>Post-quantum signatures</td> \n    <td>ML-DSA-65, SLH-DSA-128s, Ed25519</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>25 segment types</td> \n    <td>VEC, INDEX, KERNEL, EBPF, WASM, COW_MAP, and 19 more</td> \n    <td>❌</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Platform &amp; Deployment</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>Pinecone</th> \n    <th>Qdrant</th> \n    <th>Milvus</th> \n    <th>ChromaDB</th> \n    <th>Weaviate</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Browser / WASM</td> \n    <td>✅ WebGPU, 58 KB</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Edge standalone</td> \n    <td>✅ rvLite</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Node.js native</td> \n    <td>✅ NAPI-RS</td> \n    <td>❌</td> \n    <td>Client only</td> \n    <td>Client only</td> \n    <td>✅</td> \n    <td>Client only</td> \n   </tr> \n   <tr> \n    <td>PostgreSQL extension</td> \n    <td>✅ 230+ SQL functions</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>iOS App Clip</td> \n    <td>✅ QR → RVF in &lt;15 MB</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Cloud deployment</td> \n    <td>Cloud Run, Kubernetes</td> \n    <td>Managed only</td> \n    <td>Docker, K8s</td> \n    <td>Docker, K8s</td> \n    <td>Docker</td> \n    <td>Docker, K8s</td> \n   </tr> \n   <tr> \n    <td>FPGA acceleration</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Prometheus metrics</td> \n    <td>✅ built-in</td> \n    <td>Dashboard</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>✅</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Specialized Applications</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>All Others</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Genomics (rvDNA)</td> \n    <td>Variant calling, k-mer search in 12 ms, browser WASM</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Neural trading</td> \n    <td>Kelly Criterion + LSTM-Transformer prediction</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Scientific OCR (SciPix)</td> \n    <td>LaTeX/MathML extraction from papers</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Spiking neural networks</td> \n    <td>Neuromorphic computing, BTSP learning</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Bio-inspired nervous system</td> \n    <td>5-layer adaptive system with EWC plasticity</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>DAG workflows</td> \n    <td>Self-learning directed graph execution</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Cognitum Gate</td> \n    <td>Cognitive AI gateway with TileZero acceleration</td> \n    <td>❌</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Licensing &amp; Cost</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th></th> \n    <th>RuVector</th> \n    <th>Pinecone</th> \n    <th>Qdrant</th> \n    <th>Milvus</th> \n    <th>ChromaDB</th> \n    <th>Weaviate</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>License</td> \n    <td>MIT (free forever)</td> \n    <td>Proprietary</td> \n    <td>Apache 2.0</td> \n    <td>Apache 2.0</td> \n    <td>Apache 2.0</td> \n    <td>BSD-3</td> \n   </tr> \n   <tr> \n    <td>Self-hosted</td> \n    <td>✅</td> \n    <td>❌ managed only</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n   </tr> \n   <tr> \n    <td>Pricing model</td> \n    <td>Free</td> \n    <td>Per-vector/query</td> \n    <td>Free + Cloud</td> \n    <td>Free + managed</td> \n    <td>Free + Cloud</td> \n    <td>Free + Cloud</td> \n   </tr> \n  </tbody> \n </table> \n <p>* Memory with PQ8 compression. Benchmarks on Apple M2 / Intel i7.</p> \n</details> \n<h2>Features</h2> \n<p>Everything RuVector can do — organized by category. Vector search, graph queries, LLM inference, distributed systems, deployment targets, and the self-learning stack that ties it all together.</p> \n<details> \n ⚡ Core Features &amp; Capabilities \n <h3>Core Capabilities</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Vector Search</strong></td> \n    <td>HNSW index, &lt;0.5ms latency, SIMD acceleration</td> \n    <td>Fast enough for real-time apps</td> \n   </tr> \n   <tr> \n    <td><strong>Cypher Queries</strong></td> \n    <td><code>MATCH</code>, <code>WHERE</code>, <code>CREATE</code>, <code>RETURN</code></td> \n    <td>Familiar Neo4j syntax</td> \n   </tr> \n   <tr> \n    <td><strong>GNN Layers</strong></td> \n    <td>Neural network on index topology</td> \n    <td>Search improves with usage</td> \n   </tr> \n   <tr> \n    <td><strong>Hyperedges</strong></td> \n    <td>Connect 3+ nodes at once</td> \n    <td>Model complex relationships</td> \n   </tr> \n   <tr> \n    <td><strong>Metadata Filtering</strong></td> \n    <td>Filter vectors by properties</td> \n    <td>Combine semantic + structured search</td> \n   </tr> \n   <tr> \n    <td><strong>Collections</strong></td> \n    <td>Namespace isolation, multi-tenancy</td> \n    <td>Organize vectors by project/user</td> \n   </tr> \n   <tr> \n    <td><strong>Hyperbolic HNSW</strong></td> \n    <td>Poincaré ball indexing for hierarchies</td> \n    <td>Better tree/taxonomy embeddings</td> \n   </tr> \n   <tr> \n    <td><strong>Sparse Vectors</strong></td> \n    <td>BM25/TF-IDF hybrid search</td> \n    <td>Combine keyword + semantic</td> \n   </tr> \n  </tbody> \n </table> \n <h3>LLM Runtime</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>ruvllm</strong></td> \n    <td>Local LLM inference with GGUF models</td> \n    <td>Run AI without cloud APIs</td> \n   </tr> \n   <tr> \n    <td><strong>Metal/CUDA/ANE</strong></td> \n    <td>Hardware acceleration on Mac/NVIDIA/Apple</td> \n    <td>10-50x faster inference</td> \n   </tr> \n   <tr> \n    <td><strong>ruvllm-wasm</strong></td> \n    <td>Browser LLM with WebGPU acceleration</td> \n    <td>Client-side AI, zero latency</td> \n   </tr> \n   <tr> \n    <td><strong>RuvLTRA Models</strong></td> \n    <td>Pre-trained GGUF for routing &amp; embeddings</td> \n    <td>&lt;10ms inference → <a href=\"https://huggingface.co/ruv/ruvltra\">HuggingFace</a></td> \n   </tr> \n   <tr> \n    <td><strong>Streaming Tokens</strong></td> \n    <td>Real-time token generation</td> \n    <td>Responsive chat UX</td> \n   </tr> \n   <tr> \n    <td><strong>Quantization</strong></td> \n    <td>Q4, Q5, Q8 model support</td> \n    <td>Run 7B models in 4GB RAM</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\">npm install @ruvector/ruvllm        # Node.js\ncargo add ruvllm                    # Rust\n</code></pre> \n <h3>Platform &amp; Edge</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">RVF Cognitive Container</a></strong></td> \n    <td>Single <code>.rvf</code> file: store, boot, branch, prove</td> \n    <td>Replaces Docker + DB + audit system</td> \n   </tr> \n   <tr> \n    <td><strong>rvLite</strong></td> \n    <td>Standalone 2MB edge database</td> \n    <td>IoT, mobile, embedded</td> \n   </tr> \n   <tr> \n    <td><strong>PostgreSQL Extension</strong></td> \n    <td>77+ SQL functions, pgvector replacement</td> \n    <td>Drop-in upgrade for existing DBs</td> \n   </tr> \n   <tr> \n    <td><strong>MCP Server</strong></td> \n    <td>Model Context Protocol integration</td> \n    <td>AI assistant tool calling</td> \n   </tr> \n   <tr> \n    <td><strong>WASM/Browser</strong></td> \n    <td>Full client-side vector search</td> \n    <td>Offline-first apps</td> \n   </tr> \n   <tr> \n    <td><strong>Node.js Bindings</strong></td> \n    <td>Native napi-rs, zero-copy</td> \n    <td>No serialization overhead</td> \n   </tr> \n   <tr> \n    <td><strong>HTTP/gRPC Server</strong></td> \n    <td>REST API with streaming</td> \n    <td>Easy microservice integration</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\">cargo install rvf-cli                    # RVF CLI (17 commands)\ncargo add rvf-runtime                    # RVF Rust library\nnpm install @ruvector/rvf                # RVF TypeScript SDK\ndocker pull ruvnet/ruvector-postgres     # PostgreSQL\nnpm install rvlite                       # Edge DB\nnpx ruvector mcp start                   # MCP Server\n</code></pre> \n <h3>Distributed Systems</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Raft Consensus</strong></td> \n    <td>Leader election, log replication</td> \n    <td>Strong consistency for metadata</td> \n   </tr> \n   <tr> \n    <td><strong>Auto-Sharding</strong></td> \n    <td>Consistent hashing, shard migration</td> \n    <td>Scale to billions of vectors</td> \n   </tr> \n   <tr> \n    <td><strong>Multi-Master Replication</strong></td> \n    <td>Write to any node, conflict resolution</td> \n    <td>High availability, no SPOF</td> \n   </tr> \n   <tr> \n    <td><strong>Snapshots</strong></td> \n    <td>Point-in-time backups, incremental</td> \n    <td>Disaster recovery</td> \n   </tr> \n   <tr> \n    <td><strong>Cluster Metrics</strong></td> \n    <td>Prometheus-compatible monitoring</td> \n    <td>Observability at scale</td> \n   </tr> \n   <tr> \n    <td><strong>Burst Scaling</strong></td> \n    <td>10-50x capacity for traffic spikes</td> \n    <td>Handle viral moments</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\">cargo add ruvector-raft ruvector-cluster ruvector-replication\n</code></pre> \n <h3>AI &amp; ML</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Tensor Compression</strong></td> \n    <td>f32→f16→PQ8→PQ4→Binary</td> \n    <td>2-32x memory reduction</td> \n   </tr> \n   <tr> \n    <td><strong>Differentiable Search</strong></td> \n    <td>Soft attention k-NN</td> \n    <td>End-to-end trainable</td> \n   </tr> \n   <tr> \n    <td><strong>Semantic Router</strong></td> \n    <td>Route queries to optimal endpoints</td> \n    <td>Multi-model AI orchestration</td> \n   </tr> \n   <tr> \n    <td><strong>Hybrid Routing</strong></td> \n    <td>Keyword-first + embedding fallback</td> \n    <td><strong>90% accuracy</strong> for agent routing</td> \n   </tr> \n   <tr> \n    <td><strong>Tiny Dancer</strong></td> \n    <td>FastGRNN neural inference</td> \n    <td>Optimize LLM inference costs</td> \n   </tr> \n   <tr> \n    <td><strong>Adaptive Routing</strong></td> \n    <td>Learn optimal routing strategies</td> \n    <td>Minimize latency, maximize accuracy</td> \n   </tr> \n   <tr> \n    <td><strong>SONA</strong></td> \n    <td>Two-tier LoRA + EWC++ + ReasoningBank</td> \n    <td>Runtime learning without retraining</td> \n   </tr> \n   <tr> \n    <td><strong>Local Embeddings</strong></td> \n    <td>8+ ONNX models built-in</td> \n    <td>No external API needed</td> \n   </tr> \n   <tr> \n    <td><strong><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-verified\">Verified Proofs</a></strong></td> \n    <td>82-byte proof attestations per vector op</td> \n    <td>Structural trust, not just assertions</td> \n   </tr> \n   <tr> \n    <td><strong><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-transformer\">Graph Transformers</a></strong></td> \n    <td>8 proof-gated modules: physics, bio, manifold, temporal, economic</td> \n    <td>Every graph mutation is mathematically verified</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Specialized Processing</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>SciPix OCR</strong></td> \n    <td>LaTeX/MathML from scientific docs</td> \n    <td>Index research papers</td> \n   </tr> \n   <tr> \n    <td><strong>DAG Workflows</strong></td> \n    <td>Self-learning directed acyclic graphs</td> \n    <td>Complex pipeline orchestration</td> \n   </tr> \n   <tr> \n    <td><strong>Cognitum Gate</strong></td> \n    <td>Cognitive AI gateway + TileZero</td> \n    <td>Unified AI model routing</td> \n   </tr> \n   <tr> \n    <td><strong>FPGA Transformer</strong></td> \n    <td>Hardware-accelerated inference</td> \n    <td>Ultra-low latency serving</td> \n   </tr> \n   <tr> \n    <td><strong>ruQu Quantum</strong></td> \n    <td>Quantum error correction via min-cut</td> \n    <td>Future-proof algorithms</td> \n   </tr> \n   <tr> \n    <td><strong>Mincut-Gated Transformer</strong></td> \n    <td>Dynamic attention via graph optimization</td> \n    <td><strong>50% compute reduction</strong></td> \n   </tr> \n   <tr> \n    <td><strong>Sparse Inference</strong></td> \n    <td>Efficient sparse matrix operations</td> \n    <td>10x faster for sparse data</td> \n   </tr> \n   <tr> \n    <td><strong>Sublinear Solver</strong></td> \n    <td>8 sparse algorithms, O(log n)</td> \n    <td>Powers coherence, GNN, spectral</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Self-Learning &amp; Adaptation</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Self-Learning Hooks</strong></td> \n    <td>Q-learning + neural patterns + HNSW</td> \n    <td>System improves automatically</td> \n   </tr> \n   <tr> \n    <td><strong>ReasoningBank</strong></td> \n    <td>Trajectory learning with verdict judgment</td> \n    <td>Learn from successes/failures</td> \n   </tr> \n   <tr> \n    <td><strong>Economy System</strong></td> \n    <td>Tokenomics, CRDT-based distributed state</td> \n    <td>Incentivize agent behavior</td> \n   </tr> \n   <tr> \n    <td><strong>Nervous System</strong></td> \n    <td>Event-driven reactive architecture</td> \n    <td>Real-time adaptation</td> \n   </tr> \n   <tr> \n    <td><strong>Agentic Synthesis</strong></td> \n    <td>Multi-agent workflow composition</td> \n    <td>Emergent problem solving</td> \n   </tr> \n   <tr> \n    <td><strong>EWC++</strong></td> \n    <td>Elastic weight consolidation</td> \n    <td>Prevent catastrophic forgetting</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\">npx @ruvector/cli hooks init      # Install self-learning hooks\nnpx @ruvector/cli hooks install   # Configure for Claude Code\n</code></pre> \n <h3>Attention Mechanisms (<code>@ruvector/attention</code>)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>46 Mechanisms</strong></td> \n    <td>Dot-product, multi-head, flash, linear, sparse, cross-attention, CGT sheaf</td> \n    <td>Cover all transformer and GNN use cases</td> \n   </tr> \n   <tr> \n    <td><strong>Graph Attention</strong></td> \n    <td>RoPE, edge-featured, local-global, neighborhood</td> \n    <td>Purpose-built for graph neural networks</td> \n   </tr> \n   <tr> \n    <td><strong>Hyperbolic Attention</strong></td> \n    <td>Poincaré ball operations, curved-space math</td> \n    <td>Better embeddings for hierarchical data</td> \n   </tr> \n   <tr> \n    <td><strong>SIMD Optimized</strong></td> \n    <td>Native Rust with AVX2/NEON acceleration</td> \n    <td>2-10x faster than pure JS</td> \n   </tr> \n   <tr> \n    <td><strong>Streaming &amp; Caching</strong></td> \n    <td>Chunk-based processing, KV-cache</td> \n    <td>Constant memory, 10x faster inference</td> \n   </tr> \n  </tbody> \n </table> \n <blockquote> \n  <p><strong>Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention/README.md\">Attention Module Docs</a></p> \n </blockquote> \n <h4>Core Attention Mechanisms</h4> \n <p>Standard attention layers for sequence modeling and transformers.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Mechanism</th> \n    <th>Complexity</th> \n    <th>Memory</th> \n    <th>Best For</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>DotProductAttention</strong></td> \n    <td>O(n²)</td> \n    <td>O(n²)</td> \n    <td>Basic attention for small-medium sequences</td> \n   </tr> \n   <tr> \n    <td><strong>MultiHeadAttention</strong></td> \n    <td>O(n²·h)</td> \n    <td>O(n²·h)</td> \n    <td>BERT, GPT-style transformers</td> \n   </tr> \n   <tr> \n    <td><strong>FlashAttention</strong></td> \n    <td>O(n²)</td> \n    <td>O(n)</td> \n    <td>Long sequences with limited GPU memory</td> \n   </tr> \n   <tr> \n    <td><strong>LinearAttention</strong></td> \n    <td>O(n·d)</td> \n    <td>O(n·d)</td> \n    <td>8K+ token sequences, real-time streaming</td> \n   </tr> \n   <tr> \n    <td><strong>HyperbolicAttention</strong></td> \n    <td>O(n²)</td> \n    <td>O(n²)</td> \n    <td>Tree-like data: taxonomies, org charts</td> \n   </tr> \n   <tr> \n    <td><strong>MoEAttention</strong></td> \n    <td>O(n·k)</td> \n    <td>O(n·k)</td> \n    <td>Large models with sparse expert routing</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Graph Attention Mechanisms</h4> \n <p>Attention layers designed for graph-structured data and GNNs.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Mechanism</th> \n    <th>Complexity</th> \n    <th>Best For</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>GraphRoPeAttention</strong></td> \n    <td>O(n²)</td> \n    <td>Position-aware graph transformers</td> \n   </tr> \n   <tr> \n    <td><strong>EdgeFeaturedAttention</strong></td> \n    <td>O(n²·e)</td> \n    <td>Molecules, knowledge graphs with edge data</td> \n   </tr> \n   <tr> \n    <td><strong>DualSpaceAttention</strong></td> \n    <td>O(n²)</td> \n    <td>Hybrid flat + hierarchical embeddings</td> \n   </tr> \n   <tr> \n    <td><strong>LocalGlobalAttention</strong></td> \n    <td>O(n·k + n)</td> \n    <td>100K+ node graphs, scalable GNNs</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Specialized Mechanisms</h4> \n <p>Task-specific attention variants for efficiency and multi-modal learning.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Mechanism</th> \n    <th>Type</th> \n    <th>Best For</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>SparseAttention</strong></td> \n    <td>Efficiency</td> \n    <td>Long docs, low-memory inference</td> \n   </tr> \n   <tr> \n    <td><strong>CrossAttention</strong></td> \n    <td>Multi-modal</td> \n    <td>Image-text, encoder-decoder models</td> \n   </tr> \n   <tr> \n    <td><strong>NeighborhoodAttention</strong></td> \n    <td>Graph</td> \n    <td>Local message passing in GNNs</td> \n   </tr> \n   <tr> \n    <td><strong>HierarchicalAttention</strong></td> \n    <td>Structure</td> \n    <td>Multi-level docs (section → paragraph)</td> \n   </tr> \n   <tr> \n    <td><strong>CGTSheafAttention</strong></td> \n    <td>Coherence</td> \n    <td>Consistency-gated graph transformers</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Hyperbolic Math Functions</h4> \n <p>Operations for Poincaré ball embeddings—curved space that naturally represents hierarchies.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Function</th> \n    <th>Description</th> \n    <th>Use Case</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>expMap(v, c)</code></td> \n    <td>Map to hyperbolic space</td> \n    <td>Initialize embeddings</td> \n   </tr> \n   <tr> \n    <td><code>logMap(p, c)</code></td> \n    <td>Map to flat space</td> \n    <td>Compute gradients</td> \n   </tr> \n   <tr> \n    <td><code>mobiusAddition(x, y, c)</code></td> \n    <td>Add vectors in curved space</td> \n    <td>Aggregate features</td> \n   </tr> \n   <tr> \n    <td><code>poincareDistance(x, y, c)</code></td> \n    <td>Measure hyperbolic distance</td> \n    <td>Compute similarity</td> \n   </tr> \n   <tr> \n    <td><code>projectToPoincareBall(p, c)</code></td> \n    <td>Ensure valid coordinates</td> \n    <td>Prevent numerical errors</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Async &amp; Batch Operations</h4> \n <p>Utilities for high-throughput inference and training optimization.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Operation</th> \n    <th>Description</th> \n    <th>Performance</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>asyncBatchCompute()</code></td> \n    <td>Process batches in parallel</td> \n    <td>3-5x faster</td> \n   </tr> \n   <tr> \n    <td><code>streamingAttention()</code></td> \n    <td>Process in chunks</td> \n    <td>Fixed memory usage</td> \n   </tr> \n   <tr> \n    <td><code>HardNegativeMiner</code></td> \n    <td>Find hard training examples</td> \n    <td>Better contrastive learning</td> \n   </tr> \n   <tr> \n    <td><code>AttentionCache</code></td> \n    <td>Cache key-value pairs</td> \n    <td>10x faster inference</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\"># Install attention module\nnpm install @ruvector/attention\n\n# CLI commands\nnpx ruvector attention list                    # List all 39 mechanisms\nnpx ruvector attention info flash              # Details on FlashAttention\nnpx ruvector attention benchmark               # Performance comparison\nnpx ruvector attention compute -t dot -d 128   # Run attention computation\nnpx ruvector attention hyperbolic -a distance -v \"[0.1,0.2]\" -b \"[0.3,0.4]\"\n</code></pre> \n <h3>Coherence Gate (<code>prime-radiant</code>)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Sheaf Laplacian</strong></td> \n    <td>Measures consistency via E(S) = Σ wₑ · ‖ρᵤ(xᵤ) - ρᵥ(xᵥ)‖²</td> \n    <td>Mathematical proof of coherence</td> \n   </tr> \n   <tr> \n    <td><strong>Compute Ladder</strong></td> \n    <td>Reflex (&lt;1ms) → Retrieval (~10ms) → Heavy (~100ms) → Human</td> \n    <td>Route by confidence level</td> \n   </tr> \n   <tr> \n    <td><strong>LLM Hallucination Gate</strong></td> \n    <td>Block incoherent responses with witnesses</td> \n    <td>Refuse generation when math says contradiction</td> \n   </tr> \n   <tr> \n    <td><strong>GPU/SIMD Acceleration</strong></td> \n    <td>wgpu + AVX-512/NEON + vec4 WGSL kernels</td> \n    <td>4-16x speedup on coherence checks</td> \n   </tr> \n   <tr> \n    <td><strong>Governance Audit</strong></td> \n    <td>Blake3 hash chain, cryptographic witnesses</td> \n    <td>Every decision is provable</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Coherence vs Confidence</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Traditional AI</th> \n    <th>Prime-Radiant</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>\"I'm 85% confident\"</td> \n    <td>\"Zero contradictions found\"</td> \n   </tr> \n   <tr> \n    <td>Can be confidently wrong</td> \n    <td>Knows when it doesn't know</td> \n   </tr> \n   <tr> \n    <td>Guesses about the future</td> \n    <td>Proves consistency right now</td> \n   </tr> \n   <tr> \n    <td>Trust the model</td> \n    <td>Trust the math</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Compute Ladder Routing</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Energy</th> \n    <th>Lane</th> \n    <th>Latency</th> \n    <th>Action</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>&lt; 0.1</td> \n    <td>Reflex</td> \n    <td>&lt; 1ms</td> \n    <td>Immediate approval</td> \n   </tr> \n   <tr> \n    <td>0.1-0.4</td> \n    <td>Retrieval</td> \n    <td>~10ms</td> \n    <td>Fetch more evidence</td> \n   </tr> \n   <tr> \n    <td>0.4-0.7</td> \n    <td>Heavy</td> \n    <td>~100ms</td> \n    <td>Deep analysis</td> \n   </tr> \n   <tr> \n    <td>&gt; 0.7</td> \n    <td>Human</td> \n    <td>async</td> \n    <td>Escalate to review</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\"># Install coherence engine\ncargo add prime-radiant\n\n# With GPU acceleration\ncargo add prime-radiant --features gpu,simd\n</code></pre> \n</details> \n<h2>Deployment</h2> \n<p>Run RuVector wherever your application lives — as a server, a PostgreSQL extension, a browser library, an edge database, or a self-booting container.</p> \n<details> \n 🚀 Deployment Options \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>HTTP/gRPC Server</strong></td> \n    <td>REST API, streaming support</td> \n    <td>Easy integration</td> \n   </tr> \n   <tr> \n    <td><strong>WASM/Browser</strong></td> \n    <td>Full client-side support</td> \n    <td>Run AI search offline</td> \n   </tr> \n   <tr> \n    <td><strong>Node.js Bindings</strong></td> \n    <td>Native napi-rs bindings</td> \n    <td>No serialization overhead</td> \n   </tr> \n   <tr> \n    <td><strong>FFI Bindings</strong></td> \n    <td>C-compatible interface</td> \n    <td>Use from Python, Go, etc.</td> \n   </tr> \n   <tr> \n    <td><strong>CLI Tools</strong></td> \n    <td>Benchmarking, testing, management</td> \n    <td>DevOps-friendly</td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<h2>Performance</h2> \n<p>Real numbers from real benchmarks — measured on Apple M4 Pro (48GB RAM) with Criterion.rs statistical sampling.</p> \n<details> \n 📈 Performance Benchmarks \n <h3>Vector Search (HNSW)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Configuration</th> \n    <th>QPS</th> \n    <th>p50 Latency</th> \n    <th>p99 Latency</th> \n    <th>Recall</th> \n    <th>Dataset</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Single-threaded</strong></td> \n    <td>394</td> \n    <td>1.80ms</td> \n    <td>1.84ms</td> \n    <td>100%</td> \n    <td>50K vectors, 384D</td> \n   </tr> \n   <tr> \n    <td><strong>Multi-threaded (16)</strong></td> \n    <td>3,597</td> \n    <td>2.86ms</td> \n    <td>8.47ms</td> \n    <td>100%</td> \n    <td>50K vectors, 384D</td> \n   </tr> \n   <tr> \n    <td><strong>Optimized (SIMD)</strong></td> \n    <td>1,216</td> \n    <td>0.78ms</td> \n    <td>0.78ms</td> \n    <td>100%</td> \n    <td>10K vectors, 384D</td> \n   </tr> \n   <tr> \n    <td>Python baseline</td> \n    <td>77</td> \n    <td>11.88ms</td> \n    <td>11.88ms</td> \n    <td>100%</td> \n    <td>10K vectors, 384D</td> \n   </tr> \n   <tr> \n    <td>Brute force</td> \n    <td>12</td> \n    <td>77.76ms</td> \n    <td>77.76ms</td> \n    <td>100%</td> \n    <td>10K vectors, 384D</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>15.7x faster than Python</strong> — 100% recall at every configuration.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Search k</th> \n    <th>p50 Latency</th> \n    <th>Throughput</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>k=1</td> \n    <td>18.9µs</td> \n    <td>53K QPS</td> \n   </tr> \n   <tr> \n    <td>k=10</td> \n    <td>25.2µs</td> \n    <td>40K QPS</td> \n   </tr> \n   <tr> \n    <td>k=100</td> \n    <td>77.9µs</td> \n    <td>13K QPS</td> \n   </tr> \n  </tbody> \n </table> \n <h3>SIMD Distance Calculations (NEON)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Metric</th> \n    <th>128D</th> \n    <th>384D</th> \n    <th>768D</th> \n    <th>1536D</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Euclidean</strong></td> \n    <td>14.9ns (67M/s)</td> \n    <td>55.3ns (18M/s)</td> \n    <td>115.3ns (8.7M/s)</td> \n    <td>279.6ns (3.6M/s)</td> \n   </tr> \n   <tr> \n    <td><strong>Cosine</strong></td> \n    <td>16.4ns (61M/s)</td> \n    <td>60.4ns (17M/s)</td> \n    <td>128.8ns (7.8M/s)</td> \n    <td>302.9ns (3.3M/s)</td> \n   </tr> \n   <tr> \n    <td><strong>Dot Product</strong></td> \n    <td>12.0ns (83M/s)</td> \n    <td>52.7ns (19M/s)</td> \n    <td>112.2ns (8.9M/s)</td> \n    <td>292.3ns (3.4M/s)</td> \n   </tr> \n  </tbody> \n </table> \n <p>SIMD speedup: <strong>2.9x</strong> (Euclidean/Dot Product), <strong>5.95x</strong> (Cosine).</p> \n <h3>Quantization</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Mode</th> \n    <th>Compression</th> \n    <th>Encode (384D)</th> \n    <th>Distance (384D)</th> \n    <th>Memory per 1M vectors</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>None (f32)</strong></td> \n    <td>1x</td> \n    <td>—</td> \n    <td>55.3ns</td> \n    <td>1.46 GB</td> \n   </tr> \n   <tr> \n    <td><strong>Scalar (INT8)</strong></td> \n    <td>4x</td> \n    <td>213ns</td> \n    <td>31ns</td> \n    <td>366 MB</td> \n   </tr> \n   <tr> \n    <td><strong>INT4</strong></td> \n    <td>8x</td> \n    <td>—</td> \n    <td>—</td> \n    <td>183 MB</td> \n   </tr> \n   <tr> \n    <td><strong>Binary</strong></td> \n    <td>32x</td> \n    <td>208ns</td> \n    <td>0.9ns</td> \n    <td>46 MB</td> \n   </tr> \n  </tbody> \n </table> \n <p>Binary quantization: <strong>sub-nanosecond</strong> distance calculations.</p> \n <h3>Insert Throughput</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Operation</th> \n    <th>Latency</th> \n    <th>Throughput</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Single insert (384D)</strong></td> \n    <td>4.63ms</td> \n    <td>216/s</td> \n   </tr> \n   <tr> \n    <td><strong>Batch 100</strong></td> \n    <td>34.1ms</td> \n    <td>2,928/s</td> \n   </tr> \n   <tr> \n    <td><strong>Batch 500</strong></td> \n    <td>72.8ms</td> \n    <td>6,865/s</td> \n   </tr> \n   <tr> \n    <td><strong>Batch 1000</strong></td> \n    <td>152.0ms</td> \n    <td>6,580/s</td> \n   </tr> \n  </tbody> \n </table> \n <p>Batch inserts: <strong>30x faster</strong> than single inserts.</p> \n <h3>LLM Inference (ruvllm)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Operation</th> \n    <th>Configuration</th> \n    <th>Latency</th> \n    <th>vs Target</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Flash Attention</strong></td> \n    <td>256 seq</td> \n    <td>840µs</td> \n    <td>2.4x better than 2ms target</td> \n   </tr> \n   <tr> \n    <td><strong>RMSNorm</strong></td> \n    <td>4096 dim</td> \n    <td>620ns</td> \n    <td>16x better than 10µs target</td> \n   </tr> \n   <tr> \n    <td><strong>GEMV</strong></td> \n    <td>4096x4096</td> \n    <td>1.36ms</td> \n    <td>3.7x better than 5ms target</td> \n   </tr> \n   <tr> \n    <td><strong>MicroLoRA forward</strong></td> \n    <td>rank=2, 4096 dim</td> \n    <td>8.56µs (scalar) / 2.61µs (SIMD)</td> \n    <td>117x–383x better than 1ms target</td> \n   </tr> \n   <tr> \n    <td><strong>RoPE</strong></td> \n    <td>128 dim, 32 tokens</td> \n    <td>1.33µs</td> \n    <td>9.6x better than 50µs target</td> \n   </tr> \n  </tbody> \n </table> \n <table> \n  <thead> \n   <tr> \n    <th>GEMV Scaling (M4 Pro)</th> \n    <th>Threads</th> \n    <th>Speedup</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td></td> \n    <td>1</td> \n    <td>1.0x</td> \n   </tr> \n   <tr> \n    <td></td> \n    <td>4</td> \n    <td>3.4x</td> \n   </tr> \n   <tr> \n    <td></td> \n    <td>8</td> \n    <td>6.1x</td> \n   </tr> \n   <tr> \n    <td></td> \n    <td>10</td> \n    <td><strong>12.7x</strong></td> \n   </tr> \n  </tbody> \n </table> \n <h3>ef_search Tuning</h3> \n <table> \n  <thead> \n   <tr> \n    <th>ef_search</th> \n    <th>QPS</th> \n    <th>p50 Latency</th> \n    <th>Recall</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>50</td> \n    <td>674</td> \n    <td>1.35ms</td> \n    <td>100%</td> \n   </tr> \n   <tr> \n    <td>100</td> \n    <td>596</td> \n    <td>1.37ms</td> \n    <td>100%</td> \n   </tr> \n   <tr> \n    <td>200</td> \n    <td>572</td> \n    <td>1.40ms</td> \n    <td>100%</td> \n   </tr> \n   <tr> \n    <td>400</td> \n    <td>434</td> \n    <td>1.97ms</td> \n    <td>100%</td> \n   </tr> \n   <tr> \n    <td>800</td> \n    <td>434</td> \n    <td>1.77ms</td> \n    <td>100%</td> \n   </tr> \n  </tbody> \n </table> \n <p>100% recall across all ef_search values — choose speed vs safety margin.</p> \n <h3>Cloud Scale (Production)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Metric</th> \n    <th>Value</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Concurrent Streams</strong></td> \n    <td>500M baseline, burst to 25B (50x)</td> \n   </tr> \n   <tr> \n    <td><strong>Global Latency</strong></td> \n    <td>p50 &lt;10ms, p99 &lt;50ms</td> \n   </tr> \n   <tr> \n    <td><strong>Availability</strong></td> \n    <td>99.99% SLA across 15 regions</td> \n   </tr> \n   <tr> \n    <td><strong>Throughput per Region</strong></td> \n    <td>100K+ QPS</td> \n   </tr> \n   <tr> \n    <td><strong>Memory Efficiency</strong></td> \n    <td>2-32x adaptive compression</td> \n   </tr> \n   <tr> \n    <td><strong>Index Build</strong></td> \n    <td>1M vectors/min (parallel HNSW)</td> \n   </tr> \n   <tr> \n    <td><strong>Replication Lag</strong></td> \n    <td>&lt;100ms (multi-master async)</td> \n   </tr> \n  </tbody> \n </table> \n <p><em>Full results: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/bench_results/\">bench_results/</a> | <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/benchmarks/\">docs/benchmarks/</a> | Run: <code>cargo bench -p ruvector-core</code></em></p> \n</details> \n<h2>Compression</h2> \n<p>RuVector automatically manages memory like a CPU cache — hot data stays at full precision, cold data compresses in the background. No manual tuning required.</p> \n<details> \n 🗜️ Adaptive Compression Tiers \n <p><strong>The architecture adapts to your data.</strong> Hot paths get full precision and maximum compute. Cold paths compress automatically and throttle resources. Recent data stays crystal clear; historical data optimizes itself in the background.</p> \n <p>Think of it like your computer's memory hierarchy—frequently accessed data lives in fast cache, while older files move to slower, denser storage. RuVector does this automatically for your vectors:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Access Frequency</th> \n    <th>Format</th> \n    <th>Compression</th> \n    <th>What Happens</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Hot</strong> (&gt;80%)</td> \n    <td>f32</td> \n    <td>1x</td> \n    <td>Full precision, instant retrieval</td> \n   </tr> \n   <tr> \n    <td><strong>Warm</strong> (40-80%)</td> \n    <td>f16</td> \n    <td>2x</td> \n    <td>Slight compression, imperceptible latency</td> \n   </tr> \n   <tr> \n    <td><strong>Cool</strong> (10-40%)</td> \n    <td>PQ8</td> \n    <td>8x</td> \n    <td>Smart quantization, ~1ms overhead</td> \n   </tr> \n   <tr> \n    <td><strong>Cold</strong> (1-10%)</td> \n    <td>PQ4</td> \n    <td>16x</td> \n    <td>Heavy compression, still fast search</td> \n   </tr> \n   <tr> \n    <td><strong>Archive</strong> (&lt;1%)</td> \n    <td>Binary</td> \n    <td>32x</td> \n    <td>Maximum density, batch retrieval</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>No configuration needed.</strong> RuVector tracks access patterns and automatically promotes/demotes vectors between tiers. Your hot data stays fast; your cold data shrinks.</p> \n</details> \n<h2>Use Cases</h2> \n<details> \n From AI-powered search to genomics, from real-time recommendations to knowledge graphs — see what people are building with RuVector. \n <h3>AI &amp; LLM Applications</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>RAG Pipelines</strong></td> \n    <td>Local vector search + local LLM — zero cloud costs, search improves from every query</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/ruvLLM\">examples/ruvLLM</a></td> \n   </tr> \n   <tr> \n    <td><strong>AI Agent Memory</strong></td> \n    <td>GNN-backed HNSW memory that agents share and learn from across sessions</td> \n    <td><a href=\"https://github.com/ruvnet/agentic-flow\">Agentic-Flow</a></td> \n   </tr> \n   <tr> \n    <td><strong>Agent Routing</strong></td> \n    <td>Semantic router with SONA self-learning picks the right agent in &lt;1ms</td> \n    <td><a href=\"https://github.com/ruvnet/claude-flow\">Claude-Flow</a></td> \n   </tr> \n   <tr> \n    <td><strong>Self-Learning Chatbots</strong></td> \n    <td>ReasoningBank + EWC++ — learns from conversations without forgetting previous ones</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/meta-cognition-spiking-neural-network\">examples/meta-cognition</a></td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-javascript\">// RAG with local LLM — zero cloud costs, search gets smarter over time\nconst db = new RuVector({ dimensions: 384 });\nconst llm = new RuvLLM({ model: 'ruvltra-small-0.5b-q4_k_m.gguf' });\n\nconst context = await db.search(questionEmbedding, { k: 5 }); // GNN-enhanced\nconst response = await llm.generate(`Context: ${context}\\n\\nQ: ${question}`);\n</code></pre> \n <h3>Search &amp; Discovery</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Semantic Search</strong></td> \n    <td>Sub-millisecond HNSW with SIMD acceleration — 80K QPS on 8 cores</td> \n    <td>Core feature</td> \n   </tr> \n   <tr> \n    <td><strong>Hybrid Search</strong></td> \n    <td>BM25 keywords + vector embeddings in one query, GNN reranking</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/api/\">docs/api</a></td> \n   </tr> \n   <tr> \n    <td><strong>Image / Audio / Video</strong></td> \n    <td>Any embedding model works — CLIP, Whisper, CLAP — with metadata filtering</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/wasm-react\">examples/wasm-react</a></td> \n   </tr> \n   <tr> \n    <td><strong>Code Search</strong></td> \n    <td>Local ONNX embeddings + graph queries find semantically similar code</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/nodejs\">examples/nodejs</a></td> \n   </tr> \n   <tr> \n    <td><strong>E-commerce</strong></td> \n    <td>Product search with price/category filters applied during search, not after</td> \n    <td>Core feature</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-javascript\">// Hybrid search: keyword + semantic with filtering\nconst results = await db.search(query, {\n  k: 10,\n  filter: { category: 'electronics', price: { $lt: 500 } },\n  hybridAlpha: 0.7,  // 70% semantic, 30% keyword\n  rerank: true       // GNN-enhanced reranking\n});\n</code></pre> \n <h3>Recommendations &amp; Knowledge Graphs</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Product Recommendations</strong></td> \n    <td>Neo4j-compatible Cypher queries with GNN scoring — no separate graph DB</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/graph\">examples/graph</a></td> \n   </tr> \n   <tr> \n    <td><strong>Knowledge Graphs</strong></td> \n    <td>Hypergraph with Cypher + W3C SPARQL 1.1, hyperedge relationships</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/api/CYPHER_REFERENCE.md\">docs/api/CYPHER_REFERENCE.md</a></td> \n   </tr> \n   <tr> \n    <td><strong>Document Q&amp;A</strong></td> \n    <td>Chunking, embeddings, RAG pipeline — all local, all self-improving</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/refrag-pipeline\">examples/refrag-pipeline</a></td> \n   </tr> \n   <tr> \n    <td><strong>Research Discovery</strong></td> \n    <td>Citation graph traversal, concept linking, multi-hop reasoning</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/scipix\">examples/scipix</a></td> \n   </tr> \n   <tr> \n    <td><strong>Content Personalization</strong></td> \n    <td>User embeddings + collaborative filtering adapt in real time</td> \n    <td>Real-time adaptation</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-cypher\">-- Neo4j-style graph query — runs inside RuVector, no separate database\nMATCH (user:User {id: $userId})-[:VIEWED]-&gt;(item:Product)\nMATCH (item)-[:SIMILAR_TO]-&gt;(rec:Product)\nWHERE NOT (user)-[:PURCHASED]-&gt;(rec)\nRETURN rec ORDER BY rec.gnn_score DESC LIMIT 10\n</code></pre> \n <h3>Edge, Browser &amp; IoT</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Browser AI</strong></td> \n    <td>Full LLM + vector search in WASM — no server, runs in any browser</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/wasm-vanilla\">examples/wasm-vanilla</a></td> \n   </tr> \n   <tr> \n    <td><strong>IoT / Sensors</strong></td> \n    <td>2MB footprint, <code>no_std</code> support, offline-first with sync on reconnect</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge\">examples/edge</a></td> \n   </tr> \n   <tr> \n    <td><strong>Mobile Apps</strong></td> \n    <td>Embed as a library — offline search, on-device learning</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge-net\">examples/edge-net</a></td> \n   </tr> \n   <tr> \n    <td><strong>Streaming Data</strong></td> \n    <td>Real-time indexing with dynamic min-cut for live data feeds</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/neural-trader\">examples/neural-trader</a></td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-javascript\">// Full AI in the browser — no server required\nimport init, { RuvLLMWasm } from '@ruvector/ruvllm-wasm';\nawait init();\nconst llm = await RuvLLMWasm.new(true); // WebGPU enabled\nconst response = await llm.generate('Explain quantum computing', { max_tokens: 200 });\n</code></pre> \n <h3>Self-Learning &amp; Fine-Tuning</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Per-Request Adaptation</strong></td> \n    <td>MicroLoRA adapts model weights in &lt;1ms based on user feedback</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/ruvllm/FINE_TUNING.md\">docs/ruvllm/FINE_TUNING.md</a></td> \n   </tr> \n   <tr> \n    <td><strong>Contrastive Training</strong></td> \n    <td>Triplet loss with hard negative mining — learns what's similar and what isn't</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/npm/packages/ruvllm\">npm/packages/ruvllm</a></td> \n   </tr> \n   <tr> \n    <td><strong>Memory Preservation</strong></td> \n    <td>EWC++ prevents catastrophic forgetting — learns new things without losing old ones</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/sona\">crates/sona</a></td> \n   </tr> \n   <tr> \n    <td><strong>Task-Specific Adapters</strong></td> \n    <td>5 built-in adapters (Coder, Researcher, Security, Architect, Reviewer)</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/training/\">docs/training</a></td> \n   </tr> \n   <tr> \n    <td><strong>Browser Fine-Tuning</strong></td> \n    <td>MicroLoRA in WASM — &lt;50KB adapters persist to localStorage</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm-wasm\">crates/ruvllm-wasm</a></td> \n   </tr> \n  </tbody> \n </table> \n <table> \n  <thead> \n   <tr> \n    <th>Tier</th> \n    <th>How It Works</th> \n    <th>Speed</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Instant</strong></td> \n    <td>MicroLoRA (rank 1-2) per request</td> \n    <td>&lt;1ms</td> \n   </tr> \n   <tr> \n    <td><strong>Background</strong></td> \n    <td>Adapter merge + EWC++ consolidation</td> \n    <td>~100ms</td> \n   </tr> \n   <tr> \n    <td><strong>Deep</strong></td> \n    <td>Full training pipeline</td> \n    <td>Minutes</td> \n   </tr> \n  </tbody> \n </table> \n <h3>AI Safety &amp; Coherence</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Agent Safety Gates</strong></td> \n    <td>256-tile WASM fabric — Permit / Defer / Deny decisions in &lt;1ms</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/cognitum-gate-tilezero\">crates/cognitum-gate</a></td> \n   </tr> \n   <tr> \n    <td><strong>Cryptographic Audit</strong></td> \n    <td>Hash-chained witness receipts — tamper-proof record of every AI action</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto\">crates/rvf-crypto</a></td> \n   </tr> \n   <tr> \n    <td><strong>Coherence Checking</strong></td> \n    <td>Min-cut aggregation detects when AI agents drift from intended behavior</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/mincut\">examples/mincut</a></td> \n   </tr> \n   <tr> \n    <td><strong>Post-Quantum Security</strong></td> \n    <td>ML-DSA-65, SLH-DSA-128s, Ed25519 signatures on every operation</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto\">crates/rvf-crypto</a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Neuromorphic &amp; Scientific Computing</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Spiking Neural Networks</strong></td> \n    <td>LIF neurons with STDP learning — 10-50x more energy efficient than ANNs</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/meta-cognition-spiking-neural-network\">examples/meta-cognition</a></td> \n   </tr> \n   <tr> \n    <td><strong>Neuromorphic Search</strong></td> \n    <td>micro-hnsw: brain-inspired vector search in 11.8KB WASM</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/micro-hnsw-wasm\">crates/micro-hnsw</a></td> \n   </tr> \n   <tr> \n    <td><strong>Algorithmic Trading</strong></td> \n    <td>Neural Trader with time-series analysis and real-time decision making</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/neural-trader\">examples/neural-trader</a></td> \n   </tr> \n   <tr> \n    <td><strong>Quantum Computing</strong></td> \n    <td>ruQu quantum circuit simulation with min-cut coherence</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruQu\">crates/ruQu</a></td> \n   </tr> \n   <tr> \n    <td><strong>Genomics</strong></td> \n    <td>DNA sequence similarity, variant calling, population analysis</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/dna\">examples/dna</a></td> \n   </tr> \n   <tr> \n    <td><strong>Graph Transformers</strong></td> \n    <td>8 verified modules — physics, bio, manifold, temporal, economic, and more</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-transformer\">crates/ruvector-graph-transformer</a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Distributed &amp; Enterprise</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>PostgreSQL Drop-In</strong></td> \n    <td>230+ SQL functions — replace pgvector with self-learning search</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-postgres\">crates/ruvector-postgres</a></td> \n   </tr> \n   <tr> \n    <td><strong>Multi-Region HA</strong></td> \n    <td>Raft consensus, multi-master replication, automatic failover</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/cloud-architecture/\">docs/cloud-architecture</a></td> \n   </tr> \n   <tr> \n    <td><strong>Burst Scaling</strong></td> \n    <td>10-50x auto-scaling with quorum writes and load balancing</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/google-cloud\">examples/google-cloud</a></td> \n   </tr> \n   <tr> \n    <td><strong>Cognitive Containers</strong></td> \n    <td>Single <code>.rvf</code> file boots as a microservice in 125ms — data + code + lineage</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf\">crates/rvf</a></td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-sql\">-- Drop-in PostgreSQL replacement with self-improving search\nCREATE EXTENSION ruvector;\nCREATE TABLE documents (id SERIAL PRIMARY KEY, content TEXT, embedding VECTOR(384));\nCREATE INDEX ON documents USING hnsw_gnn (embedding);\n\nSELECT * FROM documents ORDER BY embedding &lt;-&gt; query_vector LIMIT 10;\n</code></pre> \n <h3>Agentic Workflows</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Version Control for AI</strong></td> \n    <td>Agentic Jujutsu — branch, merge, and diff AI model states</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/agentic-jujutsu\">examples/agentic-jujutsu</a></td> \n   </tr> \n   <tr> \n    <td><strong>Self-Learning Pipelines</strong></td> \n    <td>DAG workflows that learn optimal execution paths over time</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-dag\">crates/ruvector-dag</a></td> \n   </tr> \n   <tr> \n    <td><strong>Web Scraping → Embeddings</strong></td> \n    <td>Apify integration — scrape, embed, and index in one pipeline</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/apify\">examples/apify</a></td> \n   </tr> \n   <tr> \n    <td><strong>Synthetic Data</strong></td> \n    <td>Agentic synthesis and generation for training data</td> \n    <td><a href=\"https://github.com/ruvnet/agentic-flow\">Agentic-Flow</a></td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<h2>Installation</h2> \n<p>RuVector runs on Node.js, Rust, browsers, PostgreSQL, and Docker. Pick the package that fits your stack.</p> \n<table> \n <thead> \n  <tr> \n   <th>Platform</th> \n   <th>Command</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><strong>npm</strong></td> \n   <td><code>npm install ruvector</code></td> \n  </tr> \n  <tr> \n   <td><strong>npm (SONA)</strong></td> \n   <td><code>npm install @ruvector/sona</code></td> \n  </tr> \n  <tr> \n   <td><strong>npm (Genomics)</strong></td> \n   <td><code>npm install @ruvector/rvdna</code></td> \n  </tr> \n  <tr> \n   <td><strong>npm (RVF)</strong></td> \n   <td><code>npm install @ruvector/rvf</code></td> \n  </tr> \n  <tr> \n   <td><strong>Browser/WASM</strong></td> \n   <td><code>npm install ruvector-wasm</code></td> \n  </tr> \n  <tr> \n   <td><strong>Rust</strong></td> \n   <td><code>cargo add ruvector-core ruvector-graph ruvector-gnn</code></td> \n  </tr> \n  <tr> \n   <td><strong>Rust (RVF)</strong></td> \n   <td><code>cargo add rvf-runtime</code></td> \n  </tr> \n  <tr> \n   <td><strong>Rust (Genomics)</strong></td> \n   <td><code>cargo add rvdna</code></td> \n  </tr> \n  <tr> \n   <td><strong>Rust (SONA)</strong></td> \n   <td><code>cargo add ruvector-sona</code></td> \n  </tr> \n  <tr> \n   <td><strong>Rust (LLM)</strong></td> \n   <td><code>cargo add ruvllm</code></td> \n  </tr> \n  <tr> \n   <td><strong>RVF CLI</strong></td> \n   <td><code>cargo install rvf-cli</code></td> \n  </tr> \n  <tr> \n   <td><strong>RVF MCP</strong></td> \n   <td><code>npx @ruvector/rvf-mcp-server --transport stdio</code></td> \n  </tr> \n </tbody> \n</table> \n<hr /> \n<h2>Package Reference</h2> \n<details> \n 📖 Documentation \n <h4>Getting Started</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Topic</th> \n    <th>Link</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Getting Started</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/guides/GETTING_STARTED.md\">docs/guides/GETTING_STARTED.md</a></td> \n   </tr> \n   <tr> \n    <td>API Reference</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/api/\">docs/api/</a></td> \n   </tr> \n   <tr> \n    <td>Cypher Reference</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/api/CYPHER_REFERENCE.md\">docs/api/CYPHER_REFERENCE.md</a></td> \n   </tr> \n   <tr> \n    <td>Performance Tuning</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/optimization/PERFORMANCE_TUNING_GUIDE.md\">docs/optimization/PERFORMANCE_TUNING_GUIDE.md</a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Core Components</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Topic</th> \n    <th>Link</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>GNN Architecture</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/gnn/\">docs/gnn/</a></td> \n   </tr> \n   <tr> \n    <td>HNSW Indexing</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/hnsw/\">docs/hnsw/</a></td> \n   </tr> \n   <tr> \n    <td>DAG System</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/dag/\">docs/dag/</a></td> \n   </tr> \n   <tr> \n    <td>Nervous System</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/nervous-system/\">docs/nervous-system/</a></td> \n   </tr> \n   <tr> \n    <td>Sparse Inference</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/sparse-inference/\">docs/sparse-inference/</a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Bindings &amp; Integration</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Topic</th> \n    <th>Link</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Node.js API</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn-node/README.md\">crates/ruvector-gnn-node/README.md</a></td> \n   </tr> \n   <tr> \n    <td>WASM API</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn-wasm/README.md\">crates/ruvector-gnn-wasm/README.md</a></td> \n   </tr> \n   <tr> \n    <td>PostgreSQL</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/postgres/\">docs/postgres/</a></td> \n   </tr> \n   <tr> \n    <td>Self-Learning Hooks</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/hooks/\">docs/hooks/</a></td> \n   </tr> \n   <tr> \n    <td>Integration Guides</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/integration/\">docs/integration/</a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>LLM &amp; AI</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Topic</th> \n    <th>Link</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>RuvLLM</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/ruvllm/\">docs/ruvllm/</a></td> \n   </tr> \n   <tr> \n    <td>Training Guides</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/training/\">docs/training/</a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Operations</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Topic</th> \n    <th>Link</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Architecture</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/architecture/\">docs/architecture/</a></td> \n   </tr> \n   <tr> \n    <td>Cloud Deployment</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/cloud-architecture/\">docs/cloud-architecture/</a></td> \n   </tr> \n   <tr> \n    <td>Security</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/security/\">docs/security/</a></td> \n   </tr> \n   <tr> \n    <td>Benchmarks</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/benchmarks/\">docs/benchmarks/</a></td> \n   </tr> \n   <tr> \n    <td>Testing</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/testing/\">docs/testing/</a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Research</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Topic</th> \n    <th>Link</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Research Papers</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/research/\">docs/research/</a></td> \n   </tr> \n   <tr> \n    <td>GNN V2 Features</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/research/gnn-v2/\">docs/research/gnn-v2/</a></td> \n   </tr> \n   <tr> \n    <td>Min-Cut Algorithms</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/research/mincut/\">docs/research/mincut/</a></td> \n   </tr> \n   <tr> \n    <td>SPARQL Support</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/research/sparql/\">docs/research/sparql/</a></td> \n   </tr> \n   <tr> \n    <td>Latent Space</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/research/latent-space/\">docs/research/latent-space/</a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Architecture Decision Records (ADRs)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>ADR</th> \n    <th>Status</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-001-ruvector-core-architecture.md\">ADR-001</a></td> \n    <td>Accepted</td> \n    <td>Core architecture design</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-002-ruvllm-integration.md\">ADR-002</a></td> \n    <td>Accepted</td> \n    <td>RuvLLM integration</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-003-simd-optimization-strategy.md\">ADR-003</a></td> \n    <td>Accepted</td> \n    <td>SIMD optimization strategy</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-004-kv-cache-management.md\">ADR-004</a></td> \n    <td>Accepted</td> \n    <td>KV cache management</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-005-wasm-runtime-integration.md\">ADR-005</a></td> \n    <td>Accepted</td> \n    <td>WASM runtime integration</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-006-memory-management.md\">ADR-006</a></td> \n    <td>Accepted</td> \n    <td>Memory management</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-007-security-review-technical-debt.md\">ADR-007</a></td> \n    <td>Accepted</td> \n    <td>Security review</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-008-mistral-rs-integration.md\">ADR-008</a></td> \n    <td><strong>New</strong></td> \n    <td>Mistral-rs backend integration</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-009-structured-output.md\">ADR-009</a></td> \n    <td><strong>New</strong></td> \n    <td>Structured output (SOTA)</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-010-function-calling.md\">ADR-010</a></td> \n    <td><strong>New</strong></td> \n    <td>Function calling (SOTA)</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-011-prefix-caching.md\">ADR-011</a></td> \n    <td><strong>New</strong></td> \n    <td>Prefix caching (SOTA)</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-012-security-remediation.md\">ADR-012</a></td> \n    <td><strong>New</strong></td> \n    <td>Security remediation</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-013-huggingface-publishing.md\">ADR-013</a></td> \n    <td><strong>New</strong></td> \n    <td>HuggingFace publishing</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-030-rvf-cognitive-container.md\">ADR-030</a></td> \n    <td><strong>Accepted</strong></td> \n    <td>RVF cognitive container architecture</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-031-rvcow-branching-and-real-cognitive-containers.md\">ADR-031</a></td> \n    <td><strong>Accepted</strong></td> \n    <td>RVCOW branching &amp; real containers</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-045-lean-agentic-integration.md\">ADR-045</a></td> \n    <td><strong>Accepted</strong></td> \n    <td>Lean-agentic formal verification integration</td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<details> \n 📦 npm Packages (49+ Packages) \n <h4>Core Packages</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/ruvector\">ruvector</a></td> \n    <td>All-in-one CLI &amp; package</td> \n    <td><a href=\"https://www.npmjs.com/package/ruvector\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/ruvector.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/ruvector\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/ruvector.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/core\">@ruvector/core</a></td> \n    <td>Core vector database with HNSW</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/core\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/core.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/core\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/core.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/node\">@ruvector/node</a></td> \n    <td>Unified Node.js bindings</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/node\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/node.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/node\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/ruvector-extensions\">ruvector-extensions</a></td> \n    <td>Advanced features: embeddings, UI</td> \n    <td><a href=\"https://www.npmjs.com/package/ruvector-extensions\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/ruvector-extensions.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/ruvector-extensions\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/ruvector-extensions.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Graph &amp; GNN</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn\">@ruvector/gnn</a></td> \n    <td>Graph Neural Network layers</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/gnn.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/gnn.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-node\">@ruvector/graph-node</a></td> \n    <td>Hypergraph with Cypher queries</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-node\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/graph-node.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-node\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/graph-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-wasm\">@ruvector/graph-wasm</a></td> \n    <td>Browser graph queries</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/graph-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/graph-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-data-generator\">@ruvector/graph-data-generator</a></td> \n    <td>AI-powered synthetic graph data</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-data-generator\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/graph-data-generator.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-data-generator\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/graph-data-generator.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>AI Routing &amp; Attention</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer\">@ruvector/tiny-dancer</a></td> \n    <td>FastGRNN neural routing</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/tiny-dancer.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/tiny-dancer.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router\">@ruvector/router</a></td> \n    <td>Semantic router + HNSW</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/router.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/router.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention\">@ruvector/attention</a></td> \n    <td>46 attention mechanisms</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/attention.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/attention.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Learning &amp; Neural</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/sona\">@ruvector/sona</a></td> \n    <td>Self-Optimizing Neural Architecture</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/sona\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/sona.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/sona\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/sona.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/spiking-neural\">@ruvector/spiking-neural</a></td> \n    <td>Spiking neural networks (SNN)</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/spiking-neural\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/spiking-neural.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/spiking-neural\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/spiking-neural.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>LLM Runtime</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm\">@ruvector/ruvllm</a></td> \n    <td>LLM orchestration + SONA</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ruvllm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/ruvllm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm-cli\">@ruvector/ruvllm-cli</a></td> \n    <td>LLM CLI: inference, benchmarks</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm-cli\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ruvllm-cli.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm-cli\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/ruvllm-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm-wasm\">@ruvector/ruvllm-wasm</a></td> \n    <td>Browser LLM inference</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ruvllm-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/ruvllm-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Distributed Systems</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/cluster\">@ruvector/cluster</a></td> \n    <td>Distributed clustering</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/cluster\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/cluster.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/cluster\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/cluster.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/server\">@ruvector/server</a></td> \n    <td>HTTP/gRPC server</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/server\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/server.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/server\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/server.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/raft\">@ruvector/raft</a></td> \n    <td>Raft consensus</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/raft\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/raft.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/raft\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/raft.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/replication\">@ruvector/replication</a></td> \n    <td>Multi-master replication</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/replication\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/replication.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/replication\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/replication.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/burst-scaling\">@ruvector/burst-scaling</a></td> \n    <td>10-50x burst scaling</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/burst-scaling\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/burst-scaling.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/burst-scaling\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/burst-scaling.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Edge &amp; Standalone</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/rvlite\">rvlite</a></td> \n    <td>SQLite-style edge DB</td> \n    <td><a href=\"https://www.npmjs.com/package/rvlite\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/rvlite.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/rvlite\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/rvlite.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rudag\">@ruvector/rudag</a></td> \n    <td>Self-learning DAG</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rudag\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rudag.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rudag\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/rudag.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Genomics &amp; Health</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvdna\">@ruvector/rvdna</a></td> \n    <td>AI-native genomic analysis + .rvdna format</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvdna\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rvdna.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvdna\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/rvdna.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>RVF Cognitive Containers</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf\">@ruvector/rvf</a></td> \n    <td>Unified TypeScript SDK</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rvf.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/rvf.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-node\">@ruvector/rvf-node</a></td> \n    <td>Node.js N-API native bindings</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-node\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rvf-node.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-node\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/rvf-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-wasm\">@ruvector/rvf-wasm</a></td> \n    <td>WASM browser package</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rvf-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/rvf-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-mcp-server\">@ruvector/rvf-mcp-server</a></td> \n    <td>MCP server for AI agents</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-mcp-server\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rvf-mcp-server.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-mcp-server\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/rvf-mcp-server.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Agentic &amp; Synthetic Data</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\">@ruvector/agentic-synth</a></td> \n    <td>AI synthetic data generator</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/agentic-synth.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/agentic-synth.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-integration\">@ruvector/agentic-integration</a></td> \n    <td>Distributed agent coordination</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-integration\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/agentic-integration.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-integration\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/agentic-integration.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@cognitum/gate\">@cognitum/gate</a></td> \n    <td>AI coherence gate</td> \n    <td><a href=\"https://www.npmjs.com/package/@cognitum/gate\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@cognitum/gate.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@cognitum/gate\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@cognitum/gate.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>CLI Tools</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/cli\">@ruvector/cli</a></td> \n    <td>CLI + self-learning hooks</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/cli\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/cli.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/cli\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/postgres-cli\">@ruvector/postgres-cli</a></td> \n    <td>PostgreSQL extension CLI</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/postgres-cli\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/postgres-cli.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/postgres-cli\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/postgres-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/scipix\">@ruvector/scipix</a></td> \n    <td>Scientific OCR client</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/scipix\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/scipix.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/scipix\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/scipix.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>WASM Packages</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm\">@ruvector/wasm</a></td> \n    <td>Unified WASM meta-package</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm-unified\">@ruvector/wasm-unified</a></td> \n    <td>Unified TypeScript API</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm-unified\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/wasm-unified.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm-unified\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/wasm-unified.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn-wasm\">@ruvector/gnn-wasm</a></td> \n    <td>GNN WASM bindings</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/gnn-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/gnn-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-wasm\">@ruvector/attention-wasm</a></td> \n    <td>Attention WASM bindings</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/attention-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/attention-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-unified-wasm\">@ruvector/attention-unified-wasm</a></td> \n    <td>All 46 attention mechanisms</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-unified-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/attention-unified-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-unified-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/attention-unified-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer-wasm\">@ruvector/tiny-dancer-wasm</a></td> \n    <td>AI routing WASM</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/tiny-dancer-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/tiny-dancer-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router-wasm\">@ruvector/router-wasm</a></td> \n    <td>Semantic router WASM</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/router-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/router-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/learning-wasm\">@ruvector/learning-wasm</a></td> \n    <td>Learning module WASM</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/learning-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/learning-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/learning-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/learning-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/economy-wasm\">@ruvector/economy-wasm</a></td> \n    <td>Tokenomics WASM</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/economy-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/economy-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/economy-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/economy-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/exotic-wasm\">@ruvector/exotic-wasm</a></td> \n    <td>Exotic features WASM</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/exotic-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/exotic-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/exotic-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/exotic-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/nervous-system-wasm\">@ruvector/nervous-system-wasm</a></td> \n    <td>Nervous system WASM</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/nervous-system-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/nervous-system-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/nervous-system-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/nervous-system-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/ruvector-attention-wasm\">ruvector-attention-wasm</a></td> \n    <td>WASM attention (Flash, MoE, Hyperbolic, CGT Sheaf)</td> \n    <td><a href=\"https://www.npmjs.com/package/ruvector-attention-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/ruvector-attention-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/ruvector-attention-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/ruvector-attention-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<details> \n 🦀 Rust Crates (83 Packages) \n <p>All crates are published to <a href=\"https://crates.io\">crates.io</a> under the <code>ruvector-*</code> and <code>rvf-*</code> namespaces.</p> \n <h3>Core Crates</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-core\">ruvector-core</a></td> \n    <td>Vector database engine with HNSW indexing</td> \n    <td><a href=\"https://crates.io/crates/ruvector-core\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-core.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-collections\">ruvector-collections</a></td> \n    <td>Collection and namespace management</td> \n    <td><a href=\"https://crates.io/crates/ruvector-collections\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-collections.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-filter\">ruvector-filter</a></td> \n    <td>Vector filtering and metadata queries</td> \n    <td><a href=\"https://crates.io/crates/ruvector-filter\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-filter.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-metrics\">ruvector-metrics</a></td> \n    <td>Performance metrics and monitoring</td> \n    <td><a href=\"https://crates.io/crates/ruvector-metrics\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-metrics.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-snapshot\">ruvector-snapshot</a></td> \n    <td>Snapshot and persistence management</td> \n    <td><a href=\"https://crates.io/crates/ruvector-snapshot\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-snapshot.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-node\">ruvector-node</a></td> \n    <td>Node.js bindings via NAPI-RS</td> \n    <td><a href=\"https://crates.io/crates/ruvector-node\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-wasm\">ruvector-wasm</a></td> \n    <td>WASM bindings for browser/edge</td> \n    <td><a href=\"https://crates.io/crates/ruvector-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-cli\">ruvector-cli</a></td> \n    <td>CLI and MCP server</td> \n    <td><a href=\"https://crates.io/crates/ruvector-cli\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-bench\">ruvector-bench</a></td> \n    <td>Benchmarking suite</td> \n    <td><a href=\"https://crates.io/crates/ruvector-bench\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-bench.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Graph &amp; GNN</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph\">ruvector-graph</a></td> \n    <td>Hypergraph database with Neo4j-style Cypher</td> \n    <td><a href=\"https://crates.io/crates/ruvector-graph\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-graph.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-node\">ruvector-graph-node</a></td> \n    <td>Node.js bindings for graph operations</td> \n    <td><a href=\"https://crates.io/crates/ruvector-graph-node\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-graph-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-wasm\">ruvector-graph-wasm</a></td> \n    <td>WASM bindings for browser graph queries</td> \n    <td><a href=\"https://crates.io/crates/ruvector-graph-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-graph-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn\">ruvector-gnn</a></td> \n    <td>Graph Neural Network layers and training</td> \n    <td><a href=\"https://crates.io/crates/ruvector-gnn\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-gnn.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn-node\">ruvector-gnn-node</a></td> \n    <td>Node.js bindings for GNN inference</td> \n    <td><a href=\"https://crates.io/crates/ruvector-gnn-node\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-gnn-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn-wasm\">ruvector-gnn-wasm</a></td> \n    <td>WASM bindings for browser GNN</td> \n    <td><a href=\"https://crates.io/crates/ruvector-gnn-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-gnn-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Attention Mechanisms</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention\">ruvector-attention</a></td> \n    <td>46 attention mechanisms (Flash, Hyperbolic, MoE, Graph)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-attention\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-attention.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention-node\">ruvector-attention-node</a></td> \n    <td>Node.js bindings for attention mechanisms</td> \n    <td><a href=\"https://crates.io/crates/ruvector-attention-node\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-attention-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention-wasm\">ruvector-attention-wasm</a></td> \n    <td>WASM bindings for browser attention</td> \n    <td><a href=\"https://crates.io/crates/ruvector-attention-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-attention-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention-cli\">ruvector-attention-cli</a></td> \n    <td>CLI for attention testing and benchmarking</td> \n    <td><a href=\"https://crates.io/crates/ruvector-attention-cli\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-attention-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>LLM Runtime (ruvllm)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm\">ruvllm</a></td> \n    <td>LLM serving runtime with SONA, paged attention, KV cache, BitNet</td> \n    <td><a href=\"https://crates.io/crates/ruvllm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvllm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm-cli\">ruvllm-cli</a></td> \n    <td>CLI for model inference and benchmarking</td> \n    <td><a href=\"https://crates.io/crates/ruvllm-cli\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvllm-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm-wasm\">ruvllm-wasm</a></td> \n    <td>WASM bindings for browser LLM inference</td> \n    <td><a href=\"https://crates.io/crates/ruvllm-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvllm-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Features:</strong> Candle backend, Metal/CUDA acceleration, Apple Neural Engine, GGUF support, SONA learning, <strong>BitNet 1.58-bit quantization</strong> (TL1 kernels, AVX2/WASM).</p> \n <pre><code class=\"language-bash\">cargo add ruvllm --features inference-metal  # Mac with Metal\ncargo add ruvllm --features inference-cuda   # NVIDIA GPU\n</code></pre> \n <p><strong>RuvLTRA Models</strong> — Pre-trained GGUF models optimized for Claude Code workflows:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Model</th> \n    <th>Size</th> \n    <th>Use Case</th> \n    <th>Link</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>ruvltra-claude-code-0.5b-q4_k_m</td> \n    <td>398 MB</td> \n    <td>Agent routing</td> \n    <td><a href=\"https://huggingface.co/ruv/ruvltra\">HuggingFace</a></td> \n   </tr> \n   <tr> \n    <td>ruvltra-small-0.5b-q4_k_m</td> \n    <td>398 MB</td> \n    <td>Embeddings</td> \n    <td><a href=\"https://huggingface.co/ruv/ruvltra\">HuggingFace</a></td> \n   </tr> \n   <tr> \n    <td>ruvltra-medium-1.1b-q4_k_m</td> \n    <td>800 MB</td> \n    <td>Classification</td> \n    <td><a href=\"https://huggingface.co/ruv/ruvltra\">HuggingFace</a></td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\"># Download and use\nwget https://huggingface.co/ruv/ruvltra/resolve/main/ruvltra-small-0.5b-q4_k_m.gguf\n</code></pre> \n <h3>Distributed Systems</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-cluster\">ruvector-cluster</a></td> \n    <td>Cluster management and coordination</td> \n    <td><a href=\"https://crates.io/crates/ruvector-cluster\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-cluster.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-raft\">ruvector-raft</a></td> \n    <td>Raft consensus implementation</td> \n    <td><a href=\"https://crates.io/crates/ruvector-raft\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-raft.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-replication\">ruvector-replication</a></td> \n    <td>Data replication and synchronization</td> \n    <td><a href=\"https://crates.io/crates/ruvector-replication\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-replication.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-server\">ruvector-server</a></td> \n    <td>REST/gRPC API server</td> \n    <td><a href=\"https://crates.io/crates/ruvector-server\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-server.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>AI Agent Routing (Tiny Dancer)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-tiny-dancer-core\">ruvector-tiny-dancer-core</a></td> \n    <td>FastGRNN neural inference for AI routing</td> \n    <td><a href=\"https://crates.io/crates/ruvector-tiny-dancer-core\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-tiny-dancer-core.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-tiny-dancer-node\">ruvector-tiny-dancer-node</a></td> \n    <td>Node.js bindings for AI routing</td> \n    <td><a href=\"https://crates.io/crates/ruvector-tiny-dancer-node\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-tiny-dancer-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-tiny-dancer-wasm\">ruvector-tiny-dancer-wasm</a></td> \n    <td>WASM bindings for browser AI routing</td> \n    <td><a href=\"https://crates.io/crates/ruvector-tiny-dancer-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-tiny-dancer-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Router (Semantic Routing)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-router-core\">ruvector-router-core</a></td> \n    <td>Core semantic routing engine</td> \n    <td><a href=\"https://crates.io/crates/ruvector-router-core\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-router-core.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-router-cli\">ruvector-router-cli</a></td> \n    <td>CLI for router testing and benchmarking</td> \n    <td><a href=\"https://crates.io/crates/ruvector-router-cli\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-router-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-router-ffi\">ruvector-router-ffi</a></td> \n    <td>FFI bindings for other languages</td> \n    <td><a href=\"https://crates.io/crates/ruvector-router-ffi\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-router-ffi.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-router-wasm\">ruvector-router-wasm</a></td> \n    <td>WASM bindings for browser routing</td> \n    <td><a href=\"https://crates.io/crates/ruvector-router-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-router-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Hybrid Routing</strong> achieves <strong>90% accuracy</strong> for agent routing using keyword-first strategy with embedding fallback. See <a href=\"https://github.com/ruvnet/ruvector/issues/122\">Issue #122</a> for benchmarks and the <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/#-ruvllm-training--fine-tuning-tutorials\">training tutorials</a> for fine-tuning guides.</p> \n <h3>Dynamic Min-Cut (December 2025 Breakthrough)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-mincut\">ruvector-mincut</a></td> \n    <td>Subpolynomial fully-dynamic min-cut (<a href=\"https://arxiv.org/abs/2512.13105\">arXiv:2512.13105</a>)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-mincut\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-mincut.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-mincut-node\">ruvector-mincut-node</a></td> \n    <td>Node.js bindings for min-cut</td> \n    <td><a href=\"https://crates.io/crates/ruvector-mincut-node\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-mincut-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-mincut-wasm\">ruvector-mincut-wasm</a></td> \n    <td>WASM bindings for browser min-cut</td> \n    <td><a href=\"https://crates.io/crates/ruvector-mincut-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-mincut-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>First deterministic exact fully-dynamic min-cut</strong> with verified <strong>n^0.12 subpolynomial</strong> update scaling:</p> \n <ul> \n  <li><strong>Brain connectivity</strong> — Detect Alzheimer's markers by tracking neural pathway changes in milliseconds</li> \n  <li><strong>Network resilience</strong> — Predict outages before they happen, route around failures instantly</li> \n  <li><strong>AI agent coordination</strong> — Find communication bottlenecks in multi-agent systems</li> \n  <li><strong>Neural network pruning</strong> — Identify which connections can be removed without losing accuracy</li> \n  <li><strong>448+ tests</strong>, 256-core parallel optimization, 8KB per core (compile-time verified)</li> \n </ul> \n <pre><code class=\"language-rust\">use ruvector_mincut::{DynamicMinCut, Graph};\n\nlet mut graph = Graph::new();\ngraph.add_edge(0, 1, 10.0);\ngraph.add_edge(1, 2, 5.0);\n\nlet mincut = DynamicMinCut::new(&amp;graph);\nlet (value, cut_edges) = mincut.compute();\n// Updates in subpolynomial time as edges change\n</code></pre> \n <h3>Quantum Coherence (ruQu)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruQu\">ruqu</a></td> \n    <td>Classical nervous system for quantum machines - coherence via min-cut</td> \n    <td><a href=\"https://crates.io/crates/ruqu\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruqu.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/cognitum-gate-kernel\">cognitum-gate-kernel</a></td> \n    <td>Anytime-valid coherence gate kernel</td> \n    <td><a href=\"https://crates.io/crates/cognitum-gate-kernel\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/cognitum-gate-kernel.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/cognitum-gate-tilezero\">cognitum-gate-tilezero</a></td> \n    <td>TileZero arbiter for coherence decisions</td> \n    <td><a href=\"https://crates.io/crates/cognitum-gate-tilezero\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/cognitum-gate-tilezero.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/mcp-gate\">mcp-gate</a></td> \n    <td>MCP server for coherence gate integration</td> \n    <td><a href=\"https://crates.io/crates/mcp-gate\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/mcp-gate.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/prime-radiant\">prime-radiant</a></td> \n    <td>Universal coherence engine - sheaf Laplacian AI safety &amp; hallucination detection</td> \n    <td><a href=\"https://crates.io/crates/prime-radiant\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/prime-radiant.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>ruQu Features:</strong> Real-time quantum coherence assessment, MWPM decoder integration, mincut-gated attention (50% FLOPs reduction).</p> \n <pre><code class=\"language-rust\">use ruqu::{CoherenceGate, SyndromeFilter};\n\nlet gate = CoherenceGate::new();\nlet syndrome = gate.assess_coherence(&amp;quantum_state)?;\n</code></pre> \n <h3>Advanced Math &amp; Inference</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-math\">ruvector-math</a></td> \n    <td>Core math utilities, SIMD operations</td> \n    <td><a href=\"https://crates.io/crates/ruvector-math\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-math.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-math-wasm\">ruvector-math-wasm</a></td> \n    <td>WASM bindings for math operations</td> \n    <td><a href=\"https://crates.io/crates/ruvector-math-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-math-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-sparse-inference\">ruvector-sparse-inference</a></td> \n    <td>Sparse tensor inference engine</td> \n    <td><a href=\"https://crates.io/crates/ruvector-sparse-inference\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-sparse-inference.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-sparse-inference-wasm\">ruvector-sparse-inference-wasm</a></td> \n    <td>WASM bindings for sparse inference</td> \n    <td><a href=\"https://crates.io/crates/ruvector-sparse-inference-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-sparse-inference-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-hyperbolic-hnsw\">ruvector-hyperbolic-hnsw</a></td> \n    <td>HNSW in hyperbolic space (Poincaré/Lorentz)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-hyperbolic-hnsw\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-hyperbolic-hnsw.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-hyperbolic-hnsw-wasm\">ruvector-hyperbolic-hnsw-wasm</a></td> \n    <td>WASM bindings for hyperbolic HNSW</td> \n    <td><a href=\"https://crates.io/crates/ruvector-hyperbolic-hnsw-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-hyperbolic-hnsw-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-dither\">ruvector-dither</a></td> \n    <td>Deterministic golden-ratio and pi-digit dithering for quantization (<code>no_std</code>)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-dither\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-dither.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>FPGA &amp; Hardware Acceleration</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-fpga-transformer\">ruvector-fpga-transformer</a></td> \n    <td>FPGA-optimized transformer inference</td> \n    <td><a href=\"https://crates.io/crates/ruvector-fpga-transformer\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-fpga-transformer.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-fpga-transformer-wasm\">ruvector-fpga-transformer-wasm</a></td> \n    <td>WASM simulation of FPGA transformer</td> \n    <td><a href=\"https://crates.io/crates/ruvector-fpga-transformer-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-fpga-transformer-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-mincut-gated-transformer\">ruvector-mincut-gated-transformer</a></td> \n    <td>MinCut-gated attention for 50% compute reduction</td> \n    <td><a href=\"https://crates.io/crates/ruvector-mincut-gated-transformer\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-mincut-gated-transformer.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-mincut-gated-transformer-wasm\">ruvector-mincut-gated-transformer-wasm</a></td> \n    <td>WASM bindings for mincut-gated transformer</td> \n    <td><a href=\"https://crates.io/crates/ruvector-mincut-gated-transformer-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-mincut-gated-transformer-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Neuromorphic &amp; Bio-Inspired Learning</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-nervous-system\">ruvector-nervous-system</a></td> \n    <td>Spiking neural networks with BTSP learning &amp; EWC plasticity</td> \n    <td><a href=\"https://crates.io/crates/ruvector-nervous-system\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-nervous-system.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-nervous-system-wasm\">ruvector-nervous-system-wasm</a></td> \n    <td>WASM bindings for neuromorphic learning</td> \n    <td><a href=\"https://crates.io/crates/ruvector-nervous-system-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-nervous-system-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-learning-wasm\">ruvector-learning-wasm</a></td> \n    <td>MicroLoRA adaptation (&lt;100µs latency)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-learning-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-learning-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-economy-wasm\">ruvector-economy-wasm</a></td> \n    <td>CRDT-based autonomous credit economy</td> \n    <td><a href=\"https://crates.io/crates/ruvector-economy-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-economy-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-exotic-wasm\">ruvector-exotic-wasm</a></td> \n    <td>Exotic AI primitives (strange loops, time crystals)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-exotic-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-exotic-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention-unified-wasm\">ruvector-attention-unified-wasm</a></td> \n    <td>Unified 18+ attention mechanisms (Neural, DAG, Mamba SSM)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-attention-unified-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-attention-unified-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/micro-hnsw-wasm\">micro-hnsw-wasm</a></td> \n    <td>Neuromorphic HNSW with spiking neurons (11.8KB WASM)</td> \n    <td><a href=\"https://crates.io/crates/micro-hnsw-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/micro-hnsw-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/thermorust\">thermorust</a></td> \n    <td>Thermodynamic neural motif engine — Ising/soft-spin Hamiltonians, Langevin dynamics, Landauer dissipation</td> \n    <td><a href=\"https://crates.io/crates/thermorust\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/thermorust.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Bio-inspired features:</strong></p> \n <ul> \n  <li><strong>Spiking Neural Networks (SNNs)</strong> — 10-50x energy efficiency vs traditional ANNs</li> \n  <li><strong>BTSP Learning</strong> — Behavioral Time-Scale Synaptic Plasticity for rapid adaptation</li> \n  <li><strong>MicroLoRA</strong> — Sub-microsecond fine-tuning for per-operator learning</li> \n  <li><strong>Mamba SSM</strong> — State Space Model attention for linear-time sequences</li> \n </ul> \n <h3>Cognitive Robotics</h3> \n <details> \n  Perception, planning, behavior trees, and swarm coordination for autonomous robots \n  <table> \n   <thead> \n    <tr> \n     <th>Crate</th> \n     <th>Description</th> \n     <th>crates.io</th> \n    </tr> \n   </thead> \n   <tbody> \n    <tr> \n     <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-robotics\">ruvector-robotics</a></td> \n     <td>Cognitive robotics platform — perception, A* planning, behavior trees, swarm coordination</td> \n     <td><a href=\"https://crates.io/crates/ruvector-robotics\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-robotics.svg?sanitize=true\" /></a></td> \n    </tr> \n   </tbody> \n  </table> \n  <p><strong>Modules:</strong></p> \n  <table> \n   <thead> \n    <tr> \n     <th>Module</th> \n     <th>What It Does</th> \n    </tr> \n   </thead> \n   <tbody> \n    <tr> \n     <td><strong>bridge</strong></td> \n     <td>OccupancyGrid, PointCloud, SensorFrame, SceneGraph data types with spatial kNN</td> \n    </tr> \n    <tr> \n     <td><strong>perception</strong></td> \n     <td>Scene-graph construction from point clouds, obstacle detection pipeline</td> \n    </tr> \n    <tr> \n     <td><strong>planning</strong></td> \n     <td>A* grid search (octile heuristic) and potential-field velocity commands</td> \n    </tr> \n    <tr> \n     <td><strong>cognitive</strong></td> \n     <td>Perceive-think-act-learn loop with utility-based reasoning</td> \n    </tr> \n    <tr> \n     <td><strong>domain_expansion</strong></td> \n     <td>Cross-domain transfer learning via Meta Thompson Sampling and Beta priors</td> \n    </tr> \n   </tbody> \n  </table> \n  <p><strong>Key features:</strong> 290 tests, clippy-clean, <code>no_std</code>-friendly types, optional <code>domain-expansion</code> feature flag for cross-domain transfer, pluggable <code>PotentialFieldConfig</code> for obstacle avoidance, Byzantine-tolerant swarm coordination via <code>ruvector-domain-expansion</code>.</p> \n  <pre><code class=\"language-rust\">use ruvector_robotics::planning::{astar, potential_field, PotentialFieldConfig};\nuse ruvector_robotics::bridge::OccupancyGrid;\n\nlet grid = OccupancyGrid::new(100, 100, 0.1);\nlet path = astar(&amp;grid, (5, 5), (90, 90))?;\nlet cmd = potential_field(&amp;[0.0, 0.0, 0.0], &amp;[5.0, 3.0, 0.0], &amp;[], &amp;PotentialFieldConfig::default());\n</code></pre> \n </details> \n <h3>Self-Learning (SONA)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/sona\">sona</a></td> \n    <td>Self-Optimizing Neural Architecture - LoRA, EWC++, ReasoningBank</td> \n    <td><a href=\"https://crates.io/crates/ruvector-sona\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-sona.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>SONA Features:</strong> Two-tier LoRA adaptation, Elastic Weight Consolidation (EWC++), ReasoningBank for trajectory learning, runtime-adaptive learning for LLM routers.</p> \n <h3>Genomics &amp; Health (rvDNA)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/dna\">rvdna</a></td> \n    <td>AI-native genomic analysis — variant calling, protein translation, HNSW k-mer search, <code>.rvdna</code> format</td> \n    <td><a href=\"https://crates.io/crates/rvdna\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvdna.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>rvDNA Features:</strong> 12 ms full pipeline on 5 real human genes, Bayesian variant calling (155 ns/SNP), Horvath epigenetic clock, CYP2D6 pharmacogenomics, <code>.rvdna</code> binary format with pre-computed AI features, WASM support for browser-based diagnostics. <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/dna/README.md\">Full README</a></p> \n <h3>RVF Cognitive Containers</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-types\">rvf-types</a></td> \n    <td>20 segment headers, COW/membership/delta types (<code>no_std</code>)</td> \n    <td><a href=\"https://crates.io/crates/rvf-types\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-types.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-wire\">rvf-wire</a></td> \n    <td>Wire format read/write (<code>no_std</code>)</td> \n    <td><a href=\"https://crates.io/crates/rvf-wire\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-wire.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-manifest\">rvf-manifest</a></td> \n    <td>Two-level manifest, FileIdentity, COW pointers</td> \n    <td><a href=\"https://crates.io/crates/rvf-manifest\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-manifest.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-quant\">rvf-quant</a></td> \n    <td>Scalar, product, binary quantization</td> \n    <td><a href=\"https://crates.io/crates/rvf-quant\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-quant.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-index\">rvf-index</a></td> \n    <td>HNSW progressive indexing (Layer A/B/C)</td> \n    <td><a href=\"https://crates.io/crates/rvf-index\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-index.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto\">rvf-crypto</a></td> \n    <td>SHAKE-256, Ed25519, witness chains, attestation</td> \n    <td><a href=\"https://crates.io/crates/rvf-crypto\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-crypto.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-runtime\">rvf-runtime</a></td> \n    <td>Full store API, COW engine, compaction</td> \n    <td><a href=\"https://crates.io/crates/rvf-runtime\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-runtime.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-kernel\">rvf-kernel</a></td> \n    <td>Linux kernel builder, initramfs, Docker pipeline</td> \n    <td><a href=\"https://crates.io/crates/rvf-kernel\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-kernel.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-ebpf\">rvf-ebpf</a></td> \n    <td>Real BPF programs (XDP, socket filter, TC)</td> \n    <td><a href=\"https://crates.io/crates/rvf-ebpf\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-ebpf.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-federation\">rvf-federation</a></td> \n    <td>Federated transfer learning — PII stripping, differential privacy, FedAvg/FedProx</td> \n    <td><a href=\"https://crates.io/crates/rvf-federation\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-federation.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-launch\">rvf-launch</a></td> \n    <td>QEMU microvm launcher, KVM/TCG</td> \n    <td><a href=\"https://crates.io/crates/rvf-launch\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-launch.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-server\">rvf-server</a></td> \n    <td>HTTP REST + TCP streaming server</td> \n    <td><a href=\"https://crates.io/crates/rvf-server\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-server.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-import\">rvf-import</a></td> \n    <td>JSON, CSV, NumPy importers</td> \n    <td><a href=\"https://crates.io/crates/rvf-import\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-import.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-cli\">rvf-cli</a></td> \n    <td>Unified CLI with 17 subcommands</td> \n    <td><a href=\"https://crates.io/crates/rvf-cli\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>RVF Features:</strong> Single-file cognitive containers that boot as Linux microservices, COW branching at cluster granularity, eBPF acceleration, witness chains, post-quantum signatures, federated transfer learning with differential privacy, 28 segment types. <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">Full README</a></p> \n <h3>Formal Verification</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-verified\">ruvector-verified</a></td> \n    <td>Proof-carrying vector operations with lean-agentic dependent types (~500ns proofs)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-verified\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-verified.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-verified-wasm\">ruvector-verified-wasm</a></td> \n    <td>WASM bindings for browser/edge formal verification</td> \n    <td><a href=\"https://crates.io/crates/ruvector-verified-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-verified-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Verification Features:</strong> 82-byte proof attestations, 3-tier gated proof routing (Reflex &lt;10ns / Standard &lt;1us / Deep &lt;100us), FastTermArena with O(1) dedup, batch dimension verification (~11ns/vector), type-safe pipeline composition. <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-verified/README.md\">Full README</a></p> \n <details> \n  Formal Verification Details \n  <p><strong>How it works:</strong> Every vector operation produces a machine-checked proof term using lean-agentic dependent types. Proofs are constructed at compile-time semantics but execute at runtime with sub-microsecond overhead, then serialized into 82-byte attestations that can be embedded in RVF witness chains.</p> \n  <table> \n   <thead> \n    <tr> \n     <th>Operation</th> \n     <th>Latency</th> \n     <th>Description</th> \n    </tr> \n   </thead> \n   <tbody> \n    <tr> \n     <td><code>ProofEnvironment::new()</code></td> \n     <td>~470ns</td> \n     <td>Initialize proof context with type declarations</td> \n    </tr> \n    <tr> \n     <td><code>prove_dim_eq(a, b)</code></td> \n     <td>~496ns</td> \n     <td>Dimension equality proof with FxHash caching</td> \n    </tr> \n    <tr> \n     <td><code>verify_batch_dimensions()</code></td> \n     <td>~11ns/vec</td> \n     <td>Batch verification for N vectors</td> \n    </tr> \n    <tr> \n     <td><code>compose_chain()</code></td> \n     <td>~1.2us</td> \n     <td>Type-safe pipeline composition</td> \n    </tr> \n    <tr> \n     <td><code>create_attestation()</code></td> \n     <td>~180ns</td> \n     <td>82-byte formal proof witness</td> \n    </tr> \n    <tr> \n     <td><code>FastTermArena::intern()</code></td> \n     <td>~1.6ns hit</td> \n     <td>O(1) dedup with 4-wide linear probe</td> \n    </tr> \n    <tr> \n     <td><code>gated::route_proof()</code></td> \n     <td>&lt;10ns</td> \n     <td>3-tier routing: Reflex / Standard / Deep</td> \n    </tr> \n   </tbody> \n  </table> \n  <p><strong>10 Exotic Application Domains</strong> (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/verified-applications\">examples/verified-applications</a>):</p> \n  <ol> \n   <li><strong>Autonomous Weapons Filter</strong> — certified targeting pipeline blocks tampered sensors</li> \n   <li><strong>Medical Diagnostics</strong> — proof-carrying ECG analysis with patient-keyed attestations</li> \n   <li><strong>Financial Order Routing</strong> — verified trade execution with proof-hash audit trail</li> \n   <li><strong>Multi-Agent Contracts</strong> — dimension + metric + depth contracts enforced by proof</li> \n   <li><strong>Distributed Sensor Swarm</strong> — coherence verification across heterogeneous nodes</li> \n   <li><strong>Quantization Proof</strong> — certify quantization error within formal tolerance bounds</li> \n   <li><strong>Verifiable Synthetic Memory</strong> — AGI memory with per-embedding proof attestations</li> \n   <li><strong>Cryptographic Vector Signatures</strong> — model-keyed signatures with contract matching</li> \n   <li><strong>Simulation Integrity</strong> — proof receipt per step for reproducible physics</li> \n   <li><strong>Legal Forensics</strong> — court-admissible replay bundles with structural invariants</li> \n  </ol> \n  <pre><code class=\"language-rust\">use ruvector_verified::{ProofEnvironment, vector_types, proof_store};\n\nlet mut env = ProofEnvironment::new();\nlet proof = vector_types::prove_dim_eq(&amp;mut env, 384, 384).unwrap();\nlet att = proof_store::create_attestation(&amp;env, proof);\nassert_eq!(att.to_bytes().len(), 82); // 82-byte formal witness\n</code></pre> \n </details> \n <p><strong>Self-booting example</strong> — the <code>claude_code_appliance</code> builds a complete AI dev environment as one file:</p> \n <pre><code class=\"language-bash\">cd examples/rvf &amp;&amp; cargo run --example claude_code_appliance\n</code></pre> \n <p>Final file: <strong>5.1 MB single <code>.rvf</code></strong> — boots Linux, serves queries, runs Claude Code. One file. Boots on QEMU/Firecracker. Runs SSH. Serves vectors. Installs Claude Code. Proves every step with a cryptographic witness chain.</p> \n <h3>Graph Transformer</h3> \n <p><a href=\"https://crates.io/crates/ruvector-graph-transformer\"><img alt=\"Crates.io\" src=\"https://img.shields.io/crates/v/ruvector-graph-transformer.svg?sanitize=true\" /></a></p> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>Registry</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-transformer\">ruvector-graph-transformer</a></td> \n    <td>Unified graph transformer with proof-gated mutation substrate (8 modules, 186 tests)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-graph-transformer\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-graph-transformer.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-transformer-wasm\">ruvector-graph-transformer-wasm</a></td> \n    <td>WASM bindings for browser-side graph transformers</td> \n    <td><a href=\"https://crates.io/crates/ruvector-graph-transformer-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-graph-transformer-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-transformer-node\">ruvector-graph-transformer-node</a></td> \n    <td>Node.js NAPI-RS bindings (22+ methods, 20 tests)</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-transformer\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/graph-transformer.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>What it does:</strong> Every time you modify a graph — adding a node, changing an edge weight, updating a vector — the graph transformer requires a formal proof that the operation is valid <em>before</em> it executes. Think of it like a type system for graph mutations: you can't accidentally corrupt your data because the system mathematically verifies every change.</p> \n <p>On top of that proof layer, 8 specialized modules handle different aspects of graph intelligence:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Module</th> \n    <th>What It Does (Plain English)</th> \n    <th>Feature Flag</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Proof-Gated Mutation</strong></td> \n    <td>Locks graph data behind mathematical proofs — no proof, no access</td> \n    <td>always on</td> \n   </tr> \n   <tr> \n    <td><strong>Sublinear Attention</strong></td> \n    <td>Finds the most important nodes without checking every single one — scales to millions</td> \n    <td><code>sublinear</code></td> \n   </tr> \n   <tr> \n    <td><strong>Physics-Informed</strong></td> \n    <td>Applies physics equations (conservation of energy, symmetry) to message passing</td> \n    <td><code>physics</code></td> \n   </tr> \n   <tr> \n    <td><strong>Biological</strong></td> \n    <td>Models neurons that only fire when excited enough — naturally sparse, energy-efficient</td> \n    <td><code>biological</code></td> \n   </tr> \n   <tr> \n    <td><strong>Self-Organizing</strong></td> \n    <td>Graphs that grow and reorganize themselves like biological development</td> \n    <td><code>self-organizing</code></td> \n   </tr> \n   <tr> \n    <td><strong>Verified Training</strong></td> \n    <td>Training with a receipt — if a gradient step would break an invariant, it's automatically rolled back</td> \n    <td><code>verified-training</code></td> \n   </tr> \n   <tr> \n    <td><strong>Manifold</strong></td> \n    <td>Operates in curved spaces (like the surface of a sphere) instead of just flat Euclidean space</td> \n    <td><code>manifold</code></td> \n   </tr> \n   <tr> \n    <td><strong>Temporal-Causal</strong></td> \n    <td>Enforces that information flows forward in time — no peeking at the future</td> \n    <td><code>temporal</code></td> \n   </tr> \n   <tr> \n    <td><strong>Economic</strong></td> \n    <td>Uses game theory to allocate attention fairly — Nash equilibrium for node importance</td> \n    <td><code>economic</code></td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\"># Rust\ncargo add ruvector-graph-transformer --features full\n\n# Node.js\nnpm install @ruvector/graph-transformer\n</code></pre> \n <pre><code class=\"language-rust\">use ruvector_graph_transformer::sublinear_attention::SublinearGraphAttention;\nuse ruvector_graph_transformer::config::SublinearConfig;\n\nlet attn = SublinearGraphAttention::new(128, SublinearConfig::default());\nlet outputs = attn.lsh_attention(&amp;features).unwrap(); // O(n log n)\n</code></pre> \n <pre><code class=\"language-javascript\">const { GraphTransformer } = require('@ruvector/graph-transformer');\nconst gt = new GraphTransformer();\n\n// Every operation produces a proof receipt\nconst proof = gt.proveDimension(128, 128);\nconst attestation = gt.createAttestation(proof.proof_id); // 82 bytes\n</code></pre> \n <details> \n  Graph Transformer Architecture Details \n  <p><strong>Proof-gated mutation</strong> means <code>state_n -&gt; proof(invariant) -&gt; mutation -&gt; state_n+1</code>. The <code>ProofGate&lt;T&gt;</code> type wraps any value so you literally cannot access the inner data without first producing a valid proof. This is enforced at the type level in Rust and at runtime in WASM/Node.js.</p> \n  <p><strong>Sublinear attention</strong> uses three strategies: (1) locality-sensitive hashing groups similar nodes into buckets, (2) personalized PageRank samples the most relevant neighbors via random walks, (3) spectral sparsification prunes edges that don't contribute to graph connectivity. All three reduce O(n^2) full attention to O(n log n).</p> \n  <p><strong>Verified training</strong> uses a delta-apply architecture: gradients go to a scratch buffer first, invariants are checked against the proposed weights (loss stability, weight norms, Lipschitz bounds, energy gates), and only if all checks pass are the weights committed. If any check fails and <code>fail_closed = true</code>, the step is rejected and the old weights are preserved. Every successful step produces a <code>TrainingCertificate</code> with BLAKE3 hashes of weights, config, and dataset manifest.</p> \n  <p><strong>10 ADRs</strong> document every design decision: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-046-graph-transformer-architecture.md\">ADR-046</a> through <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-055-manifold-graph-layers.md\">ADR-055</a>.</p> \n </details> \n <h3>Personal AI Memory (OSpipe)</h3> \n <p><a href=\"https://www.npmjs.com/package/@ruvector/ospipe\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ospipe.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/@ruvector/ospipe-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ospipe-wasm.svg?sanitize=true\" /></a></p> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Registry</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/OSpipe\">ospipe</a></td> \n    <td>RuVector-enhanced personal AI memory for Screenpipe</td> \n    <td><a href=\"https://crates.io/crates/ospipe\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ospipe.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ospipe\">@ruvector/ospipe</a></td> \n    <td>TypeScript SDK with retry, timeout, and AbortSignal</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ospipe\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ospipe.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ospipe-wasm\">@ruvector/ospipe-wasm</a></td> \n    <td>WASM bindings for browser deployment (145 KB)</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ospipe-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ospipe-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\">npm install @ruvector/ospipe         # TypeScript SDK\nnpm install @ruvector/ospipe-wasm    # Browser WASM\ncargo add ospipe                     # Rust crate\n</code></pre> \n <p><strong>Replaces Screenpipe's SQLite/FTS5 backend with semantic vector search.</strong> Ask your computer what you saw, heard, and did -- with semantic understanding.</p> \n <details> \n  OSpipe Features &amp; Capabilities \n  <table> \n   <thead> \n    <tr> \n     <th>Feature</th> \n     <th>Description</th> \n    </tr> \n   </thead> \n   <tbody> \n    <tr> \n     <td><strong>HNSW Vector Search</strong></td> \n     <td>61us p50 query latency via <code>ruvector-core</code></td> \n    </tr> \n    <tr> \n     <td><strong>Knowledge Graph</strong></td> \n     <td>Entity extraction (persons, URLs, emails, mentions) via <code>ruvector-graph</code></td> \n    </tr> \n    <tr> \n     <td><strong>Attention Reranking</strong></td> \n     <td>Content prioritization via <code>ruvector-attention</code></td> \n    </tr> \n    <tr> \n     <td><strong>Quantum Diversity</strong></td> \n     <td>MMR + quantum-inspired result selection via <code>ruqu-algorithms</code></td> \n    </tr> \n    <tr> \n     <td><strong>GNN Learning</strong></td> \n     <td>Search quality improves over time via <code>ruvector-gnn</code></td> \n    </tr> \n    <tr> \n     <td><strong>PII Safety Gate</strong></td> \n     <td>Auto-redacts credit cards, SSNs, emails before storage</td> \n    </tr> \n    <tr> \n     <td><strong>Frame Deduplication</strong></td> \n     <td>Cosine similarity sliding window eliminates near-duplicates</td> \n    </tr> \n    <tr> \n     <td><strong>Query Router</strong></td> \n     <td>Auto-routes to Semantic, Keyword, Graph, Temporal, or Hybrid backend</td> \n    </tr> \n    <tr> \n     <td><strong>4-Tier Quantization</strong></td> \n     <td>f32 -&gt; int8 -&gt; product -&gt; binary (97% memory savings over time)</td> \n    </tr> \n    <tr> \n     <td><strong>REST API</strong></td> \n     <td>Axum server with <code>/v2/search</code>, <code>/v2/route</code>, <code>/v2/stats</code>, <code>/v2/health</code></td> \n    </tr> \n    <tr> \n     <td><strong>WASM Support</strong></td> \n     <td>Runs in browser (145 KB), bundles from 11.8 KB (micro) to 350 KB (full)</td> \n    </tr> \n    <tr> \n     <td><strong>Cross-Platform</strong></td> \n     <td>Native: Linux, macOS, Windows; WASM: any browser</td> \n    </tr> \n   </tbody> \n  </table> \n  <p><strong>Comparison: Screenpipe vs OSpipe</strong></p> \n  <table> \n   <thead> \n    <tr> \n     <th></th> \n     <th>Screenpipe (FTS5)</th> \n     <th>OSpipe (RuVector)</th> \n    </tr> \n   </thead> \n   <tbody> \n    <tr> \n     <td>Search</td> \n     <td>Keyword (FTS5)</td> \n     <td>Semantic + Keyword + Graph + Temporal</td> \n    </tr> \n    <tr> \n     <td>Latency</td> \n     <td>~1ms (FTS5)</td> \n     <td>61us (HNSW p50)</td> \n    </tr> \n    <tr> \n     <td>Relations</td> \n     <td>None</td> \n     <td>Knowledge Graph (Cypher)</td> \n    </tr> \n    <tr> \n     <td>PII</td> \n     <td>Basic</td> \n     <td>Credit card, SSN, email redaction</td> \n    </tr> \n    <tr> \n     <td>Dedup</td> \n     <td>None</td> \n     <td>Cosine similarity sliding window</td> \n    </tr> \n    <tr> \n     <td>Browser</td> \n     <td>None</td> \n     <td>WASM (11.8 KB - 350 KB)</td> \n    </tr> \n    <tr> \n     <td>Quantization</td> \n     <td>None</td> \n     <td>4-tier age-based (f32 -&gt; binary)</td> \n    </tr> \n   </tbody> \n  </table> \n  <p><strong>Integrates 10 RuVector crates:</strong> ruvector-core, ruvector-filter, ruvector-cluster, ruvector-delta-core, ruvector-router-core, cognitum-gate-kernel, ruvector-graph, ruvector-attention, ruvector-gnn, ruqu-algorithms.</p> \n </details> \n <pre><code class=\"language-rust\">use ospipe::config::OsPipeConfig;\nuse ospipe::pipeline::ingestion::IngestionPipeline;\nuse ospipe::capture::CapturedFrame;\n\nlet config = OsPipeConfig::default();\nlet mut pipeline = IngestionPipeline::new(config)?;\n\n// Ingest a screen capture\nlet frame = CapturedFrame::new_screen(\"Firefox\", \"Meeting Notes\", \"auth discussion: JWT with refresh tokens\", 0);\npipeline.ingest(frame)?;\n\n// Semantic search\nlet results = pipeline.search(\"what was the authentication discussion?\", 5)?;\n</code></pre> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/OSpipe/README.md\">OSpipe README</a> for full documentation, TypeScript/WASM quickstart, and configuration reference.</p> \n <h3>Standalone Edge Database (rvLite)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvlite\">rvlite</a></td> \n    <td>Standalone 2MB edge database with SQL, SPARQL, Cypher</td> \n    <td><a href=\"https://crates.io/crates/rvlite\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvlite.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>rvLite Features:</strong> Powered by RuVector WASM, supports SQL/SPARQL/Cypher queries, ideal for IoT, mobile, and embedded systems.</p> \n <h3>PostgreSQL Extension</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-postgres\">ruvector-postgres</a></td> \n    <td>pgvector replacement with 230+ SQL functions, SIMD, Flash Attention</td> \n    <td><a href=\"https://crates.io/crates/ruvector-postgres\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-postgres.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>PostgreSQL Features:</strong> Drop-in pgvector replacement, GNN layers, hybrid search, multi-tenancy, self-healing, self-learning capabilities.</p> \n <pre><code class=\"language-bash\">docker pull ruvnet/ruvector-postgres    # Docker image\ncargo add ruvector-postgres             # Rust crate\n</code></pre> \n <h3>Self-Learning Query DAG (ruvector-dag)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-dag\">ruvector-dag</a></td> \n    <td>Neural self-learning DAG for automatic query optimization</td> \n    <td><a href=\"https://crates.io/crates/ruvector-dag\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-dag.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-dag-wasm\">ruvector-dag-wasm</a></td> \n    <td>WASM bindings for browser DAG optimization (58KB gzipped)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-dag-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-dag-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Make your queries faster automatically.</strong> RuVector DAG learns from every query execution and continuously optimizes performance—no manual tuning required.</p> \n <ul> \n  <li><strong>7 Attention Mechanisms</strong>: Automatically selects the best strategy (Topological, Causal Cone, Critical Path, MinCut Gated, etc.)</li> \n  <li><strong>SONA Learning</strong>: Self-Optimizing Neural Architecture adapts in &lt;100μs per query</li> \n  <li><strong>MinCut Control</strong>: Rising \"tension\" triggers automatic strategy switching and predictive healing</li> \n  <li><strong>50-80% Latency Reduction</strong>: Queries improve over time without code changes</li> \n </ul> \n <pre><code class=\"language-rust\">use ruvector_dag::{QueryDag, OperatorNode};\nuse ruvector_dag::attention::{AttentionSelector, SelectionPolicy};\n\nlet mut dag = QueryDag::new();\nlet scan = dag.add_node(OperatorNode::hnsw_scan(0, \"vectors_idx\", 64));\nlet filter = dag.add_node(OperatorNode::filter(1, \"score &gt; 0.5\"));\ndag.add_edge(scan, filter).unwrap();\n\n// System learns which attention mechanism works best\nlet selector = AttentionSelector::new();\nlet scores = selector.select_and_apply(SelectionPolicy::Adaptive, &amp;dag)?;\n</code></pre> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-dag/README.md\">ruvector-dag README</a> for full documentation.</p> \n <h3>Temporal Tensor Store</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-temporal-tensor\">ruvector-temporal-tensor</a></td> \n    <td>Time-series tensor storage with tiered quantization</td> \n    <td><a href=\"https://crates.io/crates/ruvector-temporal-tensor\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-temporal-tensor.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-temporal-tensor-wasm\">ruvector-temporal-tensor-wasm</a></td> \n    <td>WASM bindings for browser temporal tensors</td> \n    <td><a href=\"https://crates.io/crates/ruvector-temporal-tensor-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-temporal-tensor-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>High-performance temporal embedding storage</strong> optimized for AI agent memory systems:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Block-Based Storage</strong></td> \n    <td>4KB aligned blocks with SIMD-optimized I/O (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/temporal-tensor-store/ADR-018-block-based-storage-engine.md\">ADR-018</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>Tiered Quantization</strong></td> \n    <td>F32 → F16 → INT8 → INT4 with &lt;1% accuracy loss (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/temporal-tensor-store/ADR-019-tiered-quantization-formats.md\">ADR-019</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>Temporal Scoring</strong></td> \n    <td>Access frequency + recency decay for automatic tier migration (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/temporal-tensor-store/ADR-020-temporal-scoring-tier-migration.md\">ADR-020</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>Delta Compression</strong></td> \n    <td>60-80% storage reduction via temporal differencing (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/temporal-tensor-store/ADR-021-delta-compression-reconstruction.md\">ADR-021</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>Cross-Platform WASM</strong></td> \n    <td>Unified API for browser, Node.js, and edge (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/temporal-tensor-store/ADR-022-wasm-api-cross-platform.md\">ADR-022</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>AgentDB Integration</strong></td> \n    <td>Native coherence scoring and memory persistence</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Performance Targets:</strong> &gt;100K writes/sec, &lt;1ms p99 read latency, 4-32x compression (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/temporal-tensor-store/ADR-023-benchmarking-acceptance-criteria.md\">ADR-023</a>)</p> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/architecture/temporal-tensor-store-ddd.md\">Domain-Driven Design</a> for architecture details.</p> \n <h3>Delta Behavior (Behavioral Change Tracking)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-delta-core\">ruvector-delta-core</a></td> \n    <td>Core delta types and traits for behavioral vector change tracking</td> \n    <td><a href=\"https://crates.io/crates/ruvector-delta-core\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-delta-core.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-delta-graph\">ruvector-delta-graph</a></td> \n    <td>Delta operations for graph structures — edge and node changes</td> \n    <td><a href=\"https://crates.io/crates/ruvector-delta-graph\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-delta-graph.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-delta-index\">ruvector-delta-index</a></td> \n    <td>Delta-aware HNSW index with incremental updates and repair</td> \n    <td><a href=\"https://crates.io/crates/ruvector-delta-index\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-delta-index.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-delta-consensus\">ruvector-delta-consensus</a></td> \n    <td>Distributed delta consensus using CRDTs and causal ordering</td> \n    <td><a href=\"https://crates.io/crates/ruvector-delta-consensus\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-delta-consensus.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-delta-wasm\">ruvector-delta-wasm</a></td> \n    <td>WASM bindings for delta operations on vectors</td> \n    <td><a href=\"https://crates.io/crates/ruvector-delta-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-delta-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Delta Behavior</strong> tracks how vectors change over time — the mathematics of systems that refuse to collapse. Incremental HNSW updates without full rebuilds, CRDT-based distributed consensus, and causal ordering for conflict-free replication.</p> \n <h3>Profiling &amp; Diagnostics</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/profiling\">profiling</a></td> \n    <td>Real-time coherence assessment via dynamic min-cut</td> \n    <td><a href=\"https://crates.io/crates/profiling\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/profiling.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>CRV Signal Line Protocol</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-crv\">ruvector-crv</a></td> \n    <td>6-stage CRV signal line methodology for vector search</td> \n    <td><a href=\"https://crates.io/crates/ruvector-crv\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-crv.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Maps CRV stages to ruvector subsystems:</strong></p> \n <ul> \n  <li>Stage I (Ideograms) → Poincaré ball hyperbolic embeddings</li> \n  <li>Stage II (Sensory) → Multi-head attention vectors</li> \n  <li>Stage III (Dimensional) → GNN graph topology</li> \n  <li>Stage IV (Emotional) → SNN temporal encoding</li> \n  <li>Stage V (Interrogation) → Differentiable search</li> \n  <li>Stage VI (3D Model) → MinCut partitioning</li> \n </ul> \n <h3>Quantum Simulation Engine (ruQu)</h3> \n <p><a href=\"https://www.npmjs.com/package/@ruvector/ruqu-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ruqu-wasm.svg?sanitize=true\" /></a></p> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruqu-core\">ruqu-core</a></td> \n    <td>State-vector simulator with SIMD, noise models</td> \n    <td><a href=\"https://crates.io/crates/ruqu-core\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruqu-core.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruqu-algorithms\">ruqu-algorithms</a></td> \n    <td>VQE, Grover's search, QAOA MaxCut, Surface Code QEC</td> \n    <td><a href=\"https://crates.io/crates/ruqu-algorithms\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruqu-algorithms.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruqu-exotic\">ruqu-exotic</a></td> \n    <td>Quantum-classical hybrids: decay, interference, syndrome</td> \n    <td><a href=\"https://crates.io/crates/ruqu-exotic\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruqu-exotic.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruqu-wasm\">ruqu-wasm</a></td> \n    <td>WebAssembly bindings (npm: <code>@ruvector/ruqu-wasm</code>)</td> \n    <td><a href=\"https://crates.io/crates/ruqu-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruqu-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\">npm install @ruvector/ruqu-wasm    # Browser/Node.js\ncargo add ruqu-core                # Rust\n</code></pre> \n <p><strong>Pure Rust quantum simulation</strong> with 25-qubit WASM support:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>State-Vector Simulator</strong></td> \n    <td>Complex128 amplitudes, SIMD acceleration (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/quantum-engine/ADR-QE-001-quantum-engine-core-architecture.md\">QE-001</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>VQE Algorithm</strong></td> \n    <td>Variational Quantum Eigensolver for chemistry (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/quantum-engine/ADR-QE-005-vqe-algorithm-support.md\">QE-005</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>Grover's Search</strong></td> \n    <td>Quadratic speedup for unstructured search (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/quantum-engine/ADR-QE-006-grover-search-implementation.md\">QE-006</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>QAOA MaxCut</strong></td> \n    <td>Quantum approximate optimization (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/quantum-engine/ADR-QE-007-qaoa-maxcut-implementation.md\">QE-007</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>Surface Code QEC</strong></td> \n    <td>Topological error correction (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/quantum-engine/ADR-QE-008-surface-code-error-correction.md\">QE-008</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>MinCut Coherence</strong></td> \n    <td>Quantum-classical integration via dynamic min-cut (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/quantum-engine/ADR-QE-012-mincut-coherence-integration.md\">QE-012</a>)</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-rust\">use ruqu_core::{QuantumState, Gate, Circuit};\n\nlet mut circuit = Circuit::new(3);\ncircuit.add_gate(Gate::H, 0);           // Hadamard\ncircuit.add_gate(Gate::CNOT, 0, 1);     // Entangle\ncircuit.add_gate(Gate::CNOT, 1, 2);     // GHZ state\n\nlet state = circuit.execute()?;\nlet result = state.measure_all();        // Collapse to |000⟩ or |111⟩\n</code></pre> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/quantum-engine/\">Quantum Engine ADRs</a> for full documentation.</p> \n <h3>Distributed Systems (Raft &amp; Replication)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-raft\">ruvector-raft</a></td> \n    <td>Raft consensus with leader election &amp; log replication</td> \n    <td><a href=\"https://crates.io/crates/ruvector-raft\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-raft.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-replication\">ruvector-replication</a></td> \n    <td>Multi-master replication with vector clocks</td> \n    <td><a href=\"https://crates.io/crates/ruvector-replication\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-replication.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-cluster\">ruvector-cluster</a></td> \n    <td>Cluster coordination and sharding</td> \n    <td><a href=\"https://crates.io/crates/ruvector-cluster\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-cluster.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Build distributed vector databases</strong> with strong consistency guarantees:</p> \n <ul> \n  <li><strong>Raft Consensus</strong> — Leader election, log replication, automatic failover</li> \n  <li><strong>Vector Clocks</strong> — Causal ordering for conflict detection</li> \n  <li><strong>Conflict Resolution</strong> — Last-Write-Wins, custom merge functions, CRDT support</li> \n  <li><strong>Change Data Capture</strong> — Stream changes to replicas in real-time</li> \n  <li><strong>Automatic Failover</strong> — Promote replicas on primary failure</li> \n </ul> \n <pre><code class=\"language-typescript\">import { RaftNode, ReplicaSet, VectorClock } from '@ruvector/raft';\nimport { ReplicationManager, ConflictStrategy } from '@ruvector/replication';\n\n// Raft consensus cluster\nconst node = new RaftNode({\n  nodeId: 'node-1',\n  peers: ['node-2', 'node-3'],\n  electionTimeout: [150, 300],\n});\n\nawait node.start();\nconst entry = await node.propose({ op: 'insert', vector: embedding });\n\n// Multi-master replication\nconst replicaSet = new ReplicaSet();\nreplicaSet.addReplica('primary', 'localhost:5001', 'primary');\nreplicaSet.addReplica('replica-1', 'localhost:5002', 'replica');\n\nconst manager = new ReplicationManager(replicaSet, {\n  conflictStrategy: ConflictStrategy.LastWriteWins,\n  syncMode: 'async',\n});\n\nawait manager.write('vectors', { id: 'v1', data: embedding });\n</code></pre> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/npm/packages/raft/README.md\">npm/packages/raft/README.md</a> and <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/npm/packages/replication/README.md\">npm/packages/replication/README.md</a> for full documentation.</p> \n <h3>Standalone Vector Database (rvLite)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvlite\">rvlite</a></td> \n    <td>SQLite-style vector database for browsers &amp; edge</td> \n    <td><a href=\"https://crates.io/crates/rvlite\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvlite.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Runs anywhere JavaScript runs</strong> — browsers, Node.js, Deno, Bun, Cloudflare Workers, Vercel Edge:</p> \n <ul> \n  <li><strong>SQL + SPARQL + Cypher</strong> unified query interface</li> \n  <li><strong>Zero dependencies</strong> — thin orchestration over existing WASM crates</li> \n  <li><strong>Self-learning</strong> via SONA ReasoningBank integration</li> \n </ul> \n <pre><code class=\"language-typescript\">import { RvLite } from '@rvlite/wasm';\n\nconst db = await RvLite.create();\nawait db.sql(`CREATE TABLE docs (id SERIAL, embedding VECTOR(384))`);\nawait db.sparql(`SELECT ?s WHERE { ?s rdf:type ex:Document }`);\nawait db.cypher(`MATCH (d:Doc)-[:SIMILAR]-&gt;(r) RETURN r`);\n</code></pre> \n <h3>Self-Optimizing Neural Architecture (SONA)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n    <th>npm</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/sona\">ruvector-sona</a></td> \n    <td>Runtime-adaptive learning with LoRA, EWC++, and ReasoningBank</td> \n    <td><a href=\"https://crates.io/crates/ruvector-sona\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-sona.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/sona\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/sona.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>SONA</strong> enables AI systems to continuously improve from user feedback without expensive retraining:</p> \n <ul> \n  <li><strong>Two-tier LoRA</strong>: MicroLoRA (rank 1-2) for instant adaptation, BaseLoRA (rank 4-16) for long-term learning</li> \n  <li><strong>EWC++</strong>: Elastic Weight Consolidation prevents catastrophic forgetting</li> \n  <li><strong>ReasoningBank</strong>: K-means++ clustering stores and retrieves successful reasoning patterns</li> \n  <li><strong>Lock-free Trajectories</strong>: ~50ns overhead per step with crossbeam ArrayQueue</li> \n  <li><strong>Sub-millisecond Learning</strong>: &lt;0.8ms per trajectory processing</li> \n </ul> \n <pre><code class=\"language-bash\"># Rust\ncargo add ruvector-sona\n\n# Node.js\nnpm install @ruvector/sona\n</code></pre> \n <pre><code class=\"language-rust\">use ruvector_sona::{SonaEngine, SonaConfig};\n\nlet engine = SonaEngine::new(SonaConfig::default());\nlet traj_id = engine.start_trajectory(query_embedding);\nengine.record_step(traj_id, node_id, 0.85, 150);\nengine.end_trajectory(traj_id, 0.90);\nengine.learn_from_feedback(LearningSignal::positive(50.0, 0.95));\n</code></pre> \n <pre><code class=\"language-javascript\">// Node.js\nconst { SonaEngine } = require('@ruvector/sona');\n\nconst engine = new SonaEngine(256); // 256 hidden dimensions\nconst trajId = engine.beginTrajectory([0.1, 0.2, ...]);\nengine.addTrajectoryStep(trajId, activations, attention, 0.9);\nengine.endTrajectory(trajId, 0.95);\n</code></pre> \n</details> \n<hr /> \n<h2>Platform Features</h2> \n<details> \n <strong>🔀 Self-Learning DAG (Query Optimization)</strong> \n <p><a href=\"https://crates.io/crates/ruvector-dag\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-dag.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/@ruvector/rudag\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rudag.svg?sanitize=true\" /></a></p> \n <p><strong>Make your queries faster automatically.</strong> RuVector DAG learns from every query execution and continuously optimizes performance—no manual tuning required.</p> \n <h3>What is RuVector DAG?</h3> \n <p>A <strong>self-learning query optimization system</strong>—like a \"nervous system\" for your database queries that:</p> \n <ol> \n  <li><strong>Watches</strong> how queries execute and identifies bottlenecks</li> \n  <li><strong>Learns</strong> which optimization strategies work best for different query patterns</li> \n  <li><strong>Adapts</strong> in real-time, switching strategies when conditions change</li> \n  <li><strong>Heals</strong> itself by detecting anomalies and fixing problems before they impact users</li> \n </ol> \n <p>Unlike traditional query optimizers that use static rules, RuVector DAG learns from actual execution patterns and gets smarter over time.</p> \n <h3>Key Benefits</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Benefit</th> \n    <th>What It Does</th> \n    <th>Result</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Automatic Improvement</strong></td> \n    <td>Queries get faster without code changes</td> \n    <td><strong>50-80% latency reduction</strong> after learning</td> \n   </tr> \n   <tr> \n    <td><strong>Zero-Downtime Adaptation</strong></td> \n    <td>Adapts to pattern changes automatically</td> \n    <td>No manual index rebuilds</td> \n   </tr> \n   <tr> \n    <td><strong>Predictive Prevention</strong></td> \n    <td>Detects rising \"tension\" early</td> \n    <td>Intervenes <em>before</em> slowdowns</td> \n   </tr> \n   <tr> \n    <td><strong>Works Everywhere</strong></td> \n    <td>PostgreSQL, Browser (58KB WASM), Embedded</td> \n    <td>Universal deployment</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Use Cases</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>Why RuVector DAG Helps</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Vector Search Applications</strong></td> \n    <td>Optimize similarity searches that traditional databases struggle with</td> \n   </tr> \n   <tr> \n    <td><strong>High-Traffic APIs</strong></td> \n    <td>Automatically adapt to changing query patterns throughout the day</td> \n   </tr> \n   <tr> \n    <td><strong>Real-Time Analytics</strong></td> \n    <td>Learn which aggregation paths are fastest for your specific data</td> \n   </tr> \n   <tr> \n    <td><strong>Edge/Embedded Systems</strong></td> \n    <td>58KB WASM build runs in browsers and IoT devices</td> \n   </tr> \n   <tr> \n    <td><strong>Multi-Tenant Platforms</strong></td> \n    <td>Learn per-tenant query patterns without manual tuning</td> \n   </tr> \n  </tbody> \n </table> \n <h3>How It Works</h3> \n <pre><code>Query comes in → DAG analyzes execution plan → Best attention mechanism selected\n                                                          ↓\nQuery executes → Results returned → Learning system records what worked\n                                                          ↓\n                    Next similar query benefits from learned optimizations\n</code></pre> \n <p>The system maintains a <strong>MinCut tension</strong> score that acts as a health indicator. When tension rises, the system automatically switches to more aggressive optimization strategies and triggers predictive healing.</p> \n <h3>7 DAG Attention Mechanisms</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Mechanism</th> \n    <th>When to Use</th> \n    <th>Trigger</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Topological</strong></td> \n    <td>Default baseline</td> \n    <td>Low variance</td> \n   </tr> \n   <tr> \n    <td><strong>Causal Cone</strong></td> \n    <td>Downstream impact analysis</td> \n    <td>Write-heavy patterns</td> \n   </tr> \n   <tr> \n    <td><strong>Critical Path</strong></td> \n    <td>Latency-bound queries</td> \n    <td>p99 &gt; 2x p50</td> \n   </tr> \n   <tr> \n    <td><strong>MinCut Gated</strong></td> \n    <td>Bottleneck-aware weighting</td> \n    <td>Cut tension rising</td> \n   </tr> \n   <tr> \n    <td><strong>Hierarchical Lorentz</strong></td> \n    <td>Deep hierarchical queries</td> \n    <td>Depth &gt; 10</td> \n   </tr> \n   <tr> \n    <td><strong>Parallel Branch</strong></td> \n    <td>Wide parallel execution</td> \n    <td>Branch count &gt; 3</td> \n   </tr> \n   <tr> \n    <td><strong>Temporal BTSP</strong></td> \n    <td>Time-series workloads</td> \n    <td>Temporal patterns</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <p><strong>Rust:</strong></p> \n <pre><code class=\"language-rust\">use ruvector_dag::{QueryDag, OperatorNode, OperatorType};\nuse ruvector_dag::attention::{TopologicalAttention, DagAttention};\n\n// Build a query DAG\nlet mut dag = QueryDag::new();\nlet scan = dag.add_node(OperatorNode::hnsw_scan(0, \"vectors_idx\", 64));\nlet filter = dag.add_node(OperatorNode::filter(1, \"score &gt; 0.5\"));\nlet result = dag.add_node(OperatorNode::new(2, OperatorType::Result));\n\ndag.add_edge(scan, filter).unwrap();\ndag.add_edge(filter, result).unwrap();\n\n// Compute attention scores\nlet attention = TopologicalAttention::new(Default::default());\nlet scores = attention.forward(&amp;dag).unwrap();\n</code></pre> \n <p><strong>Node.js:</strong></p> \n <pre><code class=\"language-javascript\">import { QueryDag, TopologicalAttention } from '@ruvector/rudag';\n\n// Build DAG\nconst dag = new QueryDag();\nconst scan = dag.addNode({ type: 'hnsw_scan', table: 'vectors', k: 64 });\nconst filter = dag.addNode({ type: 'filter', condition: 'score &gt; 0.5' });\ndag.addEdge(scan, filter);\n\n// Apply attention\nconst attention = new TopologicalAttention();\nconst scores = attention.forward(dag);\nconsole.log('Attention scores:', scores);\n</code></pre> \n <p><strong>Browser (WASM - 58KB):</strong></p> \n <pre><code class=\"language-html\">&lt;script type=\"module\"&gt;\nimport init, { QueryDag, TopologicalAttention } from '@ruvector/rudag-wasm';\n\nawait init();\nconst dag = new QueryDag();\n// ... same API as Node.js\n&lt;/script&gt;\n</code></pre> \n <h3>SONA Learning Integration</h3> \n <p>SONA (Self-Optimizing Neural Architecture) runs post-query in background, never blocking execution:</p> \n <pre><code class=\"language-rust\">use ruvector_dag::sona::{DagSonaEngine, SonaConfig};\n\nlet config = SonaConfig {\n    embedding_dim: 256,\n    lora_rank: 2,           // Rank-2 for &lt;100μs updates\n    ewc_lambda: 5000.0,     // Catastrophic forgetting prevention\n    trajectory_capacity: 10_000,\n};\nlet mut sona = DagSonaEngine::new(config);\n\n// Pre-query: Get enhanced embedding (fast path)\nlet enhanced = sona.pre_query(&amp;dag);\n\n// Execute query...\nlet execution_time = execute_query(&amp;dag);\n\n// Post-query: Record trajectory (async, background)\nsona.post_query(&amp;dag, execution_time, baseline_time, \"topological\");\n</code></pre> \n <h3>Self-Healing</h3> \n <p>Reactive (Z-score anomaly detection) + Predictive (rising MinCut tension triggers early intervention):</p> \n <pre><code class=\"language-rust\">use ruvector_dag::healing::{HealingOrchestrator, AnomalyConfig, PredictiveConfig};\n\nlet mut orchestrator = HealingOrchestrator::new();\n\n// Reactive: Z-score anomaly detection\norchestrator.add_detector(\"query_latency\", AnomalyConfig {\n    z_threshold: 3.0,\n    window_size: 100,\n    min_samples: 10,\n});\n\n// Predictive: Rising cut tension triggers early intervention\norchestrator.enable_predictive(PredictiveConfig {\n    tension_threshold: 0.6,    // Intervene before 0.7 crisis\n    variance_threshold: 1.5,   // Rising variance = trouble coming\n    lookahead_window: 50,      // Predict 50 queries ahead\n});\n</code></pre> \n <h3>Query Convergence Example</h3> \n <p>A slow query converges over several runs:</p> \n <pre><code class=\"language-text\">[run 1] query: SELECT * FROM vectors WHERE embedding &lt;-&gt; $1 &lt; 0.5\n        attention: topological (default)\n        mincut_tension: 0.23\n        latency: 847ms (improvement: 0.4%)\n\n[run 4] mincut_tension: 0.71 &gt; 0.7 (THRESHOLD)\n        --&gt; switching attention: topological -&gt; mincut_gated\n        latency: 412ms (improvement: 51.5%)\n\n[run 10] attention: mincut_gated\n         mincut_tension: 0.22 (stable)\n         latency: 156ms (improvement: 81.6%)\n</code></pre> \n <h3>Performance Targets</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Component</th> \n    <th>Target</th> \n    <th>Notes</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Attention (100 nodes)</td> \n    <td>&lt;100μs</td> \n    <td>All 7 mechanisms</td> \n   </tr> \n   <tr> \n    <td>MicroLoRA adaptation</td> \n    <td>&lt;100μs</td> \n    <td>Rank-2, per-operator</td> \n   </tr> \n   <tr> \n    <td>Pattern search (10K)</td> \n    <td>&lt;2ms</td> \n    <td>K-means++ indexing</td> \n   </tr> \n   <tr> \n    <td>MinCut update</td> \n    <td>O(n^0.12)</td> \n    <td>Subpolynomial amortized</td> \n   </tr> \n   <tr> \n    <td>Anomaly detection</td> \n    <td>&lt;50μs</td> \n    <td>Z-score, streaming</td> \n   </tr> \n   <tr> \n    <td>WASM size</td> \n    <td>58KB</td> \n    <td>Gzipped, browser-ready</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Installation</h3> \n <pre><code class=\"language-bash\"># Rust\ncargo add ruvector-dag\n\n# Node.js\nnpm install @ruvector/rudag\n\n# WASM (browser)\nnpm install @ruvector/rudag-wasm\n</code></pre> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-dag/README.md\">ruvector-dag README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>📦 rvLite - Standalone Edge Database</strong> \n <p><a href=\"https://crates.io/crates/rvlite\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvlite.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/@ruvector/rvlite\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rvlite.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/@ruvector/rvlite\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/rvlite.svg?sanitize=true\" /></a></p> \n <p><strong>A complete vector database that runs anywhere JavaScript runs</strong> — browsers, Node.js, Deno, Bun, Cloudflare Workers, Vercel Edge Functions.</p> \n <h3>What is rvLite?</h3> \n <p>rvLite is a <strong>lightweight, standalone vector database</strong> that runs entirely in WebAssembly. It provides SQL, SPARQL, and Cypher query interfaces, along with graph neural networks and self-learning capabilities—all in under 3MB.</p> \n <h3>Key Features</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>SQL Interface</strong></td> \n    <td>Familiar <code>SELECT</code>, <code>INSERT</code>, <code>WHERE</code></td> \n    <td>No learning curve</td> \n   </tr> \n   <tr> \n    <td><strong>SPARQL Support</strong></td> \n    <td>W3C-compliant RDF queries</td> \n    <td>Knowledge graphs in browser</td> \n   </tr> \n   <tr> \n    <td><strong>Cypher Queries</strong></td> \n    <td>Neo4j-style graph queries</td> \n    <td>Graph traversals anywhere</td> \n   </tr> \n   <tr> \n    <td><strong>GNN Embeddings</strong></td> \n    <td>Graph neural network layers</td> \n    <td>Self-learning search</td> \n   </tr> \n   <tr> \n    <td><strong>ReasoningBank</strong></td> \n    <td>Trajectory learning</td> \n    <td>Gets smarter over time</td> \n   </tr> \n   <tr> \n    <td><strong>SIMD Optimized</strong></td> \n    <td>Vector operations accelerated</td> \n    <td>Native-like performance</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Runs Everywhere</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Platform</th> \n    <th>Status</th> \n    <th>Use Case</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Browsers</strong></td> \n    <td>✅</td> \n    <td>Offline-first apps</td> \n   </tr> \n   <tr> \n    <td><strong>Node.js</strong></td> \n    <td>✅</td> \n    <td>Server-side</td> \n   </tr> \n   <tr> \n    <td><strong>Deno</strong></td> \n    <td>✅</td> \n    <td>Edge functions</td> \n   </tr> \n   <tr> \n    <td><strong>Bun</strong></td> \n    <td>✅</td> \n    <td>Fast runtime</td> \n   </tr> \n   <tr> \n    <td><strong>Cloudflare Workers</strong></td> \n    <td>✅</td> \n    <td>Edge computing</td> \n   </tr> \n   <tr> \n    <td><strong>Vercel Edge</strong></td> \n    <td>✅</td> \n    <td>Serverless</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Architecture</h3> \n <pre><code>┌─────────────────────────────────────────┐\n│  RvLite (Orchestration)                 │\n│  ├─ SQL executor                        │\n│  ├─ SPARQL executor                     │\n│  ├─ Cypher executor                     │\n│  └─ Unified WASM API                    │\n└──────────────┬──────────────────────────┘\n               │ depends on (100% reuse)\n               ▼\n┌──────────────────────────────────────────┐\n│  Existing WASM Crates                    │\n├──────────────────────────────────────────┤\n│  • ruvector-core (vectors, SIMD)         │\n│  • ruvector-graph-wasm (Cypher)          │\n│  • ruvector-gnn-wasm (GNN layers)        │\n│  • sona (ReasoningBank learning)         │\n│  • micro-hnsw-wasm (ultra-fast HNSW)     │\n└──────────────────────────────────────────┘\n</code></pre> \n <h3>Quick Start</h3> \n <pre><code class=\"language-typescript\">import { RvLite } from '@ruvector/rvlite';\n\n// Create database\nconst db = await RvLite.create();\n\n// SQL with vector search\nawait db.sql(`\n  CREATE TABLE docs (\n    id SERIAL PRIMARY KEY,\n    content TEXT,\n    embedding VECTOR(384)\n  )\n`);\n\nawait db.sql(`\n  SELECT id, content, embedding &lt;=&gt; $1 AS distance\n  FROM docs\n  ORDER BY distance\n  LIMIT 10\n`, [queryVector]);\n\n// Cypher graph queries\nawait db.cypher(`\n  CREATE (a:Person {name: 'Alice'})-[:KNOWS]-&gt;(b:Person {name: 'Bob'})\n`);\n\n// SPARQL RDF queries\nawait db.sparql(`\n  SELECT ?name WHERE {\n    ?person foaf:name ?name .\n  }\n`);\n\n// GNN embeddings\nconst embeddings = await db.gnn.computeEmbeddings('social_network', [\n  db.gnn.createLayer('gcn', { inputDim: 128, outputDim: 64 })\n]);\n\n// Self-learning with ReasoningBank\nawait db.learning.recordTrajectory({ state: [0.1], action: 2, reward: 1.0 });\nawait db.learning.train({ algorithm: 'q-learning', iterations: 1000 });\n</code></pre> \n <h3>Size Budget</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Component</th> \n    <th>Size</th> \n    <th>Purpose</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>ruvector-core</td> \n    <td>~500KB</td> \n    <td>Vectors, SIMD</td> \n   </tr> \n   <tr> \n    <td>SQL parser</td> \n    <td>~200KB</td> \n    <td>Query parsing</td> \n   </tr> \n   <tr> \n    <td>SPARQL executor</td> \n    <td>~300KB</td> \n    <td>RDF queries</td> \n   </tr> \n   <tr> \n    <td>Cypher (graph-wasm)</td> \n    <td>~600KB</td> \n    <td>Graph queries</td> \n   </tr> \n   <tr> \n    <td>GNN layers</td> \n    <td>~300KB</td> \n    <td>Neural networks</td> \n   </tr> \n   <tr> \n    <td>ReasoningBank (sona)</td> \n    <td>~300KB</td> \n    <td>Self-learning</td> \n   </tr> \n   <tr> \n    <td><strong>Total</strong></td> \n    <td><strong>~2.3MB</strong></td> \n    <td>Gzipped</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Installation</h3> \n <pre><code class=\"language-bash\"># npm\nnpm install @ruvector/rvlite\n\n# Rust\ncargo add rvlite\n\n# Build WASM\nwasm-pack build --target web --release\n</code></pre> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvlite/README.md\">rvlite README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🌐 Edge-Net - Collective AI Computing Network</strong> \n <p><a href=\"https://www.npmjs.com/package/@ruvector/edge-net\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/edge-net.svg?sanitize=true\" /></a></p> \n <p><strong>Share, Contribute, Compute Together</strong> — A distributed computing platform that enables collective resource sharing for AI workloads.</p> \n <h3>What is Edge-Net?</h3> \n <p>Edge-Net creates a <strong>collective computing network</strong> where participants share idle browser resources to power distributed AI workloads:</p> \n <pre><code>┌─────────────────────────────────────────────────────────────────────────────┐\n│              EDGE-NET: COLLECTIVE AI COMPUTING NETWORK                      │\n├─────────────────────────────────────────────────────────────────────────────┤\n│   ┌─────────────┐       ┌─────────────┐       ┌─────────────┐              │\n│   │  Your       │       │  Collective │       │  AI Tasks   │              │\n│   │  Browser    │◄─────►│  Network    │◄─────►│  Completed  │              │\n│   │  (Idle CPU) │  P2P  │  (1000s)    │       │  for You    │              │\n│   └─────────────┘       └─────────────┘       └─────────────┘              │\n│         │                     │                     │                       │\n│   Contribute ──────► Earn rUv Units ──────► Use for AI Workloads           │\n└─────────────────────────────────────────────────────────────────────────────┘\n</code></pre> \n <h3>How It Works</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Step</th> \n    <th>What Happens</th> \n    <th>Result</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>1. <strong>Contribute</strong></td> \n    <td>Share unused CPU cycles when browsing</td> \n    <td>Help the network</td> \n   </tr> \n   <tr> \n    <td>2. <strong>Earn</strong></td> \n    <td>Accumulate rUv (Resource Utility Vouchers)</td> \n    <td>Build credits</td> \n   </tr> \n   <tr> \n    <td>3. <strong>Use</strong></td> \n    <td>Spend rUv to run AI tasks</td> \n    <td>Access collective power</td> \n   </tr> \n   <tr> \n    <td>4. <strong>Network Grows</strong></td> \n    <td>More participants = more power</td> \n    <td>Everyone benefits</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Why Collective AI Computing?</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Traditional AI</th> \n    <th>Collective Edge-Net</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Expensive GPU servers</td> \n    <td>Free idle browser CPUs</td> \n   </tr> \n   <tr> \n    <td>Centralized data centers</td> \n    <td>Distributed global network</td> \n   </tr> \n   <tr> \n    <td>Pay-per-use pricing</td> \n    <td>Contribution-based access</td> \n   </tr> \n   <tr> \n    <td>Single point of failure</td> \n    <td>Resilient P2P mesh</td> \n   </tr> \n   <tr> \n    <td>Limited by your hardware</td> \n    <td>Scale with the collective</td> \n   </tr> \n  </tbody> \n </table> \n <h3>AI Intelligence Stack</h3> \n <pre><code>┌─────────────────────────────────────────────────────────────────────────────┐\n│                        AI INTELLIGENCE STACK                                 │\n├─────────────────────────────────────────────────────────────────────────────┤\n│  ┌─────────────────────────────────────────────────────────────────────┐   │\n│  │                    MicroLoRA Adapter Pool (from ruvLLM)              │   │\n│  │  • LRU-managed pool (16 slots) • &lt;50µs rank-1 forward               │   │\n│  │  • 4-bit/8-bit quantization    • 2,236+ ops/sec                     │   │\n│  └─────────────────────────────────────────────────────────────────────┘   │\n│  ┌─────────────────────────────────────────────────────────────────────┐   │\n│  │                    SONA - Self-Optimizing Neural Architecture         │   │\n│  │  • Instant Loop: Per-request MicroLoRA adaptation                    │   │\n│  │  • Background Loop: Hourly K-means consolidation                     │   │\n│  │  • Deep Loop: Weekly EWC++ (prevents catastrophic forgetting)        │   │\n│  └─────────────────────────────────────────────────────────────────────┘   │\n│  ┌──────────────────────┐  ┌──────────────────────┐  ┌─────────────────┐  │\n│  │   HNSW Vector Index  │  │  Federated Learning  │  │ ReasoningBank   │  │\n│  │   • 150x faster      │  │  • Byzantine tolerant│  │ • Pattern learn │  │\n│  │   • O(log N) search  │  │  • Diff privacy      │  │ • 87x energy    │  │\n│  └──────────────────────┘  └──────────────────────┘  └─────────────────┘  │\n└─────────────────────────────────────────────────────────────────────────────┘\n</code></pre> \n <h3>Core AI Tasks</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Task Type</th> \n    <th>Use Case</th> \n    <th>Performance</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Vector Search</strong></td> \n    <td>Find similar items</td> \n    <td>150x speedup via HNSW</td> \n   </tr> \n   <tr> \n    <td><strong>Embeddings</strong></td> \n    <td>Text understanding</td> \n    <td>Semantic vectors</td> \n   </tr> \n   <tr> \n    <td><strong>Semantic Match</strong></td> \n    <td>Intent detection</td> \n    <td>Classify meaning</td> \n   </tr> \n   <tr> \n    <td><strong>LoRA Inference</strong></td> \n    <td>Task adaptation</td> \n    <td>&lt;100µs forward</td> \n   </tr> \n   <tr> \n    <td><strong>Pattern Learning</strong></td> \n    <td>Self-optimization</td> \n    <td>ReasoningBank trajectories</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Pi-Key Identity System</h3> \n <p>Ultra-compact cryptographic identity using mathematical constants:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Key Type</th> \n    <th>Size</th> \n    <th>Purpose</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>π (Pi-Key)</strong></td> \n    <td>40 bytes</td> \n    <td>Permanent identity</td> \n   </tr> \n   <tr> \n    <td><strong>e (Session)</strong></td> \n    <td>34 bytes</td> \n    <td>Encrypted sessions</td> \n   </tr> \n   <tr> \n    <td><strong>φ (Genesis)</strong></td> \n    <td>21 bytes</td> \n    <td>Network origin markers</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Self-Optimizing Features</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Mechanism</th> \n    <th>Benefit</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Task Routing</strong></td> \n    <td>Multi-head attention</td> \n    <td>Work goes to best nodes</td> \n   </tr> \n   <tr> \n    <td><strong>Topology Optimization</strong></td> \n    <td>Self-organizing mesh</td> \n    <td>Network adapts to load</td> \n   </tr> \n   <tr> \n    <td><strong>Q-Learning Security</strong></td> \n    <td>Reinforcement learning</td> \n    <td>Learns to defend threats</td> \n   </tr> \n   <tr> \n    <td><strong>Economic Balance</strong></td> \n    <td>rUv token system</td> \n    <td>Self-sustaining economy</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <p><strong>Add to Your Website:</strong></p> \n <pre><code class=\"language-html\">&lt;script type=\"module\"&gt;\n  import init, { EdgeNetNode, EdgeNetConfig } from '@ruvector/edge-net';\n\n  async function joinCollective() {\n    await init();\n\n    // Join the collective\n    const node = new EdgeNetConfig('my-website')\n      .cpuLimit(0.3)          // Contribute 30% CPU when idle\n      .memoryLimit(256 * 1024 * 1024)  // 256MB max\n      .respectBattery(true)   // Reduce on battery\n      .build();\n\n    node.start();\n\n    // Monitor participation\n    setInterval(() =&gt; {\n      console.log(`Contributed: ${node.ruvBalance()} rUv`);\n    }, 10000);\n  }\n\n  joinCollective();\n&lt;/script&gt;\n</code></pre> \n <p><strong>Use the Collective's AI Power:</strong></p> \n <pre><code class=\"language-javascript\">// Submit an AI task to the collective\nconst result = await node.submitTask('vector_search', {\n  query: embeddings,\n  k: 10,\n  index: 'shared-knowledge-base'\n}, 5);  // Spend up to 5 rUv\n\nconsole.log('Similar items:', result);\n</code></pre> \n <p><strong>Monitor Your Contribution:</strong></p> \n <pre><code class=\"language-javascript\">const stats = node.getStats();\nconsole.log(`\n  rUv Earned: ${stats.ruv_earned}\n  rUv Spent: ${stats.ruv_spent}\n  Tasks Completed: ${stats.tasks_completed}\n  Reputation: ${(stats.reputation * 100).toFixed(1)}%\n`);\n</code></pre> \n <h3>Key Features</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Benefit</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Idle CPU Utilization</strong></td> \n    <td>Use resources that would otherwise be wasted</td> \n   </tr> \n   <tr> \n    <td><strong>Browser-Based</strong></td> \n    <td>No installation, runs in any modern browser</td> \n   </tr> \n   <tr> \n    <td><strong>Adjustable Contribution</strong></td> \n    <td>Control how much you share (10-50% CPU)</td> \n   </tr> \n   <tr> \n    <td><strong>Battery Aware</strong></td> \n    <td>Automatically reduces on battery power</td> \n   </tr> \n   <tr> \n    <td><strong>Fair Distribution</strong></td> \n    <td>Work routed based on capability matching</td> \n   </tr> \n   <tr> \n    <td><strong>Privacy-First</strong></td> \n    <td>Pi-Key cryptographic identity</td> \n   </tr> \n   <tr> \n    <td><strong>Federated Learning</strong></td> \n    <td>Learn collectively without sharing data</td> \n   </tr> \n   <tr> \n    <td><strong>Byzantine Tolerance</strong></td> \n    <td>Resilient to malicious nodes</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Installation</h3> \n <pre><code class=\"language-bash\"># npm\nnpm install @ruvector/edge-net\n\n# Or include directly\n&lt;script src=\"https://unpkg.com/@ruvector/edge-net\"&gt;&lt;/script&gt;\n</code></pre> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge-net/README.md\">edge-net README</a></p> \n </blockquote> \n</details> \n<hr /> \n<h2>AI &amp; Machine Learning</h2> \n<details> \n <strong>🎲 Agentic-Synth - AI Synthetic Data Generation</strong> \n <p><a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/agentic-synth.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/agentic-synth.svg?sanitize=true\" /></a></p> \n <p><strong>AI-Powered Synthetic Data Generation at Scale</strong> — Generate unlimited, high-quality synthetic data for training AI models, testing systems, and building robust agentic applications.</p> \n <h3>Why Agentic-Synth?</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Problem</th> \n    <th>Solution</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Real data is <strong>expensive</strong> to collect</td> \n    <td>Generate <strong>unlimited</strong> synthetic data</td> \n   </tr> \n   <tr> \n    <td><strong>Privacy-sensitive</strong> with compliance risks</td> \n    <td><strong>Fully synthetic</strong>, no PII concerns</td> \n   </tr> \n   <tr> \n    <td><strong>Slow</strong> to generate at scale</td> \n    <td><strong>10-100x faster</strong> than manual creation</td> \n   </tr> \n   <tr> \n    <td><strong>Insufficient</strong> for edge cases</td> \n    <td><strong>Customizable</strong> schemas for any scenario</td> \n   </tr> \n   <tr> \n    <td><strong>Hard to reproduce</strong> across environments</td> \n    <td><strong>Reproducible</strong> with seed values</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Key Features</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Multi-Model Support</strong></td> \n    <td>Gemini, OpenRouter, GPT, Claude, and 50+ models via DSPy.ts</td> \n   </tr> \n   <tr> \n    <td><strong>Context Caching</strong></td> \n    <td>95%+ performance improvement with intelligent LRU cache</td> \n   </tr> \n   <tr> \n    <td><strong>Smart Model Routing</strong></td> \n    <td>Automatic load balancing, failover, and cost optimization</td> \n   </tr> \n   <tr> \n    <td><strong>DSPy.ts Integration</strong></td> \n    <td>Self-learning optimization with 20-25% quality improvement</td> \n   </tr> \n   <tr> \n    <td><strong>Streaming</strong></td> \n    <td>AsyncGenerator for real-time data flow</td> \n   </tr> \n   <tr> \n    <td><strong>Memory Efficient</strong></td> \n    <td>&lt;50MB for datasets up to 10K records</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Data Generation Types</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Type</th> \n    <th>Use Cases</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Time-Series</strong></td> \n    <td>Financial data, IoT sensors, metrics</td> \n   </tr> \n   <tr> \n    <td><strong>Events</strong></td> \n    <td>Logs, user actions, system events</td> \n   </tr> \n   <tr> \n    <td><strong>Structured</strong></td> \n    <td>JSON, CSV, databases, APIs</td> \n   </tr> \n   <tr> \n    <td><strong>Embeddings</strong></td> \n    <td>Vector data for RAG systems</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-bash\"># Install\nnpm install @ruvector/agentic-synth\n\n# Or run instantly with npx\nnpx @ruvector/agentic-synth generate --count 100\n\n# Interactive mode\nnpx @ruvector/agentic-synth interactive\n</code></pre> \n <h3>Basic Usage</h3> \n <pre><code class=\"language-typescript\">import { AgenticSynth } from '@ruvector/agentic-synth';\n\n// Initialize with your preferred model\nconst synth = new AgenticSynth({\n  model: 'gemini-pro',\n  apiKey: process.env.GEMINI_API_KEY\n});\n\n// Generate structured data\nconst users = await synth.generate({\n  schema: {\n    name: 'string',\n    email: 'email',\n    age: 'number:18-65',\n    role: ['admin', 'user', 'guest']\n  },\n  count: 1000\n});\n\n// Generate time-series data\nconst stockData = await synth.timeSeries({\n  fields: ['open', 'high', 'low', 'close', 'volume'],\n  interval: '1h',\n  count: 500,\n  volatility: 0.02\n});\n\n// Stream large datasets\nfor await (const batch of synth.stream({ count: 100000, batchSize: 1000 })) {\n  await processData(batch);\n}\n</code></pre> \n <h3>Self-Learning with DSPy</h3> \n <pre><code class=\"language-typescript\">import { AgenticSynth, DSPyOptimizer } from '@ruvector/agentic-synth';\n\n// Enable self-learning optimization\nconst synth = new AgenticSynth({\n  model: 'gemini-pro',\n  optimizer: new DSPyOptimizer({\n    learningRate: 0.1,\n    qualityThreshold: 0.85\n  })\n});\n\n// Quality improves automatically over time\nconst data = await synth.generate({\n  schema: { ... },\n  count: 1000,\n  optimize: true  // Enable learning\n});\n\nconsole.log(`Quality score: ${data.metrics.quality}`);\n// First run: 0.72\n// After 100 runs: 0.94 (+25% improvement)\n</code></pre> \n <h3>Performance</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Metric</th> \n    <th>Value</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>With caching</strong></td> \n    <td>98.2% faster</td> \n   </tr> \n   <tr> \n    <td><strong>P99 latency</strong></td> \n    <td>2500ms → 45ms</td> \n   </tr> \n   <tr> \n    <td><strong>Memory</strong></td> \n    <td>&lt;50MB for 10K records</td> \n   </tr> \n   <tr> \n    <td><strong>Throughput</strong></td> \n    <td>1000+ records/sec</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Ecosystem Integration</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Purpose</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>RuVector</strong></td> \n    <td>Native vector database for RAG</td> \n   </tr> \n   <tr> \n    <td><strong>DSPy.ts</strong></td> \n    <td>Prompt optimization</td> \n   </tr> \n   <tr> \n    <td><strong>Agentic-Jujutsu</strong></td> \n    <td>Version-controlled generation</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Installation</h3> \n <pre><code class=\"language-bash\"># npm\nnpm install @ruvector/agentic-synth\n\n# Examples package (50+ production examples)\nnpm install @ruvector/agentic-synth-examples\n</code></pre> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/npm/packages/agentic-synth/README.md\">agentic-synth README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>📈 Neural Trader - AI Trading System</strong> \n <p><a href=\"https://www.npmjs.com/package/neural-trader\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/neural-trader.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/neural-trader\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/neural-trader.svg?sanitize=true\" /></a></p> \n <p><strong>Production-ready neural trading system</strong> combining state-of-the-art ML for automated trading, sports betting, and portfolio management. Zero external ML dependencies, sub-millisecond latency.</p> \n <h3>Core AI/ML Engines</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Engine</th> \n    <th>Description</th> \n    <th>Performance</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Fractional Kelly</strong></td> \n    <td>Optimal position sizing with risk-adjusted bet scaling</td> \n    <td>588,885 ops/s</td> \n   </tr> \n   <tr> \n    <td><strong>LSTM-Transformer</strong></td> \n    <td>Deep learning price prediction (temporal + attention)</td> \n    <td>1,468 seq/s</td> \n   </tr> \n   <tr> \n    <td><strong>DRL Portfolio</strong></td> \n    <td>Reinforcement learning ensemble (PPO/SAC/A2C)</td> \n    <td>17,043 steps/s</td> \n   </tr> \n   <tr> \n    <td><strong>Sentiment Alpha</strong></td> \n    <td>Real-time sentiment analysis for alpha generation</td> \n    <td>3,764 pipeline/s</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Why Neural Trader?</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Traditional ML</th> \n    <th>Neural Trader</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>TensorFlow/PyTorch required</td> \n    <td><strong>Zero dependencies</strong></td> \n   </tr> \n   <tr> \n    <td>1.2MB+ bundle size</td> \n    <td><strong>45KB</strong> bundle</td> \n   </tr> \n   <tr> \n    <td>2.1ms LSTM inference</td> \n    <td><strong>0.68ms</strong> inference</td> \n   </tr> \n   <tr> \n    <td>Complex deployment</td> \n    <td><strong>Works in browser &amp; Node.js</strong></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Research-Backed Algorithms</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Algorithm</th> \n    <th>Research Finding</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Kelly Criterion</strong></td> \n    <td>1/5th fractional achieves 98% ROI with 85% less risk of ruin</td> \n   </tr> \n   <tr> \n    <td><strong>LSTM-Transformer</strong></td> \n    <td>Temporal + attention fusion outperforms single architectures</td> \n   </tr> \n   <tr> \n    <td><strong>DRL Ensemble</strong></td> \n    <td>PPO/SAC/A2C voting reduces variance vs single agent</td> \n   </tr> \n   <tr> \n    <td><strong>Sentiment Alpha</strong></td> \n    <td>3% annual excess returns documented in academia</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-javascript\">import { KellyCriterion, HybridLSTMTransformer, DRLPortfolioManager } from 'neural-trader';\n\n// Kelly position sizing\nconst kelly = new KellyCriterion();\nconst stake = kelly.calculateStake(9000, 0.55, 2.0, 0.2);  // 1/5th Kelly\n// → $180 recommended stake (2% of bankroll)\n\n// LSTM-Transformer prediction\nconst model = new HybridLSTMTransformer({\n  lstm: { hiddenSize: 64, layers: 2 },\n  transformer: { heads: 4, layers: 2 }\n});\nconst prediction = model.predict(candles);\n// → { signal: 'BUY', confidence: 0.73, direction: 'bullish' }\n\n// DRL portfolio allocation\nconst manager = new DRLPortfolioManager({ numAssets: 10 });\nawait manager.train(marketData, { episodes: 100 });\nconst allocation = manager.getAction(currentState);\n// → [0.15, 0.12, 0.08, ...] optimal weights\n</code></pre> \n <h3>Use Cases</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Stock Trading</strong></td> \n    <td>DAG-based pipeline with parallel execution</td> \n   </tr> \n   <tr> \n    <td><strong>Sports Betting</strong></td> \n    <td>Kelly sizing with ML calibration</td> \n   </tr> \n   <tr> \n    <td><strong>Crypto Trading</strong></td> \n    <td>DRL portfolio for 20+ assets</td> \n   </tr> \n   <tr> \n    <td><strong>News Trading</strong></td> \n    <td>Real-time sentiment stream processing</td> \n   </tr> \n   <tr> \n    <td><strong>Portfolio Rebalancing</strong></td> \n    <td>Reinforcement learning allocation</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Package Ecosystem (20+)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>neural-trader</code></td> \n    <td>Core engine with native HNSW, SIMD</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/core</code></td> \n    <td>Ultra-low latency Rust + Node.js bindings</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/strategies</code></td> \n    <td>Strategy management and backtesting</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/execution</code></td> \n    <td>Trade execution and order management</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/mcp</code></td> \n    <td>MCP server with 87+ trading tools</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/risk</code></td> \n    <td>VaR, stress testing, risk metrics</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/portfolio</code></td> \n    <td>Markowitz, Risk Parity optimization</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/neural</code></td> \n    <td>Neural network training</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/brokers</code></td> \n    <td>Alpaca, Interactive Brokers</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/sports-betting</code></td> \n    <td>Arbitrage, Kelly, odds analysis</td> \n   </tr> \n  </tbody> \n </table> \n <h3>CLI Interface</h3> \n <pre><code class=\"language-bash\"># Real-time trading\nnode cli.js run --strategy=hybrid --symbol=AAPL --capital=100000\n\n# Backtest historical performance\nnode cli.js backtest --days=252 --capital=50000 --strategy=drl\n\n# Paper trading simulation\nnode cli.js paper --capital=100000 --strategy=sentiment\n\n# Performance benchmarks\nnode cli.js benchmark --iterations=100\n</code></pre> \n <h3>Exotic Examples</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Example</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Multi-Agent Swarm</strong></td> \n    <td>Distributed trading intelligence with consensus</td> \n   </tr> \n   <tr> \n    <td><strong>GNN Correlation Network</strong></td> \n    <td>Graph neural network correlation analysis</td> \n   </tr> \n   <tr> \n    <td><strong>Attention Regime Detection</strong></td> \n    <td>Transformer-based market regime classification</td> \n   </tr> \n   <tr> \n    <td><strong>Quantum Portfolio</strong></td> \n    <td>QAOA &amp; quantum annealing optimization</td> \n   </tr> \n   <tr> \n    <td><strong>Hyperbolic Embeddings</strong></td> \n    <td>Poincaré disk market embeddings</td> \n   </tr> \n   <tr> \n    <td><strong>Atomic Arbitrage</strong></td> \n    <td>Cross-exchange with MEV protection</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Performance</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Module</th> \n    <th>Latency</th> \n    <th>Throughput</th> \n    <th>Status</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Kelly Engine</td> \n    <td>0.014ms</td> \n    <td>71,295/s</td> \n    <td>✓ Ready</td> \n   </tr> \n   <tr> \n    <td>LSTM-Transformer</td> \n    <td>0.681ms</td> \n    <td>1,468/s</td> \n    <td>✓ Ready</td> \n   </tr> \n   <tr> \n    <td>DRL Portfolio</td> \n    <td>0.059ms</td> \n    <td>17,043/s</td> \n    <td>✓ Ready</td> \n   </tr> \n   <tr> \n    <td>Sentiment Alpha</td> \n    <td>0.266ms</td> \n    <td>3,764/s</td> \n    <td>✓ Ready</td> \n   </tr> \n   <tr> \n    <td>Full Pipeline</td> \n    <td>4.68ms</td> \n    <td>214/s</td> \n    <td>✓ Ready</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Installation</h3> \n <pre><code class=\"language-bash\"># npm\nnpm install neural-trader\n\n# Full ecosystem\nnpm install @neural-trader/core @neural-trader/strategies @neural-trader/mcp\n</code></pre> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/neural-trader/README.md\">neural-trader README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🥋 Agentic-Jujutsu - Quantum-Resistant Version Control</strong> \n <p><a href=\"https://www.npmjs.com/package/agentic-jujutsu\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/agentic-jujutsu.svg?sanitize=true\" /></a> <a href=\"https://opensource.org/licenses/MIT\"><img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true\" /></a></p> \n <h3>What is Agentic-Jujutsu?</h3> \n <p>Agentic-Jujutsu is a <strong>quantum-resistant, self-learning version control system</strong> designed for AI agents. It combines lock-free concurrent operations with ReasoningBank trajectory learning for continuous improvement.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Traditional Git</th> \n    <th>Agentic-Jujutsu</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Lock-based commits</td> \n    <td>Lock-free (23x faster)</td> \n   </tr> \n   <tr> \n    <td>Manual conflict resolution</td> \n    <td>87% automatic resolution</td> \n   </tr> \n   <tr> \n    <td>Static operations</td> \n    <td>Self-learning from patterns</td> \n   </tr> \n   <tr> \n    <td>No quantum protection</td> \n    <td>SHA3-512 + HQC-128</td> \n   </tr> \n   <tr> \n    <td>Sequential agents</td> \n    <td>Concurrent multi-agent</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Key Features</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Performance</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Concurrent Commits</strong></td> \n    <td>350 ops/s</td> \n    <td>23x faster than Git (15 ops/s)</td> \n   </tr> \n   <tr> \n    <td><strong>Context Switching</strong></td> \n    <td>&lt;100ms</td> \n    <td>10x faster than Git (500-1000ms)</td> \n   </tr> \n   <tr> \n    <td><strong>Conflict Resolution</strong></td> \n    <td>87% auto</td> \n    <td>AI-powered pattern matching</td> \n   </tr> \n   <tr> \n    <td><strong>Quantum Security</strong></td> \n    <td>&lt;1ms verify</td> \n    <td>SHA3-512 fingerprints, HQC-128 encryption</td> \n   </tr> \n   <tr> \n    <td><strong>ReasoningBank</strong></td> \n    <td>Continuous</td> \n    <td>Trajectory learning with verdicts</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-bash\"># Install\nnpm install agentic-jujutsu\n\n# Basic usage\nnpx agentic-jujutsu\n</code></pre> \n <pre><code class=\"language-typescript\">import { JjWrapper } from 'agentic-jujutsu';\n\nconst jj = new JjWrapper();\n\n// Start learning trajectory\njj.startTrajectory('Implement feature X');\n\n// Make changes and commit\nawait jj.newCommit('Add authentication module');\njj.addToTrajectory();\n\n// Finalize with success score\njj.finalizeTrajectory(0.9, 'Feature implemented successfully');\n\n// Get AI-powered suggestions\nconst suggestions = await jj.getSuggestions();\n</code></pre> \n <h3>Multi-Agent Coordination</h3> \n <pre><code class=\"language-typescript\">// Concurrent commits without locks\nconst agents = ['agent-1', 'agent-2', 'agent-3'];\nawait Promise.all(agents.map(agent =&gt;\n  jj.newCommit(`Changes from ${agent}`)\n));\n// All commits succeed - no lock waiting!\n</code></pre> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/agentic-jujutsu/README.md\">agentic-jujutsu README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🔬 SciPix - Scientific Document OCR</strong> \n <p><a href=\"https://crates.io/crates/ruvector-scipix\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-scipix.svg?sanitize=true\" /></a> <a href=\"https://docs.rs/ruvector-scipix\"><img alt=\"docs.rs\" src=\"https://docs.rs/ruvector-scipix/badge.svg?sanitize=true\" /></a> <a href=\"https://opensource.org/licenses/MIT\"><img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true\" /></a></p> \n <h3>What is SciPix?</h3> \n <p>SciPix is a <strong>blazing-fast, memory-safe OCR engine</strong> written in pure Rust, purpose-built for scientific documents, mathematical equations, and technical diagrams.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>SciPix</th> \n    <th>Tesseract</th> \n    <th>Mathpix</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Simple Text OCR</td> \n    <td><strong>50ms</strong></td> \n    <td>120ms</td> \n    <td>200ms*</td> \n   </tr> \n   <tr> \n    <td>Math Equation</td> \n    <td><strong>80ms</strong></td> \n    <td>N/A</td> \n    <td>150ms*</td> \n   </tr> \n   <tr> \n    <td>Batch (100 images)</td> \n    <td><strong>2.1s</strong></td> \n    <td>8.5s</td> \n    <td>N/A</td> \n   </tr> \n   <tr> \n    <td>Memory Usage</td> \n    <td><strong>45MB</strong></td> \n    <td>180MB</td> \n    <td>Cloud</td> \n   </tr> \n   <tr> \n    <td>LaTeX Output</td> \n    <td>Yes</td> \n    <td>No</td> \n    <td>Yes</td> \n   </tr> \n  </tbody> \n </table> \n <p>*API latency, not processing time</p> \n <h3>Key Features</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>ONNX Runtime</strong></td> \n    <td>GPU-accelerated with CUDA, TensorRT, CoreML</td> \n   </tr> \n   <tr> \n    <td><strong>LaTeX Output</strong></td> \n    <td>Mathematical equation recognition with LaTeX, MathML, AsciiMath</td> \n   </tr> \n   <tr> \n    <td><strong>SIMD Optimized</strong></td> \n    <td>4x faster image preprocessing with AVX2, SSE4, NEON</td> \n   </tr> \n   <tr> \n    <td><strong>REST API</strong></td> \n    <td>Production-ready HTTP server with rate limiting</td> \n   </tr> \n   <tr> \n    <td><strong>MCP Server</strong></td> \n    <td>Integrate with Claude, ChatGPT via Model Context Protocol</td> \n   </tr> \n   <tr> \n    <td><strong>WebAssembly</strong></td> \n    <td>Run OCR directly in browsers</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-bash\"># Add to Cargo.toml\ncargo add ruvector-scipix\n\n# With features\nruvector-scipix = { version = \"0.1.16\", features = [\"ocr\", \"math\", \"optimize\"] }\n</code></pre> \n <pre><code class=\"language-rust\">use ruvector_scipix::{SciPixOcr, OcrConfig};\n\n// Initialize OCR engine\nlet ocr = SciPixOcr::new(OcrConfig::default())?;\n\n// Process scientific image\nlet result = ocr.process_image(\"equation.png\")?;\nprintln!(\"LaTeX: {}\", result.latex);\nprintln!(\"Confidence: {:.2}%\", result.confidence * 100.0);\n</code></pre> \n <h3>Use Cases</h3> \n <ul> \n  <li><strong>Academic Paper Digitization</strong> - Extract text and equations from scanned research papers</li> \n  <li><strong>Math Homework Assistance</strong> - Convert handwritten equations to LaTeX for AI tutoring</li> \n  <li><strong>Technical Documentation</strong> - Process engineering diagrams and scientific charts</li> \n  <li><strong>AI/LLM Integration</strong> - Feed scientific content to language models via MCP</li> \n </ul> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/scipix/README.md\">scipix README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🧠 Meta-Cognition SNN - Spiking Neural Networks</strong> \n <h3>What is Meta-Cognition SNN?</h3> \n <p>A hybrid AI architecture combining <strong>Spiking Neural Networks (SNN)</strong>, <strong>SIMD-optimized vector operations</strong>, and <strong>5 attention mechanisms</strong> with meta-cognitive self-discovery capabilities.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Capability</th> \n    <th>Performance</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Spiking Neural Networks</strong></td> \n    <td>10-50x faster</td> \n    <td>LIF neurons + STDP learning with N-API SIMD</td> \n   </tr> \n   <tr> \n    <td><strong>SIMD Vector Operations</strong></td> \n    <td>5-54x faster</td> \n    <td>Loop-unrolled distance/dot product calculations</td> \n   </tr> \n   <tr> \n    <td><strong>5 Attention Mechanisms</strong></td> \n    <td>Sub-millisecond</td> \n    <td>Multi-Head, Flash, Linear, Hyperbolic, MoE</td> \n   </tr> \n   <tr> \n    <td><strong>Vector Search</strong></td> \n    <td>150x faster</td> \n    <td>RuVector-powered semantic search</td> \n   </tr> \n   <tr> \n    <td><strong>Meta-Cognition</strong></td> \n    <td>Autonomous</td> \n    <td>Self-discovering emergent capabilities</td> \n   </tr> \n  </tbody> \n </table> \n <h3>SIMD Performance</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Operation</th> \n    <th>Speedup</th> \n    <th>Notes</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>LIF Updates</td> \n    <td><strong>16.7x</strong></td> \n    <td>Leaky integrate-and-fire neurons</td> \n   </tr> \n   <tr> \n    <td>Synaptic Forward</td> \n    <td><strong>14.9x</strong></td> \n    <td>Forward propagation</td> \n   </tr> \n   <tr> \n    <td>STDP Learning</td> \n    <td><strong>26.3x</strong></td> \n    <td>Spike-timing dependent plasticity</td> \n   </tr> \n   <tr> \n    <td>Distance (128d)</td> \n    <td><strong>54x</strong></td> \n    <td>Euclidean distance calculation</td> \n   </tr> \n   <tr> \n    <td>Full Simulation</td> \n    <td><strong>18.4x</strong></td> \n    <td>End-to-end SNN simulation</td> \n   </tr> \n  </tbody> \n </table> \n <h3>5 Attention Mechanisms</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Mechanism</th> \n    <th>Best For</th> \n    <th>Latency</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Flash</strong></td> \n    <td>Long sequences</td> \n    <td>0.023ms</td> \n   </tr> \n   <tr> \n    <td><strong>MoE</strong></td> \n    <td>Specialized domains</td> \n    <td>0.021ms</td> \n   </tr> \n   <tr> \n    <td><strong>Multi-Head</strong></td> \n    <td>Complex patterns</td> \n    <td>0.047ms</td> \n   </tr> \n   <tr> \n    <td><strong>Linear</strong></td> \n    <td>Real-time processing</td> \n    <td>0.075ms</td> \n   </tr> \n   <tr> \n    <td><strong>Hyperbolic</strong></td> \n    <td>Hierarchical data</td> \n    <td>0.222ms</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-bash\"># Install and run demos\ncd examples/meta-cognition-spiking-neural-network\nnpm install\nnode demos/run-all.js\n</code></pre> \n <pre><code class=\"language-javascript\">const { createFeedforwardSNN, rateEncoding } = require('./demos/snn/lib/SpikingNeuralNetwork');\n\n// Create SNN with SIMD optimization\nconst snn = createFeedforwardSNN([100, 50, 10], {\n  dt: 1.0,\n  tau: 20.0,\n  a_plus: 0.005,\n  lateral_inhibition: true\n});\n\n// Train with STDP\nconst input = rateEncoding(pattern, snn.dt, 100);\nsnn.step(input);\n</code></pre> \n <h3>6 Emergent Discoveries</h3> \n <ol> \n  <li>Multi-Scale Attention Hierarchy (Novelty: 5/5)</li> \n  <li>Spike Synchronization Patterns</li> \n  <li>Attention-Gated Spike Propagation</li> \n  <li>Temporal Coherence Emergence</li> \n  <li>Emergent Sparsity (80% fewer active neurons)</li> \n  <li>Meta-Plasticity (faster learning on later tasks)</li> \n </ol> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/meta-cognition-spiking-neural-network/README.md\">meta-cognition-snn README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🤖 RuvLLM - Self-Learning LLM Orchestration</strong> \n <p><a href=\"https://www.rust-lang.org/\"><img alt=\"Rust\" src=\"https://img.shields.io/badge/rust-1.77%2B-orange.svg?sanitize=true\" /></a> <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg?sanitize=true\" /></a> <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/#\"><img alt=\"HuggingFace\" src=\"https://img.shields.io/badge/export-HuggingFace-yellow.svg?sanitize=true\" /></a></p> \n <h3>What is RuvLLM?</h3> \n <p>RuvLLM is a <strong>self-learning language model orchestration system</strong> that combines frozen foundation models with adaptive memory and intelligent routing. Unlike traditional LLMs that rely solely on static parameters, RuvLLM continuously improves from every interaction.</p> \n <blockquote> \n  <p><em>\"The intelligence is not in one model anymore. It is in the loop.\"</em></p> \n </blockquote> \n <h3>SONA: 3-Tier Temporal Learning</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Loop</th> \n    <th>Frequency</th> \n    <th>Latency</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>A: Instant</strong></td> \n    <td>Per-request</td> \n    <td>&lt;100μs</td> \n    <td>MicroLoRA adaptation (rank 1-2)</td> \n   </tr> \n   <tr> \n    <td><strong>B: Background</strong></td> \n    <td>Hourly</td> \n    <td>~1.3ms</td> \n    <td>K-means++ clustering, base LoRA (rank 4-16)</td> \n   </tr> \n   <tr> \n    <td><strong>C: Deep</strong></td> \n    <td>Weekly</td> \n    <td>N/A</td> \n    <td>EWC++ consolidation, concept hierarchies</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Core Components</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Component</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>LFM2 Cortex</strong></td> \n    <td>Frozen reasoning engine (135M-2.6B params)</td> \n   </tr> \n   <tr> \n    <td><strong>Ruvector Memory</strong></td> \n    <td>Adaptive synaptic mesh with HNSW indexing</td> \n   </tr> \n   <tr> \n    <td><strong>FastGRNN Router</strong></td> \n    <td>Intelligent model selection circuit</td> \n   </tr> \n   <tr> \n    <td><strong>Graph Attention</strong></td> \n    <td>8-head attention with edge features</td> \n   </tr> \n   <tr> \n    <td><strong>SONA Engine</strong></td> \n    <td>LoRA + EWC++ + ReasoningBank</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Performance (CPU-Only)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Metric</th> \n    <th>Value</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Initialization</strong></td> \n    <td>3.71ms</td> \n   </tr> \n   <tr> \n    <td><strong>Average Query</strong></td> \n    <td>0.09ms</td> \n   </tr> \n   <tr> \n    <td><strong>Session Query</strong></td> \n    <td>0.04ms</td> \n   </tr> \n   <tr> \n    <td><strong>Throughput</strong></td> \n    <td>~38,000 q/s</td> \n   </tr> \n   <tr> \n    <td><strong>Memory</strong></td> \n    <td>~50MB</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-rust\">use ruvllm::{RuvLLMOrchestrator, OrchestratorConfig};\n\n// Initialize orchestrator\nlet config = OrchestratorConfig::default();\nlet orchestrator = RuvLLMOrchestrator::new(config)?;\n\n// Query with automatic learning\nlet response = orchestrator.query(\"Explain quantum entanglement\").await?;\nprintln!(\"{}\", response.text);\n\n// Response improves over time as SONA learns patterns\n</code></pre> \n <h3>Federated Learning</h3> \n <pre><code class=\"language-rust\">use rvf_federation::{ExportBuilder, DiffPrivacyEngine, FederationPolicy};\n\n// Build a privacy-preserving federated export\nlet mut dp = DiffPrivacyEngine::gaussian(1.0, 1e-5, 1.0, 10.0)?;\nlet export = ExportBuilder::new(\"contributor_pseudo\".into(), \"code_review\".into())\n    .add_priors(local_engine.extract_priors())\n    .add_weights(sona_weights)\n    .with_policy(FederationPolicy::default())  // quality gate + min observations\n    .build(&amp;mut dp)?;                          // PII strip → DP noise → manifest\n\n// Import and merge federated learning from another contributor\nlet merger = ImportMerger::new();\nmerger.validate(&amp;remote_export)?;              // signature + witness chain check\nmerger.merge_priors(&amp;mut local, &amp;remote_export.priors, 1);  // version-aware merge\n</code></pre> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-federation\"><code>rvf-federation</code></a> for FedAvg/FedProx aggregation, Byzantine tolerance, RDP privacy accounting, and PII stripping pipeline.</p> \n <h3>Dynamic Embedding Fine-Tuning</h3> \n <p>RuvLLM's adaptive learning system enables real-time model improvement without retraining.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Description</th> \n    <th>Latency</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>MicroLoRA</strong></td> \n    <td>Per-request adaptation (rank 1-2), &lt;50KB adapters</td> \n    <td>&lt;1ms</td> \n   </tr> \n   <tr> \n    <td><strong>Contrastive Training</strong></td> \n    <td>Triplet loss with hard negatives for embedding optimization</td> \n    <td>Batch</td> \n   </tr> \n   <tr> \n    <td><strong>Task-Specific Adapters</strong></td> \n    <td>Pre-tuned for Coder, Researcher, Security, Architect, Reviewer</td> \n    <td>Hot-swap</td> \n   </tr> \n   <tr> \n    <td><strong>EWC++ Consolidation</strong></td> \n    <td>Prevents catastrophic forgetting during continuous learning</td> \n    <td>Background</td> \n   </tr> \n   <tr> \n    <td><strong>Adapter Merging</strong></td> \n    <td>Average, Weighted, SLERP, TIES, DARE strategies</td> \n    <td>On-demand</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-javascript\">// Contrastive fine-tuning for agent routing\nimport { ContrastiveTrainer } from '@ruvector/ruvllm';\n\nconst trainer = new ContrastiveTrainer({\n  margin: 0.5,\n  hardNegativeRatio: 0.7\n});\n\n// Learn: task → correct agent, not wrong agent\ntrainer.addTriplet(taskEmb, correctAgentEmb, wrongAgentEmb, true);\nconst model = trainer.train();\n</code></pre> \n <pre><code class=\"language-rust\">// Task-specific adapter hot-swapping\nuse ruvllm::lora::RuvLtraAdapters;\n\nlet adapters = RuvLtraAdapters::new();\nlet coder = adapters.create_lora(\"coder\", 768)?;      // Rank 16, code patterns\nlet security = adapters.create_lora(\"security\", 768)?; // Rank 16, vulnerability detection\n\n// Hot-swap at runtime without model reload\norchestrator.set_adapter(coder);\nlet code_response = orchestrator.query(\"Implement binary search\").await?;\n\norchestrator.set_adapter(security);\nlet audit_response = orchestrator.query(\"Audit this code for vulnerabilities\").await?;\n</code></pre> \n <h3>Advanced Features</h3> \n <ul> \n  <li><strong>SIMD Inference</strong>: AVX2/AVX512/SSE4.1 optimization</li> \n  <li><strong>Q4 Quantization</strong>: 4-bit weights for memory efficiency</li> \n  <li><strong>HuggingFace Export</strong>: Export LoRA weights and preference pairs</li> \n  <li><strong>Multi-Model Routing</strong>: SmolLM, Qwen2, TinyLlama selection</li> \n  <li><strong>WASM Support</strong>: Run SONA in browsers and edge devices</li> \n  <li><strong>Browser Fine-Tuning</strong>: MicroLoRA WASM with localStorage persistence</li> \n </ul> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/ruvLLM/README.md\">ruvLLM README</a> | <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/ruvllm/FINE_TUNING.md\">Fine-Tuning Guide</a> | <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/training/task_specific_lora_adapters.md\">Task Adapters</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🗜️ REFRAG - Compress-Sense-Expand RAG</strong> \n <h3>What is REFRAG?</h3> \n <p>REFRAG implements the <strong>Compress-Sense-Expand architecture</strong> from <a href=\"https://arxiv.org/abs/2509.01092\">arXiv:2509.01092</a>, achieving <strong>~30x RAG latency reduction</strong> by storing pre-computed representation tensors instead of raw text.</p> \n <h3>Architecture</h3> \n <pre><code>┌────────────────┐    ┌────────────────┐    ┌────────────────┐\n│   COMPRESS     │───▶│     SENSE      │───▶│    EXPAND      │\n│    Layer       │    │     Layer      │    │    Layer       │\n└────────────────┘    └────────────────┘    └────────────────┘\n\nBinary tensor         Policy network        Dimension projection\nstorage with          decides COMPRESS      (768 → 4096 dims)\nzero-copy access      vs EXPAND\n</code></pre> \n <h3>Compression Strategies</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Strategy</th> \n    <th>Compression</th> \n    <th>Use Case</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>None</code></td> \n    <td>1x</td> \n    <td>Maximum precision</td> \n   </tr> \n   <tr> \n    <td><code>Float16</code></td> \n    <td>2x</td> \n    <td>Good balance</td> \n   </tr> \n   <tr> \n    <td><code>Int8</code></td> \n    <td>4x</td> \n    <td>Memory constrained</td> \n   </tr> \n   <tr> \n    <td><code>Binary</code></td> \n    <td>32x</td> \n    <td>Extreme compression</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Policy Networks</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Policy</th> \n    <th>Latency</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>ThresholdPolicy</code></td> \n    <td>~2μs</td> \n    <td>Cosine similarity threshold</td> \n   </tr> \n   <tr> \n    <td><code>LinearPolicy</code></td> \n    <td>~5μs</td> \n    <td>Single layer classifier</td> \n   </tr> \n   <tr> \n    <td><code>MLPPolicy</code></td> \n    <td>~15μs</td> \n    <td>Two-layer neural network</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-bash\"># Run demo\ncargo run --bin refrag-demo\n\n# Run benchmarks\ncargo run --bin refrag-benchmark --release\n</code></pre> \n <pre><code class=\"language-rust\">use refrag_pipeline_example::{RefragStore, RefragEntry};\n\n// Create REFRAG-enabled store\nlet store = RefragStore::new(384, 768)?;\n\n// Insert with representation tensor\nlet entry = RefragEntry::new(\"doc_1\", search_vector, \"The quick brown fox...\")\n    .with_tensor(tensor_bytes, \"llama3-8b\");\nstore.insert(entry)?;\n\n// Hybrid search (policy-based COMPRESS/EXPAND)\nlet results = store.search_hybrid(&amp;query, 10, Some(0.85))?;\n\nfor result in results {\n    match result.response_type {\n        RefragResponseType::Compress =&gt; {\n            // Tensor directly injectable into LLM context\n            println!(\"Tensor: {} dims\", result.tensor_dims.unwrap());\n        }\n        RefragResponseType::Expand =&gt; {\n            // Original text when full context needed\n            println!(\"Text: {}\", result.content.unwrap());\n        }\n    }\n}\n</code></pre> \n <h3>Target LLM Dimensions</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Source</th> \n    <th>Target</th> \n    <th>LLM</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>768</td> \n    <td>4096</td> \n    <td>LLaMA-3 8B</td> \n   </tr> \n   <tr> \n    <td>768</td> \n    <td>8192</td> \n    <td>LLaMA-3 70B</td> \n   </tr> \n   <tr> \n    <td>1536</td> \n    <td>8192</td> \n    <td>GPT-4</td> \n   </tr> \n  </tbody> \n </table> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/refrag-pipeline/README.md\">refrag-pipeline README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🐦 7sense - Bioacoustic Intelligence Platform</strong> \n <p><a href=\"https://www.rust-lang.org\"><img alt=\"Rust\" src=\"https://img.shields.io/badge/rust-1.75+-orange.svg?sanitize=true\" /></a> <a href=\"\"><img alt=\"Tests\" src=\"https://img.shields.io/badge/tests-329%20passed-brightgreen.svg?sanitize=true\" /></a> <a href=\"\"><img alt=\"Coverage\" src=\"https://img.shields.io/badge/coverage-85%25-green.svg?sanitize=true\" /></a></p> \n <h3>What is 7sense?</h3> \n <p>7sense transforms <strong>bird calls into navigable geometric space</strong> using cutting-edge AI and vector search. It converts audio recordings of bird songs into rich, searchable embeddings using Perch 2.0 neural networks and ultra-fast HNSW indexing.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Traditional Monitoring</th> \n    <th>7sense</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Expert human listeners</td> \n    <td>Instant AI species ID</td> \n   </tr> \n   <tr> \n    <td>Basic spectrogram analysis</td> \n    <td>Neural embeddings (1536-dim)</td> \n   </tr> \n   <tr> \n    <td>Limited scale</td> \n    <td>Millions of recordings</td> \n   </tr> \n   <tr> \n    <td>Manual pattern finding</td> \n    <td>Automated discovery</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Performance Targets</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Metric</th> \n    <th>Target</th> \n    <th>Status</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>HNSW Search Speedup</td> \n    <td>150x vs brute force</td> \n    <td>Achieved</td> \n   </tr> \n   <tr> \n    <td>Query Latency (p99)</td> \n    <td>&lt; 50ms</td> \n    <td>Achieved</td> \n   </tr> \n   <tr> \n    <td>Recall@10</td> \n    <td>&gt;= 0.95</td> \n    <td>Achieved</td> \n   </tr> \n   <tr> \n    <td>Embedding Throughput</td> \n    <td>&gt; 100 segments/sec</td> \n    <td>Achieved</td> \n   </tr> \n   <tr> \n    <td>Memory per 1M vectors</td> \n    <td>&lt; 6 GB</td> \n    <td>Achieved</td> \n   </tr> \n  </tbody> \n </table> \n <h3>9 Rust Crates</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>sevensense-core</code></td> \n    <td>Species taxonomy, temporal types</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-audio</code></td> \n    <td>WAV/MP3/FLAC, Mel spectrograms</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-embedding</code></td> \n    <td>Perch 2.0 ONNX, 1536-dim vectors</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-vector</code></td> \n    <td>HNSW with 150x speedup</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-learning</code></td> \n    <td>GNN training, EWC regularization</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-analysis</code></td> \n    <td>HDBSCAN clustering, Markov models</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-interpretation</code></td> \n    <td>Evidence packs, species narratives</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-api</code></td> \n    <td>GraphQL, REST, WebSocket streaming</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-benches</code></td> \n    <td>Criterion.rs performance suites</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-bash\"># Build and run\ncd examples/vibecast-7sense\ncargo build --release\ncargo run -p sevensense-api --release\n</code></pre> \n <pre><code class=\"language-rust\">use sevensense_audio::AudioProcessor;\nuse sevensense_embedding::EmbeddingPipeline;\nuse sevensense_vector::HnswIndex;\n\n// Load and process audio\nlet processor = AudioProcessor::new(Default::default());\nlet segments = processor.process_file(\"recording.wav\").await?;\n\n// Generate Perch 2.0 embeddings\nlet pipeline = EmbeddingPipeline::new(Default::default()).await?;\nlet embeddings = pipeline.embed_segments(&amp;segments).await?;\n\n// Search for similar calls (150x faster)\nlet index = HnswIndex::new(Default::default());\nindex.add_batch(&amp;embeddings)?;\nlet neighbors = index.search(&amp;embeddings[0], 10)?;\n\nprintln!(\"Found {} similar bird calls\", neighbors.len());\n</code></pre> \n <h3>Use Cases</h3> \n <ul> \n  <li><strong>Species Identification</strong> - Instant predictions with confidence scores</li> \n  <li><strong>Pattern Discovery</strong> - Find similar calls across millions of recordings</li> \n  <li><strong>Behavioral Insights</strong> - Detect singing patterns, dialects, anomalies</li> \n  <li><strong>Conservation Monitoring</strong> - Track biodiversity at scale</li> \n </ul> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/vibecast-7sense/README.md\">7sense README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🧬 EXO-AI - Advanced Cognitive Substrate</strong> \n <p><a href=\"https://crates.io/crates/exo-core\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/exo-core.svg?sanitize=true\" /></a> <a href=\"https://docs.rs/exo-core\"><img alt=\"docs.rs\" src=\"https://docs.rs/exo-core/badge.svg?sanitize=true\" /></a> <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg?sanitize=true\" /></a></p> \n <h3>What is EXO-AI?</h3> \n <p>EXO-AI 2025 is a comprehensive <strong>cognitive substrate</strong> implementing cutting-edge theories from neuroscience, physics, and consciousness research. It provides 9 interconnected Rust crates totaling ~15,800+ lines of research-grade code.</p> \n <blockquote> \n  <p>Traditional AI systems process information. EXO-AI aims to understand it.</p> \n </blockquote> \n <h3>SIMD-Accelerated Performance</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Operation</th> \n    <th>Speedup</th> \n    <th>Notes</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Distance (128d)</td> \n    <td><strong>54x</strong></td> \n    <td>AVX2/NEON optimized</td> \n   </tr> \n   <tr> \n    <td>Cosine Similarity</td> \n    <td><strong>2.73x</strong></td> \n    <td>Batch operations</td> \n   </tr> \n   <tr> \n    <td>Pattern Matching</td> \n    <td><strong>8-54x</strong></td> \n    <td>Loop-unrolled</td> \n   </tr> \n   <tr> \n    <td>Meta-Simulation</td> \n    <td><strong>13+ quadrillion/s</strong></td> \n    <td>From ultra-low-latency-sim</td> \n   </tr> \n  </tbody> \n </table> \n <h3>9 Rust Crates</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>exo-core</code></td> \n    <td>IIT consciousness (Φ) measurement &amp; Landauer thermodynamics</td> \n   </tr> \n   <tr> \n    <td><code>exo-temporal</code></td> \n    <td>Temporal memory with causal tracking &amp; consolidation</td> \n   </tr> \n   <tr> \n    <td><code>exo-hypergraph</code></td> \n    <td>Topological analysis with persistent homology</td> \n   </tr> \n   <tr> \n    <td><code>exo-manifold</code></td> \n    <td>SIREN networks + SIMD-accelerated retrieval</td> \n   </tr> \n   <tr> \n    <td><code>exo-exotic</code></td> \n    <td>10 cutting-edge cognitive experiments</td> \n   </tr> \n   <tr> \n    <td><code>exo-federation</code></td> \n    <td>Post-quantum federated cognitive mesh</td> \n   </tr> \n   <tr> \n    <td><code>exo-backend-classical</code></td> \n    <td>SIMD-accelerated compute backend</td> \n   </tr> \n   <tr> \n    <td><code>exo-wasm</code></td> \n    <td>Browser &amp; edge WASM deployment</td> \n   </tr> \n   <tr> \n    <td><code>exo-node</code></td> \n    <td>Node.js bindings via NAPI-RS</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Key Theories Implemented</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Theory</th> \n    <th>Implementation</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>IIT (Integrated Information Theory)</strong></td> \n    <td>Consciousness level (Φ) measurement</td> \n   </tr> \n   <tr> \n    <td><strong>Landauer's Principle</strong></td> \n    <td>Computational thermodynamics</td> \n   </tr> \n   <tr> \n    <td><strong>Free Energy Principle</strong></td> \n    <td>Friston's predictive processing</td> \n   </tr> \n   <tr> \n    <td><strong>Strange Loops</strong></td> \n    <td>Hofstadter's self-referential patterns</td> \n   </tr> \n   <tr> \n    <td><strong>Morphogenesis</strong></td> \n    <td>Pattern formation emergence</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-toml\">[dependencies]\nexo-core = \"0.1\"\nexo-temporal = \"0.1\"\nexo-exotic = \"0.1\"\nexo-manifold = \"0.1\"  # SIMD acceleration!\n</code></pre> \n <pre><code class=\"language-rust\">use exo_core::consciousness::{ConsciousnessSubstrate, IITConfig};\nuse exo_core::thermodynamics::CognitiveThermometer;\n\n// Measure integrated information (Φ)\nlet substrate = ConsciousnessSubstrate::new(IITConfig::default());\nsubstrate.add_pattern(pattern);\nlet phi = substrate.compute_phi();\nprintln!(\"Consciousness level (Φ): {:.4}\", phi);\n\n// Track computational thermodynamics\nlet thermo = CognitiveThermometer::new(300.0); // Kelvin\nlet cost = thermo.landauer_cost_bits(1024);\nprintln!(\"Landauer cost: {:.2e} J\", cost);\n</code></pre> \n <h3>SIMD Pattern Retrieval</h3> \n <pre><code class=\"language-rust\">use exo_manifold::{ManifoldEngine, cosine_similarity_simd, batch_distances};\n\n// 54x faster similarity search\nlet query = vec![0.5; 768];\nlet results = engine.retrieve(&amp;query, 10)?;\n\n// Batch distance computation\nlet distances = batch_distances(&amp;query, &amp;database);  // 8-54x speedup\n</code></pre> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/exo-ai-2025/README.md\">exo-ai README</a></p> \n </blockquote> \n</details> \n<hr /> \n<h2>Database Extensions</h2> \n<details> \n <strong>🐘 PostgreSQL Extension</strong> \n <p><a href=\"https://crates.io/crates/ruvector-postgres\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-postgres.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/@ruvector/postgres-cli\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/postgres-cli.svg?sanitize=true\" /></a> <a href=\"https://hub.docker.com/r/ruvnet/ruvector-postgres\"><img alt=\"Docker Hub\" src=\"https://img.shields.io/docker/pulls/ruvnet/ruvector-postgres?label=docker%20pulls\" /></a> <a href=\"https://hub.docker.com/r/ruvnet/ruvector-postgres\"><img alt=\"Docker\" src=\"https://img.shields.io/docker/v/ruvnet/ruvector-postgres?label=docker\" /></a></p> \n <p><strong>The most advanced PostgreSQL vector extension</strong> — a drop-in pgvector replacement with 143 SQL functions, hardware-accelerated SIMD operations, and built-in AI capabilities. Transform your existing PostgreSQL database into a full-featured vector search engine with GNN layers, attention mechanisms, and self-learning capabilities.</p> \n <pre><code class=\"language-bash\"># Quick Install from Docker Hub\ndocker run -d --name ruvector \\\n  -e POSTGRES_PASSWORD=secret \\\n  -p 5432:5432 \\\n  ruvnet/ruvector-postgres:latest\n\n# Connect and use\npsql -h localhost -U ruvector -d ruvector_test\n\n# Create extension\nCREATE EXTENSION ruvector;\n</code></pre> \n <p><strong>Why RuVector Postgres?</strong></p> \n <ul> \n  <li><strong>Zero Migration</strong> — Works with existing pgvector code, just swap the extension</li> \n  <li><strong>10x More Functions</strong> — 143 SQL functions vs pgvector's ~20</li> \n  <li><strong>2x Faster</strong> — AVX-512/AVX2/NEON SIMD acceleration</li> \n  <li><strong>AI-Native</strong> — GNN layers, 46 attention mechanisms, local embeddings</li> \n  <li><strong>Self-Learning</strong> — Improves search quality over time with ReasoningBank</li> \n </ul> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>pgvector</th> \n    <th>RuVector Postgres</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>SQL Functions</td> \n    <td>~20</td> \n    <td><strong>143</strong></td> \n   </tr> \n   <tr> \n    <td>SIMD Acceleration</td> \n    <td>Basic</td> \n    <td>AVX-512/AVX2/NEON (~2x faster)</td> \n   </tr> \n   <tr> \n    <td>Index Types</td> \n    <td>HNSW, IVFFlat</td> \n    <td>HNSW, IVFFlat + Hyperbolic</td> \n   </tr> \n   <tr> \n    <td>Attention Mechanisms</td> \n    <td>❌</td> \n    <td>46 types (Flash, Linear, Graph)</td> \n   </tr> \n   <tr> \n    <td>GNN Layers</td> \n    <td>❌</td> \n    <td>GCN, GraphSAGE, GAT, GIN</td> \n   </tr> \n   <tr> \n    <td>Sparse Vectors</td> \n    <td>❌</td> \n    <td>BM25, TF-IDF, SPLADE</td> \n   </tr> \n   <tr> \n    <td>Self-Learning</td> \n    <td>❌</td> \n    <td>ReasoningBank, trajectory learning</td> \n   </tr> \n   <tr> \n    <td>Local Embeddings</td> \n    <td>❌</td> \n    <td>6 fastembed models built-in</td> \n   </tr> \n   <tr> \n    <td>Multi-Tenancy</td> \n    <td>❌</td> \n    <td>Built-in namespace isolation</td> \n   </tr> \n   <tr> \n    <td>Quantization</td> \n    <td>❌</td> \n    <td>Scalar, Product, Binary (4-32x compression)</td> \n   </tr> \n  </tbody> \n </table> \n <details> \n  <strong>🐳 Docker Hub (Recommended)</strong> \n  <p><strong>Pull from Docker Hub:</strong> <a href=\"https://hub.docker.com/r/ruvnet/ruvector-postgres\">hub.docker.com/r/ruvnet/ruvector-postgres</a></p> \n  <pre><code class=\"language-bash\"># Quick start\ndocker run -d --name ruvector \\\n  -e POSTGRES_PASSWORD=secret \\\n  -p 5432:5432 \\\n  ruvnet/ruvector-postgres:latest\n\n# Connect\npsql -h localhost -U ruvector -d ruvector_test\n\n# Create extension\nCREATE EXTENSION ruvector;\n</code></pre> \n  <p><strong>Environment Variables:</strong></p> \n  <table> \n   <thead> \n    <tr> \n     <th>Variable</th> \n     <th>Default</th> \n     <th>Description</th> \n    </tr> \n   </thead> \n   <tbody> \n    <tr> \n     <td><code>POSTGRES_USER</code></td> \n     <td><code>ruvector</code></td> \n     <td>Database user</td> \n    </tr> \n    <tr> \n     <td><code>POSTGRES_PASSWORD</code></td> \n     <td><code>ruvector</code></td> \n     <td>Database password</td> \n    </tr> \n    <tr> \n     <td><code>POSTGRES_DB</code></td> \n     <td><code>ruvector_test</code></td> \n     <td>Default database</td> \n    </tr> \n   </tbody> \n  </table> \n  <p><strong>Docker Compose:</strong></p> \n  <pre><code class=\"language-yaml\">version: '3.8'\nservices:\n  ruvector-postgres:\n    image: ruvnet/ruvector-postgres:latest\n    environment:\n      POSTGRES_PASSWORD: secret\n      POSTGRES_DB: ruvector_test\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n\nvolumes:\n  pgdata:\n</code></pre> \n  <p><strong>Available Tags:</strong></p> \n  <ul> \n   <li><code>ruvnet/ruvector-postgres:latest</code> - PostgreSQL + RuVector 0.3.0</li> \n   <li><code>ruvnet/ruvector-postgres:0.3.0</code> - Current release (143 SQL functions)</li> \n   <li><code>ruvnet/ruvector-postgres:2.0.0</code> - Previous release</li> \n  </ul> \n </details> \n <details> \n  <strong>📦 npm CLI</strong> \n  <pre><code class=\"language-bash\"># Install globally\nnpm install -g @ruvector/postgres-cli\n\n# Or use npx\nnpx @ruvector/postgres-cli --help\n\n# Commands available as 'ruvector-pg' or 'rvpg'\nruvector-pg --version\nrvpg --help\n</code></pre> \n  <p><strong>CLI Commands:</strong></p> \n  <pre><code class=\"language-bash\"># Install extension to existing PostgreSQL\nruvector-pg install\n\n# Create vector table with HNSW index\nruvector-pg vector create table embeddings --dim 1536 --index hnsw\n\n# Import vectors from file\nruvector-pg vector import embeddings data.json\n\n# Search vectors\nruvector-pg vector search embeddings --query \"0.1,0.2,...\" --limit 10\n\n# Benchmark performance\nruvector-pg bench --iterations 1000\n\n# Check extension status\nruvector-pg status\n</code></pre> \n  <p><strong>Programmatic Usage:</strong></p> \n  <pre><code class=\"language-typescript\">import { RuvectorPG } from '@ruvector/postgres-cli';\n\nconst client = new RuvectorPG({\n  host: 'localhost',\n  port: 5432,\n  database: 'vectors',\n  user: 'postgres',\n  password: 'secret'\n});\n\n// Create table with HNSW index\nawait client.createTable('embeddings', {\n  dimensions: 1536,\n  indexType: 'hnsw',\n  distanceMetric: 'cosine'\n});\n\n// Insert vectors\nawait client.insert('embeddings', {\n  id: '1',\n  vector: [0.1, 0.2, ...],\n  metadata: { source: 'openai' }\n});\n\n// Search\nconst results = await client.search('embeddings', queryVector, { limit: 10 });\n</code></pre> \n </details> \n <details> \n  <strong>🦀 Rust Crate</strong> \n  <pre><code class=\"language-bash\"># Install pgrx (PostgreSQL extension framework)\ncargo install cargo-pgrx --version \"0.12.9\" --locked\ncargo pgrx init\n\n# Build and install extension\ncd crates/ruvector-postgres\ncargo pgrx install --release\n\n# Or install specific PostgreSQL version\ncargo pgrx install --release --pg-config /usr/lib/postgresql/17/bin/pg_config\n</code></pre> \n  <p><strong>Cargo.toml:</strong></p> \n  <pre><code class=\"language-toml\">[dependencies]\nruvector-postgres = \"2.0\"\n\n# Optional features\n[features]\ndefault = [\"pg17\"]\npg16 = [\"ruvector-postgres/pg16\"]\npg15 = [\"ruvector-postgres/pg15\"]\n\n# AI features (opt-in)\nai-complete = [\"ruvector-postgres/ai-complete\"]  # All AI features\nlearning = [\"ruvector-postgres/learning\"]         # Self-learning\nattention = [\"ruvector-postgres/attention\"]       # 46 attention mechanisms\ngnn = [\"ruvector-postgres/gnn\"]                   # Graph neural networks\nhyperbolic = [\"ruvector-postgres/hyperbolic\"]     # Hyperbolic embeddings\nembeddings = [\"ruvector-postgres/embeddings\"]     # Local embedding generation\nsolver = [\"ruvector-postgres/solver\"]                   # Sublinear solvers\nmath-distances = [\"ruvector-postgres/math-distances\"]   # Math distances &amp; spectral\ntda = [\"ruvector-postgres/tda\"]                         # Topological data analysis\nsona-learning = [\"ruvector-postgres/sona-learning\"]     # Sona learning\ndomain-expansion = [\"ruvector-postgres/domain-expansion\"] # Domain expansion\nanalytics-complete = [\"solver\", \"math-distances\", \"tda\"] # All analytics\n</code></pre> \n  <p><strong>Build with all features:</strong></p> \n  <pre><code class=\"language-bash\">cargo pgrx install --release --features \"ai-complete,embeddings,analytics-complete,attention-extended,sona-learning,domain-expansion\"\n</code></pre> \n </details> \n <details> \n  <strong>📝 SQL Examples</strong> \n  <pre><code class=\"language-sql\">-- Enable extension\nCREATE EXTENSION ruvector;\n\n-- Create table with vector column\nCREATE TABLE documents (\n  id SERIAL PRIMARY KEY,\n  content TEXT,\n  embedding VECTOR(1536)\n);\n\n-- Create HNSW index\nCREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops)\n  WITH (m = 16, ef_construction = 200);\n\n-- Insert vectors\nINSERT INTO documents (content, embedding)\nVALUES ('Hello world', '[0.1, 0.2, ...]'::vector);\n\n-- Semantic search (cosine similarity)\nSELECT id, content, embedding &lt;=&gt; '[0.1, 0.2, ...]'::vector AS distance\nFROM documents\nORDER BY distance\nLIMIT 10;\n\n-- Hybrid search (vector + full-text)\nSELECT id, content\nFROM documents\nWHERE to_tsvector(content) @@ to_tsquery('machine &amp; learning')\nORDER BY embedding &lt;=&gt; query_embedding\nLIMIT 10;\n\n-- GNN-enhanced search (with learning)\nSELECT * FROM ruvector_gnn_search(\n  'documents',\n  '[0.1, 0.2, ...]'::vector,\n  10,  -- limit\n  'gcn' -- gnn_type: gcn, graphsage, gat, gin\n);\n\n-- Generate embeddings locally (no API needed)\nSELECT ruvector_embed('all-MiniLM-L6-v2', 'Your text here');\n\n-- Flash attention\nSELECT ruvector_flash_attention(query, key, value);\n</code></pre> \n </details> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-postgres/README.md\">ruvector-postgres README</a> for full SQL API reference (143 functions).</p> \n</details> \n<hr /> \n<h2>Developer Tools</h2> \n<details> \n 🛠️ Tools &amp; Utilities \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-bench\">ruvector-bench</a></td> \n    <td>Benchmarking suite for vector operations</td> \n    <td><a href=\"https://crates.io/crates/ruvector-bench\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-bench.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-metrics\">ruvector-metrics</a></td> \n    <td>Observability, metrics, and monitoring</td> \n    <td><a href=\"https://crates.io/crates/ruvector-metrics\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-metrics.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-filter\">ruvector-filter</a></td> \n    <td>Metadata filtering and query predicates</td> \n    <td><a href=\"https://crates.io/crates/ruvector-filter\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-filter.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-collections\">ruvector-collections</a></td> \n    <td>Multi-tenant collection management</td> \n    <td><a href=\"https://crates.io/crates/ruvector-collections\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-collections.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-snapshot\">ruvector-snapshot</a></td> \n    <td>Point-in-time snapshots and backups</td> \n    <td><a href=\"https://crates.io/crates/ruvector-snapshot\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-snapshot.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/micro-hnsw-wasm\">micro-hnsw-wasm</a></td> \n    <td>Lightweight HNSW implementation for WASM</td> \n    <td><a href=\"https://crates.io/crates/micro-hnsw-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/micro-hnsw-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Embedded &amp; IoT</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>Target</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge\">ruvector-esp32</a></td> \n    <td>ESP32/ESP-IDF vector search</td> \n    <td>ESP32, ESP32-S3</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvlite\">rvlite</a></td> \n    <td>SQLite-style edge DB (no_std compatible)</td> \n    <td>ARM, RISC-V, WASM</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/micro-hnsw-wasm\">micro-hnsw-wasm</a></td> \n    <td>&lt;50KB HNSW for constrained devices</td> \n    <td>WASM, embedded</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-rust\">// ESP32 example (no_std)\n#![no_std]\nuse rvlite::RvLite;\n\nlet db = RvLite::new(128);  // 128-dim vectors\ndb.insert(0, &amp;embedding);\nlet results = db.search(&amp;query, 5);\n</code></pre> \n</details> \n<hr /> \n<h2>Browser &amp; Edge (WASM)</h2> \n<details> \n 🌐 WASM Packages (Browser &amp; Edge) \n <p>Specialized WebAssembly modules for browser and edge deployment. These packages bring advanced AI and distributed computing primitives to JavaScript/TypeScript with near-native performance.</p> \n <h3>Quick Install (All Browser WASM)</h3> \n <pre><code class=\"language-bash\"># Core vector search\nnpm install ruvector-wasm @ruvector/rvlite\n\n# AI &amp; Neural\nnpm install @ruvector/gnn-wasm @ruvector/attention-wasm @ruvector/sona-wasm\n\n# Graph &amp; Algorithms\nnpm install @ruvector/graph-wasm @ruvector/mincut-wasm @ruvector/hyperbolic-hnsw-wasm\n\n# Exotic AI\nnpm install @ruvector/economy-wasm @ruvector/exotic-wasm @ruvector/nervous-system-wasm\n\n# LLM (browser inference)\nnpm install @ruvector/ruvllm-wasm\n</code></pre> \n <table> \n  <thead> \n   <tr> \n    <th>Category</th> \n    <th>Packages</th> \n    <th>Total Size</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Core</strong></td> \n    <td>ruvector-wasm, rvlite</td> \n    <td>~200KB</td> \n   </tr> \n   <tr> \n    <td><strong>AI/Neural</strong></td> \n    <td>gnn, attention, sona</td> \n    <td>~300KB</td> \n   </tr> \n   <tr> \n    <td><strong>Graph</strong></td> \n    <td>graph, mincut, hyperbolic-hnsw</td> \n    <td>~250KB</td> \n   </tr> \n   <tr> \n    <td><strong>Exotic</strong></td> \n    <td>economy, exotic, nervous-system</td> \n    <td>~350KB</td> \n   </tr> \n   <tr> \n    <td><strong>LLM</strong></td> \n    <td>ruvllm-wasm</td> \n    <td>~500KB</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Installation</h3> \n <pre><code class=\"language-bash\"># Install individual packages\nnpm install @ruvector/learning-wasm\nnpm install @ruvector/economy-wasm\nnpm install @ruvector/exotic-wasm\nnpm install @ruvector/nervous-system-wasm\nnpm install @ruvector/attention-unified-wasm\n\n# Or build from source\ncd crates/ruvector-learning-wasm\nwasm-pack build --target web\n</code></pre> \n <h3>ruvector-learning-wasm</h3> \n <p><strong>MicroLoRA, BTSP, and HDC for self-learning AI systems.</strong></p> \n <p>Ultra-fast Low-Rank Adaptation (LoRA) optimized for WASM execution with &lt;100us adaptation latency. Designed for real-time per-operator learning in query optimization and AI agent systems.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Performance</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>MicroLoRA</strong></td> \n    <td>&lt;100us latency</td> \n    <td>Rank-2 LoRA matrices for instant weight adaptation</td> \n   </tr> \n   <tr> \n    <td><strong>Per-Operator Scoping</strong></td> \n    <td>Zero-allocation hot paths</td> \n    <td>Separate adapters for different operator types</td> \n   </tr> \n   <tr> \n    <td><strong>Trajectory Tracking</strong></td> \n    <td>Lock-free buffers</td> \n    <td>Record learning trajectories for replay</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Architecture:</strong></p> \n <pre><code>Input Embedding (256-dim)\n       |\n       v\n  +---------+\n  | A: d x 2 |  Down projection\n  +---------+\n       |\n       v\n  +---------+\n  | B: 2 x d |  Up projection\n  +---------+\n       |\n       v\nDelta W = alpha * (A @ B)\n       |\n       v\nOutput = Input + Delta W\n</code></pre> \n <p><strong>JavaScript/TypeScript Example:</strong></p> \n <pre><code class=\"language-typescript\">import init, { WasmMicroLoRA } from '@ruvector/learning-wasm';\n\nawait init();\n\n// Create MicroLoRA engine (256-dim, alpha=0.1, lr=0.01)\nconst lora = new WasmMicroLoRA(256, 0.1, 0.01);\n\n// Forward pass with adaptation\nconst input = new Float32Array(256).fill(0.5);\nconst output = lora.forward_array(input);\n\n// Adapt based on gradient signal\nconst gradient = new Float32Array(256).fill(0.1);\nlora.adapt_array(gradient);\n\n// Adapt with reward signal for RL\nlora.adapt_with_reward(0.8);  // 80% improvement\n\nconsole.log(`Adaptations: ${lora.adapt_count()}`);\nconsole.log(`Delta norm: ${lora.delta_norm()}`);\n</code></pre> \n <h3>ruvector-economy-wasm</h3> \n <p><strong>CRDT-based autonomous credit economy for distributed compute networks.</strong></p> \n <p>P2P-safe concurrent transactions using Conflict-free Replicated Data Types (CRDTs). Features a 10x-to-1x early adopter contribution curve and stake/slash mechanisms for participation incentives.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>CRDT Ledger</strong></td> \n    <td>G-Counter (earned) + PN-Counter (spent) for P2P consistency</td> \n   </tr> \n   <tr> \n    <td><strong>Contribution Curve</strong></td> \n    <td>10x early adopter multiplier decaying to 1x baseline</td> \n   </tr> \n   <tr> \n    <td><strong>Stake/Slash</strong></td> \n    <td>Participation requirements with slashing for bad actors</td> \n   </tr> \n   <tr> \n    <td><strong>Reputation Scoring</strong></td> \n    <td>Multi-factor: accuracy * uptime * stake_weight</td> \n   </tr> \n   <tr> \n    <td><strong>Merkle Verification</strong></td> \n    <td>SHA-256 state root for quick ledger verification</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Architecture:</strong></p> \n <pre><code>+------------------------+\n|     CreditLedger       |  &lt;-- CRDT-based P2P-safe ledger\n|  +------------------+  |\n|  | G-Counter: Earned|  |  &lt;-- Monotonically increasing\n|  | PN-Counter: Spent|  |  &lt;-- Can handle disputes/refunds\n|  | Stake: Locked    |  |  &lt;-- Participation requirement\n|  | State Root       |  |  &lt;-- Merkle root for verification\n|  +------------------+  |\n+------------------------+\n          |\n          v\n+------------------------+\n|  ContributionCurve     |  &lt;-- Exponential decay: 10x -&gt; 1x\n+------------------------+\n          |\n          v\n+------------------------+\n|   ReputationScore      |  &lt;-- accuracy * uptime * stake_weight\n+------------------------+\n</code></pre> \n <p><strong>JavaScript/TypeScript Example:</strong></p> \n <pre><code class=\"language-typescript\">import init, {\n  CreditLedger,\n  ReputationScore,\n  contribution_multiplier\n} from '@ruvector/economy-wasm';\n\nawait init();\n\n// Create a new ledger for a node\nconst ledger = new CreditLedger(\"node-123\");\n\n// Earn credits (with early adopter multiplier)\nledger.creditWithMultiplier(100, \"task:abc\");\nconsole.log(`Balance: ${ledger.balance()}`);\nconsole.log(`Multiplier: ${ledger.currentMultiplier()}x`);\n\n// Stake for participation\nledger.stake(50);\nconsole.log(`Staked: ${ledger.stakedAmount()}`);\n\n// Check multiplier for network compute hours\nconst mult = contribution_multiplier(50000.0);  // 50K hours\nconsole.log(`Network multiplier: ${mult}x`);  // ~8.5x\n\n// Track reputation\nconst rep = new ReputationScore(0.95, 0.98, 1000);\nconsole.log(`Composite score: ${rep.composite_score()}`);\n\n// P2P merge with another ledger (CRDT operation)\nconst otherEarned = new Uint8Array([/* serialized earned counter */]);\nconst otherSpent = new Uint8Array([/* serialized spent counter */]);\nconst mergedCount = ledger.merge(otherEarned, otherSpent);\n</code></pre> \n <h3>ruvector-exotic-wasm</h3> \n <p><strong>Exotic AI mechanisms for emergent behavior in distributed systems.</strong></p> \n <p>Novel coordination primitives inspired by decentralized governance, developmental biology, and quantum physics.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Mechanism</th> \n    <th>Inspiration</th> \n    <th>Use Case</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Neural Autonomous Organization (NAO)</strong></td> \n    <td>DAOs + oscillatory sync</td> \n    <td>Decentralized AI agent governance</td> \n   </tr> \n   <tr> \n    <td><strong>Morphogenetic Network</strong></td> \n    <td>Developmental biology</td> \n    <td>Emergent network topology</td> \n   </tr> \n   <tr> \n    <td><strong>Time Crystal Coordinator</strong></td> \n    <td>Quantum time crystals</td> \n    <td>Robust distributed coordination</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>NAO Features:</strong></p> \n <ul> \n  <li>Stake-weighted quadratic voting</li> \n  <li>Oscillatory synchronization for coherence</li> \n  <li>Quorum-based consensus (configurable threshold)</li> \n </ul> \n <p><strong>Morphogenetic Network Features:</strong></p> \n <ul> \n  <li>Cellular differentiation through morphogen gradients</li> \n  <li>Emergent network topology via growth/pruning</li> \n  <li>Synaptic pruning for optimization</li> \n </ul> \n <p><strong>Time Crystal Features:</strong></p> \n <ul> \n  <li>Period-doubled oscillations for stable coordination</li> \n  <li>Floquet engineering for noise resilience</li> \n  <li>Phase-locked agent synchronization</li> \n </ul> \n <p><strong>JavaScript/TypeScript Example:</strong></p> \n <pre><code class=\"language-typescript\">import init, {\n  WasmNAO,\n  WasmMorphogeneticNetwork,\n  WasmTimeCrystal,\n  ExoticEcosystem\n} from '@ruvector/exotic-wasm';\n\nawait init();\n\n// Neural Autonomous Organization\nconst nao = new WasmNAO(0.7);  // 70% quorum\nnao.addMember(\"agent_1\", 100);  // 100 stake\nnao.addMember(\"agent_2\", 50);\n\nconst propId = nao.propose(\"Upgrade memory backend\");\nnao.vote(propId, \"agent_1\", 0.9);  // 90% approval weight\nnao.vote(propId, \"agent_2\", 0.6);\n\nif (nao.execute(propId)) {\n  console.log(\"Proposal executed!\");\n}\n\n// Morphogenetic Network\nconst net = new WasmMorphogeneticNetwork(100, 100);  // 100x100 grid\nnet.seedSignaling(50, 50);  // Seed signaling cell at center\n\nfor (let i = 0; i &lt; 1000; i++) {\n  net.grow(0.1);  // 10% growth rate\n}\nnet.differentiate();\nnet.prune(0.1);  // 10% pruning threshold\n\n// Time Crystal Coordinator\nconst crystal = new WasmTimeCrystal(10, 100);  // 10 oscillators, 100ms period\ncrystal.crystallize();\n\nfor (let i = 0; i &lt; 200; i++) {\n  const pattern = crystal.tick();\n  // Use pattern for coordination decisions\n}\n\nconsole.log(`Synchronization: ${crystal.orderParameter()}`);\n\n// Combined Ecosystem (all three working together)\nconst eco = new ExoticEcosystem(5, 50, 8);  // 5 agents, 50x50 grid, 8 oscillators\neco.crystallize();\n\nfor (let i = 0; i &lt; 100; i++) {\n  eco.step();\n}\n\nconsole.log(eco.summaryJson());\n</code></pre> \n <h3>ruvector-nervous-system-wasm</h3> \n <p><strong>Bio-inspired neural system components for browser execution.</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Component</th> \n    <th>Performance</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>BTSP</strong></td> \n    <td>Immediate</td> \n    <td>Behavioral Timescale Synaptic Plasticity for one-shot learning</td> \n   </tr> \n   <tr> \n    <td><strong>HDC</strong></td> \n    <td>&lt;50ns bind, &lt;100ns similarity</td> \n    <td>Hyperdimensional Computing with 10,000-bit vectors</td> \n   </tr> \n   <tr> \n    <td><strong>WTA</strong></td> \n    <td>&lt;1us</td> \n    <td>Winner-Take-All for instant decisions</td> \n   </tr> \n   <tr> \n    <td><strong>K-WTA</strong></td> \n    <td>&lt;10us</td> \n    <td>K-Winner-Take-All for sparse distributed coding</td> \n   </tr> \n   <tr> \n    <td><strong>Global Workspace</strong></td> \n    <td>&lt;10us</td> \n    <td>4-7 item attention bottleneck (Miller's Law)</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Hyperdimensional Computing:</strong></p> \n <ul> \n  <li>10,000-bit binary hypervectors</li> \n  <li>10^40 representational capacity</li> \n  <li>XOR binding (associative, commutative, self-inverse)</li> \n  <li>Hamming distance similarity with SIMD optimization</li> \n </ul> \n <p><strong>Biological References:</strong></p> \n <ul> \n  <li>BTSP: Bittner et al. 2017 - Hippocampal place fields</li> \n  <li>HDC: Kanerva 1988, Plate 2003 - Hyperdimensional computing</li> \n  <li>WTA: Cortical microcircuits - Lateral inhibition</li> \n  <li>Global Workspace: Baars 1988, Dehaene 2014 - Consciousness</li> \n </ul> \n <p><strong>JavaScript/TypeScript Example:</strong></p> \n <pre><code class=\"language-typescript\">import init, {\n  BTSPLayer,\n  Hypervector,\n  HdcMemory,\n  WTALayer,\n  KWTALayer,\n  GlobalWorkspace,\n  WorkspaceItem,\n} from '@ruvector/nervous-system-wasm';\n\nawait init();\n\n// One-shot learning with BTSP\nconst btsp = new BTSPLayer(100, 2000.0);  // 100 dim, 2000ms tau\nconst pattern = new Float32Array(100).fill(0.1);\nbtsp.one_shot_associate(pattern, 1.0);  // Immediate association\nconst output = btsp.forward(pattern);\n\n// Hyperdimensional Computing\nconst apple = Hypervector.random();\nconst orange = Hypervector.random();\nconst fruit = apple.bind(orange);  // XOR binding\n\nconst similarity = apple.similarity(orange);  // ~0.0 (orthogonal)\nconsole.log(`Similarity: ${similarity}`);  // Random vectors are orthogonal\n\n// HDC Memory\nconst memory = new HdcMemory();\nmemory.store(\"apple\", apple);\nmemory.store(\"orange\", orange);\n\nconst results = memory.retrieve(apple, 0.9);  // threshold 0.9\nconst topK = memory.top_k(fruit, 3);  // top-3 similar\n\n// Instant decisions with WTA\nconst wta = new WTALayer(1000, 0.5, 0.8);  // 1000 neurons, threshold, inhibition\nconst activations = new Float32Array(1000);\n// ... fill activations ...\nconst winner = wta.compete(activations);\n\n// Sparse coding with K-WTA\nconst kwta = new KWTALayer(1000, 50);  // 1000 neurons, k=50 winners\nconst winners = kwta.select(activations);\n\n// Attention bottleneck with Global Workspace\nconst workspace = new GlobalWorkspace(7);  // Miller's Law: 7 +/- 2\nconst item = new WorkspaceItem(\n  new Float32Array([1, 2, 3]),  // content\n  0.9,  // salience\n  1,    // source\n  Date.now()  // timestamp\n);\nworkspace.broadcast(item);\n</code></pre> \n <h3>ruvector-attention-unified-wasm</h3> \n <p><strong>Unified API for 18+ attention mechanisms across Neural, DAG, Graph, and SSM domains.</strong></p> \n <p>A single WASM interface that routes to the appropriate attention implementation based on your data structure and requirements.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Category</th> \n    <th>Mechanisms</th> \n    <th>Best For</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Neural</strong></td> \n    <td>Scaled Dot-Product, Multi-Head, Hyperbolic, Linear, Flash, Local-Global, MoE</td> \n    <td>Transformers, sequences</td> \n   </tr> \n   <tr> \n    <td><strong>DAG</strong></td> \n    <td>Topological, Causal Cone, Critical Path, MinCut-Gated, Hierarchical Lorentz, Parallel Branch, Temporal BTSP</td> \n    <td>Query DAGs, workflows</td> \n   </tr> \n   <tr> \n    <td><strong>Graph</strong></td> \n    <td>GAT, GCN, GraphSAGE</td> \n    <td>GNNs, knowledge graphs</td> \n   </tr> \n   <tr> \n    <td><strong>SSM</strong></td> \n    <td>Mamba</td> \n    <td>Long sequences, streaming</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Mechanism Selection:</strong></p> \n <pre><code>+------------------+     +-------------------+\n|   Your Data      | --&gt; | UnifiedAttention  | --&gt; Optimal Mechanism\n+------------------+     +-------------------+\n                               |\n        +----------------------+----------------------+\n        |                      |                      |\n   +----v----+           +-----v-----+          +-----v----+\n   | Neural  |           |    DAG    |          |  Graph   |\n   +---------+           +-----------+          +----------+\n   | dot_prod|           | topological|         | gat      |\n   | multi_hd|           | causal_cone|         | gcn      |\n   | flash   |           | mincut_gtd |         | graphsage|\n   +---------+           +-----------+          +----------+\n</code></pre> \n <p><strong>JavaScript/TypeScript Example:</strong></p> \n <pre><code class=\"language-typescript\">import init, {\n  UnifiedAttention,\n  availableMechanisms,\n  getStats,\n  softmax,\n  temperatureSoftmax,\n  cosineSimilarity,\n  // Neural attention\n  ScaledDotProductAttention,\n  MultiHeadAttention,\n  // DAG attention\n  TopologicalAttention,\n  MinCutGatedAttention,\n  // Graph attention\n  GraphAttention,\n  // SSM\n  MambaSSM,\n} from '@ruvector/attention-unified-wasm';\n\nawait init();\n\n// List all available mechanisms\nconsole.log(availableMechanisms());\n// { neural: [...], dag: [...], graph: [...], ssm: [...] }\n\nconsole.log(getStats());\n// { total_mechanisms: 18, neural_count: 7, dag_count: 7, ... }\n\n// Unified selector - routes to appropriate implementation\nconst attention = new UnifiedAttention(\"multi_head\");\nconsole.log(`Category: ${attention.category}`);  // \"neural\"\nconsole.log(`Supports sequences: ${attention.supportsSequences()}`);  // true\nconsole.log(`Supports graphs: ${attention.supportsGraphs()}`);  // false\n\n// For DAG structures\nconst dagAttention = new UnifiedAttention(\"topological\");\nconsole.log(`Category: ${dagAttention.category}`);  // \"dag\"\nconsole.log(`Supports graphs: ${dagAttention.supportsGraphs()}`);  // true\n\n// Hyperbolic attention for hierarchical data\nconst hypAttention = new UnifiedAttention(\"hierarchical_lorentz\");\nconsole.log(`Supports hyperbolic: ${hypAttention.supportsHyperbolic()}`);  // true\n\n// Utility functions\nconst logits = [1.0, 2.0, 3.0, 4.0];\nconst probs = softmax(logits);\nconsole.log(`Probabilities sum to: ${probs.reduce((a, b) =&gt; a + b)}`);  // 1.0\n\n// Temperature-scaled softmax (lower = more peaked)\nconst sharperProbs = temperatureSoftmax(logits, 0.5);\n\n// Cosine similarity\nconst vecA = [1.0, 0.0, 0.0];\nconst vecB = [1.0, 0.0, 0.0];\nconsole.log(`Similarity: ${cosineSimilarity(vecA, vecB)}`);  // 1.0\n</code></pre> \n <h3>WASM Package Summary</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Size Target</th> \n    <th>Key Features</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>@ruvector/learning-wasm</code></td> \n    <td>&lt;50KB</td> \n    <td>MicroLoRA (&lt;100us), trajectory tracking</td> \n   </tr> \n   <tr> \n    <td><code>@ruvector/economy-wasm</code></td> \n    <td>&lt;100KB</td> \n    <td>CRDT ledger, 10x-&gt;1x curve, stake/slash</td> \n   </tr> \n   <tr> \n    <td><code>@ruvector/exotic-wasm</code></td> \n    <td>&lt;150KB</td> \n    <td>NAO, Morphogenetic, Time Crystal</td> \n   </tr> \n   <tr> \n    <td><code>@ruvector/nervous-system-wasm</code></td> \n    <td>&lt;100KB</td> \n    <td>BTSP, HDC (10K-bit), WTA, Global Workspace</td> \n   </tr> \n   <tr> \n    <td><code>@ruvector/attention-unified-wasm</code></td> \n    <td>&lt;200KB</td> \n    <td>18+ attention mechanisms, unified API</td> \n   </tr> \n   <tr> \n    <td><code>@ruvnet/ruvector-verified-wasm</code></td> \n    <td>&lt;80KB</td> \n    <td>Formal proof verification in browser/edge</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Common Patterns:</strong></p> \n <pre><code class=\"language-typescript\">// All packages follow the same initialization pattern\nimport init, { /* exports */ } from '@ruvector/&lt;package&gt;-wasm';\nawait init();\n\n// Version check\nimport { version } from '@ruvector/&lt;package&gt;-wasm';\nconsole.log(`Version: ${version()}`);\n\n// Feature discovery\nimport { available_mechanisms } from '@ruvector/&lt;package&gt;-wasm';\nconsole.log(available_mechanisms());\n</code></pre> \n</details> \n<hr /> \n<h2>Self-Learning Systems</h2> \n<details> \n 🧠 Self-Learning Intelligence Hooks \n <p><strong>Make your AI assistant smarter over time.</strong></p> \n <p>When you use Claude Code (or any AI coding assistant), it starts fresh every session. It doesn't remember which approaches worked, which files you typically edit together, or what errors you've seen before.</p> \n <p><strong>RuVector Hooks fixes this.</strong> It's a lightweight intelligence layer that:</p> \n <ol> \n  <li><strong>Remembers what works</strong> — Tracks which agent types succeed for different tasks</li> \n  <li><strong>Learns from mistakes</strong> — Records error patterns and suggests fixes you've used before</li> \n  <li><strong>Predicts your workflow</strong> — Knows that after editing <code>api.rs</code>, you usually edit <code>api_test.rs</code></li> \n  <li><strong>Coordinates teams</strong> — Manages multi-agent swarms for complex tasks</li> \n </ol> \n <p>Think of it as giving your AI assistant a memory and intuition about your codebase.</p> \n <h4>How It Works</h4> \n <pre><code>┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐\n│  Claude Code    │────▶│  RuVector Hooks  │────▶│   Intelligence  │\n│  (PreToolUse)   │     │   (pre-edit)     │     │      Layer      │\n└─────────────────┘     └──────────────────┘     └─────────────────┘\n                                                         │\n         ┌───────────────────────────────────────────────┘\n         ▼\n┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐\n│   Q-Learning    │     │  Vector Memory   │     │  Swarm Graph    │\n│   α=0.1 γ=0.95  │     │  64-dim embed    │     │  Coordination   │\n└─────────────────┘     └──────────────────┘     └─────────────────┘\n</code></pre> \n <p>The hooks integrate with Claude Code's event system:</p> \n <ul> \n  <li><strong>PreToolUse</strong> → Provides guidance before edits (agent routing, related files)</li> \n  <li><strong>PostToolUse</strong> → Records outcomes for learning (success/failure, patterns)</li> \n  <li><strong>SessionStart/Stop</strong> → Manages session state and metrics export</li> \n </ul> \n <h4>Technical Specifications</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Component</th> \n    <th>Implementation</th> \n    <th>Details</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Q-Learning</strong></td> \n    <td>Temporal Difference</td> \n    <td>α=0.1, γ=0.95, ε=0.1 (ε-greedy exploration)</td> \n   </tr> \n   <tr> \n    <td><strong>Embeddings</strong></td> \n    <td>Hash-based vectors</td> \n    <td>64 dimensions, normalized, cosine similarity</td> \n   </tr> \n   <tr> \n    <td><strong>LRU Cache</strong></td> \n    <td><code>lru</code> crate</td> \n    <td>1000 entries, ~10x faster Q-value lookups</td> \n   </tr> \n   <tr> \n    <td><strong>Compression</strong></td> \n    <td><code>flate2</code> gzip</td> \n    <td>70-83% storage reduction, fast compression</td> \n   </tr> \n   <tr> \n    <td><strong>Storage</strong></td> \n    <td>JSON / PostgreSQL</td> \n    <td>Auto-fallback, 5000 memory entry limit</td> \n   </tr> \n   <tr> \n    <td><strong>Cross-platform</strong></td> \n    <td>Rust + TypeScript</td> \n    <td>Windows (USERPROFILE), Unix (HOME)</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Performance</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Metric</th> \n    <th>Value</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Q-value lookup (cached)</td> \n    <td>&lt;1µs</td> \n   </tr> \n   <tr> \n    <td>Q-value lookup (uncached)</td> \n    <td>~50µs</td> \n   </tr> \n   <tr> \n    <td>Memory search (1000 entries)</td> \n    <td>&lt;5ms</td> \n   </tr> \n   <tr> \n    <td>Storage compression ratio</td> \n    <td>70-83%</td> \n   </tr> \n   <tr> \n    <td>Session start overhead</td> \n    <td>&lt;10ms</td> \n   </tr> \n  </tbody> \n </table> \n <table> \n  <thead> \n   <tr> \n    <th>Crate/Package</th> \n    <th>Description</th> \n    <th>Status</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-cli\">ruvector-cli hooks</a></td> \n    <td>Rust CLI with 34 hooks commands</td> \n    <td><a href=\"https://crates.io/crates/ruvector-cli\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/npm/packages/cli\">@ruvector/cli hooks</a></td> \n    <td>npm CLI with 29 hooks commands</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/cli\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/cli.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Quick Start</h4> \n <pre><code class=\"language-bash\"># Rust CLI\ncargo install ruvector-cli\nruvector hooks init\nruvector hooks install\n\n# npm CLI\nnpx @ruvector/cli hooks init\nnpx @ruvector/cli hooks install\n</code></pre> \n <h4>Core Capabilities</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Description</th> \n    <th>Technical Details</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Q-Learning Routing</strong></td> \n    <td>Routes tasks to best agent with learned confidence scores</td> \n    <td>TD learning with α=0.1, γ=0.95, ε-greedy exploration</td> \n   </tr> \n   <tr> \n    <td><strong>Semantic Memory</strong></td> \n    <td>Vector-based memory with embeddings for context retrieval</td> \n    <td>64-dim hash embeddings, cosine similarity, top-k search</td> \n   </tr> \n   <tr> \n    <td><strong>Error Learning</strong></td> \n    <td>Records error patterns and suggests fixes</td> \n    <td>Pattern matching for E0308, E0433, TS2322, etc.</td> \n   </tr> \n   <tr> \n    <td><strong>File Sequences</strong></td> \n    <td>Predicts next files to edit based on historical patterns</td> \n    <td>Markov chain transitions, frequency-weighted suggestions</td> \n   </tr> \n   <tr> \n    <td><strong>Swarm Coordination</strong></td> \n    <td>Registers agents, tracks coordination edges, optimizes</td> \n    <td>Graph-based topology, weighted edges, task assignment</td> \n   </tr> \n   <tr> \n    <td><strong>LRU Cache</strong></td> \n    <td>1000-entry cache for faster Q-value lookups</td> \n    <td>~10x speedup, automatic eviction, RefCell for interior mutability</td> \n   </tr> \n   <tr> \n    <td><strong>Gzip Compression</strong></td> \n    <td>Storage savings with automatic compression</td> \n    <td>flate2 fast mode, 70-83% reduction, transparent load/save</td> \n   </tr> \n   <tr> \n    <td><strong>Batch Saves</strong></td> \n    <td>Dirty flag tracking to reduce disk I/O</td> \n    <td>Only writes when data changes, force_save() override</td> \n   </tr> \n   <tr> \n    <td><strong>Shell Completions</strong></td> \n    <td>Tab completion for all commands</td> \n    <td>bash, zsh, fish, PowerShell support</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Supported Error Codes</h4> \n <p>The intelligence layer has built-in knowledge for common error patterns:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Language</th> \n    <th>Error Codes</th> \n    <th>Auto-Suggested Fixes</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Rust</strong></td> \n    <td>E0308, E0433, E0425, E0277, E0382</td> \n    <td>Type mismatches, missing imports, borrow checker</td> \n   </tr> \n   <tr> \n    <td><strong>TypeScript</strong></td> \n    <td>TS2322, TS2339, TS2345, TS7006</td> \n    <td>Type assignments, property access, argument types</td> \n   </tr> \n   <tr> \n    <td><strong>Python</strong></td> \n    <td>ImportError, AttributeError, TypeError</td> \n    <td>Module imports, attribute access, type errors</td> \n   </tr> \n   <tr> \n    <td><strong>Go</strong></td> \n    <td>undefined, cannot use, not enough arguments</td> \n    <td>Variable scope, type conversion, function calls</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Commands Reference</h4> \n <pre><code class=\"language-bash\"># Setup\nruvector hooks init [--force] [--postgres]  # Initialize hooks (--postgres for DB schema)\nruvector hooks install                   # Install into Claude settings\n\n# Core\nruvector hooks stats                     # Show intelligence statistics\nruvector hooks session-start [--resume]  # Start/resume a session\nruvector hooks session-end               # End session with metrics\n\n# Memory\nruvector hooks remember -t edit \"content\"  # Store in semantic memory\nruvector hooks recall \"query\" -k 5         # Search memory semantically\n\n# Learning\nruvector hooks learn &lt;state&gt; &lt;action&gt; --reward 0.8  # Record trajectory\nruvector hooks suggest &lt;state&gt; --actions \"a,b,c\"    # Get action suggestion\nruvector hooks route \"implement caching\" --file src/cache.rs  # Route to agent\n\n# Claude Code Hooks\nruvector hooks pre-edit &lt;file&gt;           # Pre-edit intelligence hook\nruvector hooks post-edit &lt;file&gt; --success  # Post-edit learning hook\nruvector hooks pre-command &lt;cmd&gt;         # Pre-command hook\nruvector hooks post-command &lt;cmd&gt; --success  # Post-command hook\nruvector hooks suggest-context           # UserPromptSubmit context injection\nruvector hooks track-notification        # Track notification patterns\nruvector hooks pre-compact [--auto]      # Pre-compact hook (auto/manual)\n\n# Claude Code v2.0.55+ Features\nruvector hooks lsp-diagnostic --file &lt;f&gt; --severity error  # LSP diagnostics\nruvector hooks suggest-ultrathink \"complex task\"  # Recommend extended reasoning\nruvector hooks async-agent --action spawn --agent-id &lt;id&gt;  # Async sub-agents\n\n# Intelligence\nruvector hooks record-error &lt;cmd&gt; &lt;stderr&gt;  # Record error pattern\nruvector hooks suggest-fix E0308           # Get fix for error code\nruvector hooks suggest-next &lt;file&gt; -n 3    # Predict next files\nruvector hooks should-test &lt;file&gt;          # Check if tests needed\n\n# Swarm\nruvector hooks swarm-register &lt;id&gt; &lt;type&gt;  # Register agent\nruvector hooks swarm-coordinate &lt;src&gt; &lt;tgt&gt;  # Record coordination\nruvector hooks swarm-optimize \"task1,task2\"  # Optimize distribution\nruvector hooks swarm-recommend \"rust\"      # Recommend agent for task\nruvector hooks swarm-heal &lt;agent-id&gt;       # Handle agent failure\nruvector hooks swarm-stats                 # Show swarm statistics\n\n# Optimization (Rust only)\nruvector hooks compress                   # Compress storage (70-83% savings)\nruvector hooks cache-stats                # Show LRU cache statistics\nruvector hooks completions bash           # Generate shell completions\n</code></pre> \n <h4>Tutorial: Claude Code Integration</h4> \n <p><strong>1. Initialize and install hooks:</strong></p> \n <pre><code class=\"language-bash\">ruvector hooks init\nruvector hooks install --settings-dir .claude\n</code></pre> \n <p>This creates <code>.claude/settings.json</code> with hook configurations:</p> \n <pre><code class=\"language-json\">{\n  \"hooks\": {\n    \"PreToolUse\": [\n      { \"matcher\": \"Edit|Write|MultiEdit\", \"hooks\": [\"ruvector hooks pre-edit \\\"$TOOL_INPUT_FILE_PATH\\\"\"] },\n      { \"matcher\": \"Bash\", \"hooks\": [\"ruvector hooks pre-command \\\"$TOOL_INPUT_COMMAND\\\"\"] }\n    ],\n    \"PostToolUse\": [\n      { \"matcher\": \"Edit|Write|MultiEdit\", \"hooks\": [\"ruvector hooks post-edit ... --success\"] },\n      { \"matcher\": \"Bash\", \"hooks\": [\"ruvector hooks post-command ... --success\"] }\n    ],\n    \"SessionStart\": [\"ruvector hooks session-start\"],\n    \"Stop\": [\"ruvector hooks session-end --export-metrics\"],\n    \"PreCompact\": [\"ruvector hooks pre-compact\"]\n  }\n}\n</code></pre> \n <p><strong>All 7 Claude Code hooks covered:</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Hook</th> \n    <th>When It Fires</th> \n    <th>What RuVector Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>PreToolUse</code></td> \n    <td>Before file edit, command, or Task</td> \n    <td>Suggests agent, shows related files, validates agent assignments</td> \n   </tr> \n   <tr> \n    <td><code>PostToolUse</code></td> \n    <td>After file edit or command</td> \n    <td>Records outcome, updates Q-values, injects context</td> \n   </tr> \n   <tr> \n    <td><code>SessionStart</code></td> \n    <td>When session begins/resumes</td> \n    <td>Loads intelligence, shows stats (startup vs resume)</td> \n   </tr> \n   <tr> \n    <td><code>Stop</code></td> \n    <td>When session ends</td> \n    <td>Saves state, exports metrics</td> \n   </tr> \n   <tr> \n    <td><code>PreCompact</code></td> \n    <td>Before context compaction</td> \n    <td>Preserves critical memories (auto vs manual)</td> \n   </tr> \n   <tr> \n    <td><code>UserPromptSubmit</code></td> \n    <td>Before processing user prompt</td> \n    <td>Injects learned patterns as context</td> \n   </tr> \n   <tr> \n    <td><code>Notification</code></td> \n    <td>On system notifications</td> \n    <td>Tracks notification patterns</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Advanced Features:</strong></p> \n <ul> \n  <li><strong>Stdin JSON Parsing</strong>: Hooks receive full JSON via stdin (session_id, tool_input, tool_response)</li> \n  <li><strong>Context Injection</strong>: PostToolUse returns <code>additionalContext</code> to inject into Claude's context</li> \n  <li><strong>Timeout Optimization</strong>: All hooks have optimized timeouts (1-5 seconds vs 60s default)</li> \n </ul> \n <p><strong>2. Use routing for intelligent agent selection:</strong></p> \n <pre><code class=\"language-bash\"># Route a task to the best agent\n$ ruvector hooks route \"implement vector search\" --file src/lib.rs\n{\n  \"recommended\": \"rust-developer\",\n  \"confidence\": 0.85,\n  \"reasoning\": \"learned from 47 similar edits\"\n}\n</code></pre> \n <p><strong>3. Learn from outcomes:</strong></p> \n <pre><code class=\"language-bash\"># Record successful outcome\nruvector hooks learn \"edit-rs-lib\" \"rust-developer\" --reward 1.0\n\n# Record failed outcome\nruvector hooks learn \"edit-rs-lib\" \"typescript-dev\" --reward -0.5\n</code></pre> \n <p><strong>4. Get error fix suggestions:</strong></p> \n <pre><code class=\"language-bash\">$ ruvector hooks suggest-fix E0308\n{\n  \"code\": \"E0308\",\n  \"type\": \"type_mismatch\",\n  \"fixes\": [\n    \"Check return type matches function signature\",\n    \"Use .into() or .as_ref() for type conversion\",\n    \"Verify generic type parameters\"\n  ]\n}\n</code></pre> \n <h4>Tutorial: Swarm Coordination</h4> \n <p><strong>1. Register agents:</strong></p> \n <pre><code class=\"language-bash\">ruvector hooks swarm-register agent-1 rust-developer --capabilities \"rust,async,testing\"\nruvector hooks swarm-register agent-2 typescript-dev --capabilities \"ts,react,node\"\nruvector hooks swarm-register agent-3 reviewer --capabilities \"review,security,performance\"\n</code></pre> \n <p><strong>2. Record coordination patterns:</strong></p> \n <pre><code class=\"language-bash\"># Agent-1 hands off to Agent-3 for review\nruvector hooks swarm-coordinate agent-1 agent-3 --weight 0.9\n</code></pre> \n <p><strong>3. Optimize task distribution:</strong></p> \n <pre><code class=\"language-bash\">$ ruvector hooks swarm-optimize \"implement-api,write-tests,code-review\"\n{\n  \"assignments\": {\n    \"implement-api\": \"agent-1\",\n    \"write-tests\": \"agent-1\",\n    \"code-review\": \"agent-3\"\n  }\n}\n</code></pre> \n <p><strong>4. Handle failures with self-healing:</strong></p> \n <pre><code class=\"language-bash\"># Mark agent as failed and redistribute\nruvector hooks swarm-heal agent-2\n</code></pre> \n <h4>PostgreSQL Storage (Optional)</h4> \n <p>For production deployments, use PostgreSQL instead of JSON files:</p> \n <pre><code class=\"language-bash\"># Set connection URL\nexport RUVECTOR_POSTGRES_URL=\"postgres://user:pass@localhost/ruvector\"\n\n# Initialize PostgreSQL schema (automatic)\nruvector hooks init --postgres\n\n# Or apply schema manually\npsql $RUVECTOR_POSTGRES_URL -f crates/ruvector-cli/sql/hooks_schema.sql\n\n# Build CLI with postgres feature\ncargo build -p ruvector-cli --features postgres\n</code></pre> \n <p>The PostgreSQL backend provides:</p> \n <ul> \n  <li>Vector embeddings with native <code>ruvector</code> type</li> \n  <li>Q-learning functions (<code>ruvector_hooks_update_q</code>, <code>ruvector_hooks_best_action</code>)</li> \n  <li>Swarm coordination tables with foreign key relationships</li> \n  <li>Automatic memory cleanup (keeps last 5000 entries)</li> \n </ul> \n</details> \n<hr /> \n<h2>Additional Modules</h2> \n<details> \n 🔬 Scientific OCR (SciPix) \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Install</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/scipix\">ruvector-scipix</a></td> \n    <td>Rust OCR engine for scientific documents</td> \n    <td><code>cargo add ruvector-scipix</code></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/scipix\">@ruvector/scipix</a></td> \n    <td>TypeScript client for SciPix API</td> \n    <td><code>npm install @ruvector/scipix</code></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>SciPix</strong> extracts text and mathematical equations from images, converting them to LaTeX, MathML, or plain text.</p> \n <p><strong>Features:</strong></p> \n <ul> \n  <li><strong>Multi-format output</strong> — LaTeX, MathML, AsciiMath, plain text, structured JSON</li> \n  <li><strong>Batch processing</strong> — Process multiple images with parallel execution</li> \n  <li><strong>Content detection</strong> — Equations, tables, diagrams, mixed content</li> \n  <li><strong>Confidence scoring</strong> — Per-region confidence levels (high/medium/low)</li> \n  <li><strong>PDF support</strong> — Extract from multi-page PDFs with page selection</li> \n </ul> \n <pre><code class=\"language-typescript\">import { SciPixClient, OutputFormat } from '@ruvector/scipix';\n\nconst client = new SciPixClient({\n  baseUrl: 'http://localhost:8080',\n  apiKey: 'your-api-key',\n});\n\n// OCR an image file\nconst result = await client.ocrFile('./equation.png', {\n  formats: [OutputFormat.LaTeX, OutputFormat.MathML],\n  detectEquations: true,\n});\n\nconsole.log('LaTeX:', result.latex);\nconsole.log('Confidence:', result.confidence);\n\n// Quick LaTeX extraction\nconst latex = await client.extractLatex('./math.png');\n\n// Batch processing\nconst batchResult = await client.batchOcr({\n  images: [\n    { source: 'base64...', id: 'eq1' },\n    { source: 'base64...', id: 'eq2' },\n  ],\n  defaultOptions: { formats: [OutputFormat.LaTeX] },\n});\n</code></pre> \n <pre><code class=\"language-bash\"># Rust CLI usage\nscipix-cli ocr --input equation.png --format latex\nscipix-cli serve --port 3000\n\n# MCP server for Claude/AI assistants\nscipix-cli mcp\nclaude mcp add scipix -- scipix-cli mcp\n</code></pre> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/npm/packages/scipix/README.md\">npm/packages/scipix/README.md</a> for full documentation.</p> \n</details> \n<details> \n 🔗 ONNX Embeddings \n <table> \n  <thead> \n   <tr> \n    <th>Example</th> \n    <th>Description</th> \n    <th>Path</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/onnx-embeddings\">ruvector-onnx-embeddings</a></td> \n    <td>Production-ready ONNX embedding generation in pure Rust</td> \n    <td><code>examples/onnx-embeddings</code></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>ONNX Embeddings</strong> provides native embedding generation using ONNX Runtime — no Python required. Supports 8+ pretrained models (all-MiniLM, BGE, E5, GTE), multiple pooling strategies, GPU acceleration (CUDA, TensorRT, CoreML, WebGPU), and direct RuVector index integration for RAG pipelines.</p> \n <pre><code class=\"language-rust\">use ruvector_onnx_embeddings::{Embedder, PretrainedModel};\n\n#[tokio::main]\nasync fn main() -&gt; anyhow::Result&lt;()&gt; {\n    // Create embedder with default model (all-MiniLM-L6-v2)\n    let mut embedder = Embedder::default_model().await?;\n\n    // Generate embedding (384 dimensions)\n    let embedding = embedder.embed_one(\"Hello, world!\")?;\n\n    // Compute semantic similarity\n    let sim = embedder.similarity(\n        \"I love programming in Rust\",\n        \"Rust is my favorite language\"\n    )?;\n    println!(\"Similarity: {:.4}\", sim); // ~0.85\n\n    Ok(())\n}\n</code></pre> \n <p><strong>Supported Models:</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Model</th> \n    <th>Dimension</th> \n    <th>Speed</th> \n    <th>Best For</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>AllMiniLmL6V2</code></td> \n    <td>384</td> \n    <td>Fast</td> \n    <td>General purpose (default)</td> \n   </tr> \n   <tr> \n    <td><code>BgeSmallEnV15</code></td> \n    <td>384</td> \n    <td>Fast</td> \n    <td>Search &amp; retrieval</td> \n   </tr> \n   <tr> \n    <td><code>AllMpnetBaseV2</code></td> \n    <td>768</td> \n    <td>Accurate</td> \n    <td>Production RAG</td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<details> \n 🔧 Bindings &amp; Tools \n <p><strong>Native bindings and tools</strong> for integrating RuVector into any environment — Node.js, browsers, CLI, or as an HTTP/gRPC server.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-node\">ruvector-node</a></td> \n    <td>Native Node.js bindings via napi-rs</td> \n    <td><a href=\"https://crates.io/crates/ruvector-node\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-wasm\">ruvector-wasm</a></td> \n    <td>WASM bindings for browsers &amp; edge</td> \n    <td><a href=\"https://crates.io/crates/ruvector-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm-wasm\">ruvllm-wasm</a></td> \n    <td>Browser LLM inference with WebGPU</td> \n    <td><a href=\"https://crates.io/crates/ruvllm-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvllm-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-cli\">ruvector-cli</a></td> \n    <td>Command-line interface</td> \n    <td><a href=\"https://crates.io/crates/ruvector-cli\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-server\">ruvector-server</a></td> \n    <td>HTTP/gRPC server</td> \n    <td><a href=\"https://crates.io/crates/ruvector-server\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-server.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Node.js (Native Performance)</strong></p> \n <pre><code class=\"language-bash\">npm install @ruvector/node\n</code></pre> \n <pre><code class=\"language-javascript\">const { RuVector } = require('@ruvector/node');\nconst db = new RuVector({ dimensions: 1536 });\ndb.insert('doc1', embedding, { title: 'Hello' });\nconst results = db.search(queryEmbedding, 10);\n</code></pre> \n <p><strong>Browser (WASM)</strong></p> \n <pre><code class=\"language-bash\">npm install @ruvector/wasm\n</code></pre> \n <pre><code class=\"language-javascript\">import { RuVectorWasm } from '@ruvector/wasm';\nconst db = await RuVectorWasm.create({ dimensions: 384 });\nawait db.insert('doc1', embedding);\nconst results = await db.search(query, 5);\n</code></pre> \n <p><strong>CLI</strong></p> \n <pre><code class=\"language-bash\">cargo install ruvector-cli\nruvector init mydb --dim 1536\nruvector insert mydb --file embeddings.json\nruvector search mydb --query \"[0.1, 0.2, ...]\" --limit 10\n</code></pre> \n <p><strong>HTTP Server</strong></p> \n <pre><code class=\"language-bash\">cargo install ruvector-server\nruvector-server --port 8080 --data ./vectors\n\n# REST API\ncurl -X POST http://localhost:8080/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"vector\": [0.1, 0.2, ...], \"limit\": 10}'\n</code></pre> \n</details> \n<hr /> \n<h2>Examples &amp; Tutorials</h2> \n<details> \n 📚 Production Examples \n <p>34 production-ready examples demonstrating RuVector integration patterns.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Example</th> \n    <th>Description</th> \n    <th>Type</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/security_hardened.rvf\"><strong>security_hardened.rvf</strong></a></td> \n    <td><strong>Security RVF: 22 capabilities — TEE, AIDefence, eBPF, RBAC, Paranoid policy</strong></td> \n    <td>Rust/RVF</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/agentic-jujutsu\">agentic-jujutsu</a></td> \n    <td>Quantum-resistant version control for AI agents (23x faster than Git)</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/mincut\">mincut</a></td> \n    <td>6 self-organizing network demos: strange loops, time crystals, causal discovery</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/subpolynomial-time\">subpolynomial-time</a></td> \n    <td>n^0.12 subpolynomial algorithm demos</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/exo-ai-2025\">exo-ai-2025</a></td> \n    <td>Cognitive substrate with 9 neural-symbolic crates + 11 research experiments</td> \n    <td>Rust/TS</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/neural-trader\">neural-trader</a></td> \n    <td>AI trading with DRL + sentiment analysis + SONA learning</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/ultra-low-latency-sim\">ultra-low-latency-sim</a></td> \n    <td>13+ quadrillion meta-simulations/sec with SIMD</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/meta-cognition-spiking-neural-network\">meta-cognition-spiking-neural-network</a></td> \n    <td>Spiking neural network with meta-cognitive learning (10-50x speedup)</td> \n    <td>npm</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/spiking-network\">spiking-network</a></td> \n    <td>Biologically-inspired spiking neural networks</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/ruvLLM\">ruvLLM</a></td> \n    <td>LLM integration patterns for RAG and AI agents</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/onnx-embeddings\">onnx-embeddings</a></td> \n    <td>Production ONNX embedding generation without Python</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/onnx-embeddings-wasm\">onnx-embeddings-wasm</a></td> \n    <td>WASM ONNX embeddings for browsers</td> \n    <td>WASM</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/refrag-pipeline\">refrag-pipeline</a></td> \n    <td>RAG pipeline with vector search and document processing</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/scipix\">scipix</a></td> \n    <td>Scientific OCR: equations → LaTeX/MathML with ONNX inference</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/graph\">graph</a></td> \n    <td>Graph database examples with Cypher queries</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge\">edge</a></td> \n    <td>364KB WASM edge deployment</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge-full\">edge-full</a></td> \n    <td>Full-featured edge vector DB</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge-net\">edge-net</a></td> \n    <td>Networked edge deployment with zero-cost swarms</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/vibecast-7sense\">vibecast-7sense</a></td> \n    <td>7-sense perception AI application</td> \n    <td>TypeScript</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/apify\">apify</a></td> \n    <td>13 Apify actors: trading, memory engine, synth data, market research</td> \n    <td>npm</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/google-cloud\">google-cloud</a></td> \n    <td>GCP templates for Cloud Run, GKE, Vertex AI</td> \n    <td>Terraform</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/wasm-react\">wasm-react</a></td> \n    <td>React integration with WASM vector operations</td> \n    <td>WASM</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/wasm-vanilla\">wasm-vanilla</a></td> \n    <td>Vanilla JS WASM example for browser vector search</td> \n    <td>WASM</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/wasm\">wasm</a></td> \n    <td>Core WASM examples and bindings</td> \n    <td>WASM</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/nodejs\">nodejs</a></td> \n    <td>Node.js integration examples</td> \n    <td>Node.js</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/rust\">rust</a></td> \n    <td>Core Rust usage examples</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/dna\">dna</a></td> \n    <td>rvDNA: AI-native genomic analysis, variant calling, <code>.rvdna</code> format</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/delta-behavior\">delta-behavior</a></td> \n    <td>Mathematics of systems that refuse to collapse — behavioral change tracking</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/data\">data</a></td> \n    <td>Dataset discovery framework — graph-based pattern finding in massive datasets</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/prime-radiant\">prime-radiant</a></td> \n    <td>Prime-Radiant coherence engine examples and usage demos</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/benchmarks\">benchmarks</a></td> \n    <td>Comprehensive benchmarks for temporal reasoning and vector operations</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/vwm-viewer\">vwm-viewer</a></td> \n    <td>Visual vector world model viewer (HTML Canvas)</td> \n    <td>HTML</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/verified-applications\"><strong>verified-applications</strong></a></td> \n    <td><strong>10 exotic domains: weapons filter, medical diagnostics, financial routing, agent contracts, sensor swarm, quantization proof, AGI memory, vector signatures, simulation integrity, legal forensics</strong></td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/rvf-kernel-optimized\">rvf-kernel-optimized</a></td> \n    <td>Verified + hyper-optimized Linux kernel RVF with proof-carrying ingest</td> \n    <td>Rust</td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<details> \n 🎓 Tutorials \n <h3>Tutorial 1: Vector Search in 60 Seconds</h3> \n <pre><code class=\"language-javascript\">import { VectorDB } from 'ruvector';\n\n// Create DB with 384-dimensional vectors\nconst db = new VectorDB(384);\n\n// Add vectors\ndb.insert('doc1', [0.1, 0.2, ...]);  // 384 floats\ndb.insert('doc2', [0.3, 0.1, ...]);\n\n// Search (returns top 5 nearest neighbors)\nconst results = db.search(queryVector, 5);\n// -&gt; [{ id: 'doc1', score: 0.95 }, { id: 'doc2', score: 0.87 }]\n</code></pre> \n <h3>Tutorial 2: Graph Queries with Cypher</h3> \n <pre><code class=\"language-javascript\">import { GraphDB } from 'ruvector';\n\nconst graph = new GraphDB();\n\n// Create nodes and relationships\ngraph.query(`\n  CREATE (a:Person {name: 'Alice', embedding: $emb1})\n  CREATE (b:Person {name: 'Bob', embedding: $emb2})\n  CREATE (a)-[:KNOWS {since: 2020}]-&gt;(b)\n`, { emb1: aliceVector, emb2: bobVector });\n\n// Hybrid query: graph traversal + vector similarity\nconst results = graph.query(`\n  MATCH (p:Person)-[:KNOWS*1..3]-&gt;(friend)\n  WHERE vector.similarity(friend.embedding, $query) &gt; 0.8\n  RETURN friend.name, vector.similarity(friend.embedding, $query) as score\n  ORDER BY score DESC\n`, { query: queryVector });\n</code></pre> \n <h3>Tutorial 3: Self-Learning with SONA</h3> \n <pre><code class=\"language-rust\">use ruvector_sona::{SonaEngine, SonaConfig};\n\n// Initialize SONA with LoRA adapters\nlet sona = SonaEngine::with_config(SonaConfig {\n    hidden_dim: 256,\n    lora_rank: 8,\n    ewc_lambda: 0.4,  // Elastic Weight Consolidation\n    ..Default::default()\n});\n\n// Record successful action\nlet mut trajectory = sona.begin_trajectory(query_embedding);\ntrajectory.add_step(result_embedding, vec![], 1.0);  // reward=1.0\nsona.end_trajectory(trajectory, true);  // success=true\n\n// SONA learns and improves future predictions\nsona.force_learn();\n\n// Later: get improved predictions\nlet prediction = sona.predict(&amp;new_query_embedding);\n</code></pre> \n <h3>Tutorial 4: Dynamic Min-Cut (n^0.12 Updates)</h3> \n <pre><code class=\"language-rust\">use ruvector_mincut::{DynamicMinCut, Graph};\n\n// Build graph\nlet mut graph = Graph::new(100);  // 100 nodes\ngraph.add_edge(0, 1, 10.0);\ngraph.add_edge(1, 2, 5.0);\ngraph.add_edge(0, 2, 15.0);\n\n// Compute initial min-cut\nlet mut mincut = DynamicMinCut::new(&amp;graph);\nlet (value, cut_edges) = mincut.compute();\nprintln!(\"Min-cut value: {}\", value);  // -&gt; 15.0\n\n// Dynamic update - subpolynomial time O(n^0.12)!\ngraph.update_edge(1, 2, 20.0);\nlet (new_value, _) = mincut.recompute();  // Much faster than recomputing from scratch\n</code></pre> \n <h3>Tutorial 5: 39 Attention Mechanisms</h3> \n <pre><code class=\"language-rust\">use ruvector_attention::{\n    Attention, FlashAttention, LinearAttention,\n    HyperbolicAttention, GraphAttention, MinCutGatedAttention\n};\n\n// FlashAttention - O(n) memory, fastest for long sequences\nlet flash = FlashAttention::new(512, 8);  // dim=512, heads=8\nlet output = flash.forward(&amp;query, &amp;key, &amp;value);\n\n// LinearAttention - O(n) time complexity\nlet linear = LinearAttention::new(512, 8);\n\n// HyperbolicAttention - for hierarchical data (Poincaré ball)\nlet hyper = HyperbolicAttention::new(512, 8, Curvature(-1.0));\n\n// GraphAttention - respects graph structure\nlet gat = GraphAttention::new(512, 8, &amp;adjacency_matrix);\n\n// MinCutGatedAttention - 50% compute reduction via sparsity\nlet mincut_gated = MinCutGatedAttention::new(512, 8, sparsity: 0.5);\nlet sparse_output = mincut_gated.forward(&amp;query, &amp;key, &amp;value);\n</code></pre> \n <h3>Tutorial 6: Spiking Neural Networks</h3> \n <pre><code class=\"language-javascript\">import { SpikingNetwork, HDCEncoder } from '@ruvector/spiking-neural';\n\n// High-Dimensional Computing encoder (10K-bit vectors)\nconst encoder = new HDCEncoder(10000);\nconst encoded = encoder.encode(\"hello world\");\n\n// Spiking network with BTSP learning\nconst network = new SpikingNetwork({\n  layers: [784, 256, 10],\n  learning: 'btsp',  // Behavioral Time-Scale Plasticity\n  threshold: 1.0\n});\n\n// Train with spike timing\nnetwork.train(spikes, labels, { epochs: 10 });\n\n// Inference\nconst output = network.forward(inputSpikes);\n</code></pre> \n <h3>Tutorial 7: Claude Code Hooks Integration</h3> \n <pre><code class=\"language-bash\"># 1. Initialize hooks\nnpx @ruvector/cli hooks init\n\n# 2. Install into Claude settings\nnpx @ruvector/cli hooks install\n\n# 3. Hooks now capture:\n#    - File edits (pre/post)\n#    - Commands (pre/post)\n#    - Sessions (start/end)\n#    - Errors and fixes\n\n# 4. Query learned patterns\nnpx @ruvector/cli hooks recall \"authentication error\"\n# -&gt; Returns similar past solutions\n\n# 5. Get AI routing suggestions\nnpx @ruvector/cli hooks route \"implement caching\"\n# -&gt; Suggests: rust-developer (confidence: 0.89)\n</code></pre> \n <h3>Tutorial 8: Edge Deployment with rvLite</h3> \n <pre><code class=\"language-javascript\">import { RvLite } from '@ruvector/rvlite';\n\n// Create persistent edge database (IndexedDB in browser)\nconst db = await RvLite.create({\n  path: 'my-vectors.db',\n  dimensions: 384\n});\n\n// Works offline - all computation local\nawait db.insert('doc1', embedding1, { title: 'Hello' });\nawait db.insert('doc2', embedding2, { title: 'World' });\n\n// Semantic search with metadata filtering\nconst results = await db.search(queryEmbedding, {\n  limit: 10,\n  filter: { title: { $contains: 'Hello' } }\n});\n\n// Sync when online\nawait db.sync('https://api.example.com/vectors');\n</code></pre> \n</details> \n<details> \n 🍕 WASM &amp; Utility Packages \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm\">@ruvector/wasm</a></td> \n    <td>WASM core vector DB</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn-wasm\">@ruvector/gnn-wasm</a></td> \n    <td>WASM GNN layers</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/gnn-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/gnn-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-wasm\">@ruvector/graph-wasm</a></td> \n    <td>WASM graph DB</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/graph-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/graph-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-wasm\">@ruvector/attention-wasm</a></td> \n    <td>WASM attention</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/attention-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/attention-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer-wasm\">@ruvector/tiny-dancer-wasm</a></td> \n    <td>WASM AI routing</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/tiny-dancer-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/tiny-dancer-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router-wasm\">@ruvector/router-wasm</a></td> \n    <td>WASM semantic router</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/router-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/router-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/postgres-cli\">@ruvector/postgres-cli</a></td> \n    <td>Postgres extension CLI</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/postgres-cli\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/postgres-cli.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/postgres-cli\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/postgres-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\">@ruvector/agentic-synth</a></td> \n    <td>Synthetic data generator</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/agentic-synth.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/agentic-synth.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-data-generator\">@ruvector/graph-data-generator</a></td> \n    <td>Graph data generation</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-data-generator\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/graph-data-generator.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-data-generator\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/graph-data-generator.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-integration\">@ruvector/agentic-integration</a></td> \n    <td>Agentic workflows</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-integration\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/agentic-integration.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-integration\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/agentic-integration.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/rvlite\">rvlite</a></td> \n    <td>SQLite-style edge DB (SQL/SPARQL/Cypher)</td> \n    <td><a href=\"https://www.npmjs.com/package/rvlite\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/rvlite.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/rvlite\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/rvlite.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Platform-specific native bindings</strong> (auto-detected):</p> \n <ul> \n  <li><code>@ruvector/node-linux-x64-gnu</code>, <code>@ruvector/node-linux-arm64-gnu</code>, <code>@ruvector/node-darwin-x64</code>, <code>@ruvector/node-darwin-arm64</code>, <code>@ruvector/node-win32-x64-msvc</code></li> \n  <li><code>@ruvector/gnn-linux-x64-gnu</code>, <code>@ruvector/gnn-linux-arm64-gnu</code>, <code>@ruvector/gnn-darwin-x64</code>, <code>@ruvector/gnn-darwin-arm64</code>, <code>@ruvector/gnn-win32-x64-msvc</code></li> \n  <li><code>@ruvector/tiny-dancer-linux-x64-gnu</code>, <code>@ruvector/tiny-dancer-linux-arm64-gnu</code>, <code>@ruvector/tiny-dancer-darwin-x64</code>, <code>@ruvector/tiny-dancer-darwin-arm64</code>, <code>@ruvector/tiny-dancer-win32-x64-msvc</code></li> \n  <li><code>@ruvector/router-linux-x64-gnu</code>, <code>@ruvector/router-linux-arm64-gnu</code>, <code>@ruvector/router-darwin-x64</code>, <code>@ruvector/router-darwin-arm64</code>, <code>@ruvector/router-win32-x64-msvc</code></li> \n  <li><code>@ruvector/attention-linux-x64-gnu</code>, <code>@ruvector/attention-linux-arm64-gnu</code>, <code>@ruvector/attention-darwin-x64</code>, <code>@ruvector/attention-darwin-arm64</code>, <code>@ruvector/attention-win32-x64-msvc</code></li> \n  <li><code>@ruvector/ruvllm-linux-x64-gnu</code>, <code>@ruvector/ruvllm-linux-arm64-gnu</code>, <code>@ruvector/ruvllm-darwin-x64</code>, <code>@ruvector/ruvllm-darwin-arm64</code>, <code>@ruvector/ruvllm-win32-x64-msvc</code></li> \n </ul> \n <p>See <a href=\"https://github.com/ruvnet/ruvector/issues/20\">GitHub Issue #20</a> for multi-platform npm package roadmap.</p> \n <pre><code class=\"language-bash\"># Install all-in-one package\nnpm install ruvector\n\n# Or install individual packages\nnpm install @ruvector/core @ruvector/gnn @ruvector/graph-node\n\n# List all available packages\nnpx ruvector install\n</code></pre> \n <pre><code class=\"language-javascript\">const ruvector = require('ruvector');\n\n// Vector search\nconst db = new ruvector.VectorDB(128);\ndb.insert('doc1', embedding1);\nconst results = db.search(queryEmbedding, 10);\n\n// Graph queries (Cypher)\ndb.execute(\"CREATE (a:Person {name: 'Alice'})-[:KNOWS]-&gt;(b:Person {name: 'Bob'})\");\ndb.execute(\"MATCH (p:Person)-[:KNOWS]-&gt;(friend) RETURN friend.name\");\n\n// GNN-enhanced search\nconst layer = new ruvector.GNNLayer(128, 256, 4);\nconst enhanced = layer.forward(query, neighbors, weights);\n\n// Compression (2-32x memory savings)\nconst compressed = ruvector.compress(embedding, 0.3);\n\n// Tiny Dancer: AI agent routing\nconst router = new ruvector.Router();\nconst decision = router.route(candidates, { optimize: 'cost' });\n</code></pre> \n</details> \n<details> \n 🦀 Rust Usage Examples \n <pre><code class=\"language-bash\">cargo add ruvector-graph ruvector-gnn\n</code></pre> \n <pre><code class=\"language-rust\">use ruvector_graph::{GraphDB, NodeBuilder};\nuse ruvector_gnn::{RuvectorLayer, differentiable_search};\n\nlet db = GraphDB::new();\n\nlet doc = NodeBuilder::new(\"doc1\")\n    .label(\"Document\")\n    .property(\"embedding\", vec![0.1, 0.2, 0.3])\n    .build();\ndb.create_node(doc)?;\n\n// GNN layer\nlet layer = RuvectorLayer::new(128, 256, 4, 0.1);\nlet enhanced = layer.forward(&amp;query, &amp;neighbors, &amp;weights);\n</code></pre> \n <pre><code class=\"language-rust\">use ruvector_raft::{RaftNode, RaftNodeConfig};\nuse ruvector_cluster::{ClusterManager, ConsistentHashRing};\nuse ruvector_replication::{SyncManager, SyncMode};\n\n// Configure a 5-node Raft cluster\nlet config = RaftNodeConfig {\n    node_id: \"node-1\".into(),\n    cluster_members: vec![\"node-1\", \"node-2\", \"node-3\", \"node-4\", \"node-5\"]\n        .into_iter().map(Into::into).collect(),\n    election_timeout_min: 150,  // ms\n    election_timeout_max: 300,  // ms\n    heartbeat_interval: 50,     // ms\n};\nlet raft = RaftNode::new(config);\n\n// Auto-sharding with consistent hashing (150 virtual nodes per real node)\nlet ring = ConsistentHashRing::new(64, 3); // 64 shards, replication factor 3\nlet shard = ring.get_shard(\"my-vector-key\");\n\n// Multi-master replication with conflict resolution\nlet sync = SyncManager::new(SyncMode::SemiSync { min_replicas: 2 });\n</code></pre> \n</details> \n<details> \n 🎓 RuvLLM Training &amp; RLM Fine-Tuning Tutorials  \n <h4>Hybrid Routing (90% Accuracy)</h4> \n <p>RuvLTRA achieves <strong>90% routing accuracy</strong> using a keyword-first strategy with embedding fallback:</p> \n <pre><code class=\"language-javascript\">// Optimal routing: Keywords first, embeddings as tiebreaker\nfunction routeTask(task, taskEmbedding, agentEmbeddings) {\n  const keywordScores = getKeywordScores(task);\n  const maxKw = Math.max(...Object.values(keywordScores));\n\n  if (maxKw &gt; 0) {\n    const candidates = Object.entries(keywordScores)\n      .filter(([_, score]) =&gt; score === maxKw)\n      .map(([agent]) =&gt; agent);\n\n    if (candidates.length === 1) return { agent: candidates[0] };\n    return pickByEmbedding(candidates, taskEmbedding, agentEmbeddings);\n  }\n\n  return embeddingSimilarity(taskEmbedding, agentEmbeddings);\n}\n</code></pre> \n <p>Run the benchmark: <code>node npm/packages/ruvllm/scripts/hybrid-model-compare.js</code></p> \n <h4>Generate Training Data</h4> \n <pre><code class=\"language-bash\"># Using CLI (recommended)\nnpx @ruvector/ruvllm train stats              # View dataset statistics\nnpx @ruvector/ruvllm train dataset            # Export training data\nnpx @ruvector/ruvllm train contrastive        # Run full training pipeline\n\n# With options\nnpx @ruvector/ruvllm train dataset --output ./my-training\nnpx @ruvector/ruvllm train contrastive --epochs 20 --batch-size 32 --lr 0.0001\n</code></pre> \n <p><strong>Programmatic API:</strong></p> \n <pre><code class=\"language-javascript\">import { ContrastiveTrainer, generateTrainingDataset, getDatasetStats } from '@ruvector/ruvllm';\n\nconst stats = getDatasetStats();\nconsole.log(`${stats.totalExamples} examples, ${stats.agentTypes} agent types`);\n\nconst trainer = new ContrastiveTrainer({ epochs: 10, margin: 0.5 });\ntrainer.addTriplet(anchor, anchorEmb, positive, positiveEmb, negative, negativeEmb, true);\nconst result = trainer.train();\ntrainer.exportTrainingData('./output');\n</code></pre> \n <h4>Fine-Tune with LoRA</h4> \n <pre><code class=\"language-bash\">pip install transformers peft datasets accelerate\n\npython -m peft.lora_train \\\n  --model_name Qwen/Qwen2.5-0.5B-Instruct \\\n  --dataset ./data/training/routing-examples.jsonl \\\n  --output_dir ./ruvltra-routing-lora \\\n  --lora_r 8 --lora_alpha 16 \\\n  --num_train_epochs 3 \\\n  --learning_rate 2e-4\n</code></pre> \n <h4>Convert to GGUF</h4> \n <pre><code class=\"language-bash\"># Merge LoRA weights\npython -c \"\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM\nbase = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-0.5B-Instruct')\nmodel = PeftModel.from_pretrained(base, './ruvltra-routing-lora')\nmodel.merge_and_unload().save_pretrained('./ruvltra-routing-merged')\n\"\n\n# Convert and quantize\npython llama.cpp/convert_hf_to_gguf.py ./ruvltra-routing-merged --outfile ruvltra-routing-f16.gguf\n./llama.cpp/llama-quantize ruvltra-routing-f16.gguf ruvltra-routing-q4_k_m.gguf Q4_K_M\n</code></pre> \n <h4>Contrastive Embedding Training</h4> \n <p><strong>Using RuvLLM CLI (recommended):</strong></p> \n <pre><code class=\"language-bash\"># Full contrastive training pipeline with triplet loss\nnpx @ruvector/ruvllm train contrastive --output ./training-output\n\n# Exports: triplets.jsonl, embeddings.json, lora_config.json, train.sh\n</code></pre> \n <p><strong>Using Python (for GPU training):</strong></p> \n <pre><code class=\"language-python\">from sentence_transformers import SentenceTransformer, losses, InputExample\nfrom torch.utils.data import DataLoader\n\ntrain_examples = [\n    InputExample(texts=[\"implement login\", \"build auth component\"], label=1.0),\n    InputExample(texts=[\"implement login\", \"write unit tests\"], label=0.0),\n]\n\nmodel = SentenceTransformer(\"Qwen/Qwen2.5-0.5B-Instruct\")\ntrain_loss = losses.CosineSimilarityLoss(model)\nmodel.fit([(DataLoader(train_examples, batch_size=16), train_loss)], epochs=5)\n</code></pre> \n <p><strong>Resources:</strong> <a href=\"https://github.com/ruvnet/ruvector/issues/122\">Issue #122</a> | <a href=\"https://arxiv.org/abs/2106.09685\">LoRA Paper</a> | <a href=\"https://www.sbert.net/docs/training/overview.html\">Sentence Transformers</a></p> \n <h4>Rust Training Module</h4> \n <p>For production-scale dataset generation, use the Rust training module (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm/src/training/README.md\">full docs</a>):</p> \n <pre><code class=\"language-rust\">use ruvllm::training::{DatasetGenerator, DatasetConfig};\n\nlet config = DatasetConfig {\n    examples_per_category: 100,\n    enable_augmentation: true,\n    seed: 42,\n    ..Default::default()\n};\n\nlet dataset = DatasetGenerator::new(config).generate();\nlet (train, val, test) = dataset.split(0.7, 0.15, 0.15, 42);\ndataset.export_jsonl(\"training.jsonl\")?;\n</code></pre> \n <p><strong>Features:</strong></p> \n <ul> \n  <li><strong>5 agent categories</strong>: Coder, Researcher, Security, Architecture, Reviewer (20% each)</li> \n  <li><strong>Model routing</strong>: Haiku (simple) → Sonnet (moderate) → Opus (complex/security)</li> \n  <li><strong>Data augmentation</strong>: Paraphrasing, complexity variations, domain transfer</li> \n  <li><strong>8 technical domains</strong>: Web, Systems, DataScience, Mobile, DevOps, Security, Database, API</li> \n  <li><strong>Quality scores</strong>: 0.80-0.96 based on template quality and category</li> \n  <li><strong>Performance</strong>: ~10,000 examples/second, ~50 MB/s JSONL export</li> \n </ul> \n <pre><code class=\"language-bash\">cargo run --example generate_claude_dataset --release\n# Outputs: train.jsonl, val.jsonl, test.jsonl, stats.json\n</code></pre> \n</details> \n<hr /> \n<h2>Project</h2> \n<details> \n 📁 Project Structure \n <pre><code>crates/\n├── ruvector-core/           # Vector DB engine (HNSW, storage)\n├── ruvector-graph/          # Graph DB + Cypher parser + Hyperedges\n├── ruvector-gnn/            # GNN layers, compression, training\n├── ruvector-tiny-dancer-core/  # AI agent routing (FastGRNN)\n├── ruvector-*-wasm/         # WebAssembly bindings\n├── ruvector-*-node/         # Node.js bindings (napi-rs)\n└── rvf/                     # RVF Cognitive Containers (13 crates)\n    ├── rvf-types/           #   Segment types, headers (no_std)\n    ├── rvf-runtime/         #   Store API, COW engine, compaction\n    ├── rvf-kernel/          #   Linux kernel builder\n    ├── rvf-ebpf/            #   eBPF programs (XDP/TC/socket)\n    ├── rvf-launch/          #   QEMU microvm launcher\n    ├── rvf-cli/             #   CLI with 17 subcommands\n    └── ...                  #   wire, manifest, index, quant, crypto, server, import\n</code></pre> \n</details> \n<h2>Contributing</h2> \n<p>We welcome contributions! See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/development/CONTRIBUTING.md\">CONTRIBUTING.md</a>.</p> \n<pre><code class=\"language-bash\"># Run tests\ncargo test --workspace\n\n# Run benchmarks\ncargo bench --workspace\n\n# Build WASM\ncargo build -p ruvector-gnn-wasm --target wasm32-unknown-unknown\n</code></pre> \n<h2>License</h2> \n<p>MIT License — free for commercial and personal use.</p> \n<hr /> \n<div align=\"center\"> \n <p><strong>Built by <a href=\"https://ruv.io\">rUv</a></strong> • <a href=\"https://github.com/ruvnet/ruvector\">GitHub</a> • <a href=\"https://npmjs.com/package/ruvector\">npm</a> • <a href=\"https://crates.io/crates/rvf-runtime\">crates.io</a> • <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/\">Docs</a> • <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">RVF</a></p> \n <p><em>Vector search that gets smarter over time — now shipping as cognitive containers.</em></p> \n</div>",
        "source": "mshibanami.github.io",
        "published": "",
        "fetched_at": "2026-02-28T23:17:27.212460Z",
        "tags": [
          {
            "name": "transformation",
            "score": 16
          },
          {
            "name": "boundary_crossing",
            "score": 9
          },
          {
            "name": "visibility_gain",
            "score": 5
          },
          {
            "name": "value_redefinition",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 25
          },
          {
            "name": "ontology_shift",
            "score": 8
          }
        ],
        "structural_score": 67,
        "timeliness_score": 1,
        "final_score": 34.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://github.com/ruvnet/ruvector",
        "title": "ruvnet/ruvector",
        "summary": "<p>RuVector is a High Performance, Real-Time, Self-Learning, Vector Graph Neural Network, and Database built in Rust.</p><hr /><h1>RuVector — A Self-Learning, Agentic Operating System</h1> \n<p><a href=\"https://cognitum.one\"><img alt=\"CES 2026 Innovation Award\" src=\"https://img.shields.io/badge/%F0%9F%8F%85_CES_2026-Innovation_Award-gold.svg?sanitize=true\" /></a> <a href=\"https://github.com/ruvnet/ruvector\"><img alt=\"GitHub Trending\" src=\"https://img.shields.io/badge/%F0%9F%94%A5_GitHub-Trending-orange.svg?sanitize=true\" /></a></p> \n<p><a href=\"https://crates.io/crates/ruvector-core\"><img alt=\"Crates.io\" src=\"https://img.shields.io/crates/v/ruvector-core.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/ruvector\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/ruvector.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/ruvector\"><img alt=\"Downloads\" src=\"https://img.shields.io/npm/dt/ruvector.svg?label=Downloads\" /></a> <a href=\"https://www.npmjs.com/package/ruvector\"><img alt=\"Monthly Downloads\" src=\"https://img.shields.io/npm/dm/ruvector.svg?label=Monthly%20Downloads\" /></a> <a href=\"https://ruv.io\"><img alt=\"ruv.io\" src=\"https://img.shields.io/badge/ruv.io-website-purple.svg?sanitize=true\" /></a> <a href=\"https://opensource.org/licenses/MIT\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true\" /></a></p> \n<h3><strong>The self-learning, self-optimizing vector database — with graph intelligence, local AI, and PostgreSQL built in.</strong></h3> \n<blockquote> \n <p>Created by <a href=\"https://ruv.io\">rUv</a> and powering <a href=\"https://cognitum.one\">Cognitum</a>, a 🏅 <strong>CES 2026 Innovation Awards Honoree</strong> — the world's first Agentic Chip designed to be always running for AI agents. Tens of thousands of agents, near-zero power, learns from every signal. <a href=\"https://cognitum.one\">Learn more →</a></p> \n</blockquote> \n<pre><code class=\"language-bash\">npx ruvector\n</code></pre> \n<h4>Most vector databases store your data and search it — the same way, every time.</h4> \n<h4><strong>RuVector</strong> is fundamentally different. It watches how you use it and gets smarter: search results improve automatically, the system tunes itself to your workload, and it runs AI models right on your hardware — no cloud APIs, no per-query bills, GPUs optional, CPUs preferred. It drops into PostgreSQL, runs in browsers, and ships as a single file.</h4> \n<p>Open source. ❤️ Free forever.</p> \n<pre><code>User Query → [SONA Engine] → Model Response → User Feedback\n                  ↑                                 │\n                  └─────── Learning Signal ─────────┘\n                         (&lt; 1ms adaptation)\n</code></pre> \n<details> \n 🔍 RuVector vs Typical Vector Databases (20 differences) \n <table> \n  <thead> \n   <tr> \n    <th></th> \n    <th>RuVector</th> \n    <th>Typical Vector DB</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Self-Learning &amp; Optimization</strong></td> \n    <td></td> \n    <td></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn\">Search quality</a></td> \n    <td>🧠 GNN learns from every query — results improve over time</td> \n    <td>Static — same results every time</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/sona\">Self-optimizing</a></td> \n    <td>⚡ SONA auto-tunes routing, ranking, and compression to your workload</td> \n    <td>Manual tuning required</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention\">46 attention mechanisms</a></td> \n    <td>🎯 Flash, linear, graph, hyperbolic, <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attn-mincut\">mincut-gated</a> (cuts compute 50%)</td> \n    <td>Basic similarity only</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-domain-expansion\">Transfer learning</a></td> \n    <td>🔄 Knowledge transfers across domains — new tasks bootstrap from past learning</td> \n    <td>Start from scratch each time</td> \n   </tr> \n   <tr> \n    <td><strong>Graph &amp; Relationships</strong></td> \n    <td></td> \n    <td></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph\">Graph queries</a></td> \n    <td>🔗 Full Cypher engine — <code>MATCH (a)-[:KNOWS]-&gt;(b)</code> like Neo4j</td> \n    <td>Flat list of results</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-transformer\">Graph transformers</a></td> \n    <td>🔬 8 verified modules: physics, bio, manifold, temporal, economic</td> \n    <td>No graph support</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph\">Hyperedges</a></td> \n    <td>🕸️ Connect 3+ nodes at once — model group relationships natively</td> \n    <td>Pairwise only</td> \n   </tr> \n   <tr> \n    <td><strong>AI &amp; Compute</strong></td> \n    <td></td> \n    <td></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm\">Local LLMs</a></td> \n    <td>🤖 Run models on your hardware — Metal, CUDA, WebGPU, no API costs</td> \n    <td>Cloud API required (pay per call)</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-solver\">Sublinear solvers</a></td> \n    <td>📐 O(log n) PageRank, spectral methods, sparse linear systems</td> \n    <td>Not available</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/dna\">Genomics</a></td> \n    <td>🧬 Variant calling, protein translation, HNSW k-mer search in 12 ms</td> \n    <td>Not available</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruqu\">Quantum coherence</a></td> \n    <td>⚛️ Error correction via dynamic min-cut optimization</td> \n    <td>Not available</td> \n   </tr> \n   <tr> \n    <td><strong>Database &amp; Platform</strong></td> \n    <td></td> \n    <td></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-postgres\">PostgreSQL</a></td> \n    <td>🐘 230+ SQL functions — drop into your existing database, <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/postgres/\">pgvector replacement</a></td> \n    <td>Separate service to manage</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">Deploy anywhere</a></td> \n    <td>🌐 One file — servers, browsers, phones, IoT, bare metal, WASM (58 KB)</td> \n    <td>Cloud server required</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">Cognitive containers</a></td> \n    <td>🚀 Single <code>.rvf</code> file boots as a service in 125 ms — includes vectors, models, kernel</td> \n    <td>Configure a cluster</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-core\">Live updates</a></td> \n    <td>⚡ Update vectors and graph connections instantly, no downtime</td> \n    <td>Rebuild index or wait</td> \n   </tr> \n   <tr> \n    <td><strong>Operations</strong></td> \n    <td></td> \n    <td></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto\">Tamper-proof audit</a></td> \n    <td>🔐 Cryptographic witness chain records every operation automatically</td> \n    <td>Manual logging</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-cow\">Branch your data</a></td> \n    <td>🌿 Git-like COW branching — 1M vectors, 100 edits = ~2.5 MB branch</td> \n    <td>Copy everything</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-replication\">Scale out</a></td> \n    <td>📈 Raft consensus, multi-master replication, auto-sharding</td> \n    <td>Paid tiers, per-vector pricing</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto\">Post-quantum crypto</a></td> \n    <td>🛡️ ML-DSA-65 and Ed25519 signatures on every segment</td> \n    <td>Not available</td> \n   </tr> \n   <tr> \n    <td>Cost</td> \n    <td>💰 Free forever — open source (MIT)</td> \n    <td>Per-query or per-vector pricing</td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<details> \n 📋 See Full Capabilities (75 features across 10 categories) \n <p><strong>Core Vector Database</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>1</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-core\"><strong>Store vectors</strong></a></td> \n    <td>Embeddings from OpenAI, Cohere, local ONNX with HNSW indexing and SIMD acceleration</td> \n   </tr> \n   <tr> \n    <td>2</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph\"><strong>Query with Cypher</strong></a></td> \n    <td>Graph queries like Neo4j — <code>MATCH (a)-[:SIMILAR]-&gt;(b)</code> with hyperedges</td> \n   </tr> \n   <tr> \n    <td>3</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn\"><strong>The index learns</strong></a></td> \n    <td>GNN layers make search results improve over time — every query teaches the system</td> \n   </tr> \n   <tr> \n    <td>4</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-hyperbolic-hnsw\"><strong>Hyperbolic HNSW</strong></a></td> \n    <td>Hierarchy-aware search in Poincare ball space — better for trees and taxonomies</td> \n   </tr> \n   <tr> \n    <td>5</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-temporal-tensor\"><strong>Compress automatically</strong></a></td> \n    <td>2-32x memory reduction with adaptive tiered compression and temporal tensor reuse</td> \n   </tr> \n   <tr> \n    <td>6</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-filter\"><strong>Metadata filtering</strong></a></td> \n    <td>Filter search results by any field before scanning vectors — fast hybrid queries</td> \n   </tr> \n   <tr> \n    <td>7</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-collections\"><strong>Collections</strong></a></td> \n    <td>Multi-tenant, schema-managed collections — isolate data per customer or project</td> \n   </tr> \n   <tr> \n    <td>8</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-snapshot\"><strong>Snapshots</strong></a></td> \n    <td>Point-in-time backups — restore your database to any previous state</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Distributed Systems</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>9</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-raft\"><strong>Raft consensus</strong></a></td> \n    <td>Leader election and log replication — nodes agree on state even when some fail</td> \n   </tr> \n   <tr> \n    <td>10</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-replication\"><strong>Multi-master replication</strong></a></td> \n    <td>Vector clocks, conflict resolution, geo-distributed sync across data centers</td> \n   </tr> \n   <tr> \n    <td>11</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-cluster\"><strong>Cluster management</strong></a></td> \n    <td>Horizontal scaling with consistent hashing — add nodes without rebalancing everything</td> \n   </tr> \n   <tr> \n    <td>12</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-delta-consensus\"><strong>Delta consensus</strong></a></td> \n    <td>Track behavioral changes across distributed nodes with CRDTs and causal ordering</td> \n   </tr> \n   <tr> \n    <td>13</td> \n    <td><strong>Burst scaling</strong></td> \n    <td>10-50x capacity scaling for traffic spikes — absorb load then scale back down</td> \n   </tr> \n   <tr> \n    <td>14</td> \n    <td><strong>Auto-sharding</strong></td> \n    <td>Automatic data partitioning across nodes based on access patterns</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>AI &amp; Machine Learning</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>15</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm\"><strong>Run LLMs locally</strong></a></td> \n    <td>Load GGUF models and run inference on your hardware — Metal, CUDA, ANE, WebGPU</td> \n   </tr> \n   <tr> \n    <td>16</td> \n    <td><a href=\"https://huggingface.co/ruv/ruvltra\"><strong>RuvLTRA models</strong></a></td> \n    <td>Pre-trained GGUF for routing and embeddings in under 10 ms</td> \n   </tr> \n   <tr> \n    <td>17</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/sona\"><strong>SONA learning</strong></a></td> \n    <td>Self-Optimizing Neural Architecture — LoRA fine-tuning + EWC++ memory preservation</td> \n   </tr> \n   <tr> \n    <td>18</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention\"><strong>46 attention mechanisms</strong></a></td> \n    <td>Flash, linear, graph, hyperbolic, mincut-gated (cuts compute 50%)</td> \n   </tr> \n   <tr> \n    <td>19</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-router-core\"><strong>Semantic routing</strong></a></td> \n    <td>Route AI requests to the right model or handler using FastGRNN neural inference</td> \n   </tr> \n   <tr> \n    <td>20</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-sparse-inference\"><strong>Sparse inference</strong></a></td> \n    <td>PowerInfer-style engine — only activate the neurons you need, 2-10x faster on edge</td> \n   </tr> \n   <tr> \n    <td>21</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-tiny-dancer-core\"><strong>Tiny Dancer</strong></a></td> \n    <td>Production-grade agent routing with FastGRNN — lightweight alternative to full LLM</td> \n   </tr> \n   <tr> \n    <td>22</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-domain-expansion\"><strong>Domain expansion</strong></a></td> \n    <td>Cross-domain transfer learning — new tasks bootstrap from past learning automatically</td> \n   </tr> \n   <tr> \n    <td>23</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-math\"><strong>Advanced math</strong></a></td> \n    <td>Optimal transport, Sinkhorn distances, KL divergence, spectral clustering</td> \n   </tr> \n   <tr> \n    <td>24</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-coherence\"><strong>Coherence measurement</strong></a></td> \n    <td>Measure signal quality and compare attention mechanisms objectively</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Graph Transformers</strong> (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-transformer\">8 verified modules</a>)</p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>25</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-verified\"><strong>Proof-gated mutation</strong></a></td> \n    <td>Every write to graph state requires a formal proof — bugs cannot corrupt data</td> \n   </tr> \n   <tr> \n    <td>26</td> \n    <td><strong>Sublinear attention</strong></td> \n    <td>O(n log n) via LSH bucketing, PPR sampling, and spectral sparsification</td> \n   </tr> \n   <tr> \n    <td>27</td> \n    <td><strong>Physics-informed layers</strong></td> \n    <td>Hamiltonian dynamics, gauge equivariant message passing — energy conserved by construction</td> \n   </tr> \n   <tr> \n    <td>28</td> \n    <td><strong>Biological layers</strong></td> \n    <td>Spiking attention, Hebbian/STDP learning, dendritic branching</td> \n   </tr> \n   <tr> \n    <td>29</td> \n    <td><strong>Self-organizing layers</strong></td> \n    <td>Morphogenetic fields, reaction-diffusion growth — graphs that restructure themselves</td> \n   </tr> \n   <tr> \n    <td>30</td> \n    <td><strong>Verified training</strong></td> \n    <td>Training certificates, delta-apply rollback — bad gradient steps auto-reversed</td> \n   </tr> \n   <tr> \n    <td>31</td> \n    <td><strong>Manifold geometry</strong></td> \n    <td>Product manifolds S^n x H^m x R^k — work in curved spaces, not just flat</td> \n   </tr> \n   <tr> \n    <td>32</td> \n    <td><strong>Temporal-causal layers</strong></td> \n    <td>Causal masking, Granger causality extraction, continuous-time ODE integration</td> \n   </tr> \n   <tr> \n    <td>33</td> \n    <td><strong>Economic layers</strong></td> \n    <td>Nash equilibrium attention, Shapley attribution — fair value assignment in multi-agent graphs</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Cognitive Containers</strong> (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">RVF format</a>)</p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>34</td> \n    <td><strong>Self-boot as a microservice</strong></td> \n    <td>A single <code>.rvf</code> file contains vectors, models, and a Linux kernel — boots in 125 ms</td> \n   </tr> \n   <tr> \n    <td>35</td> \n    <td><strong>eBPF acceleration</strong></td> \n    <td>Hot vectors served in kernel data path via XDP, socket filter, and TC programs</td> \n   </tr> \n   <tr> \n    <td>36</td> \n    <td><strong>5.5 KB WASM runtime</strong></td> \n    <td>Same <code>.rvf</code> file runs queries in a browser tab with zero backend</td> \n   </tr> \n   <tr> \n    <td>37</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf\"><strong>COW branching</strong></a></td> \n    <td>Git-like copy-on-write — 1M vectors, 100 edits = ~2.5 MB branch</td> \n   </tr> \n   <tr> \n    <td>38</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto\"><strong>Witness chains</strong></a></td> \n    <td>Tamper-evident hash-linked audit trail records every operation automatically</td> \n   </tr> \n   <tr> \n    <td>39</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto\"><strong>Post-quantum signatures</strong></a></td> \n    <td>ML-DSA-65 and SLH-DSA-128s alongside Ed25519 — future-proof cryptography</td> \n   </tr> \n   <tr> \n    <td>40</td> \n    <td><strong>DNA-style lineage</strong></td> \n    <td>Track parent/child derivation chains with cryptographic hashes</td> \n   </tr> \n   <tr> \n    <td>41</td> \n    <td><strong>25 segment types</strong></td> \n    <td>VEC, INDEX, KERNEL, EBPF, WASM, COW_MAP, WITNESS, CRYPTO, and 17 more</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>PostgreSQL Extension</strong> (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-postgres\">230+ SQL functions</a>)</p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>42</td> \n    <td><strong>Drop-in pgvector replacement</strong></td> \n    <td>Same SQL interface but with self-learning search — no app changes needed</td> \n   </tr> \n   <tr> \n    <td>43</td> \n    <td><strong>Sublinear solvers in SQL</strong></td> \n    <td>PageRank, conjugate gradient, Laplacian solver — O(log n) to O(sqrt(n))</td> \n   </tr> \n   <tr> \n    <td>44</td> \n    <td><strong>Math distances in SQL</strong></td> \n    <td>Wasserstein, Sinkhorn, KL divergence, spectral clustering — all from SQL</td> \n   </tr> \n   <tr> \n    <td>45</td> \n    <td><strong>Topological data analysis</strong></td> \n    <td>Persistent homology, Betti numbers, embedding drift detection</td> \n   </tr> \n   <tr> \n    <td>46</td> \n    <td><strong>SONA learning in SQL</strong></td> \n    <td>Micro-LoRA trajectory learning with EWC++ forgetting prevention</td> \n   </tr> \n   <tr> \n    <td>47</td> \n    <td><strong>Extended attention in SQL</strong></td> \n    <td>O(n) linear, MoE, hyperbolic, sliding window attention — all callable from SQL</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Specialized Processing</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>48</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/scipix\"><strong>SciPix OCR</strong></a></td> \n    <td>Extract LaTeX and MathML from scientific documents and PDFs</td> \n   </tr> \n   <tr> \n    <td>49</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-dag\"><strong>DAG workflows</strong></a></td> \n    <td>Self-learning directed acyclic graph execution for multi-step pipelines</td> \n   </tr> \n   <tr> \n    <td>50</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/cognitum-gate-kernel\"><strong>Cognitum Gate</strong></a></td> \n    <td>Cognitive AI gateway with TileZero acceleration for fast routing</td> \n   </tr> \n   <tr> \n    <td>51</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-fpga-transformer\"><strong>FPGA transformer</strong></a></td> \n    <td>Hardware-accelerated transformer inference on programmable chips</td> \n   </tr> \n   <tr> \n    <td>52</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruQu\"><strong>Quantum coherence</strong></a></td> \n    <td>Error correction via dynamic min-cut optimization for quantum circuits</td> \n   </tr> \n   <tr> \n    <td>53</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-solver\"><strong>Sublinear solvers</strong></a></td> \n    <td>8 algorithms (Neumann, CG, Forward Push, TRUE, BMSSP) — O(log n) to O(sqrt(n))</td> \n   </tr> \n   <tr> \n    <td>54</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-mincut-gated-transformer\"><strong>Mincut-gated transformer</strong></a></td> \n    <td>Dynamic attention that prunes irrelevant connections using graph min-cut</td> \n   </tr> \n   <tr> \n    <td>55</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-nervous-system\"><strong>Nervous system</strong></a></td> \n    <td>5-layer bio-inspired adaptive system with spiking networks and BTSP learning</td> \n   </tr> \n   <tr> \n    <td>56</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/prime-radiant\"><strong>Prime Radiant</strong></a></td> \n    <td>Coherence engine using sheaf Laplacian math for AI safety and hallucination detection</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Genomics &amp; Health</strong> (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/dna\">rvDNA</a>)</p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>57</td> \n    <td><strong>rvDNA genomic analysis</strong></td> \n    <td>Variant calling, protein translation, HNSW k-mer search in 12 ms</td> \n   </tr> \n   <tr> \n    <td>58</td> \n    <td><strong><code>.rvdna</code> file format</strong></td> \n    <td>AI-native binary with pre-computed vectors, tensors, and embeddings</td> \n   </tr> \n   <tr> \n    <td>59</td> \n    <td><strong>Instant diagnostics</strong></td> \n    <td>Sickle cell, cancer mutations, drug dosing — runs on any device</td> \n   </tr> \n   <tr> \n    <td>60</td> \n    <td><strong>Privacy-first WASM</strong></td> \n    <td>Browser-based genomics — your DNA data never leaves the device</td> \n   </tr> \n   <tr> \n    <td>61</td> \n    <td><strong>Biomarker engine</strong></td> \n    <td>Composite polygenic risk scoring (20 SNPs, 6 gene-gene interactions, 2 us)</td> \n   </tr> \n   <tr> \n    <td>62</td> \n    <td><strong>Streaming biomarkers</strong></td> \n    <td>Real-time anomaly detection, CUSUM changepoints, trend analysis (&gt;100k readings/sec)</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Platform &amp; Integration</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>63</td> \n    <td><strong>Run anywhere</strong></td> \n    <td>Rust, Node.js, browser (WASM), edge (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvlite\">rvLite</a>), HTTP server, bare metal</td> \n   </tr> \n   <tr> \n    <td>64</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/mcp-gate\"><strong>MCP server</strong></a></td> \n    <td>Model Context Protocol for AI assistants — Claude, GPT, and other agents can use RuVector as a tool</td> \n   </tr> \n   <tr> \n    <td>65</td> \n    <td><strong>Cloud deployment</strong></td> \n    <td>One-click deploy to <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/google-cloud\">Google Cloud Run</a>, Kubernetes</td> \n   </tr> \n   <tr> \n    <td>66</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/app-clip\"><strong>iOS App Clip</strong></a></td> \n    <td>Scan a QR code to load an RVF cognitive seed on your phone — under 15 MB</td> \n   </tr> \n   <tr> \n    <td>67</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-metrics\"><strong>Prometheus metrics</strong></a></td> \n    <td>Built-in monitoring — export latency, throughput, and memory stats to Grafana</td> \n   </tr> \n   <tr> \n    <td>68</td> \n    <td><strong>90+ Rust crates + npm packages</strong></td> \n    <td>Published on <a href=\"https://crates.io/crates/rvf-runtime\">crates.io</a> and <a href=\"https://www.npmjs.com/package/@ruvector/rvf\">npm</a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Examples &amp; Applications</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>#</th> \n    <th>Capability</th> \n    <th>What It Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>69</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/neural-trader\"><strong>Neural Trader</strong></a></td> \n    <td>Algorithmic trading with Kelly Criterion position sizing and LSTM-Transformer prediction</td> \n   </tr> \n   <tr> \n    <td>70</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/meta-cognition-spiking-neural-network\"><strong>Spiking Neural Network</strong></a></td> \n    <td>Hybrid AI combining spiking networks, SIMD vector ops, and 5 attention types</td> \n   </tr> \n   <tr> \n    <td>71</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/refrag-pipeline\"><strong>ReFrag Pipeline</strong></a></td> \n    <td>Compress-Sense-Expand architecture — ~30x RAG latency reduction</td> \n   </tr> \n   <tr> \n    <td>72</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge-net\"><strong>Edge Network</strong></a></td> \n    <td>Distributed collective AI — share idle compute across devices</td> \n   </tr> \n   <tr> \n    <td>73</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/vibecast-7sense\"><strong>Vibecast 7Sense</strong></a></td> \n    <td>Transform bird calls into navigable geometric space using vector search</td> \n   </tr> \n   <tr> \n    <td>74</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/ultra-low-latency-sim\"><strong>Ultra-Low Latency Sim</strong></a></td> \n    <td>Meta-simulation achieving quadrillion simulations per second on CPU with SIMD</td> \n   </tr> \n   <tr> \n    <td>75</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/verified-applications\"><strong>Verified Applications</strong></a></td> \n    <td>10 exotic proof-carrying apps: weapons filters, legal forensics, medical diagnostics</td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<h3>Built by rUv, powered by <a href=\"https://cognitum.one\">Cognitum.one</a></h3> \n<details> \n <strong>Cognitum Hardware — The Agentic Appliance &amp; Chip</strong> \n <p><strong>Cognitum v0 — The Agentic Appliance</strong>: Run tens of thousands of always-on agents at no incremental cost beyond the box. Learns in proximity to any signal — sensors, networks, machines — at near-zero power (~5 uW/inference, &lt;15W total). Sub-millisecond response, 500x cheaper than cloud AI. No cloud bills, no per-agent fees. Like a nervous system, not a brain.</p> \n <p><strong>Cognitum v1 — The Agentic Chip</strong>: Same architecture on a single 257-core custom chip. Runs on less than 2W — a AA battery. Idle-to-8 GHz burst on demand, 2 TB/s interconnect, built-in encryption per core.</p> \n</details> \n<h3>A Complete Agentic AI Operating System</h3> \n<p>RuVector isn't a database you add to your stack — it's the entire stack. Self-learning, self-optimizing, and self-deploying. Everything an AI application needs to run, from bare metal hardware up to the application layer, in one package:</p> \n<p><strong>Intelligence</strong></p> \n<table> \n <thead> \n  <tr> \n   <th></th> \n   <th>Layer</th> \n   <th>Replaces</th> \n   <th>What It Does</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>🔄</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/sona/README.md\"><strong>Self-Learning</strong></a></td> \n   <td>Manual retraining, MLOps</td> \n   <td>SONA adapts in &lt;1 ms — LoRA fine-tuning + EWC++ memory on every request</td> \n  </tr> \n  <tr> \n   <td>⚡</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn/README.md\"><strong>Self-Optimizing</strong></a></td> \n   <td>Manual tuning, config files</td> \n   <td>Auto-tunes routing, ranking, compression, and index parameters</td> \n  </tr> \n  <tr> \n   <td>🎯</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm/README.md\"><strong>Embeddings</strong></a></td> \n   <td>OpenAI API, Cohere, static models</td> \n   <td>Contrastive training, triplet loss, real-time fine-tuning — embeddings improve as you use them</td> \n  </tr> \n  <tr> \n   <td>✅</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-verified/README.md\"><strong>Verified Training</strong></a></td> \n   <td>Manual validation</td> \n   <td>Formal proofs + statistical tests on every training step — gradients only apply if invariants pass</td> \n  </tr> \n </tbody> \n</table> \n<p><strong>Data &amp; Search</strong></p> \n<table> \n <thead> \n  <tr> \n   <th></th> \n   <th>Layer</th> \n   <th>Replaces</th> \n   <th>What It Does</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>🔍</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-core/README.md\"><strong>Search</strong></a></td> \n   <td>Pinecone, Weaviate, Qdrant</td> \n   <td>Self-learning HNSW — GNN improves results from every query</td> \n  </tr> \n  <tr> \n   <td>🗄️</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-core/README.md\"><strong>Storage</strong></a></td> \n   <td>Separate database + cache</td> \n   <td>Vector store, graph DB, key-value cache — unified engine</td> \n  </tr> \n  <tr> \n   <td>🐘</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-postgres/README.md\"><strong>PostgreSQL</strong></a></td> \n   <td>pgvector, pg_embedding</td> \n   <td>Drop-in replacement — 230+ SQL functions, same interface but search gets smarter over time</td> \n  </tr> \n  <tr> \n   <td>🔗</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph/README.md\"><strong>Graph</strong></a></td> \n   <td>Neo4j, Amazon Neptune</td> \n   <td>Cypher, W3C SPARQL 1.1, hyperedges — all built in</td> \n  </tr> \n </tbody> \n</table> \n<p><strong>AI &amp; ML</strong></p> \n<table> \n <thead> \n  <tr> \n   <th></th> \n   <th>Layer</th> \n   <th>Replaces</th> \n   <th>What It Does</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>🤖</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm/README.md\"><strong>AI Runtime</strong></a></td> \n   <td>llama.cpp, vLLM, Ollama</td> \n   <td>ruvllm — GGUF models, MicroLoRA (&lt;1 ms), speculative decoding, continuous batching, WASM</td> \n  </tr> \n  <tr> \n   <td>🧠</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention/README.md\"><strong>ML Framework</strong></a></td> \n   <td>PyTorch, TensorFlow</td> \n   <td>46 attention types, 8 graph transformers, spiking networks, sparse inference, sublinear solvers</td> \n  </tr> \n  <tr> \n   <td>🔬</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-mincut/README.md\"><strong>Coherence</strong></a></td> \n   <td>Manual testing, guardrails</td> \n   <td>Min-cut finds the weakest links in any network — detects AI drift, prunes wasted compute (50% reduction), keeps agents in sync</td> \n  </tr> \n  <tr> \n   <td>🧬</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-domain-expansion/README.md\"><strong>Domain Models</strong></a></td> \n   <td>Custom ML pipelines</td> \n   <td>Genomics (DNA variant calling), physics simulation, economic modeling, biological networks</td> \n  </tr> \n </tbody> \n</table> \n<p><strong>Infrastructure</strong></p> \n<table> \n <thead> \n  <tr> \n   <th></th> \n   <th>Layer</th> \n   <th>Replaces</th> \n   <th>What It Does</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>🔧</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-fpga-transformer/README.md\"><strong>Hardware</strong></a></td> \n   <td>CUDA toolkit, driver configs</td> \n   <td>Sparse/spiking CPU (AVX-512, NEON) — GPU for bursts (Metal, CUDA, ANE, WebGPU, FPGA)</td> \n  </tr> \n  <tr> \n   <td>🐧</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\"><strong>Kernel</strong></a></td> \n   <td>Linux + Docker + eBPF</td> \n   <td><code>.rvf</code> file boots its own kernel in 125 ms — eBPF accelerates hot paths</td> \n  </tr> \n  <tr> \n   <td>🌐</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-raft/README.md\"><strong>Coordination</strong></a></td> \n   <td>etcd, ZooKeeper, Consul</td> \n   <td>Raft consensus, multi-master replication, CRDT delta sync, auto-sharding</td> \n  </tr> \n  <tr> \n   <td>📦</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\"><strong>Packaging</strong></a></td> \n   <td>Docker, Kubernetes</td> \n   <td>One <code>.rvf</code> file = your entire service — servers, browsers, phones, IoT, bare metal</td> \n  </tr> \n </tbody> \n</table> \n<p><strong>Routing &amp; Observability</strong></p> \n<table> \n <thead> \n  <tr> \n   <th></th> \n   <th>Layer</th> \n   <th>Replaces</th> \n   <th>What It Does</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>🚦</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-tiny-dancer-core/README.md\"><strong>Routing</strong></a></td> \n   <td>API gateways, LLM routers</td> \n   <td>Semantic routing (Tiny Dancer), MCP protocol gateway, agent-to-agent discovery</td> \n  </tr> \n  <tr> \n   <td>📊</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-profiler/README.md\"><strong>Observability</strong></a></td> \n   <td>Datadog, Prometheus</td> \n   <td>Latency/power/memory profiling, coherence scoring, real-time metrics</td> \n  </tr> \n  <tr> \n   <td>🛡️</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/cognitum-gate-tilezero/README.md\"><strong>Safety</strong></a></td> \n   <td>Manual review, guardrails</td> \n   <td>Cognitum Gate — 256-tile WASM fabric, Permit/Defer/Deny in &lt;1 ms, witness receipts</td> \n  </tr> \n </tbody> \n</table> \n<p><strong>Security &amp; Trust</strong></p> \n<table> \n <thead> \n  <tr> \n   <th></th> \n   <th>Layer</th> \n   <th>Replaces</th> \n   <th>What It Does</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>🔐</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto/README.md\"><strong>Crypto</strong></a></td> \n   <td>Vault, manual audit logs</td> \n   <td>Post-quantum (ML-DSA-65, Ed25519), SHAKE-256, witness chains, hardware attestation</td> \n  </tr> \n  <tr> \n   <td>📜</td> \n   <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto/README.md\"><strong>Lineage</strong></a></td> \n   <td>No equivalent</td> \n   <td>Every operation recorded in a tamper-proof chain — full provenance from creation to deployment</td> \n  </tr> \n </tbody> \n</table> \n<p>The <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">RVF cognitive container</a> ties it all together: a single file that packages your vectors, models, data, and a bootable kernel. Drop it on any machine and it starts serving in 125 ms — no install, no dependencies. It branches like Git (only changes are copied), logs every operation in a tamper-proof chain, and runs anywhere from a browser to bare metal.</p> \n<hr /> \n<h2>How the GNN Works</h2> \n<p>Traditional vector search:</p> \n<pre><code>Query → HNSW Index → Top K Results\n</code></pre> \n<p>RuVector with GNN:</p> \n<pre><code>Query → HNSW Index → GNN Layer → Enhanced Results\n                ↑                      │\n                └──── learns from ─────┘\n</code></pre> \n<p>The GNN layer:</p> \n<ol> \n <li>Takes your query and its nearest neighbors</li> \n <li>Applies multi-head attention to weigh which neighbors matter</li> \n <li>Updates representations based on graph structure</li> \n <li>Returns better-ranked results — all in under 1ms</li> \n</ol> \n<p>This is <strong>temporal learning</strong> — the system learns from the sequence and timing of queries, not just their content. A query asked right after another carries context. Patterns that repeat get reinforced. Paths that lead to good results get stronger over time. The result: search gets faster and more accurate the more you use it, adapting in real time without retraining.</p> \n<details> \n <strong>Deep Dive: How Self-Learning Search Actually Works</strong> \n <h3>The Problem with Normal Search</h3> \n <p>Every vector database does the same thing: you give it a query, it finds the closest matches by distance, and returns them. The results never change. Search the same thing a thousand times and you get the same answer a thousand times — even if the first result was wrong and you always clicked the third one instead.</p> \n <p>RuVector is different. It watches what happens <em>after</em> the search and uses that to make the next search better.</p> \n <h3>What the GNN Actually Does</h3> \n <p>Think of your data as a city map. Each vector is a building, and the HNSW index creates roads between similar buildings. A normal search just walks the shortest road to find nearby buildings.</p> \n <p>The GNN is like a local who knows the shortcuts. It looks at the neighborhood around your destination and says: \"Yes, that building is close, but <em>this</em> one over here is what you actually want.\" It learns these shortcuts by watching which results people actually use.</p> \n <p><strong>Technically, it works in three steps:</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Step</th> \n    <th>What Happens</th> \n    <th>Plain English</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>1. Message Passing</strong></td> \n    <td>Each node collects information from its HNSW neighbors</td> \n    <td>\"Ask the neighborhood what they know\"</td> \n   </tr> \n   <tr> \n    <td><strong>2. Attention Weighting</strong></td> \n    <td>Multi-head attention scores which neighbors matter most for this specific query</td> \n    <td>\"Some neighbors are more helpful than others — figure out which ones\"</td> \n   </tr> \n   <tr> \n    <td><strong>3. Representation Update</strong></td> \n    <td>Node representations shift based on what the neighborhood says</td> \n    <td>\"Update your understanding based on what you learned\"</td> \n   </tr> \n  </tbody> \n </table> \n <p>This entire process takes <strong>under 1ms</strong> thanks to SIMD acceleration (processing 4-8 numbers at once instead of one at a time).</p> \n <h3>Temporal Learning: Time Matters</h3> \n <p>Most AI systems treat every input as independent — they don't know or care what happened 5 seconds ago. RuVector tracks the <em>sequence</em> and <em>timing</em> of queries, which reveals patterns that individual queries can't:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Pattern</th> \n    <th>What It Reveals</th> \n    <th>How RuVector Adapts</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Same user searches A then B within seconds</td> \n    <td>A and B are related, even if they're far apart in vector space</td> \n    <td>Strengthens the path between A and B</td> \n   </tr> \n   <tr> \n    <td>Many users skip result #1 and click result #3</td> \n    <td>Result #3 is actually more relevant</td> \n    <td>GNN learns to rank #3 higher next time</td> \n   </tr> \n   <tr> \n    <td>Query bursts around a topic at certain times</td> \n    <td>Temporal relevance — some things matter more at certain times</td> \n    <td>Boosts recently-active paths</td> \n   </tr> \n   <tr> \n    <td>A query that follows a specific sequence</td> \n    <td>Context from previous queries changes what \"good results\" means</td> \n    <td>Attention weights shift based on session context</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Three Types of Learning</h3> \n <p>RuVector learns at three different speeds simultaneously:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Speed</th> \n    <th>Mechanism</th> \n    <th>What It Does</th> \n    <th>Latency</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Instant</strong></td> \n    <td>MicroLoRA adaptation</td> \n    <td>Adjusts weights for this specific request based on immediate feedback</td> \n    <td>&lt;1ms</td> \n   </tr> \n   <tr> \n    <td><strong>Session</strong></td> \n    <td>GNN attention updates</td> \n    <td>Reinforces paths that led to good results during this session</td> \n    <td>~10ms (background)</td> \n   </tr> \n   <tr> \n    <td><strong>Long-term</strong></td> \n    <td>EWC++ consolidation</td> \n    <td>Permanently strengthens important patterns without forgetting old ones</td> \n    <td>~100ms (background)</td> \n   </tr> \n  </tbody> \n </table> \n <p>The key innovation is <strong>EWC++ (Elastic Weight Consolidation)</strong> — it solves the \"catastrophic forgetting\" problem. Without it, learning new patterns would erase old ones. EWC++ identifies which weights are important for existing knowledge and protects them while still allowing new learning.</p> \n <h3>Why It's Fast: The HNSW Shortcut</h3> \n <p>The GNN doesn't run on your entire dataset. It only runs on the small subgraph of HNSW neighbors that are relevant to the current query — typically 10-50 nodes out of millions. This is why it adds under 1ms of latency instead of seconds:</p> \n <pre><code>1M vectors in your database\n    → HNSW finds ~50 candidate neighbors        (0.3ms)\n    → GNN re-ranks those 50 with attention       (0.4ms)\n    → Return top K results                       (0.1ms)\n    ──────────────────────────────────────────\n    Total: &lt;1ms, and results improve over time\n</code></pre> \n <h3>What Improves Over Time</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Metric</th> \n    <th>Day 1</th> \n    <th>After 1K Queries</th> \n    <th>After 100K Queries</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Recall@10</strong></td> \n    <td>Baseline (HNSW only)</td> \n    <td>+5-8%</td> \n    <td>+12.4%</td> \n   </tr> \n   <tr> \n    <td><strong>Query latency</strong></td> \n    <td>~0.8ms</td> \n    <td>~0.7ms (hot paths cached)</td> \n    <td>~0.5ms (optimized routing)</td> \n   </tr> \n   <tr> \n    <td><strong>Relevance</strong></td> \n    <td>Distance-based only</td> \n    <td>Learns user preferences</td> \n    <td>Personalized per query pattern</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Three GNN Architectures (Pick One or Stack Them)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Architecture</th> \n    <th>Best For</th> \n    <th>How It Works</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>GCN</strong> (Graph Convolutional Network)</td> \n    <td>General-purpose re-ranking</td> \n    <td>Averages neighbor information — simple, fast, effective</td> \n   </tr> \n   <tr> \n    <td><strong>GAT</strong> (Graph Attention Network)</td> \n    <td>Queries where some neighbors matter more than others</td> \n    <td>Learns <em>which</em> neighbors to pay attention to per query</td> \n   </tr> \n   <tr> \n    <td><strong>GraphSAGE</strong></td> \n    <td>Datasets that change frequently (new vectors added often)</td> \n    <td>Can score new vectors it's never seen before, without retraining</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Runs Everywhere</h3> \n <p>The same GNN code runs natively in Rust, in Node.js via NAPI-RS bindings, and in the browser via WebAssembly. Models trained on the server can be exported and run client-side — a user's browser can do personalized re-ranking without sending queries to a server.</p> \n</details> \n<h2>Quick Start</h2> \n<h3>One-Line Install</h3> \n<pre><code class=\"language-bash\"># Interactive installer - lists all packages\nnpx ruvector install\n\n# Or install directly\nnpm install ruvector\nnpx ruvector\n\n# Self-learning hooks for Claude Code\nnpx @ruvector/cli hooks init\nnpx @ruvector/cli hooks install\n\n# LLM runtime (SONA learning, HNSW memory)\nnpm install @ruvector/ruvllm\n</code></pre> \n<h3>Node.js / Browser</h3> \n<pre><code class=\"language-bash\"># Install\nnpm install ruvector\n\n# Or try instantly\nnpx ruvector\n</code></pre> \n<hr /> \n<h3>Ecosystem: AI Agent Orchestration</h3> \n<p>RuVector powers two major AI orchestration platforms:</p> \n<table> \n <thead> \n  <tr> \n   <th>Platform</th> \n   <th>Purpose</th> \n   <th>Install</th> \n   <th>Downloads</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><a href=\"https://github.com/ruvnet/claude-flow\"><strong>ruFlo</strong></a></td> \n   <td>Enterprise multi-agent orchestration for Claude Code</td> \n   <td><code>npx ruvflo@latest</code></td> \n   <td><a href=\"https://www.npmjs.com/package/claude-flow\"><img alt=\"npm downloads\" src=\"https://img.shields.io/npm/dt/claude-flow.svg?sanitize=true\" /></a></td> \n  </tr> \n  <tr> \n   <td><a href=\"https://github.com/ruvnet/agentic-flow\"><strong>Agentic-Flow</strong></a></td> \n   <td>Run AI agents on any cloud with any model — Claude, GPT, Gemini, or local</td> \n   <td><code>npx agentic-flow@latest</code></td> \n   <td><a href=\"https://www.npmjs.com/package/agentic-flow\"><img alt=\"npm downloads\" src=\"https://img.shields.io/npm/dt/agentic-flow.svg?sanitize=true\" /></a></td> \n  </tr> \n  <tr> \n   <td><a href=\"https://github.com/ruvnet/agentdb\"><strong>AgentDB</strong></a></td> \n   <td>Give AI agents long-term memory that gets smarter over time</td> \n   <td><code>npm install agentdb@alpha</code></td> \n   <td><a href=\"https://www.npmjs.com/package/agentdb\"><img alt=\"npm downloads\" src=\"https://img.shields.io/npm/dt/agentdb.svg?sanitize=true\" /></a></td> \n  </tr> \n </tbody> \n</table> \n<details> \n <strong>Claude-Flow v3</strong> — Turn Claude Code into a collaborative AI team \n <p><strong>54+ specialized agents</strong> working together on complex software engineering tasks:</p> \n <pre><code class=\"language-bash\"># Install\nnpx ruvflo@latest init --wizard\n\n# Spawn a swarm\nnpx ruvflo@latest swarm init --topology hierarchical --max-agents 8\n</code></pre> \n <p><strong>Key Features:</strong></p> \n <ul> \n  <li><strong>SONA Learning</strong>: Sub-50ms adaptive routing, learns optimal patterns over time</li> \n  <li><strong>Queen-led Swarms</strong>: Byzantine fault-tolerant consensus with 5 protocols (Raft, Gossip, CRDT)</li> \n  <li><strong>HNSW Memory</strong>: 150x-12,500x faster pattern retrieval via RuVector</li> \n  <li><strong>175+ MCP Tools</strong>: Native Model Context Protocol integration</li> \n  <li><strong>Cost Optimization</strong>: 3-tier routing extends Claude Code quota by 2.5x</li> \n  <li><strong>Security</strong>: AIDefence threat detection (&lt;10ms), prompt injection blocking</li> \n </ul> \n</details> \n<details> \n <strong>Agentic-Flow v2</strong> — Production AI agents for any cloud \n <p><strong>66 self-learning agents</strong> with Claude Agent SDK, deployable to any cloud:</p> \n <pre><code class=\"language-bash\"># Install\nnpx agentic-flow@latest\n\n# Or with npm\nnpm install agentic-flow\n</code></pre> \n <p><strong>Key Features:</strong></p> \n <ul> \n  <li><strong>SONA Architecture</strong>: &lt;1ms adaptive learning, +55% quality improvement</li> \n  <li><strong>Flash Attention</strong>: 2.49x JS speedup, 7.47x with NAPI bindings</li> \n  <li><strong>213 MCP Tools</strong>: Swarm management, memory, GitHub integration</li> \n  <li><strong>Agent Booster</strong>: 352x faster code editing for simple transforms</li> \n  <li><strong>Multi-Provider</strong>: Claude, GPT, Gemini, Cohere, local models with failover</li> \n  <li><strong>Graph Reasoning</strong>: GNN query refinement with +12.4% recall improvement</li> \n </ul> \n</details> \n<details> \n <strong>rvDNA</strong> — AI-native genomic diagnostics, instant and available to everyone \n <p><strong>Using AI to make the world a healthier place.</strong> rvDNA puts genomic diagnostics on any device — a phone, a laptop, a browser tab — in 12 milliseconds. No cloud, no GPU, no subscription. Private by default.</p> \n <pre><code class=\"language-bash\">cargo add rvdna              # Rust\nnpm install @ruvector/rvdna  # JavaScript / TypeScript\n</code></pre> \n <table> \n  <thead> \n   <tr> \n    <th>What It Does</th> \n    <th>How</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Find mutations (sickle cell, cancer)</td> \n    <td>Bayesian variant calling, 155 ns/SNP</td> \n   </tr> \n   <tr> \n    <td>Translate DNA to protein</td> \n    <td>Full codon table + GNN contact graphs</td> \n   </tr> \n   <tr> \n    <td>Predict biological age</td> \n    <td>Horvath clock, 353 CpG sites</td> \n   </tr> \n   <tr> \n    <td>Recommend drug doses</td> \n    <td>CYP2D6 star alleles + CPIC guidelines</td> \n   </tr> \n   <tr> \n    <td>Score health risks</td> \n    <td>20 SNPs, 6 gene-gene interactions, composite risk scoring in 2 us</td> \n   </tr> \n   <tr> \n    <td>Stream biomarker data</td> \n    <td>Real-time anomaly detection, CUSUM changepoints, &gt;100k readings/sec</td> \n   </tr> \n   <tr> \n    <td>Search genomes by similarity</td> \n    <td>HNSW k-mer vectors, O(log N)</td> \n   </tr> \n   <tr> \n    <td>Store pre-computed AI features</td> \n    <td><code>.rvdna</code> binary format — open and instant</td> \n   </tr> \n  </tbody> \n </table> \n <ul> \n  <li><strong>Rust crate</strong>: <a href=\"https://crates.io/crates/rvdna\">crates.io/crates/rvdna</a></li> \n  <li><strong>npm package</strong>: <a href=\"https://www.npmjs.com/package/@ruvector/rvdna\">@ruvector/rvdna</a> (NAPI-RS native + JS fallback)</li> \n  <li><strong>Source</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/dna\">examples/dna</a></li> \n </ul> \n</details> \n<details> \n <strong>RVF Cognitive Containers</strong> — One file that stores, boots, and proves everything \n <p><strong><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">RVF (RuVector Format)</a></strong> is a universal binary substrate that merges database, model, graph engine, kernel, and attestation into a single deployable file. A <code>.rvf</code> file can store vector embeddings, carry LoRA adapter deltas, embed GNN graph state, include a bootable Linux microkernel, run queries in a 5.5 KB WASM runtime, and prove every operation through a cryptographic witness chain — all in one file that runs anywhere from a browser to bare metal.</p> \n <p>This is not a database format. It is an <strong>executable knowledge unit</strong>.</p> \n <pre><code class=\"language-bash\">cargo install rvf-cli                          # CLI tool\ncargo add rvf-runtime                          # Rust library\nnpm install @ruvector/rvf                      # TypeScript SDK\nnpx @ruvector/rvf-mcp-server --transport stdio # MCP server for AI agents\n</code></pre> \n <table> \n  <thead> \n   <tr> \n    <th>What It Does</th> \n    <th>How</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Self-boot as a microservice</td> \n    <td>Real Linux kernel in the file, boots in 125 ms on QEMU/KVM</td> \n   </tr> \n   <tr> \n    <td>Hardware-speed lookups</td> \n    <td>eBPF programs (XDP, TC, socket filter) bypass userspace entirely</td> \n   </tr> \n   <tr> \n    <td>Run in any browser</td> \n    <td>5.5 KB WASM runtime, zero backend</td> \n   </tr> \n   <tr> \n    <td>Git-like branching</td> \n    <td>COW at cluster granularity — 1M vectors, 100 edits = ~2.5 MB child</td> \n   </tr> \n   <tr> \n    <td>Tamper-evident audit</td> \n    <td>Hash-linked witness chain for every insert, query, and deletion</td> \n   </tr> \n   <tr> \n    <td>Post-quantum signatures</td> \n    <td>ML-DSA-65 and Ed25519 signing on every segment</td> \n   </tr> \n   <tr> \n    <td>DNA-style lineage</td> \n    <td>Parent/child derivation chains with cryptographic verification</td> \n   </tr> \n   <tr> \n    <td>28 segment types</td> \n    <td>VEC, INDEX, KERNEL, EBPF, WASM, COW_MAP, WITNESS, CRYPTO, FEDERATED_MANIFEST, and 19 more</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Rust crates</strong> (23): <a href=\"https://crates.io/crates/rvf-types\"><code>rvf-types</code></a> <code>rvf-wire</code> <code>rvf-manifest</code> <code>rvf-quant</code> <code>rvf-index</code> <code>rvf-crypto</code> <a href=\"https://crates.io/crates/rvf-runtime\"><code>rvf-runtime</code></a> <code>rvf-kernel</code> <code>rvf-ebpf</code> <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-federation\"><code>rvf-federation</code></a> <code>rvf-launch</code> <code>rvf-server</code> <code>rvf-import</code> <a href=\"https://crates.io/crates/rvf-cli\"><code>rvf-cli</code></a> <code>rvf-wasm</code> <code>rvf-solver-wasm</code> <code>rvf-node</code> + 6 adapters (claude-flow, agentdb, ospipe, agentic-flow, rvlite, sona)</p> \n <p><strong>npm packages</strong> (4): <a href=\"https://www.npmjs.com/package/@ruvector/rvf\"><code>@ruvector/rvf</code></a> <a href=\"https://www.npmjs.com/package/@ruvector/rvf-node\"><code>@ruvector/rvf-node</code></a> <a href=\"https://www.npmjs.com/package/@ruvector/rvf-wasm\"><code>@ruvector/rvf-wasm</code></a> <a href=\"https://www.npmjs.com/package/@ruvector/rvf-mcp-server\"><code>@ruvector/rvf-mcp-server</code></a></p> \n <ul> \n  <li><strong>Security Hardened RVF</strong> (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/security_hardened.rvf\"><code>examples/security_hardened.rvf</code></a>) — 2.1 MB sealed artifact with 22 verified capabilities: TEE attestation (SGX/SEV-SNP/TDX/ARM CCA), AIDefence (injection/jailbreak/PII/exfil), hardened Linux microkernel, eBPF firewall, Ed25519 signing, 6-role RBAC, Coherence Gate, 30-entry witness chain, Paranoid policy, COW branching, audited k-NN. See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-042-Security-RVF-AIDefence-TEE.md\">ADR-042</a>.</li> \n  <li><strong>Full documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">crates/rvf/README.md</a></li> \n  <li><strong>ADR-030</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-030-rvf-cognitive-container.md\">Cognitive Container Architecture</a></li> \n  <li><strong>ADR-031</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-031-rvcow-branching-and-real-cognitive-containers.md\">COW Branching &amp; Real Containers</a></li> \n  <li><strong>ADR-042</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-042-Security-RVF-AIDefence-TEE.md\">Security RVF — AIDefence + TEE</a></li> \n  <li><strong>56 runnable examples</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/rvf/examples/\">examples/rvf/examples/</a></li> \n </ul> \n</details> \n<details> \n <strong>Sublinear-Time Solver</strong> — math that gets faster as your data gets bigger \n <p><strong><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-solver/README.md\">ruvector-solver</a></strong> solves large math problems (like ranking pages, finding connections in graphs, or computing AI attention) in a fraction of the time traditional solvers need. Where standard approaches slow down dramatically with scale (doubling data = 8x slower), RuVector's 8 specialized algorithms barely notice the increase (doubling data = barely any slower). This is what powers the self-learning engine — fast graph math is what lets search improve in real time instead of waiting minutes to retrain.</p> \n <pre><code class=\"language-bash\">cargo add ruvector-solver --features all-algorithms\n</code></pre> \n <table> \n  <thead> \n   <tr> \n    <th>Algorithm</th> \n    <th>Complexity</th> \n    <th>Best For</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Neumann Series</strong></td> \n    <td>O(k · nnz)</td> \n    <td>Diagonally dominant, fast convergence</td> \n   </tr> \n   <tr> \n    <td><strong>Conjugate Gradient</strong></td> \n    <td>O(√κ · log(1/ε) · nnz)</td> \n    <td>Gold-standard SPD solver</td> \n   </tr> \n   <tr> \n    <td><strong>Forward Push</strong></td> \n    <td>O(1/ε)</td> \n    <td>Single-source PageRank</td> \n   </tr> \n   <tr> \n    <td><strong>Backward Push</strong></td> \n    <td>O(1/ε)</td> \n    <td>Reverse relevance computation</td> \n   </tr> \n   <tr> \n    <td><strong>Hybrid Random Walk</strong></td> \n    <td>O(√n/ε)</td> \n    <td>Pairwise relevance, Monte Carlo</td> \n   </tr> \n   <tr> \n    <td><strong>TRUE</strong></td> \n    <td>O(log n) amortized</td> \n    <td>Large-scale Laplacian systems</td> \n   </tr> \n   <tr> \n    <td><strong>BMSSP</strong></td> \n    <td>O(nnz · log n)</td> \n    <td>Multigrid hierarchical solve</td> \n   </tr> \n   <tr> \n    <td><strong>Auto Router</strong></td> \n    <td>Automatic</td> \n    <td>Selects optimal algorithm</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Key optimizations</strong>: AVX2 SIMD SpMV, fused residual kernels, bounds-check elimination, arena allocator</p> \n <p><strong>Supporting crates</strong>:</p> \n <ul> \n  <li> <p><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attn-mincut/README.md\"><code>ruvector-attn-mincut</code></a> — Min-cut gating as alternative to softmax attention</p> </li> \n  <li> <p><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-coherence/README.md\"><code>ruvector-coherence</code></a> — Coherence measurement for attention comparison</p> </li> \n  <li> <p><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-profiler/README.md\"><code>ruvector-profiler</code></a> — Memory, power, and latency benchmarking</p> </li> \n  <li> <p><strong>177 tests</strong> | 5 Criterion benchmarks | WASM + NAPI bindings</p> </li> \n  <li> <p><strong>ADR documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/research/sublinear-time-solver/\">docs/research/sublinear-time-solver/</a></p> </li> \n </ul> \n</details> \n<hr /> \n<h2>How RuVector Compares</h2> \n<p>See how RuVector stacks up against popular vector databases across 40+ features — from latency and graph queries to self-learning, cognitive containers, and PostgreSQL integration.</p> \n<details> \n 📊 Comparison with Other Vector Databases \n <p>Grouped comparison across 10 categories. RuVector is the only vector database that learns from usage, runs AI locally, and ships as a single self-booting file.</p> \n <p><strong>Performance &amp; Storage</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>Pinecone</th> \n    <th>Qdrant</th> \n    <th>Milvus</th> \n    <th>ChromaDB</th> \n    <th>Weaviate</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Latency (p50)</td> \n    <td><strong>61 us</strong></td> \n    <td>~2 ms</td> \n    <td>~1 ms</td> \n    <td>~5 ms</td> \n    <td>~50 ms</td> \n    <td>~5 ms</td> \n   </tr> \n   <tr> \n    <td>Memory (1M vectors)</td> \n    <td><strong>200 MB</strong>*</td> \n    <td>2 GB</td> \n    <td>1.5 GB</td> \n    <td>1 GB</td> \n    <td>3 GB</td> \n    <td>1.5 GB</td> \n   </tr> \n   <tr> \n    <td>SIMD acceleration</td> \n    <td>AVX-512, NEON</td> \n    <td>Partial</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>Partial</td> \n   </tr> \n   <tr> \n    <td>Auto-compression</td> \n    <td>2-32x adaptive</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>PQ only</td> \n   </tr> \n   <tr> \n    <td>Temporal tensor compression</td> \n    <td>4-10x reuse</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Sparse vectors (BM25/TF-IDF)</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>✅</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Search &amp; Query</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>Pinecone</th> \n    <th>Qdrant</th> \n    <th>Milvus</th> \n    <th>ChromaDB</th> \n    <th>Weaviate</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Vector similarity search</td> \n    <td>✅ HNSW</td> \n    <td>✅</td> \n    <td>✅ HNSW</td> \n    <td>✅ HNSW</td> \n    <td>✅</td> \n    <td>✅ HNSW</td> \n   </tr> \n   <tr> \n    <td>Metadata filtering</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n   </tr> \n   <tr> \n    <td>Graph queries (Cypher)</td> \n    <td>✅ full engine</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>SPARQL/RDF (W3C 1.1)</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Hyperedges (3+ node)</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Hyperbolic embeddings</td> \n    <td>Poincare + Lorentz</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Multi-tenancy</td> \n    <td>✅ collections</td> \n    <td>✅ namespaces</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Self-Learning &amp; AI</strong> — features unique to RuVector</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>All Others</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>GNN on HNSW — search improves from usage</td> \n    <td>✅ every query teaches the index</td> \n    <td>❌ static index</td> \n   </tr> \n   <tr> \n    <td>SONA runtime adaptation</td> \n    <td>✅ LoRA + EWC++ auto-tuning</td> \n    <td>❌ manual tuning</td> \n   </tr> \n   <tr> \n    <td>46 attention mechanisms</td> \n    <td>Flash, linear, graph, hyperbolic, mincut-gated</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Semantic routing (Tiny Dancer)</td> \n    <td>FastGRNN neural agent routing</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Sparse inference (PowerInfer-style)</td> \n    <td>2-10x faster on edge devices</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Domain expansion</td> \n    <td>Cross-domain transfer learning with bandits</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Self-learning hooks</td> \n    <td>Q-learning, neural patterns, HNSW memory</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>ReasoningBank</td> \n    <td>Trajectory learning with verdict judgment</td> \n    <td>❌</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Local AI — no cloud APIs needed</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>Pinecone</th> \n    <th>Qdrant</th> \n    <th>Milvus</th> \n    <th>ChromaDB</th> \n    <th>Weaviate</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Built-in LLM runtime</td> \n    <td>✅ ruvllm (GGUF)</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Hardware acceleration</td> \n    <td>Metal, CUDA, ANE, WebGPU</td> \n    <td>N/A</td> \n    <td>N/A</td> \n    <td>GPU indexing</td> \n    <td>N/A</td> \n    <td>N/A</td> \n   </tr> \n   <tr> \n    <td>Pre-trained models</td> \n    <td><a href=\"https://huggingface.co/ruv/ruvltra\">RuvLTRA</a> (&lt;10 ms)</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Local ONNX embeddings</td> \n    <td>8+ models, no API calls</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>text2vec modules</td> \n   </tr> \n   <tr> \n    <td>MCP server for AI agents</td> \n    <td>✅ mcp-gate</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Graph Transformers</strong> — verified graph neural network modules</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>All Others</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Proof-gated mutation</td> \n    <td>Every write requires a formal proof — bugs cannot corrupt</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Sublinear attention</td> \n    <td>O(n log n) via LSH, PPR, spectral sparsification</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Physics-informed layers</td> \n    <td>Hamiltonian dynamics, energy conserved by construction</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Biological layers</td> \n    <td>Spiking, Hebbian/STDP, dendritic branching</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Manifold geometry</td> \n    <td>Product manifolds S^n x H^m x R^k</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Temporal-causal layers</td> \n    <td>Granger causality, continuous-time ODE</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Economic layers</td> \n    <td>Nash equilibrium, Shapley attribution</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Verified training</td> \n    <td>Certificates, delta-apply rollback, fail-closed</td> \n    <td>❌</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Math &amp; Solvers</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>Pinecone</th> \n    <th>Qdrant</th> \n    <th>Milvus</th> \n    <th>ChromaDB</th> \n    <th>Weaviate</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Sublinear solvers (8 algorithms)</td> \n    <td>O(log n) to O(sqrt(n))</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Dynamic min-cut</td> \n    <td>n^0.12 complexity</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Optimal transport distances</td> \n    <td>Wasserstein, Sinkhorn, KL</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Topological data analysis</td> \n    <td>Persistent homology, Betti numbers</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Coherence measurement</td> \n    <td>Prime Radiant sheaf Laplacian</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Quantum error correction</td> \n    <td>ruQu dynamic min-cut</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Distributed Systems</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>Pinecone</th> \n    <th>Qdrant</th> \n    <th>Milvus</th> \n    <th>ChromaDB</th> \n    <th>Weaviate</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Raft consensus</td> \n    <td>✅</td> \n    <td>❌ managed</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>✅</td> \n   </tr> \n   <tr> \n    <td>Multi-master replication</td> \n    <td>✅ vector clocks</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>✅</td> \n   </tr> \n   <tr> \n    <td>Auto-sharding</td> \n    <td>✅ consistent hashing</td> \n    <td>✅ managed</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>✅</td> \n   </tr> \n   <tr> \n    <td>Delta consensus (CRDT)</td> \n    <td>✅ causal ordering</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Burst scaling (10-50x)</td> \n    <td>✅</td> \n    <td>✅ managed</td> \n    <td>❌</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Snapshots / backups</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>✅</td> \n   </tr> \n   <tr> \n    <td>Streaming API</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>✅</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Cognitive Containers (RVF)</strong> — single-file deployment unique to RuVector</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>All Others</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Self-booting microservice</td> \n    <td><code>.rvf</code> file boots in 125 ms with Linux kernel</td> \n    <td>❌ requires server setup</td> \n   </tr> \n   <tr> \n    <td>eBPF acceleration</td> \n    <td>XDP, socket filter, TC kernel data path</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>COW branching</td> \n    <td>Git-like — 1M vectors, 100 edits = ~2.5 MB branch</td> \n    <td>❌ copy everything</td> \n   </tr> \n   <tr> \n    <td>Witness chains</td> \n    <td>Tamper-evident hash-linked audit trail</td> \n    <td>❌ manual logging</td> \n   </tr> \n   <tr> \n    <td>Post-quantum signatures</td> \n    <td>ML-DSA-65, SLH-DSA-128s, Ed25519</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>25 segment types</td> \n    <td>VEC, INDEX, KERNEL, EBPF, WASM, COW_MAP, and 19 more</td> \n    <td>❌</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Platform &amp; Deployment</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>Pinecone</th> \n    <th>Qdrant</th> \n    <th>Milvus</th> \n    <th>ChromaDB</th> \n    <th>Weaviate</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Browser / WASM</td> \n    <td>✅ WebGPU, 58 KB</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Edge standalone</td> \n    <td>✅ rvLite</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Node.js native</td> \n    <td>✅ NAPI-RS</td> \n    <td>❌</td> \n    <td>Client only</td> \n    <td>Client only</td> \n    <td>✅</td> \n    <td>Client only</td> \n   </tr> \n   <tr> \n    <td>PostgreSQL extension</td> \n    <td>✅ 230+ SQL functions</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>iOS App Clip</td> \n    <td>✅ QR → RVF in &lt;15 MB</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Cloud deployment</td> \n    <td>Cloud Run, Kubernetes</td> \n    <td>Managed only</td> \n    <td>Docker, K8s</td> \n    <td>Docker, K8s</td> \n    <td>Docker</td> \n    <td>Docker, K8s</td> \n   </tr> \n   <tr> \n    <td>FPGA acceleration</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Prometheus metrics</td> \n    <td>✅ built-in</td> \n    <td>Dashboard</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>❌</td> \n    <td>✅</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Specialized Applications</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>RuVector</th> \n    <th>All Others</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Genomics (rvDNA)</td> \n    <td>Variant calling, k-mer search in 12 ms, browser WASM</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Neural trading</td> \n    <td>Kelly Criterion + LSTM-Transformer prediction</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Scientific OCR (SciPix)</td> \n    <td>LaTeX/MathML extraction from papers</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Spiking neural networks</td> \n    <td>Neuromorphic computing, BTSP learning</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Bio-inspired nervous system</td> \n    <td>5-layer adaptive system with EWC plasticity</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>DAG workflows</td> \n    <td>Self-learning directed graph execution</td> \n    <td>❌</td> \n   </tr> \n   <tr> \n    <td>Cognitum Gate</td> \n    <td>Cognitive AI gateway with TileZero acceleration</td> \n    <td>❌</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Licensing &amp; Cost</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th></th> \n    <th>RuVector</th> \n    <th>Pinecone</th> \n    <th>Qdrant</th> \n    <th>Milvus</th> \n    <th>ChromaDB</th> \n    <th>Weaviate</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>License</td> \n    <td>MIT (free forever)</td> \n    <td>Proprietary</td> \n    <td>Apache 2.0</td> \n    <td>Apache 2.0</td> \n    <td>Apache 2.0</td> \n    <td>BSD-3</td> \n   </tr> \n   <tr> \n    <td>Self-hosted</td> \n    <td>✅</td> \n    <td>❌ managed only</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n    <td>✅</td> \n   </tr> \n   <tr> \n    <td>Pricing model</td> \n    <td>Free</td> \n    <td>Per-vector/query</td> \n    <td>Free + Cloud</td> \n    <td>Free + managed</td> \n    <td>Free + Cloud</td> \n    <td>Free + Cloud</td> \n   </tr> \n  </tbody> \n </table> \n <p>* Memory with PQ8 compression. Benchmarks on Apple M2 / Intel i7.</p> \n</details> \n<h2>Features</h2> \n<p>Everything RuVector can do — organized by category. Vector search, graph queries, LLM inference, distributed systems, deployment targets, and the self-learning stack that ties it all together.</p> \n<details> \n ⚡ Core Features &amp; Capabilities \n <h3>Core Capabilities</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Vector Search</strong></td> \n    <td>HNSW index, &lt;0.5ms latency, SIMD acceleration</td> \n    <td>Fast enough for real-time apps</td> \n   </tr> \n   <tr> \n    <td><strong>Cypher Queries</strong></td> \n    <td><code>MATCH</code>, <code>WHERE</code>, <code>CREATE</code>, <code>RETURN</code></td> \n    <td>Familiar Neo4j syntax</td> \n   </tr> \n   <tr> \n    <td><strong>GNN Layers</strong></td> \n    <td>Neural network on index topology</td> \n    <td>Search improves with usage</td> \n   </tr> \n   <tr> \n    <td><strong>Hyperedges</strong></td> \n    <td>Connect 3+ nodes at once</td> \n    <td>Model complex relationships</td> \n   </tr> \n   <tr> \n    <td><strong>Metadata Filtering</strong></td> \n    <td>Filter vectors by properties</td> \n    <td>Combine semantic + structured search</td> \n   </tr> \n   <tr> \n    <td><strong>Collections</strong></td> \n    <td>Namespace isolation, multi-tenancy</td> \n    <td>Organize vectors by project/user</td> \n   </tr> \n   <tr> \n    <td><strong>Hyperbolic HNSW</strong></td> \n    <td>Poincaré ball indexing for hierarchies</td> \n    <td>Better tree/taxonomy embeddings</td> \n   </tr> \n   <tr> \n    <td><strong>Sparse Vectors</strong></td> \n    <td>BM25/TF-IDF hybrid search</td> \n    <td>Combine keyword + semantic</td> \n   </tr> \n  </tbody> \n </table> \n <h3>LLM Runtime</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>ruvllm</strong></td> \n    <td>Local LLM inference with GGUF models</td> \n    <td>Run AI without cloud APIs</td> \n   </tr> \n   <tr> \n    <td><strong>Metal/CUDA/ANE</strong></td> \n    <td>Hardware acceleration on Mac/NVIDIA/Apple</td> \n    <td>10-50x faster inference</td> \n   </tr> \n   <tr> \n    <td><strong>ruvllm-wasm</strong></td> \n    <td>Browser LLM with WebGPU acceleration</td> \n    <td>Client-side AI, zero latency</td> \n   </tr> \n   <tr> \n    <td><strong>RuvLTRA Models</strong></td> \n    <td>Pre-trained GGUF for routing &amp; embeddings</td> \n    <td>&lt;10ms inference → <a href=\"https://huggingface.co/ruv/ruvltra\">HuggingFace</a></td> \n   </tr> \n   <tr> \n    <td><strong>Streaming Tokens</strong></td> \n    <td>Real-time token generation</td> \n    <td>Responsive chat UX</td> \n   </tr> \n   <tr> \n    <td><strong>Quantization</strong></td> \n    <td>Q4, Q5, Q8 model support</td> \n    <td>Run 7B models in 4GB RAM</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\">npm install @ruvector/ruvllm        # Node.js\ncargo add ruvllm                    # Rust\n</code></pre> \n <h3>Platform &amp; Edge</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">RVF Cognitive Container</a></strong></td> \n    <td>Single <code>.rvf</code> file: store, boot, branch, prove</td> \n    <td>Replaces Docker + DB + audit system</td> \n   </tr> \n   <tr> \n    <td><strong>rvLite</strong></td> \n    <td>Standalone 2MB edge database</td> \n    <td>IoT, mobile, embedded</td> \n   </tr> \n   <tr> \n    <td><strong>PostgreSQL Extension</strong></td> \n    <td>77+ SQL functions, pgvector replacement</td> \n    <td>Drop-in upgrade for existing DBs</td> \n   </tr> \n   <tr> \n    <td><strong>MCP Server</strong></td> \n    <td>Model Context Protocol integration</td> \n    <td>AI assistant tool calling</td> \n   </tr> \n   <tr> \n    <td><strong>WASM/Browser</strong></td> \n    <td>Full client-side vector search</td> \n    <td>Offline-first apps</td> \n   </tr> \n   <tr> \n    <td><strong>Node.js Bindings</strong></td> \n    <td>Native napi-rs, zero-copy</td> \n    <td>No serialization overhead</td> \n   </tr> \n   <tr> \n    <td><strong>HTTP/gRPC Server</strong></td> \n    <td>REST API with streaming</td> \n    <td>Easy microservice integration</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\">cargo install rvf-cli                    # RVF CLI (17 commands)\ncargo add rvf-runtime                    # RVF Rust library\nnpm install @ruvector/rvf                # RVF TypeScript SDK\ndocker pull ruvnet/ruvector-postgres     # PostgreSQL\nnpm install rvlite                       # Edge DB\nnpx ruvector mcp start                   # MCP Server\n</code></pre> \n <h3>Distributed Systems</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Raft Consensus</strong></td> \n    <td>Leader election, log replication</td> \n    <td>Strong consistency for metadata</td> \n   </tr> \n   <tr> \n    <td><strong>Auto-Sharding</strong></td> \n    <td>Consistent hashing, shard migration</td> \n    <td>Scale to billions of vectors</td> \n   </tr> \n   <tr> \n    <td><strong>Multi-Master Replication</strong></td> \n    <td>Write to any node, conflict resolution</td> \n    <td>High availability, no SPOF</td> \n   </tr> \n   <tr> \n    <td><strong>Snapshots</strong></td> \n    <td>Point-in-time backups, incremental</td> \n    <td>Disaster recovery</td> \n   </tr> \n   <tr> \n    <td><strong>Cluster Metrics</strong></td> \n    <td>Prometheus-compatible monitoring</td> \n    <td>Observability at scale</td> \n   </tr> \n   <tr> \n    <td><strong>Burst Scaling</strong></td> \n    <td>10-50x capacity for traffic spikes</td> \n    <td>Handle viral moments</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\">cargo add ruvector-raft ruvector-cluster ruvector-replication\n</code></pre> \n <h3>AI &amp; ML</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Tensor Compression</strong></td> \n    <td>f32→f16→PQ8→PQ4→Binary</td> \n    <td>2-32x memory reduction</td> \n   </tr> \n   <tr> \n    <td><strong>Differentiable Search</strong></td> \n    <td>Soft attention k-NN</td> \n    <td>End-to-end trainable</td> \n   </tr> \n   <tr> \n    <td><strong>Semantic Router</strong></td> \n    <td>Route queries to optimal endpoints</td> \n    <td>Multi-model AI orchestration</td> \n   </tr> \n   <tr> \n    <td><strong>Hybrid Routing</strong></td> \n    <td>Keyword-first + embedding fallback</td> \n    <td><strong>90% accuracy</strong> for agent routing</td> \n   </tr> \n   <tr> \n    <td><strong>Tiny Dancer</strong></td> \n    <td>FastGRNN neural inference</td> \n    <td>Optimize LLM inference costs</td> \n   </tr> \n   <tr> \n    <td><strong>Adaptive Routing</strong></td> \n    <td>Learn optimal routing strategies</td> \n    <td>Minimize latency, maximize accuracy</td> \n   </tr> \n   <tr> \n    <td><strong>SONA</strong></td> \n    <td>Two-tier LoRA + EWC++ + ReasoningBank</td> \n    <td>Runtime learning without retraining</td> \n   </tr> \n   <tr> \n    <td><strong>Local Embeddings</strong></td> \n    <td>8+ ONNX models built-in</td> \n    <td>No external API needed</td> \n   </tr> \n   <tr> \n    <td><strong><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-verified\">Verified Proofs</a></strong></td> \n    <td>82-byte proof attestations per vector op</td> \n    <td>Structural trust, not just assertions</td> \n   </tr> \n   <tr> \n    <td><strong><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-transformer\">Graph Transformers</a></strong></td> \n    <td>8 proof-gated modules: physics, bio, manifold, temporal, economic</td> \n    <td>Every graph mutation is mathematically verified</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Specialized Processing</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>SciPix OCR</strong></td> \n    <td>LaTeX/MathML from scientific docs</td> \n    <td>Index research papers</td> \n   </tr> \n   <tr> \n    <td><strong>DAG Workflows</strong></td> \n    <td>Self-learning directed acyclic graphs</td> \n    <td>Complex pipeline orchestration</td> \n   </tr> \n   <tr> \n    <td><strong>Cognitum Gate</strong></td> \n    <td>Cognitive AI gateway + TileZero</td> \n    <td>Unified AI model routing</td> \n   </tr> \n   <tr> \n    <td><strong>FPGA Transformer</strong></td> \n    <td>Hardware-accelerated inference</td> \n    <td>Ultra-low latency serving</td> \n   </tr> \n   <tr> \n    <td><strong>ruQu Quantum</strong></td> \n    <td>Quantum error correction via min-cut</td> \n    <td>Future-proof algorithms</td> \n   </tr> \n   <tr> \n    <td><strong>Mincut-Gated Transformer</strong></td> \n    <td>Dynamic attention via graph optimization</td> \n    <td><strong>50% compute reduction</strong></td> \n   </tr> \n   <tr> \n    <td><strong>Sparse Inference</strong></td> \n    <td>Efficient sparse matrix operations</td> \n    <td>10x faster for sparse data</td> \n   </tr> \n   <tr> \n    <td><strong>Sublinear Solver</strong></td> \n    <td>8 sparse algorithms, O(log n)</td> \n    <td>Powers coherence, GNN, spectral</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Self-Learning &amp; Adaptation</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Self-Learning Hooks</strong></td> \n    <td>Q-learning + neural patterns + HNSW</td> \n    <td>System improves automatically</td> \n   </tr> \n   <tr> \n    <td><strong>ReasoningBank</strong></td> \n    <td>Trajectory learning with verdict judgment</td> \n    <td>Learn from successes/failures</td> \n   </tr> \n   <tr> \n    <td><strong>Economy System</strong></td> \n    <td>Tokenomics, CRDT-based distributed state</td> \n    <td>Incentivize agent behavior</td> \n   </tr> \n   <tr> \n    <td><strong>Nervous System</strong></td> \n    <td>Event-driven reactive architecture</td> \n    <td>Real-time adaptation</td> \n   </tr> \n   <tr> \n    <td><strong>Agentic Synthesis</strong></td> \n    <td>Multi-agent workflow composition</td> \n    <td>Emergent problem solving</td> \n   </tr> \n   <tr> \n    <td><strong>EWC++</strong></td> \n    <td>Elastic weight consolidation</td> \n    <td>Prevent catastrophic forgetting</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\">npx @ruvector/cli hooks init      # Install self-learning hooks\nnpx @ruvector/cli hooks install   # Configure for Claude Code\n</code></pre> \n <h3>Attention Mechanisms (<code>@ruvector/attention</code>)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>46 Mechanisms</strong></td> \n    <td>Dot-product, multi-head, flash, linear, sparse, cross-attention, CGT sheaf</td> \n    <td>Cover all transformer and GNN use cases</td> \n   </tr> \n   <tr> \n    <td><strong>Graph Attention</strong></td> \n    <td>RoPE, edge-featured, local-global, neighborhood</td> \n    <td>Purpose-built for graph neural networks</td> \n   </tr> \n   <tr> \n    <td><strong>Hyperbolic Attention</strong></td> \n    <td>Poincaré ball operations, curved-space math</td> \n    <td>Better embeddings for hierarchical data</td> \n   </tr> \n   <tr> \n    <td><strong>SIMD Optimized</strong></td> \n    <td>Native Rust with AVX2/NEON acceleration</td> \n    <td>2-10x faster than pure JS</td> \n   </tr> \n   <tr> \n    <td><strong>Streaming &amp; Caching</strong></td> \n    <td>Chunk-based processing, KV-cache</td> \n    <td>Constant memory, 10x faster inference</td> \n   </tr> \n  </tbody> \n </table> \n <blockquote> \n  <p><strong>Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention/README.md\">Attention Module Docs</a></p> \n </blockquote> \n <h4>Core Attention Mechanisms</h4> \n <p>Standard attention layers for sequence modeling and transformers.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Mechanism</th> \n    <th>Complexity</th> \n    <th>Memory</th> \n    <th>Best For</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>DotProductAttention</strong></td> \n    <td>O(n²)</td> \n    <td>O(n²)</td> \n    <td>Basic attention for small-medium sequences</td> \n   </tr> \n   <tr> \n    <td><strong>MultiHeadAttention</strong></td> \n    <td>O(n²·h)</td> \n    <td>O(n²·h)</td> \n    <td>BERT, GPT-style transformers</td> \n   </tr> \n   <tr> \n    <td><strong>FlashAttention</strong></td> \n    <td>O(n²)</td> \n    <td>O(n)</td> \n    <td>Long sequences with limited GPU memory</td> \n   </tr> \n   <tr> \n    <td><strong>LinearAttention</strong></td> \n    <td>O(n·d)</td> \n    <td>O(n·d)</td> \n    <td>8K+ token sequences, real-time streaming</td> \n   </tr> \n   <tr> \n    <td><strong>HyperbolicAttention</strong></td> \n    <td>O(n²)</td> \n    <td>O(n²)</td> \n    <td>Tree-like data: taxonomies, org charts</td> \n   </tr> \n   <tr> \n    <td><strong>MoEAttention</strong></td> \n    <td>O(n·k)</td> \n    <td>O(n·k)</td> \n    <td>Large models with sparse expert routing</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Graph Attention Mechanisms</h4> \n <p>Attention layers designed for graph-structured data and GNNs.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Mechanism</th> \n    <th>Complexity</th> \n    <th>Best For</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>GraphRoPeAttention</strong></td> \n    <td>O(n²)</td> \n    <td>Position-aware graph transformers</td> \n   </tr> \n   <tr> \n    <td><strong>EdgeFeaturedAttention</strong></td> \n    <td>O(n²·e)</td> \n    <td>Molecules, knowledge graphs with edge data</td> \n   </tr> \n   <tr> \n    <td><strong>DualSpaceAttention</strong></td> \n    <td>O(n²)</td> \n    <td>Hybrid flat + hierarchical embeddings</td> \n   </tr> \n   <tr> \n    <td><strong>LocalGlobalAttention</strong></td> \n    <td>O(n·k + n)</td> \n    <td>100K+ node graphs, scalable GNNs</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Specialized Mechanisms</h4> \n <p>Task-specific attention variants for efficiency and multi-modal learning.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Mechanism</th> \n    <th>Type</th> \n    <th>Best For</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>SparseAttention</strong></td> \n    <td>Efficiency</td> \n    <td>Long docs, low-memory inference</td> \n   </tr> \n   <tr> \n    <td><strong>CrossAttention</strong></td> \n    <td>Multi-modal</td> \n    <td>Image-text, encoder-decoder models</td> \n   </tr> \n   <tr> \n    <td><strong>NeighborhoodAttention</strong></td> \n    <td>Graph</td> \n    <td>Local message passing in GNNs</td> \n   </tr> \n   <tr> \n    <td><strong>HierarchicalAttention</strong></td> \n    <td>Structure</td> \n    <td>Multi-level docs (section → paragraph)</td> \n   </tr> \n   <tr> \n    <td><strong>CGTSheafAttention</strong></td> \n    <td>Coherence</td> \n    <td>Consistency-gated graph transformers</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Hyperbolic Math Functions</h4> \n <p>Operations for Poincaré ball embeddings—curved space that naturally represents hierarchies.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Function</th> \n    <th>Description</th> \n    <th>Use Case</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>expMap(v, c)</code></td> \n    <td>Map to hyperbolic space</td> \n    <td>Initialize embeddings</td> \n   </tr> \n   <tr> \n    <td><code>logMap(p, c)</code></td> \n    <td>Map to flat space</td> \n    <td>Compute gradients</td> \n   </tr> \n   <tr> \n    <td><code>mobiusAddition(x, y, c)</code></td> \n    <td>Add vectors in curved space</td> \n    <td>Aggregate features</td> \n   </tr> \n   <tr> \n    <td><code>poincareDistance(x, y, c)</code></td> \n    <td>Measure hyperbolic distance</td> \n    <td>Compute similarity</td> \n   </tr> \n   <tr> \n    <td><code>projectToPoincareBall(p, c)</code></td> \n    <td>Ensure valid coordinates</td> \n    <td>Prevent numerical errors</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Async &amp; Batch Operations</h4> \n <p>Utilities for high-throughput inference and training optimization.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Operation</th> \n    <th>Description</th> \n    <th>Performance</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>asyncBatchCompute()</code></td> \n    <td>Process batches in parallel</td> \n    <td>3-5x faster</td> \n   </tr> \n   <tr> \n    <td><code>streamingAttention()</code></td> \n    <td>Process in chunks</td> \n    <td>Fixed memory usage</td> \n   </tr> \n   <tr> \n    <td><code>HardNegativeMiner</code></td> \n    <td>Find hard training examples</td> \n    <td>Better contrastive learning</td> \n   </tr> \n   <tr> \n    <td><code>AttentionCache</code></td> \n    <td>Cache key-value pairs</td> \n    <td>10x faster inference</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\"># Install attention module\nnpm install @ruvector/attention\n\n# CLI commands\nnpx ruvector attention list                    # List all 39 mechanisms\nnpx ruvector attention info flash              # Details on FlashAttention\nnpx ruvector attention benchmark               # Performance comparison\nnpx ruvector attention compute -t dot -d 128   # Run attention computation\nnpx ruvector attention hyperbolic -a distance -v \"[0.1,0.2]\" -b \"[0.3,0.4]\"\n</code></pre> \n <h3>Coherence Gate (<code>prime-radiant</code>)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Sheaf Laplacian</strong></td> \n    <td>Measures consistency via E(S) = Σ wₑ · ‖ρᵤ(xᵤ) - ρᵥ(xᵥ)‖²</td> \n    <td>Mathematical proof of coherence</td> \n   </tr> \n   <tr> \n    <td><strong>Compute Ladder</strong></td> \n    <td>Reflex (&lt;1ms) → Retrieval (~10ms) → Heavy (~100ms) → Human</td> \n    <td>Route by confidence level</td> \n   </tr> \n   <tr> \n    <td><strong>LLM Hallucination Gate</strong></td> \n    <td>Block incoherent responses with witnesses</td> \n    <td>Refuse generation when math says contradiction</td> \n   </tr> \n   <tr> \n    <td><strong>GPU/SIMD Acceleration</strong></td> \n    <td>wgpu + AVX-512/NEON + vec4 WGSL kernels</td> \n    <td>4-16x speedup on coherence checks</td> \n   </tr> \n   <tr> \n    <td><strong>Governance Audit</strong></td> \n    <td>Blake3 hash chain, cryptographic witnesses</td> \n    <td>Every decision is provable</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Coherence vs Confidence</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Traditional AI</th> \n    <th>Prime-Radiant</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>\"I'm 85% confident\"</td> \n    <td>\"Zero contradictions found\"</td> \n   </tr> \n   <tr> \n    <td>Can be confidently wrong</td> \n    <td>Knows when it doesn't know</td> \n   </tr> \n   <tr> \n    <td>Guesses about the future</td> \n    <td>Proves consistency right now</td> \n   </tr> \n   <tr> \n    <td>Trust the model</td> \n    <td>Trust the math</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Compute Ladder Routing</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Energy</th> \n    <th>Lane</th> \n    <th>Latency</th> \n    <th>Action</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>&lt; 0.1</td> \n    <td>Reflex</td> \n    <td>&lt; 1ms</td> \n    <td>Immediate approval</td> \n   </tr> \n   <tr> \n    <td>0.1-0.4</td> \n    <td>Retrieval</td> \n    <td>~10ms</td> \n    <td>Fetch more evidence</td> \n   </tr> \n   <tr> \n    <td>0.4-0.7</td> \n    <td>Heavy</td> \n    <td>~100ms</td> \n    <td>Deep analysis</td> \n   </tr> \n   <tr> \n    <td>&gt; 0.7</td> \n    <td>Human</td> \n    <td>async</td> \n    <td>Escalate to review</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\"># Install coherence engine\ncargo add prime-radiant\n\n# With GPU acceleration\ncargo add prime-radiant --features gpu,simd\n</code></pre> \n</details> \n<h2>Deployment</h2> \n<p>Run RuVector wherever your application lives — as a server, a PostgreSQL extension, a browser library, an edge database, or a self-booting container.</p> \n<details> \n 🚀 Deployment Options \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>HTTP/gRPC Server</strong></td> \n    <td>REST API, streaming support</td> \n    <td>Easy integration</td> \n   </tr> \n   <tr> \n    <td><strong>WASM/Browser</strong></td> \n    <td>Full client-side support</td> \n    <td>Run AI search offline</td> \n   </tr> \n   <tr> \n    <td><strong>Node.js Bindings</strong></td> \n    <td>Native napi-rs bindings</td> \n    <td>No serialization overhead</td> \n   </tr> \n   <tr> \n    <td><strong>FFI Bindings</strong></td> \n    <td>C-compatible interface</td> \n    <td>Use from Python, Go, etc.</td> \n   </tr> \n   <tr> \n    <td><strong>CLI Tools</strong></td> \n    <td>Benchmarking, testing, management</td> \n    <td>DevOps-friendly</td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<h2>Performance</h2> \n<p>Real numbers from real benchmarks — measured on Apple M4 Pro (48GB RAM) with Criterion.rs statistical sampling.</p> \n<details> \n 📈 Performance Benchmarks \n <h3>Vector Search (HNSW)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Configuration</th> \n    <th>QPS</th> \n    <th>p50 Latency</th> \n    <th>p99 Latency</th> \n    <th>Recall</th> \n    <th>Dataset</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Single-threaded</strong></td> \n    <td>394</td> \n    <td>1.80ms</td> \n    <td>1.84ms</td> \n    <td>100%</td> \n    <td>50K vectors, 384D</td> \n   </tr> \n   <tr> \n    <td><strong>Multi-threaded (16)</strong></td> \n    <td>3,597</td> \n    <td>2.86ms</td> \n    <td>8.47ms</td> \n    <td>100%</td> \n    <td>50K vectors, 384D</td> \n   </tr> \n   <tr> \n    <td><strong>Optimized (SIMD)</strong></td> \n    <td>1,216</td> \n    <td>0.78ms</td> \n    <td>0.78ms</td> \n    <td>100%</td> \n    <td>10K vectors, 384D</td> \n   </tr> \n   <tr> \n    <td>Python baseline</td> \n    <td>77</td> \n    <td>11.88ms</td> \n    <td>11.88ms</td> \n    <td>100%</td> \n    <td>10K vectors, 384D</td> \n   </tr> \n   <tr> \n    <td>Brute force</td> \n    <td>12</td> \n    <td>77.76ms</td> \n    <td>77.76ms</td> \n    <td>100%</td> \n    <td>10K vectors, 384D</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>15.7x faster than Python</strong> — 100% recall at every configuration.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Search k</th> \n    <th>p50 Latency</th> \n    <th>Throughput</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>k=1</td> \n    <td>18.9µs</td> \n    <td>53K QPS</td> \n   </tr> \n   <tr> \n    <td>k=10</td> \n    <td>25.2µs</td> \n    <td>40K QPS</td> \n   </tr> \n   <tr> \n    <td>k=100</td> \n    <td>77.9µs</td> \n    <td>13K QPS</td> \n   </tr> \n  </tbody> \n </table> \n <h3>SIMD Distance Calculations (NEON)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Metric</th> \n    <th>128D</th> \n    <th>384D</th> \n    <th>768D</th> \n    <th>1536D</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Euclidean</strong></td> \n    <td>14.9ns (67M/s)</td> \n    <td>55.3ns (18M/s)</td> \n    <td>115.3ns (8.7M/s)</td> \n    <td>279.6ns (3.6M/s)</td> \n   </tr> \n   <tr> \n    <td><strong>Cosine</strong></td> \n    <td>16.4ns (61M/s)</td> \n    <td>60.4ns (17M/s)</td> \n    <td>128.8ns (7.8M/s)</td> \n    <td>302.9ns (3.3M/s)</td> \n   </tr> \n   <tr> \n    <td><strong>Dot Product</strong></td> \n    <td>12.0ns (83M/s)</td> \n    <td>52.7ns (19M/s)</td> \n    <td>112.2ns (8.9M/s)</td> \n    <td>292.3ns (3.4M/s)</td> \n   </tr> \n  </tbody> \n </table> \n <p>SIMD speedup: <strong>2.9x</strong> (Euclidean/Dot Product), <strong>5.95x</strong> (Cosine).</p> \n <h3>Quantization</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Mode</th> \n    <th>Compression</th> \n    <th>Encode (384D)</th> \n    <th>Distance (384D)</th> \n    <th>Memory per 1M vectors</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>None (f32)</strong></td> \n    <td>1x</td> \n    <td>—</td> \n    <td>55.3ns</td> \n    <td>1.46 GB</td> \n   </tr> \n   <tr> \n    <td><strong>Scalar (INT8)</strong></td> \n    <td>4x</td> \n    <td>213ns</td> \n    <td>31ns</td> \n    <td>366 MB</td> \n   </tr> \n   <tr> \n    <td><strong>INT4</strong></td> \n    <td>8x</td> \n    <td>—</td> \n    <td>—</td> \n    <td>183 MB</td> \n   </tr> \n   <tr> \n    <td><strong>Binary</strong></td> \n    <td>32x</td> \n    <td>208ns</td> \n    <td>0.9ns</td> \n    <td>46 MB</td> \n   </tr> \n  </tbody> \n </table> \n <p>Binary quantization: <strong>sub-nanosecond</strong> distance calculations.</p> \n <h3>Insert Throughput</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Operation</th> \n    <th>Latency</th> \n    <th>Throughput</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Single insert (384D)</strong></td> \n    <td>4.63ms</td> \n    <td>216/s</td> \n   </tr> \n   <tr> \n    <td><strong>Batch 100</strong></td> \n    <td>34.1ms</td> \n    <td>2,928/s</td> \n   </tr> \n   <tr> \n    <td><strong>Batch 500</strong></td> \n    <td>72.8ms</td> \n    <td>6,865/s</td> \n   </tr> \n   <tr> \n    <td><strong>Batch 1000</strong></td> \n    <td>152.0ms</td> \n    <td>6,580/s</td> \n   </tr> \n  </tbody> \n </table> \n <p>Batch inserts: <strong>30x faster</strong> than single inserts.</p> \n <h3>LLM Inference (ruvllm)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Operation</th> \n    <th>Configuration</th> \n    <th>Latency</th> \n    <th>vs Target</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Flash Attention</strong></td> \n    <td>256 seq</td> \n    <td>840µs</td> \n    <td>2.4x better than 2ms target</td> \n   </tr> \n   <tr> \n    <td><strong>RMSNorm</strong></td> \n    <td>4096 dim</td> \n    <td>620ns</td> \n    <td>16x better than 10µs target</td> \n   </tr> \n   <tr> \n    <td><strong>GEMV</strong></td> \n    <td>4096x4096</td> \n    <td>1.36ms</td> \n    <td>3.7x better than 5ms target</td> \n   </tr> \n   <tr> \n    <td><strong>MicroLoRA forward</strong></td> \n    <td>rank=2, 4096 dim</td> \n    <td>8.56µs (scalar) / 2.61µs (SIMD)</td> \n    <td>117x–383x better than 1ms target</td> \n   </tr> \n   <tr> \n    <td><strong>RoPE</strong></td> \n    <td>128 dim, 32 tokens</td> \n    <td>1.33µs</td> \n    <td>9.6x better than 50µs target</td> \n   </tr> \n  </tbody> \n </table> \n <table> \n  <thead> \n   <tr> \n    <th>GEMV Scaling (M4 Pro)</th> \n    <th>Threads</th> \n    <th>Speedup</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td></td> \n    <td>1</td> \n    <td>1.0x</td> \n   </tr> \n   <tr> \n    <td></td> \n    <td>4</td> \n    <td>3.4x</td> \n   </tr> \n   <tr> \n    <td></td> \n    <td>8</td> \n    <td>6.1x</td> \n   </tr> \n   <tr> \n    <td></td> \n    <td>10</td> \n    <td><strong>12.7x</strong></td> \n   </tr> \n  </tbody> \n </table> \n <h3>ef_search Tuning</h3> \n <table> \n  <thead> \n   <tr> \n    <th>ef_search</th> \n    <th>QPS</th> \n    <th>p50 Latency</th> \n    <th>Recall</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>50</td> \n    <td>674</td> \n    <td>1.35ms</td> \n    <td>100%</td> \n   </tr> \n   <tr> \n    <td>100</td> \n    <td>596</td> \n    <td>1.37ms</td> \n    <td>100%</td> \n   </tr> \n   <tr> \n    <td>200</td> \n    <td>572</td> \n    <td>1.40ms</td> \n    <td>100%</td> \n   </tr> \n   <tr> \n    <td>400</td> \n    <td>434</td> \n    <td>1.97ms</td> \n    <td>100%</td> \n   </tr> \n   <tr> \n    <td>800</td> \n    <td>434</td> \n    <td>1.77ms</td> \n    <td>100%</td> \n   </tr> \n  </tbody> \n </table> \n <p>100% recall across all ef_search values — choose speed vs safety margin.</p> \n <h3>Cloud Scale (Production)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Metric</th> \n    <th>Value</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Concurrent Streams</strong></td> \n    <td>500M baseline, burst to 25B (50x)</td> \n   </tr> \n   <tr> \n    <td><strong>Global Latency</strong></td> \n    <td>p50 &lt;10ms, p99 &lt;50ms</td> \n   </tr> \n   <tr> \n    <td><strong>Availability</strong></td> \n    <td>99.99% SLA across 15 regions</td> \n   </tr> \n   <tr> \n    <td><strong>Throughput per Region</strong></td> \n    <td>100K+ QPS</td> \n   </tr> \n   <tr> \n    <td><strong>Memory Efficiency</strong></td> \n    <td>2-32x adaptive compression</td> \n   </tr> \n   <tr> \n    <td><strong>Index Build</strong></td> \n    <td>1M vectors/min (parallel HNSW)</td> \n   </tr> \n   <tr> \n    <td><strong>Replication Lag</strong></td> \n    <td>&lt;100ms (multi-master async)</td> \n   </tr> \n  </tbody> \n </table> \n <p><em>Full results: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/bench_results/\">bench_results/</a> | <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/benchmarks/\">docs/benchmarks/</a> | Run: <code>cargo bench -p ruvector-core</code></em></p> \n</details> \n<h2>Compression</h2> \n<p>RuVector automatically manages memory like a CPU cache — hot data stays at full precision, cold data compresses in the background. No manual tuning required.</p> \n<details> \n 🗜️ Adaptive Compression Tiers \n <p><strong>The architecture adapts to your data.</strong> Hot paths get full precision and maximum compute. Cold paths compress automatically and throttle resources. Recent data stays crystal clear; historical data optimizes itself in the background.</p> \n <p>Think of it like your computer's memory hierarchy—frequently accessed data lives in fast cache, while older files move to slower, denser storage. RuVector does this automatically for your vectors:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Access Frequency</th> \n    <th>Format</th> \n    <th>Compression</th> \n    <th>What Happens</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Hot</strong> (&gt;80%)</td> \n    <td>f32</td> \n    <td>1x</td> \n    <td>Full precision, instant retrieval</td> \n   </tr> \n   <tr> \n    <td><strong>Warm</strong> (40-80%)</td> \n    <td>f16</td> \n    <td>2x</td> \n    <td>Slight compression, imperceptible latency</td> \n   </tr> \n   <tr> \n    <td><strong>Cool</strong> (10-40%)</td> \n    <td>PQ8</td> \n    <td>8x</td> \n    <td>Smart quantization, ~1ms overhead</td> \n   </tr> \n   <tr> \n    <td><strong>Cold</strong> (1-10%)</td> \n    <td>PQ4</td> \n    <td>16x</td> \n    <td>Heavy compression, still fast search</td> \n   </tr> \n   <tr> \n    <td><strong>Archive</strong> (&lt;1%)</td> \n    <td>Binary</td> \n    <td>32x</td> \n    <td>Maximum density, batch retrieval</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>No configuration needed.</strong> RuVector tracks access patterns and automatically promotes/demotes vectors between tiers. Your hot data stays fast; your cold data shrinks.</p> \n</details> \n<h2>Use Cases</h2> \n<details> \n From AI-powered search to genomics, from real-time recommendations to knowledge graphs — see what people are building with RuVector. \n <h3>AI &amp; LLM Applications</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>RAG Pipelines</strong></td> \n    <td>Local vector search + local LLM — zero cloud costs, search improves from every query</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/ruvLLM\">examples/ruvLLM</a></td> \n   </tr> \n   <tr> \n    <td><strong>AI Agent Memory</strong></td> \n    <td>GNN-backed HNSW memory that agents share and learn from across sessions</td> \n    <td><a href=\"https://github.com/ruvnet/agentic-flow\">Agentic-Flow</a></td> \n   </tr> \n   <tr> \n    <td><strong>Agent Routing</strong></td> \n    <td>Semantic router with SONA self-learning picks the right agent in &lt;1ms</td> \n    <td><a href=\"https://github.com/ruvnet/claude-flow\">Claude-Flow</a></td> \n   </tr> \n   <tr> \n    <td><strong>Self-Learning Chatbots</strong></td> \n    <td>ReasoningBank + EWC++ — learns from conversations without forgetting previous ones</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/meta-cognition-spiking-neural-network\">examples/meta-cognition</a></td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-javascript\">// RAG with local LLM — zero cloud costs, search gets smarter over time\nconst db = new RuVector({ dimensions: 384 });\nconst llm = new RuvLLM({ model: 'ruvltra-small-0.5b-q4_k_m.gguf' });\n\nconst context = await db.search(questionEmbedding, { k: 5 }); // GNN-enhanced\nconst response = await llm.generate(`Context: ${context}\\n\\nQ: ${question}`);\n</code></pre> \n <h3>Search &amp; Discovery</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Semantic Search</strong></td> \n    <td>Sub-millisecond HNSW with SIMD acceleration — 80K QPS on 8 cores</td> \n    <td>Core feature</td> \n   </tr> \n   <tr> \n    <td><strong>Hybrid Search</strong></td> \n    <td>BM25 keywords + vector embeddings in one query, GNN reranking</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/api/\">docs/api</a></td> \n   </tr> \n   <tr> \n    <td><strong>Image / Audio / Video</strong></td> \n    <td>Any embedding model works — CLIP, Whisper, CLAP — with metadata filtering</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/wasm-react\">examples/wasm-react</a></td> \n   </tr> \n   <tr> \n    <td><strong>Code Search</strong></td> \n    <td>Local ONNX embeddings + graph queries find semantically similar code</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/nodejs\">examples/nodejs</a></td> \n   </tr> \n   <tr> \n    <td><strong>E-commerce</strong></td> \n    <td>Product search with price/category filters applied during search, not after</td> \n    <td>Core feature</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-javascript\">// Hybrid search: keyword + semantic with filtering\nconst results = await db.search(query, {\n  k: 10,\n  filter: { category: 'electronics', price: { $lt: 500 } },\n  hybridAlpha: 0.7,  // 70% semantic, 30% keyword\n  rerank: true       // GNN-enhanced reranking\n});\n</code></pre> \n <h3>Recommendations &amp; Knowledge Graphs</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Product Recommendations</strong></td> \n    <td>Neo4j-compatible Cypher queries with GNN scoring — no separate graph DB</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/graph\">examples/graph</a></td> \n   </tr> \n   <tr> \n    <td><strong>Knowledge Graphs</strong></td> \n    <td>Hypergraph with Cypher + W3C SPARQL 1.1, hyperedge relationships</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/api/CYPHER_REFERENCE.md\">docs/api/CYPHER_REFERENCE.md</a></td> \n   </tr> \n   <tr> \n    <td><strong>Document Q&amp;A</strong></td> \n    <td>Chunking, embeddings, RAG pipeline — all local, all self-improving</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/refrag-pipeline\">examples/refrag-pipeline</a></td> \n   </tr> \n   <tr> \n    <td><strong>Research Discovery</strong></td> \n    <td>Citation graph traversal, concept linking, multi-hop reasoning</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/scipix\">examples/scipix</a></td> \n   </tr> \n   <tr> \n    <td><strong>Content Personalization</strong></td> \n    <td>User embeddings + collaborative filtering adapt in real time</td> \n    <td>Real-time adaptation</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-cypher\">-- Neo4j-style graph query — runs inside RuVector, no separate database\nMATCH (user:User {id: $userId})-[:VIEWED]-&gt;(item:Product)\nMATCH (item)-[:SIMILAR_TO]-&gt;(rec:Product)\nWHERE NOT (user)-[:PURCHASED]-&gt;(rec)\nRETURN rec ORDER BY rec.gnn_score DESC LIMIT 10\n</code></pre> \n <h3>Edge, Browser &amp; IoT</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Browser AI</strong></td> \n    <td>Full LLM + vector search in WASM — no server, runs in any browser</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/wasm-vanilla\">examples/wasm-vanilla</a></td> \n   </tr> \n   <tr> \n    <td><strong>IoT / Sensors</strong></td> \n    <td>2MB footprint, <code>no_std</code> support, offline-first with sync on reconnect</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge\">examples/edge</a></td> \n   </tr> \n   <tr> \n    <td><strong>Mobile Apps</strong></td> \n    <td>Embed as a library — offline search, on-device learning</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge-net\">examples/edge-net</a></td> \n   </tr> \n   <tr> \n    <td><strong>Streaming Data</strong></td> \n    <td>Real-time indexing with dynamic min-cut for live data feeds</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/neural-trader\">examples/neural-trader</a></td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-javascript\">// Full AI in the browser — no server required\nimport init, { RuvLLMWasm } from '@ruvector/ruvllm-wasm';\nawait init();\nconst llm = await RuvLLMWasm.new(true); // WebGPU enabled\nconst response = await llm.generate('Explain quantum computing', { max_tokens: 200 });\n</code></pre> \n <h3>Self-Learning &amp; Fine-Tuning</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Per-Request Adaptation</strong></td> \n    <td>MicroLoRA adapts model weights in &lt;1ms based on user feedback</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/ruvllm/FINE_TUNING.md\">docs/ruvllm/FINE_TUNING.md</a></td> \n   </tr> \n   <tr> \n    <td><strong>Contrastive Training</strong></td> \n    <td>Triplet loss with hard negative mining — learns what's similar and what isn't</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/npm/packages/ruvllm\">npm/packages/ruvllm</a></td> \n   </tr> \n   <tr> \n    <td><strong>Memory Preservation</strong></td> \n    <td>EWC++ prevents catastrophic forgetting — learns new things without losing old ones</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/sona\">crates/sona</a></td> \n   </tr> \n   <tr> \n    <td><strong>Task-Specific Adapters</strong></td> \n    <td>5 built-in adapters (Coder, Researcher, Security, Architect, Reviewer)</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/training/\">docs/training</a></td> \n   </tr> \n   <tr> \n    <td><strong>Browser Fine-Tuning</strong></td> \n    <td>MicroLoRA in WASM — &lt;50KB adapters persist to localStorage</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm-wasm\">crates/ruvllm-wasm</a></td> \n   </tr> \n  </tbody> \n </table> \n <table> \n  <thead> \n   <tr> \n    <th>Tier</th> \n    <th>How It Works</th> \n    <th>Speed</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Instant</strong></td> \n    <td>MicroLoRA (rank 1-2) per request</td> \n    <td>&lt;1ms</td> \n   </tr> \n   <tr> \n    <td><strong>Background</strong></td> \n    <td>Adapter merge + EWC++ consolidation</td> \n    <td>~100ms</td> \n   </tr> \n   <tr> \n    <td><strong>Deep</strong></td> \n    <td>Full training pipeline</td> \n    <td>Minutes</td> \n   </tr> \n  </tbody> \n </table> \n <h3>AI Safety &amp; Coherence</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Agent Safety Gates</strong></td> \n    <td>256-tile WASM fabric — Permit / Defer / Deny decisions in &lt;1ms</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/cognitum-gate-tilezero\">crates/cognitum-gate</a></td> \n   </tr> \n   <tr> \n    <td><strong>Cryptographic Audit</strong></td> \n    <td>Hash-chained witness receipts — tamper-proof record of every AI action</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto\">crates/rvf-crypto</a></td> \n   </tr> \n   <tr> \n    <td><strong>Coherence Checking</strong></td> \n    <td>Min-cut aggregation detects when AI agents drift from intended behavior</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/mincut\">examples/mincut</a></td> \n   </tr> \n   <tr> \n    <td><strong>Post-Quantum Security</strong></td> \n    <td>ML-DSA-65, SLH-DSA-128s, Ed25519 signatures on every operation</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto\">crates/rvf-crypto</a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Neuromorphic &amp; Scientific Computing</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Spiking Neural Networks</strong></td> \n    <td>LIF neurons with STDP learning — 10-50x more energy efficient than ANNs</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/meta-cognition-spiking-neural-network\">examples/meta-cognition</a></td> \n   </tr> \n   <tr> \n    <td><strong>Neuromorphic Search</strong></td> \n    <td>micro-hnsw: brain-inspired vector search in 11.8KB WASM</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/micro-hnsw-wasm\">crates/micro-hnsw</a></td> \n   </tr> \n   <tr> \n    <td><strong>Algorithmic Trading</strong></td> \n    <td>Neural Trader with time-series analysis and real-time decision making</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/neural-trader\">examples/neural-trader</a></td> \n   </tr> \n   <tr> \n    <td><strong>Quantum Computing</strong></td> \n    <td>ruQu quantum circuit simulation with min-cut coherence</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruQu\">crates/ruQu</a></td> \n   </tr> \n   <tr> \n    <td><strong>Genomics</strong></td> \n    <td>DNA sequence similarity, variant calling, population analysis</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/dna\">examples/dna</a></td> \n   </tr> \n   <tr> \n    <td><strong>Graph Transformers</strong></td> \n    <td>8 verified modules — physics, bio, manifold, temporal, economic, and more</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-transformer\">crates/ruvector-graph-transformer</a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Distributed &amp; Enterprise</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>PostgreSQL Drop-In</strong></td> \n    <td>230+ SQL functions — replace pgvector with self-learning search</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-postgres\">crates/ruvector-postgres</a></td> \n   </tr> \n   <tr> \n    <td><strong>Multi-Region HA</strong></td> \n    <td>Raft consensus, multi-master replication, automatic failover</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/cloud-architecture/\">docs/cloud-architecture</a></td> \n   </tr> \n   <tr> \n    <td><strong>Burst Scaling</strong></td> \n    <td>10-50x auto-scaling with quorum writes and load balancing</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/google-cloud\">examples/google-cloud</a></td> \n   </tr> \n   <tr> \n    <td><strong>Cognitive Containers</strong></td> \n    <td>Single <code>.rvf</code> file boots as a microservice in 125ms — data + code + lineage</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf\">crates/rvf</a></td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-sql\">-- Drop-in PostgreSQL replacement with self-improving search\nCREATE EXTENSION ruvector;\nCREATE TABLE documents (id SERIAL PRIMARY KEY, content TEXT, embedding VECTOR(384));\nCREATE INDEX ON documents USING hnsw_gnn (embedding);\n\nSELECT * FROM documents ORDER BY embedding &lt;-&gt; query_vector LIMIT 10;\n</code></pre> \n <h3>Agentic Workflows</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>What RuVector Does</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Version Control for AI</strong></td> \n    <td>Agentic Jujutsu — branch, merge, and diff AI model states</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/agentic-jujutsu\">examples/agentic-jujutsu</a></td> \n   </tr> \n   <tr> \n    <td><strong>Self-Learning Pipelines</strong></td> \n    <td>DAG workflows that learn optimal execution paths over time</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-dag\">crates/ruvector-dag</a></td> \n   </tr> \n   <tr> \n    <td><strong>Web Scraping → Embeddings</strong></td> \n    <td>Apify integration — scrape, embed, and index in one pipeline</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/apify\">examples/apify</a></td> \n   </tr> \n   <tr> \n    <td><strong>Synthetic Data</strong></td> \n    <td>Agentic synthesis and generation for training data</td> \n    <td><a href=\"https://github.com/ruvnet/agentic-flow\">Agentic-Flow</a></td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<h2>Installation</h2> \n<p>RuVector runs on Node.js, Rust, browsers, PostgreSQL, and Docker. Pick the package that fits your stack.</p> \n<table> \n <thead> \n  <tr> \n   <th>Platform</th> \n   <th>Command</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><strong>npm</strong></td> \n   <td><code>npm install ruvector</code></td> \n  </tr> \n  <tr> \n   <td><strong>npm (SONA)</strong></td> \n   <td><code>npm install @ruvector/sona</code></td> \n  </tr> \n  <tr> \n   <td><strong>npm (Genomics)</strong></td> \n   <td><code>npm install @ruvector/rvdna</code></td> \n  </tr> \n  <tr> \n   <td><strong>npm (RVF)</strong></td> \n   <td><code>npm install @ruvector/rvf</code></td> \n  </tr> \n  <tr> \n   <td><strong>Browser/WASM</strong></td> \n   <td><code>npm install ruvector-wasm</code></td> \n  </tr> \n  <tr> \n   <td><strong>Rust</strong></td> \n   <td><code>cargo add ruvector-core ruvector-graph ruvector-gnn</code></td> \n  </tr> \n  <tr> \n   <td><strong>Rust (RVF)</strong></td> \n   <td><code>cargo add rvf-runtime</code></td> \n  </tr> \n  <tr> \n   <td><strong>Rust (Genomics)</strong></td> \n   <td><code>cargo add rvdna</code></td> \n  </tr> \n  <tr> \n   <td><strong>Rust (SONA)</strong></td> \n   <td><code>cargo add ruvector-sona</code></td> \n  </tr> \n  <tr> \n   <td><strong>Rust (LLM)</strong></td> \n   <td><code>cargo add ruvllm</code></td> \n  </tr> \n  <tr> \n   <td><strong>RVF CLI</strong></td> \n   <td><code>cargo install rvf-cli</code></td> \n  </tr> \n  <tr> \n   <td><strong>RVF MCP</strong></td> \n   <td><code>npx @ruvector/rvf-mcp-server --transport stdio</code></td> \n  </tr> \n </tbody> \n</table> \n<hr /> \n<h2>Package Reference</h2> \n<details> \n 📖 Documentation \n <h4>Getting Started</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Topic</th> \n    <th>Link</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Getting Started</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/guides/GETTING_STARTED.md\">docs/guides/GETTING_STARTED.md</a></td> \n   </tr> \n   <tr> \n    <td>API Reference</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/api/\">docs/api/</a></td> \n   </tr> \n   <tr> \n    <td>Cypher Reference</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/api/CYPHER_REFERENCE.md\">docs/api/CYPHER_REFERENCE.md</a></td> \n   </tr> \n   <tr> \n    <td>Performance Tuning</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/optimization/PERFORMANCE_TUNING_GUIDE.md\">docs/optimization/PERFORMANCE_TUNING_GUIDE.md</a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Core Components</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Topic</th> \n    <th>Link</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>GNN Architecture</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/gnn/\">docs/gnn/</a></td> \n   </tr> \n   <tr> \n    <td>HNSW Indexing</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/hnsw/\">docs/hnsw/</a></td> \n   </tr> \n   <tr> \n    <td>DAG System</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/dag/\">docs/dag/</a></td> \n   </tr> \n   <tr> \n    <td>Nervous System</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/nervous-system/\">docs/nervous-system/</a></td> \n   </tr> \n   <tr> \n    <td>Sparse Inference</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/sparse-inference/\">docs/sparse-inference/</a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Bindings &amp; Integration</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Topic</th> \n    <th>Link</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Node.js API</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn-node/README.md\">crates/ruvector-gnn-node/README.md</a></td> \n   </tr> \n   <tr> \n    <td>WASM API</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn-wasm/README.md\">crates/ruvector-gnn-wasm/README.md</a></td> \n   </tr> \n   <tr> \n    <td>PostgreSQL</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/postgres/\">docs/postgres/</a></td> \n   </tr> \n   <tr> \n    <td>Self-Learning Hooks</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/hooks/\">docs/hooks/</a></td> \n   </tr> \n   <tr> \n    <td>Integration Guides</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/integration/\">docs/integration/</a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>LLM &amp; AI</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Topic</th> \n    <th>Link</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>RuvLLM</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/ruvllm/\">docs/ruvllm/</a></td> \n   </tr> \n   <tr> \n    <td>Training Guides</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/training/\">docs/training/</a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Operations</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Topic</th> \n    <th>Link</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Architecture</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/architecture/\">docs/architecture/</a></td> \n   </tr> \n   <tr> \n    <td>Cloud Deployment</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/cloud-architecture/\">docs/cloud-architecture/</a></td> \n   </tr> \n   <tr> \n    <td>Security</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/security/\">docs/security/</a></td> \n   </tr> \n   <tr> \n    <td>Benchmarks</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/benchmarks/\">docs/benchmarks/</a></td> \n   </tr> \n   <tr> \n    <td>Testing</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/testing/\">docs/testing/</a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Research</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Topic</th> \n    <th>Link</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Research Papers</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/research/\">docs/research/</a></td> \n   </tr> \n   <tr> \n    <td>GNN V2 Features</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/research/gnn-v2/\">docs/research/gnn-v2/</a></td> \n   </tr> \n   <tr> \n    <td>Min-Cut Algorithms</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/research/mincut/\">docs/research/mincut/</a></td> \n   </tr> \n   <tr> \n    <td>SPARQL Support</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/research/sparql/\">docs/research/sparql/</a></td> \n   </tr> \n   <tr> \n    <td>Latent Space</td> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/research/latent-space/\">docs/research/latent-space/</a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Architecture Decision Records (ADRs)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>ADR</th> \n    <th>Status</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-001-ruvector-core-architecture.md\">ADR-001</a></td> \n    <td>Accepted</td> \n    <td>Core architecture design</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-002-ruvllm-integration.md\">ADR-002</a></td> \n    <td>Accepted</td> \n    <td>RuvLLM integration</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-003-simd-optimization-strategy.md\">ADR-003</a></td> \n    <td>Accepted</td> \n    <td>SIMD optimization strategy</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-004-kv-cache-management.md\">ADR-004</a></td> \n    <td>Accepted</td> \n    <td>KV cache management</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-005-wasm-runtime-integration.md\">ADR-005</a></td> \n    <td>Accepted</td> \n    <td>WASM runtime integration</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-006-memory-management.md\">ADR-006</a></td> \n    <td>Accepted</td> \n    <td>Memory management</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-007-security-review-technical-debt.md\">ADR-007</a></td> \n    <td>Accepted</td> \n    <td>Security review</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-008-mistral-rs-integration.md\">ADR-008</a></td> \n    <td><strong>New</strong></td> \n    <td>Mistral-rs backend integration</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-009-structured-output.md\">ADR-009</a></td> \n    <td><strong>New</strong></td> \n    <td>Structured output (SOTA)</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-010-function-calling.md\">ADR-010</a></td> \n    <td><strong>New</strong></td> \n    <td>Function calling (SOTA)</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-011-prefix-caching.md\">ADR-011</a></td> \n    <td><strong>New</strong></td> \n    <td>Prefix caching (SOTA)</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-012-security-remediation.md\">ADR-012</a></td> \n    <td><strong>New</strong></td> \n    <td>Security remediation</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-013-huggingface-publishing.md\">ADR-013</a></td> \n    <td><strong>New</strong></td> \n    <td>HuggingFace publishing</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-030-rvf-cognitive-container.md\">ADR-030</a></td> \n    <td><strong>Accepted</strong></td> \n    <td>RVF cognitive container architecture</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-031-rvcow-branching-and-real-cognitive-containers.md\">ADR-031</a></td> \n    <td><strong>Accepted</strong></td> \n    <td>RVCOW branching &amp; real containers</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-045-lean-agentic-integration.md\">ADR-045</a></td> \n    <td><strong>Accepted</strong></td> \n    <td>Lean-agentic formal verification integration</td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<details> \n 📦 npm Packages (49+ Packages) \n <h4>Core Packages</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/ruvector\">ruvector</a></td> \n    <td>All-in-one CLI &amp; package</td> \n    <td><a href=\"https://www.npmjs.com/package/ruvector\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/ruvector.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/ruvector\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/ruvector.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/core\">@ruvector/core</a></td> \n    <td>Core vector database with HNSW</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/core\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/core.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/core\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/core.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/node\">@ruvector/node</a></td> \n    <td>Unified Node.js bindings</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/node\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/node.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/node\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/ruvector-extensions\">ruvector-extensions</a></td> \n    <td>Advanced features: embeddings, UI</td> \n    <td><a href=\"https://www.npmjs.com/package/ruvector-extensions\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/ruvector-extensions.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/ruvector-extensions\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/ruvector-extensions.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Graph &amp; GNN</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn\">@ruvector/gnn</a></td> \n    <td>Graph Neural Network layers</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/gnn.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/gnn.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-node\">@ruvector/graph-node</a></td> \n    <td>Hypergraph with Cypher queries</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-node\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/graph-node.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-node\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/graph-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-wasm\">@ruvector/graph-wasm</a></td> \n    <td>Browser graph queries</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/graph-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/graph-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-data-generator\">@ruvector/graph-data-generator</a></td> \n    <td>AI-powered synthetic graph data</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-data-generator\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/graph-data-generator.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-data-generator\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/graph-data-generator.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>AI Routing &amp; Attention</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer\">@ruvector/tiny-dancer</a></td> \n    <td>FastGRNN neural routing</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/tiny-dancer.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/tiny-dancer.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router\">@ruvector/router</a></td> \n    <td>Semantic router + HNSW</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/router.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/router.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention\">@ruvector/attention</a></td> \n    <td>46 attention mechanisms</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/attention.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/attention.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Learning &amp; Neural</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/sona\">@ruvector/sona</a></td> \n    <td>Self-Optimizing Neural Architecture</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/sona\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/sona.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/sona\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/sona.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/spiking-neural\">@ruvector/spiking-neural</a></td> \n    <td>Spiking neural networks (SNN)</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/spiking-neural\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/spiking-neural.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/spiking-neural\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/spiking-neural.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>LLM Runtime</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm\">@ruvector/ruvllm</a></td> \n    <td>LLM orchestration + SONA</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ruvllm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/ruvllm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm-cli\">@ruvector/ruvllm-cli</a></td> \n    <td>LLM CLI: inference, benchmarks</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm-cli\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ruvllm-cli.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm-cli\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/ruvllm-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm-wasm\">@ruvector/ruvllm-wasm</a></td> \n    <td>Browser LLM inference</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ruvllm-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ruvllm-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/ruvllm-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Distributed Systems</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/cluster\">@ruvector/cluster</a></td> \n    <td>Distributed clustering</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/cluster\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/cluster.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/cluster\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/cluster.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/server\">@ruvector/server</a></td> \n    <td>HTTP/gRPC server</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/server\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/server.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/server\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/server.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/raft\">@ruvector/raft</a></td> \n    <td>Raft consensus</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/raft\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/raft.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/raft\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/raft.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/replication\">@ruvector/replication</a></td> \n    <td>Multi-master replication</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/replication\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/replication.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/replication\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/replication.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/burst-scaling\">@ruvector/burst-scaling</a></td> \n    <td>10-50x burst scaling</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/burst-scaling\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/burst-scaling.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/burst-scaling\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/burst-scaling.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Edge &amp; Standalone</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/rvlite\">rvlite</a></td> \n    <td>SQLite-style edge DB</td> \n    <td><a href=\"https://www.npmjs.com/package/rvlite\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/rvlite.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/rvlite\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/rvlite.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rudag\">@ruvector/rudag</a></td> \n    <td>Self-learning DAG</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rudag\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rudag.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rudag\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/rudag.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Genomics &amp; Health</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvdna\">@ruvector/rvdna</a></td> \n    <td>AI-native genomic analysis + .rvdna format</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvdna\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rvdna.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvdna\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/rvdna.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>RVF Cognitive Containers</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf\">@ruvector/rvf</a></td> \n    <td>Unified TypeScript SDK</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rvf.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/rvf.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-node\">@ruvector/rvf-node</a></td> \n    <td>Node.js N-API native bindings</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-node\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rvf-node.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-node\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/rvf-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-wasm\">@ruvector/rvf-wasm</a></td> \n    <td>WASM browser package</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rvf-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/rvf-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-mcp-server\">@ruvector/rvf-mcp-server</a></td> \n    <td>MCP server for AI agents</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-mcp-server\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rvf-mcp-server.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/rvf-mcp-server\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/rvf-mcp-server.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Agentic &amp; Synthetic Data</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\">@ruvector/agentic-synth</a></td> \n    <td>AI synthetic data generator</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/agentic-synth.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/agentic-synth.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-integration\">@ruvector/agentic-integration</a></td> \n    <td>Distributed agent coordination</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-integration\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/agentic-integration.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-integration\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/agentic-integration.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@cognitum/gate\">@cognitum/gate</a></td> \n    <td>AI coherence gate</td> \n    <td><a href=\"https://www.npmjs.com/package/@cognitum/gate\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@cognitum/gate.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@cognitum/gate\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@cognitum/gate.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>CLI Tools</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/cli\">@ruvector/cli</a></td> \n    <td>CLI + self-learning hooks</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/cli\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/cli.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/cli\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/postgres-cli\">@ruvector/postgres-cli</a></td> \n    <td>PostgreSQL extension CLI</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/postgres-cli\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/postgres-cli.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/postgres-cli\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/postgres-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/scipix\">@ruvector/scipix</a></td> \n    <td>Scientific OCR client</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/scipix\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/scipix.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/scipix\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/scipix.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>WASM Packages</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm\">@ruvector/wasm</a></td> \n    <td>Unified WASM meta-package</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm-unified\">@ruvector/wasm-unified</a></td> \n    <td>Unified TypeScript API</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm-unified\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/wasm-unified.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm-unified\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/wasm-unified.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn-wasm\">@ruvector/gnn-wasm</a></td> \n    <td>GNN WASM bindings</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/gnn-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/gnn-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-wasm\">@ruvector/attention-wasm</a></td> \n    <td>Attention WASM bindings</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/attention-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/attention-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-unified-wasm\">@ruvector/attention-unified-wasm</a></td> \n    <td>All 46 attention mechanisms</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-unified-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/attention-unified-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-unified-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/attention-unified-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer-wasm\">@ruvector/tiny-dancer-wasm</a></td> \n    <td>AI routing WASM</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/tiny-dancer-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/tiny-dancer-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router-wasm\">@ruvector/router-wasm</a></td> \n    <td>Semantic router WASM</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/router-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/router-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/learning-wasm\">@ruvector/learning-wasm</a></td> \n    <td>Learning module WASM</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/learning-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/learning-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/learning-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/learning-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/economy-wasm\">@ruvector/economy-wasm</a></td> \n    <td>Tokenomics WASM</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/economy-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/economy-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/economy-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/economy-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/exotic-wasm\">@ruvector/exotic-wasm</a></td> \n    <td>Exotic features WASM</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/exotic-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/exotic-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/exotic-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/exotic-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/nervous-system-wasm\">@ruvector/nervous-system-wasm</a></td> \n    <td>Nervous system WASM</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/nervous-system-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/nervous-system-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/nervous-system-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/nervous-system-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/ruvector-attention-wasm\">ruvector-attention-wasm</a></td> \n    <td>WASM attention (Flash, MoE, Hyperbolic, CGT Sheaf)</td> \n    <td><a href=\"https://www.npmjs.com/package/ruvector-attention-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/ruvector-attention-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/ruvector-attention-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/ruvector-attention-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<details> \n 🦀 Rust Crates (83 Packages) \n <p>All crates are published to <a href=\"https://crates.io\">crates.io</a> under the <code>ruvector-*</code> and <code>rvf-*</code> namespaces.</p> \n <h3>Core Crates</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-core\">ruvector-core</a></td> \n    <td>Vector database engine with HNSW indexing</td> \n    <td><a href=\"https://crates.io/crates/ruvector-core\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-core.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-collections\">ruvector-collections</a></td> \n    <td>Collection and namespace management</td> \n    <td><a href=\"https://crates.io/crates/ruvector-collections\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-collections.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-filter\">ruvector-filter</a></td> \n    <td>Vector filtering and metadata queries</td> \n    <td><a href=\"https://crates.io/crates/ruvector-filter\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-filter.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-metrics\">ruvector-metrics</a></td> \n    <td>Performance metrics and monitoring</td> \n    <td><a href=\"https://crates.io/crates/ruvector-metrics\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-metrics.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-snapshot\">ruvector-snapshot</a></td> \n    <td>Snapshot and persistence management</td> \n    <td><a href=\"https://crates.io/crates/ruvector-snapshot\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-snapshot.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-node\">ruvector-node</a></td> \n    <td>Node.js bindings via NAPI-RS</td> \n    <td><a href=\"https://crates.io/crates/ruvector-node\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-wasm\">ruvector-wasm</a></td> \n    <td>WASM bindings for browser/edge</td> \n    <td><a href=\"https://crates.io/crates/ruvector-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-cli\">ruvector-cli</a></td> \n    <td>CLI and MCP server</td> \n    <td><a href=\"https://crates.io/crates/ruvector-cli\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-bench\">ruvector-bench</a></td> \n    <td>Benchmarking suite</td> \n    <td><a href=\"https://crates.io/crates/ruvector-bench\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-bench.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Graph &amp; GNN</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph\">ruvector-graph</a></td> \n    <td>Hypergraph database with Neo4j-style Cypher</td> \n    <td><a href=\"https://crates.io/crates/ruvector-graph\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-graph.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-node\">ruvector-graph-node</a></td> \n    <td>Node.js bindings for graph operations</td> \n    <td><a href=\"https://crates.io/crates/ruvector-graph-node\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-graph-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-wasm\">ruvector-graph-wasm</a></td> \n    <td>WASM bindings for browser graph queries</td> \n    <td><a href=\"https://crates.io/crates/ruvector-graph-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-graph-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn\">ruvector-gnn</a></td> \n    <td>Graph Neural Network layers and training</td> \n    <td><a href=\"https://crates.io/crates/ruvector-gnn\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-gnn.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn-node\">ruvector-gnn-node</a></td> \n    <td>Node.js bindings for GNN inference</td> \n    <td><a href=\"https://crates.io/crates/ruvector-gnn-node\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-gnn-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-gnn-wasm\">ruvector-gnn-wasm</a></td> \n    <td>WASM bindings for browser GNN</td> \n    <td><a href=\"https://crates.io/crates/ruvector-gnn-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-gnn-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Attention Mechanisms</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention\">ruvector-attention</a></td> \n    <td>46 attention mechanisms (Flash, Hyperbolic, MoE, Graph)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-attention\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-attention.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention-node\">ruvector-attention-node</a></td> \n    <td>Node.js bindings for attention mechanisms</td> \n    <td><a href=\"https://crates.io/crates/ruvector-attention-node\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-attention-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention-wasm\">ruvector-attention-wasm</a></td> \n    <td>WASM bindings for browser attention</td> \n    <td><a href=\"https://crates.io/crates/ruvector-attention-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-attention-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention-cli\">ruvector-attention-cli</a></td> \n    <td>CLI for attention testing and benchmarking</td> \n    <td><a href=\"https://crates.io/crates/ruvector-attention-cli\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-attention-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>LLM Runtime (ruvllm)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm\">ruvllm</a></td> \n    <td>LLM serving runtime with SONA, paged attention, KV cache, BitNet</td> \n    <td><a href=\"https://crates.io/crates/ruvllm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvllm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm-cli\">ruvllm-cli</a></td> \n    <td>CLI for model inference and benchmarking</td> \n    <td><a href=\"https://crates.io/crates/ruvllm-cli\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvllm-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm-wasm\">ruvllm-wasm</a></td> \n    <td>WASM bindings for browser LLM inference</td> \n    <td><a href=\"https://crates.io/crates/ruvllm-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvllm-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Features:</strong> Candle backend, Metal/CUDA acceleration, Apple Neural Engine, GGUF support, SONA learning, <strong>BitNet 1.58-bit quantization</strong> (TL1 kernels, AVX2/WASM).</p> \n <pre><code class=\"language-bash\">cargo add ruvllm --features inference-metal  # Mac with Metal\ncargo add ruvllm --features inference-cuda   # NVIDIA GPU\n</code></pre> \n <p><strong>RuvLTRA Models</strong> — Pre-trained GGUF models optimized for Claude Code workflows:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Model</th> \n    <th>Size</th> \n    <th>Use Case</th> \n    <th>Link</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>ruvltra-claude-code-0.5b-q4_k_m</td> \n    <td>398 MB</td> \n    <td>Agent routing</td> \n    <td><a href=\"https://huggingface.co/ruv/ruvltra\">HuggingFace</a></td> \n   </tr> \n   <tr> \n    <td>ruvltra-small-0.5b-q4_k_m</td> \n    <td>398 MB</td> \n    <td>Embeddings</td> \n    <td><a href=\"https://huggingface.co/ruv/ruvltra\">HuggingFace</a></td> \n   </tr> \n   <tr> \n    <td>ruvltra-medium-1.1b-q4_k_m</td> \n    <td>800 MB</td> \n    <td>Classification</td> \n    <td><a href=\"https://huggingface.co/ruv/ruvltra\">HuggingFace</a></td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\"># Download and use\nwget https://huggingface.co/ruv/ruvltra/resolve/main/ruvltra-small-0.5b-q4_k_m.gguf\n</code></pre> \n <h3>Distributed Systems</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-cluster\">ruvector-cluster</a></td> \n    <td>Cluster management and coordination</td> \n    <td><a href=\"https://crates.io/crates/ruvector-cluster\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-cluster.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-raft\">ruvector-raft</a></td> \n    <td>Raft consensus implementation</td> \n    <td><a href=\"https://crates.io/crates/ruvector-raft\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-raft.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-replication\">ruvector-replication</a></td> \n    <td>Data replication and synchronization</td> \n    <td><a href=\"https://crates.io/crates/ruvector-replication\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-replication.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-server\">ruvector-server</a></td> \n    <td>REST/gRPC API server</td> \n    <td><a href=\"https://crates.io/crates/ruvector-server\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-server.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>AI Agent Routing (Tiny Dancer)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-tiny-dancer-core\">ruvector-tiny-dancer-core</a></td> \n    <td>FastGRNN neural inference for AI routing</td> \n    <td><a href=\"https://crates.io/crates/ruvector-tiny-dancer-core\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-tiny-dancer-core.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-tiny-dancer-node\">ruvector-tiny-dancer-node</a></td> \n    <td>Node.js bindings for AI routing</td> \n    <td><a href=\"https://crates.io/crates/ruvector-tiny-dancer-node\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-tiny-dancer-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-tiny-dancer-wasm\">ruvector-tiny-dancer-wasm</a></td> \n    <td>WASM bindings for browser AI routing</td> \n    <td><a href=\"https://crates.io/crates/ruvector-tiny-dancer-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-tiny-dancer-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Router (Semantic Routing)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-router-core\">ruvector-router-core</a></td> \n    <td>Core semantic routing engine</td> \n    <td><a href=\"https://crates.io/crates/ruvector-router-core\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-router-core.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-router-cli\">ruvector-router-cli</a></td> \n    <td>CLI for router testing and benchmarking</td> \n    <td><a href=\"https://crates.io/crates/ruvector-router-cli\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-router-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-router-ffi\">ruvector-router-ffi</a></td> \n    <td>FFI bindings for other languages</td> \n    <td><a href=\"https://crates.io/crates/ruvector-router-ffi\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-router-ffi.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-router-wasm\">ruvector-router-wasm</a></td> \n    <td>WASM bindings for browser routing</td> \n    <td><a href=\"https://crates.io/crates/ruvector-router-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-router-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Hybrid Routing</strong> achieves <strong>90% accuracy</strong> for agent routing using keyword-first strategy with embedding fallback. See <a href=\"https://github.com/ruvnet/ruvector/issues/122\">Issue #122</a> for benchmarks and the <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/#-ruvllm-training--fine-tuning-tutorials\">training tutorials</a> for fine-tuning guides.</p> \n <h3>Dynamic Min-Cut (December 2025 Breakthrough)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-mincut\">ruvector-mincut</a></td> \n    <td>Subpolynomial fully-dynamic min-cut (<a href=\"https://arxiv.org/abs/2512.13105\">arXiv:2512.13105</a>)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-mincut\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-mincut.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-mincut-node\">ruvector-mincut-node</a></td> \n    <td>Node.js bindings for min-cut</td> \n    <td><a href=\"https://crates.io/crates/ruvector-mincut-node\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-mincut-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-mincut-wasm\">ruvector-mincut-wasm</a></td> \n    <td>WASM bindings for browser min-cut</td> \n    <td><a href=\"https://crates.io/crates/ruvector-mincut-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-mincut-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>First deterministic exact fully-dynamic min-cut</strong> with verified <strong>n^0.12 subpolynomial</strong> update scaling:</p> \n <ul> \n  <li><strong>Brain connectivity</strong> — Detect Alzheimer's markers by tracking neural pathway changes in milliseconds</li> \n  <li><strong>Network resilience</strong> — Predict outages before they happen, route around failures instantly</li> \n  <li><strong>AI agent coordination</strong> — Find communication bottlenecks in multi-agent systems</li> \n  <li><strong>Neural network pruning</strong> — Identify which connections can be removed without losing accuracy</li> \n  <li><strong>448+ tests</strong>, 256-core parallel optimization, 8KB per core (compile-time verified)</li> \n </ul> \n <pre><code class=\"language-rust\">use ruvector_mincut::{DynamicMinCut, Graph};\n\nlet mut graph = Graph::new();\ngraph.add_edge(0, 1, 10.0);\ngraph.add_edge(1, 2, 5.0);\n\nlet mincut = DynamicMinCut::new(&amp;graph);\nlet (value, cut_edges) = mincut.compute();\n// Updates in subpolynomial time as edges change\n</code></pre> \n <h3>Quantum Coherence (ruQu)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruQu\">ruqu</a></td> \n    <td>Classical nervous system for quantum machines - coherence via min-cut</td> \n    <td><a href=\"https://crates.io/crates/ruqu\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruqu.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/cognitum-gate-kernel\">cognitum-gate-kernel</a></td> \n    <td>Anytime-valid coherence gate kernel</td> \n    <td><a href=\"https://crates.io/crates/cognitum-gate-kernel\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/cognitum-gate-kernel.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/cognitum-gate-tilezero\">cognitum-gate-tilezero</a></td> \n    <td>TileZero arbiter for coherence decisions</td> \n    <td><a href=\"https://crates.io/crates/cognitum-gate-tilezero\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/cognitum-gate-tilezero.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/mcp-gate\">mcp-gate</a></td> \n    <td>MCP server for coherence gate integration</td> \n    <td><a href=\"https://crates.io/crates/mcp-gate\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/mcp-gate.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/prime-radiant\">prime-radiant</a></td> \n    <td>Universal coherence engine - sheaf Laplacian AI safety &amp; hallucination detection</td> \n    <td><a href=\"https://crates.io/crates/prime-radiant\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/prime-radiant.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>ruQu Features:</strong> Real-time quantum coherence assessment, MWPM decoder integration, mincut-gated attention (50% FLOPs reduction).</p> \n <pre><code class=\"language-rust\">use ruqu::{CoherenceGate, SyndromeFilter};\n\nlet gate = CoherenceGate::new();\nlet syndrome = gate.assess_coherence(&amp;quantum_state)?;\n</code></pre> \n <h3>Advanced Math &amp; Inference</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-math\">ruvector-math</a></td> \n    <td>Core math utilities, SIMD operations</td> \n    <td><a href=\"https://crates.io/crates/ruvector-math\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-math.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-math-wasm\">ruvector-math-wasm</a></td> \n    <td>WASM bindings for math operations</td> \n    <td><a href=\"https://crates.io/crates/ruvector-math-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-math-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-sparse-inference\">ruvector-sparse-inference</a></td> \n    <td>Sparse tensor inference engine</td> \n    <td><a href=\"https://crates.io/crates/ruvector-sparse-inference\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-sparse-inference.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-sparse-inference-wasm\">ruvector-sparse-inference-wasm</a></td> \n    <td>WASM bindings for sparse inference</td> \n    <td><a href=\"https://crates.io/crates/ruvector-sparse-inference-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-sparse-inference-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-hyperbolic-hnsw\">ruvector-hyperbolic-hnsw</a></td> \n    <td>HNSW in hyperbolic space (Poincaré/Lorentz)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-hyperbolic-hnsw\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-hyperbolic-hnsw.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-hyperbolic-hnsw-wasm\">ruvector-hyperbolic-hnsw-wasm</a></td> \n    <td>WASM bindings for hyperbolic HNSW</td> \n    <td><a href=\"https://crates.io/crates/ruvector-hyperbolic-hnsw-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-hyperbolic-hnsw-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-dither\">ruvector-dither</a></td> \n    <td>Deterministic golden-ratio and pi-digit dithering for quantization (<code>no_std</code>)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-dither\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-dither.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>FPGA &amp; Hardware Acceleration</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-fpga-transformer\">ruvector-fpga-transformer</a></td> \n    <td>FPGA-optimized transformer inference</td> \n    <td><a href=\"https://crates.io/crates/ruvector-fpga-transformer\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-fpga-transformer.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-fpga-transformer-wasm\">ruvector-fpga-transformer-wasm</a></td> \n    <td>WASM simulation of FPGA transformer</td> \n    <td><a href=\"https://crates.io/crates/ruvector-fpga-transformer-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-fpga-transformer-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-mincut-gated-transformer\">ruvector-mincut-gated-transformer</a></td> \n    <td>MinCut-gated attention for 50% compute reduction</td> \n    <td><a href=\"https://crates.io/crates/ruvector-mincut-gated-transformer\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-mincut-gated-transformer.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-mincut-gated-transformer-wasm\">ruvector-mincut-gated-transformer-wasm</a></td> \n    <td>WASM bindings for mincut-gated transformer</td> \n    <td><a href=\"https://crates.io/crates/ruvector-mincut-gated-transformer-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-mincut-gated-transformer-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Neuromorphic &amp; Bio-Inspired Learning</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-nervous-system\">ruvector-nervous-system</a></td> \n    <td>Spiking neural networks with BTSP learning &amp; EWC plasticity</td> \n    <td><a href=\"https://crates.io/crates/ruvector-nervous-system\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-nervous-system.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-nervous-system-wasm\">ruvector-nervous-system-wasm</a></td> \n    <td>WASM bindings for neuromorphic learning</td> \n    <td><a href=\"https://crates.io/crates/ruvector-nervous-system-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-nervous-system-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-learning-wasm\">ruvector-learning-wasm</a></td> \n    <td>MicroLoRA adaptation (&lt;100µs latency)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-learning-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-learning-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-economy-wasm\">ruvector-economy-wasm</a></td> \n    <td>CRDT-based autonomous credit economy</td> \n    <td><a href=\"https://crates.io/crates/ruvector-economy-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-economy-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-exotic-wasm\">ruvector-exotic-wasm</a></td> \n    <td>Exotic AI primitives (strange loops, time crystals)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-exotic-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-exotic-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-attention-unified-wasm\">ruvector-attention-unified-wasm</a></td> \n    <td>Unified 18+ attention mechanisms (Neural, DAG, Mamba SSM)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-attention-unified-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-attention-unified-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/micro-hnsw-wasm\">micro-hnsw-wasm</a></td> \n    <td>Neuromorphic HNSW with spiking neurons (11.8KB WASM)</td> \n    <td><a href=\"https://crates.io/crates/micro-hnsw-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/micro-hnsw-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/thermorust\">thermorust</a></td> \n    <td>Thermodynamic neural motif engine — Ising/soft-spin Hamiltonians, Langevin dynamics, Landauer dissipation</td> \n    <td><a href=\"https://crates.io/crates/thermorust\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/thermorust.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Bio-inspired features:</strong></p> \n <ul> \n  <li><strong>Spiking Neural Networks (SNNs)</strong> — 10-50x energy efficiency vs traditional ANNs</li> \n  <li><strong>BTSP Learning</strong> — Behavioral Time-Scale Synaptic Plasticity for rapid adaptation</li> \n  <li><strong>MicroLoRA</strong> — Sub-microsecond fine-tuning for per-operator learning</li> \n  <li><strong>Mamba SSM</strong> — State Space Model attention for linear-time sequences</li> \n </ul> \n <h3>Cognitive Robotics</h3> \n <details> \n  Perception, planning, behavior trees, and swarm coordination for autonomous robots \n  <table> \n   <thead> \n    <tr> \n     <th>Crate</th> \n     <th>Description</th> \n     <th>crates.io</th> \n    </tr> \n   </thead> \n   <tbody> \n    <tr> \n     <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-robotics\">ruvector-robotics</a></td> \n     <td>Cognitive robotics platform — perception, A* planning, behavior trees, swarm coordination</td> \n     <td><a href=\"https://crates.io/crates/ruvector-robotics\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-robotics.svg?sanitize=true\" /></a></td> \n    </tr> \n   </tbody> \n  </table> \n  <p><strong>Modules:</strong></p> \n  <table> \n   <thead> \n    <tr> \n     <th>Module</th> \n     <th>What It Does</th> \n    </tr> \n   </thead> \n   <tbody> \n    <tr> \n     <td><strong>bridge</strong></td> \n     <td>OccupancyGrid, PointCloud, SensorFrame, SceneGraph data types with spatial kNN</td> \n    </tr> \n    <tr> \n     <td><strong>perception</strong></td> \n     <td>Scene-graph construction from point clouds, obstacle detection pipeline</td> \n    </tr> \n    <tr> \n     <td><strong>planning</strong></td> \n     <td>A* grid search (octile heuristic) and potential-field velocity commands</td> \n    </tr> \n    <tr> \n     <td><strong>cognitive</strong></td> \n     <td>Perceive-think-act-learn loop with utility-based reasoning</td> \n    </tr> \n    <tr> \n     <td><strong>domain_expansion</strong></td> \n     <td>Cross-domain transfer learning via Meta Thompson Sampling and Beta priors</td> \n    </tr> \n   </tbody> \n  </table> \n  <p><strong>Key features:</strong> 290 tests, clippy-clean, <code>no_std</code>-friendly types, optional <code>domain-expansion</code> feature flag for cross-domain transfer, pluggable <code>PotentialFieldConfig</code> for obstacle avoidance, Byzantine-tolerant swarm coordination via <code>ruvector-domain-expansion</code>.</p> \n  <pre><code class=\"language-rust\">use ruvector_robotics::planning::{astar, potential_field, PotentialFieldConfig};\nuse ruvector_robotics::bridge::OccupancyGrid;\n\nlet grid = OccupancyGrid::new(100, 100, 0.1);\nlet path = astar(&amp;grid, (5, 5), (90, 90))?;\nlet cmd = potential_field(&amp;[0.0, 0.0, 0.0], &amp;[5.0, 3.0, 0.0], &amp;[], &amp;PotentialFieldConfig::default());\n</code></pre> \n </details> \n <h3>Self-Learning (SONA)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/sona\">sona</a></td> \n    <td>Self-Optimizing Neural Architecture - LoRA, EWC++, ReasoningBank</td> \n    <td><a href=\"https://crates.io/crates/ruvector-sona\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-sona.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>SONA Features:</strong> Two-tier LoRA adaptation, Elastic Weight Consolidation (EWC++), ReasoningBank for trajectory learning, runtime-adaptive learning for LLM routers.</p> \n <h3>Genomics &amp; Health (rvDNA)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/dna\">rvdna</a></td> \n    <td>AI-native genomic analysis — variant calling, protein translation, HNSW k-mer search, <code>.rvdna</code> format</td> \n    <td><a href=\"https://crates.io/crates/rvdna\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvdna.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>rvDNA Features:</strong> 12 ms full pipeline on 5 real human genes, Bayesian variant calling (155 ns/SNP), Horvath epigenetic clock, CYP2D6 pharmacogenomics, <code>.rvdna</code> binary format with pre-computed AI features, WASM support for browser-based diagnostics. <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/dna/README.md\">Full README</a></p> \n <h3>RVF Cognitive Containers</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-types\">rvf-types</a></td> \n    <td>20 segment headers, COW/membership/delta types (<code>no_std</code>)</td> \n    <td><a href=\"https://crates.io/crates/rvf-types\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-types.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-wire\">rvf-wire</a></td> \n    <td>Wire format read/write (<code>no_std</code>)</td> \n    <td><a href=\"https://crates.io/crates/rvf-wire\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-wire.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-manifest\">rvf-manifest</a></td> \n    <td>Two-level manifest, FileIdentity, COW pointers</td> \n    <td><a href=\"https://crates.io/crates/rvf-manifest\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-manifest.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-quant\">rvf-quant</a></td> \n    <td>Scalar, product, binary quantization</td> \n    <td><a href=\"https://crates.io/crates/rvf-quant\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-quant.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-index\">rvf-index</a></td> \n    <td>HNSW progressive indexing (Layer A/B/C)</td> \n    <td><a href=\"https://crates.io/crates/rvf-index\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-index.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-crypto\">rvf-crypto</a></td> \n    <td>SHAKE-256, Ed25519, witness chains, attestation</td> \n    <td><a href=\"https://crates.io/crates/rvf-crypto\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-crypto.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-runtime\">rvf-runtime</a></td> \n    <td>Full store API, COW engine, compaction</td> \n    <td><a href=\"https://crates.io/crates/rvf-runtime\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-runtime.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-kernel\">rvf-kernel</a></td> \n    <td>Linux kernel builder, initramfs, Docker pipeline</td> \n    <td><a href=\"https://crates.io/crates/rvf-kernel\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-kernel.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-ebpf\">rvf-ebpf</a></td> \n    <td>Real BPF programs (XDP, socket filter, TC)</td> \n    <td><a href=\"https://crates.io/crates/rvf-ebpf\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-ebpf.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-federation\">rvf-federation</a></td> \n    <td>Federated transfer learning — PII stripping, differential privacy, FedAvg/FedProx</td> \n    <td><a href=\"https://crates.io/crates/rvf-federation\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-federation.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-launch\">rvf-launch</a></td> \n    <td>QEMU microvm launcher, KVM/TCG</td> \n    <td><a href=\"https://crates.io/crates/rvf-launch\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-launch.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-server\">rvf-server</a></td> \n    <td>HTTP REST + TCP streaming server</td> \n    <td><a href=\"https://crates.io/crates/rvf-server\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-server.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-import\">rvf-import</a></td> \n    <td>JSON, CSV, NumPy importers</td> \n    <td><a href=\"https://crates.io/crates/rvf-import\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-import.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-cli\">rvf-cli</a></td> \n    <td>Unified CLI with 17 subcommands</td> \n    <td><a href=\"https://crates.io/crates/rvf-cli\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvf-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>RVF Features:</strong> Single-file cognitive containers that boot as Linux microservices, COW branching at cluster granularity, eBPF acceleration, witness chains, post-quantum signatures, federated transfer learning with differential privacy, 28 segment types. <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">Full README</a></p> \n <h3>Formal Verification</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-verified\">ruvector-verified</a></td> \n    <td>Proof-carrying vector operations with lean-agentic dependent types (~500ns proofs)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-verified\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-verified.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-verified-wasm\">ruvector-verified-wasm</a></td> \n    <td>WASM bindings for browser/edge formal verification</td> \n    <td><a href=\"https://crates.io/crates/ruvector-verified-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-verified-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Verification Features:</strong> 82-byte proof attestations, 3-tier gated proof routing (Reflex &lt;10ns / Standard &lt;1us / Deep &lt;100us), FastTermArena with O(1) dedup, batch dimension verification (~11ns/vector), type-safe pipeline composition. <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-verified/README.md\">Full README</a></p> \n <details> \n  Formal Verification Details \n  <p><strong>How it works:</strong> Every vector operation produces a machine-checked proof term using lean-agentic dependent types. Proofs are constructed at compile-time semantics but execute at runtime with sub-microsecond overhead, then serialized into 82-byte attestations that can be embedded in RVF witness chains.</p> \n  <table> \n   <thead> \n    <tr> \n     <th>Operation</th> \n     <th>Latency</th> \n     <th>Description</th> \n    </tr> \n   </thead> \n   <tbody> \n    <tr> \n     <td><code>ProofEnvironment::new()</code></td> \n     <td>~470ns</td> \n     <td>Initialize proof context with type declarations</td> \n    </tr> \n    <tr> \n     <td><code>prove_dim_eq(a, b)</code></td> \n     <td>~496ns</td> \n     <td>Dimension equality proof with FxHash caching</td> \n    </tr> \n    <tr> \n     <td><code>verify_batch_dimensions()</code></td> \n     <td>~11ns/vec</td> \n     <td>Batch verification for N vectors</td> \n    </tr> \n    <tr> \n     <td><code>compose_chain()</code></td> \n     <td>~1.2us</td> \n     <td>Type-safe pipeline composition</td> \n    </tr> \n    <tr> \n     <td><code>create_attestation()</code></td> \n     <td>~180ns</td> \n     <td>82-byte formal proof witness</td> \n    </tr> \n    <tr> \n     <td><code>FastTermArena::intern()</code></td> \n     <td>~1.6ns hit</td> \n     <td>O(1) dedup with 4-wide linear probe</td> \n    </tr> \n    <tr> \n     <td><code>gated::route_proof()</code></td> \n     <td>&lt;10ns</td> \n     <td>3-tier routing: Reflex / Standard / Deep</td> \n    </tr> \n   </tbody> \n  </table> \n  <p><strong>10 Exotic Application Domains</strong> (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/verified-applications\">examples/verified-applications</a>):</p> \n  <ol> \n   <li><strong>Autonomous Weapons Filter</strong> — certified targeting pipeline blocks tampered sensors</li> \n   <li><strong>Medical Diagnostics</strong> — proof-carrying ECG analysis with patient-keyed attestations</li> \n   <li><strong>Financial Order Routing</strong> — verified trade execution with proof-hash audit trail</li> \n   <li><strong>Multi-Agent Contracts</strong> — dimension + metric + depth contracts enforced by proof</li> \n   <li><strong>Distributed Sensor Swarm</strong> — coherence verification across heterogeneous nodes</li> \n   <li><strong>Quantization Proof</strong> — certify quantization error within formal tolerance bounds</li> \n   <li><strong>Verifiable Synthetic Memory</strong> — AGI memory with per-embedding proof attestations</li> \n   <li><strong>Cryptographic Vector Signatures</strong> — model-keyed signatures with contract matching</li> \n   <li><strong>Simulation Integrity</strong> — proof receipt per step for reproducible physics</li> \n   <li><strong>Legal Forensics</strong> — court-admissible replay bundles with structural invariants</li> \n  </ol> \n  <pre><code class=\"language-rust\">use ruvector_verified::{ProofEnvironment, vector_types, proof_store};\n\nlet mut env = ProofEnvironment::new();\nlet proof = vector_types::prove_dim_eq(&amp;mut env, 384, 384).unwrap();\nlet att = proof_store::create_attestation(&amp;env, proof);\nassert_eq!(att.to_bytes().len(), 82); // 82-byte formal witness\n</code></pre> \n </details> \n <p><strong>Self-booting example</strong> — the <code>claude_code_appliance</code> builds a complete AI dev environment as one file:</p> \n <pre><code class=\"language-bash\">cd examples/rvf &amp;&amp; cargo run --example claude_code_appliance\n</code></pre> \n <p>Final file: <strong>5.1 MB single <code>.rvf</code></strong> — boots Linux, serves queries, runs Claude Code. One file. Boots on QEMU/Firecracker. Runs SSH. Serves vectors. Installs Claude Code. Proves every step with a cryptographic witness chain.</p> \n <h3>Graph Transformer</h3> \n <p><a href=\"https://crates.io/crates/ruvector-graph-transformer\"><img alt=\"Crates.io\" src=\"https://img.shields.io/crates/v/ruvector-graph-transformer.svg?sanitize=true\" /></a></p> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>Registry</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-transformer\">ruvector-graph-transformer</a></td> \n    <td>Unified graph transformer with proof-gated mutation substrate (8 modules, 186 tests)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-graph-transformer\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-graph-transformer.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-transformer-wasm\">ruvector-graph-transformer-wasm</a></td> \n    <td>WASM bindings for browser-side graph transformers</td> \n    <td><a href=\"https://crates.io/crates/ruvector-graph-transformer-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-graph-transformer-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-graph-transformer-node\">ruvector-graph-transformer-node</a></td> \n    <td>Node.js NAPI-RS bindings (22+ methods, 20 tests)</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-transformer\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/graph-transformer.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>What it does:</strong> Every time you modify a graph — adding a node, changing an edge weight, updating a vector — the graph transformer requires a formal proof that the operation is valid <em>before</em> it executes. Think of it like a type system for graph mutations: you can't accidentally corrupt your data because the system mathematically verifies every change.</p> \n <p>On top of that proof layer, 8 specialized modules handle different aspects of graph intelligence:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Module</th> \n    <th>What It Does (Plain English)</th> \n    <th>Feature Flag</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Proof-Gated Mutation</strong></td> \n    <td>Locks graph data behind mathematical proofs — no proof, no access</td> \n    <td>always on</td> \n   </tr> \n   <tr> \n    <td><strong>Sublinear Attention</strong></td> \n    <td>Finds the most important nodes without checking every single one — scales to millions</td> \n    <td><code>sublinear</code></td> \n   </tr> \n   <tr> \n    <td><strong>Physics-Informed</strong></td> \n    <td>Applies physics equations (conservation of energy, symmetry) to message passing</td> \n    <td><code>physics</code></td> \n   </tr> \n   <tr> \n    <td><strong>Biological</strong></td> \n    <td>Models neurons that only fire when excited enough — naturally sparse, energy-efficient</td> \n    <td><code>biological</code></td> \n   </tr> \n   <tr> \n    <td><strong>Self-Organizing</strong></td> \n    <td>Graphs that grow and reorganize themselves like biological development</td> \n    <td><code>self-organizing</code></td> \n   </tr> \n   <tr> \n    <td><strong>Verified Training</strong></td> \n    <td>Training with a receipt — if a gradient step would break an invariant, it's automatically rolled back</td> \n    <td><code>verified-training</code></td> \n   </tr> \n   <tr> \n    <td><strong>Manifold</strong></td> \n    <td>Operates in curved spaces (like the surface of a sphere) instead of just flat Euclidean space</td> \n    <td><code>manifold</code></td> \n   </tr> \n   <tr> \n    <td><strong>Temporal-Causal</strong></td> \n    <td>Enforces that information flows forward in time — no peeking at the future</td> \n    <td><code>temporal</code></td> \n   </tr> \n   <tr> \n    <td><strong>Economic</strong></td> \n    <td>Uses game theory to allocate attention fairly — Nash equilibrium for node importance</td> \n    <td><code>economic</code></td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\"># Rust\ncargo add ruvector-graph-transformer --features full\n\n# Node.js\nnpm install @ruvector/graph-transformer\n</code></pre> \n <pre><code class=\"language-rust\">use ruvector_graph_transformer::sublinear_attention::SublinearGraphAttention;\nuse ruvector_graph_transformer::config::SublinearConfig;\n\nlet attn = SublinearGraphAttention::new(128, SublinearConfig::default());\nlet outputs = attn.lsh_attention(&amp;features).unwrap(); // O(n log n)\n</code></pre> \n <pre><code class=\"language-javascript\">const { GraphTransformer } = require('@ruvector/graph-transformer');\nconst gt = new GraphTransformer();\n\n// Every operation produces a proof receipt\nconst proof = gt.proveDimension(128, 128);\nconst attestation = gt.createAttestation(proof.proof_id); // 82 bytes\n</code></pre> \n <details> \n  Graph Transformer Architecture Details \n  <p><strong>Proof-gated mutation</strong> means <code>state_n -&gt; proof(invariant) -&gt; mutation -&gt; state_n+1</code>. The <code>ProofGate&lt;T&gt;</code> type wraps any value so you literally cannot access the inner data without first producing a valid proof. This is enforced at the type level in Rust and at runtime in WASM/Node.js.</p> \n  <p><strong>Sublinear attention</strong> uses three strategies: (1) locality-sensitive hashing groups similar nodes into buckets, (2) personalized PageRank samples the most relevant neighbors via random walks, (3) spectral sparsification prunes edges that don't contribute to graph connectivity. All three reduce O(n^2) full attention to O(n log n).</p> \n  <p><strong>Verified training</strong> uses a delta-apply architecture: gradients go to a scratch buffer first, invariants are checked against the proposed weights (loss stability, weight norms, Lipschitz bounds, energy gates), and only if all checks pass are the weights committed. If any check fails and <code>fail_closed = true</code>, the step is rejected and the old weights are preserved. Every successful step produces a <code>TrainingCertificate</code> with BLAKE3 hashes of weights, config, and dataset manifest.</p> \n  <p><strong>10 ADRs</strong> document every design decision: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-046-graph-transformer-architecture.md\">ADR-046</a> through <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/ADR-055-manifold-graph-layers.md\">ADR-055</a>.</p> \n </details> \n <h3>Personal AI Memory (OSpipe)</h3> \n <p><a href=\"https://www.npmjs.com/package/@ruvector/ospipe\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ospipe.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/@ruvector/ospipe-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ospipe-wasm.svg?sanitize=true\" /></a></p> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Registry</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/OSpipe\">ospipe</a></td> \n    <td>RuVector-enhanced personal AI memory for Screenpipe</td> \n    <td><a href=\"https://crates.io/crates/ospipe\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ospipe.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ospipe\">@ruvector/ospipe</a></td> \n    <td>TypeScript SDK with retry, timeout, and AbortSignal</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ospipe\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ospipe.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ospipe-wasm\">@ruvector/ospipe-wasm</a></td> \n    <td>WASM bindings for browser deployment (145 KB)</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/ospipe-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ospipe-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\">npm install @ruvector/ospipe         # TypeScript SDK\nnpm install @ruvector/ospipe-wasm    # Browser WASM\ncargo add ospipe                     # Rust crate\n</code></pre> \n <p><strong>Replaces Screenpipe's SQLite/FTS5 backend with semantic vector search.</strong> Ask your computer what you saw, heard, and did -- with semantic understanding.</p> \n <details> \n  OSpipe Features &amp; Capabilities \n  <table> \n   <thead> \n    <tr> \n     <th>Feature</th> \n     <th>Description</th> \n    </tr> \n   </thead> \n   <tbody> \n    <tr> \n     <td><strong>HNSW Vector Search</strong></td> \n     <td>61us p50 query latency via <code>ruvector-core</code></td> \n    </tr> \n    <tr> \n     <td><strong>Knowledge Graph</strong></td> \n     <td>Entity extraction (persons, URLs, emails, mentions) via <code>ruvector-graph</code></td> \n    </tr> \n    <tr> \n     <td><strong>Attention Reranking</strong></td> \n     <td>Content prioritization via <code>ruvector-attention</code></td> \n    </tr> \n    <tr> \n     <td><strong>Quantum Diversity</strong></td> \n     <td>MMR + quantum-inspired result selection via <code>ruqu-algorithms</code></td> \n    </tr> \n    <tr> \n     <td><strong>GNN Learning</strong></td> \n     <td>Search quality improves over time via <code>ruvector-gnn</code></td> \n    </tr> \n    <tr> \n     <td><strong>PII Safety Gate</strong></td> \n     <td>Auto-redacts credit cards, SSNs, emails before storage</td> \n    </tr> \n    <tr> \n     <td><strong>Frame Deduplication</strong></td> \n     <td>Cosine similarity sliding window eliminates near-duplicates</td> \n    </tr> \n    <tr> \n     <td><strong>Query Router</strong></td> \n     <td>Auto-routes to Semantic, Keyword, Graph, Temporal, or Hybrid backend</td> \n    </tr> \n    <tr> \n     <td><strong>4-Tier Quantization</strong></td> \n     <td>f32 -&gt; int8 -&gt; product -&gt; binary (97% memory savings over time)</td> \n    </tr> \n    <tr> \n     <td><strong>REST API</strong></td> \n     <td>Axum server with <code>/v2/search</code>, <code>/v2/route</code>, <code>/v2/stats</code>, <code>/v2/health</code></td> \n    </tr> \n    <tr> \n     <td><strong>WASM Support</strong></td> \n     <td>Runs in browser (145 KB), bundles from 11.8 KB (micro) to 350 KB (full)</td> \n    </tr> \n    <tr> \n     <td><strong>Cross-Platform</strong></td> \n     <td>Native: Linux, macOS, Windows; WASM: any browser</td> \n    </tr> \n   </tbody> \n  </table> \n  <p><strong>Comparison: Screenpipe vs OSpipe</strong></p> \n  <table> \n   <thead> \n    <tr> \n     <th></th> \n     <th>Screenpipe (FTS5)</th> \n     <th>OSpipe (RuVector)</th> \n    </tr> \n   </thead> \n   <tbody> \n    <tr> \n     <td>Search</td> \n     <td>Keyword (FTS5)</td> \n     <td>Semantic + Keyword + Graph + Temporal</td> \n    </tr> \n    <tr> \n     <td>Latency</td> \n     <td>~1ms (FTS5)</td> \n     <td>61us (HNSW p50)</td> \n    </tr> \n    <tr> \n     <td>Relations</td> \n     <td>None</td> \n     <td>Knowledge Graph (Cypher)</td> \n    </tr> \n    <tr> \n     <td>PII</td> \n     <td>Basic</td> \n     <td>Credit card, SSN, email redaction</td> \n    </tr> \n    <tr> \n     <td>Dedup</td> \n     <td>None</td> \n     <td>Cosine similarity sliding window</td> \n    </tr> \n    <tr> \n     <td>Browser</td> \n     <td>None</td> \n     <td>WASM (11.8 KB - 350 KB)</td> \n    </tr> \n    <tr> \n     <td>Quantization</td> \n     <td>None</td> \n     <td>4-tier age-based (f32 -&gt; binary)</td> \n    </tr> \n   </tbody> \n  </table> \n  <p><strong>Integrates 10 RuVector crates:</strong> ruvector-core, ruvector-filter, ruvector-cluster, ruvector-delta-core, ruvector-router-core, cognitum-gate-kernel, ruvector-graph, ruvector-attention, ruvector-gnn, ruqu-algorithms.</p> \n </details> \n <pre><code class=\"language-rust\">use ospipe::config::OsPipeConfig;\nuse ospipe::pipeline::ingestion::IngestionPipeline;\nuse ospipe::capture::CapturedFrame;\n\nlet config = OsPipeConfig::default();\nlet mut pipeline = IngestionPipeline::new(config)?;\n\n// Ingest a screen capture\nlet frame = CapturedFrame::new_screen(\"Firefox\", \"Meeting Notes\", \"auth discussion: JWT with refresh tokens\", 0);\npipeline.ingest(frame)?;\n\n// Semantic search\nlet results = pipeline.search(\"what was the authentication discussion?\", 5)?;\n</code></pre> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/OSpipe/README.md\">OSpipe README</a> for full documentation, TypeScript/WASM quickstart, and configuration reference.</p> \n <h3>Standalone Edge Database (rvLite)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvlite\">rvlite</a></td> \n    <td>Standalone 2MB edge database with SQL, SPARQL, Cypher</td> \n    <td><a href=\"https://crates.io/crates/rvlite\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvlite.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>rvLite Features:</strong> Powered by RuVector WASM, supports SQL/SPARQL/Cypher queries, ideal for IoT, mobile, and embedded systems.</p> \n <h3>PostgreSQL Extension</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-postgres\">ruvector-postgres</a></td> \n    <td>pgvector replacement with 230+ SQL functions, SIMD, Flash Attention</td> \n    <td><a href=\"https://crates.io/crates/ruvector-postgres\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-postgres.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>PostgreSQL Features:</strong> Drop-in pgvector replacement, GNN layers, hybrid search, multi-tenancy, self-healing, self-learning capabilities.</p> \n <pre><code class=\"language-bash\">docker pull ruvnet/ruvector-postgres    # Docker image\ncargo add ruvector-postgres             # Rust crate\n</code></pre> \n <h3>Self-Learning Query DAG (ruvector-dag)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-dag\">ruvector-dag</a></td> \n    <td>Neural self-learning DAG for automatic query optimization</td> \n    <td><a href=\"https://crates.io/crates/ruvector-dag\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-dag.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-dag-wasm\">ruvector-dag-wasm</a></td> \n    <td>WASM bindings for browser DAG optimization (58KB gzipped)</td> \n    <td><a href=\"https://crates.io/crates/ruvector-dag-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-dag-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Make your queries faster automatically.</strong> RuVector DAG learns from every query execution and continuously optimizes performance—no manual tuning required.</p> \n <ul> \n  <li><strong>7 Attention Mechanisms</strong>: Automatically selects the best strategy (Topological, Causal Cone, Critical Path, MinCut Gated, etc.)</li> \n  <li><strong>SONA Learning</strong>: Self-Optimizing Neural Architecture adapts in &lt;100μs per query</li> \n  <li><strong>MinCut Control</strong>: Rising \"tension\" triggers automatic strategy switching and predictive healing</li> \n  <li><strong>50-80% Latency Reduction</strong>: Queries improve over time without code changes</li> \n </ul> \n <pre><code class=\"language-rust\">use ruvector_dag::{QueryDag, OperatorNode};\nuse ruvector_dag::attention::{AttentionSelector, SelectionPolicy};\n\nlet mut dag = QueryDag::new();\nlet scan = dag.add_node(OperatorNode::hnsw_scan(0, \"vectors_idx\", 64));\nlet filter = dag.add_node(OperatorNode::filter(1, \"score &gt; 0.5\"));\ndag.add_edge(scan, filter).unwrap();\n\n// System learns which attention mechanism works best\nlet selector = AttentionSelector::new();\nlet scores = selector.select_and_apply(SelectionPolicy::Adaptive, &amp;dag)?;\n</code></pre> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-dag/README.md\">ruvector-dag README</a> for full documentation.</p> \n <h3>Temporal Tensor Store</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-temporal-tensor\">ruvector-temporal-tensor</a></td> \n    <td>Time-series tensor storage with tiered quantization</td> \n    <td><a href=\"https://crates.io/crates/ruvector-temporal-tensor\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-temporal-tensor.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-temporal-tensor-wasm\">ruvector-temporal-tensor-wasm</a></td> \n    <td>WASM bindings for browser temporal tensors</td> \n    <td><a href=\"https://crates.io/crates/ruvector-temporal-tensor-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-temporal-tensor-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>High-performance temporal embedding storage</strong> optimized for AI agent memory systems:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Block-Based Storage</strong></td> \n    <td>4KB aligned blocks with SIMD-optimized I/O (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/temporal-tensor-store/ADR-018-block-based-storage-engine.md\">ADR-018</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>Tiered Quantization</strong></td> \n    <td>F32 → F16 → INT8 → INT4 with &lt;1% accuracy loss (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/temporal-tensor-store/ADR-019-tiered-quantization-formats.md\">ADR-019</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>Temporal Scoring</strong></td> \n    <td>Access frequency + recency decay for automatic tier migration (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/temporal-tensor-store/ADR-020-temporal-scoring-tier-migration.md\">ADR-020</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>Delta Compression</strong></td> \n    <td>60-80% storage reduction via temporal differencing (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/temporal-tensor-store/ADR-021-delta-compression-reconstruction.md\">ADR-021</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>Cross-Platform WASM</strong></td> \n    <td>Unified API for browser, Node.js, and edge (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/temporal-tensor-store/ADR-022-wasm-api-cross-platform.md\">ADR-022</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>AgentDB Integration</strong></td> \n    <td>Native coherence scoring and memory persistence</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Performance Targets:</strong> &gt;100K writes/sec, &lt;1ms p99 read latency, 4-32x compression (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/temporal-tensor-store/ADR-023-benchmarking-acceptance-criteria.md\">ADR-023</a>)</p> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/architecture/temporal-tensor-store-ddd.md\">Domain-Driven Design</a> for architecture details.</p> \n <h3>Delta Behavior (Behavioral Change Tracking)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-delta-core\">ruvector-delta-core</a></td> \n    <td>Core delta types and traits for behavioral vector change tracking</td> \n    <td><a href=\"https://crates.io/crates/ruvector-delta-core\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-delta-core.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-delta-graph\">ruvector-delta-graph</a></td> \n    <td>Delta operations for graph structures — edge and node changes</td> \n    <td><a href=\"https://crates.io/crates/ruvector-delta-graph\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-delta-graph.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-delta-index\">ruvector-delta-index</a></td> \n    <td>Delta-aware HNSW index with incremental updates and repair</td> \n    <td><a href=\"https://crates.io/crates/ruvector-delta-index\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-delta-index.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-delta-consensus\">ruvector-delta-consensus</a></td> \n    <td>Distributed delta consensus using CRDTs and causal ordering</td> \n    <td><a href=\"https://crates.io/crates/ruvector-delta-consensus\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-delta-consensus.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-delta-wasm\">ruvector-delta-wasm</a></td> \n    <td>WASM bindings for delta operations on vectors</td> \n    <td><a href=\"https://crates.io/crates/ruvector-delta-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-delta-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Delta Behavior</strong> tracks how vectors change over time — the mathematics of systems that refuse to collapse. Incremental HNSW updates without full rebuilds, CRDT-based distributed consensus, and causal ordering for conflict-free replication.</p> \n <h3>Profiling &amp; Diagnostics</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/profiling\">profiling</a></td> \n    <td>Real-time coherence assessment via dynamic min-cut</td> \n    <td><a href=\"https://crates.io/crates/profiling\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/profiling.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>CRV Signal Line Protocol</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-crv\">ruvector-crv</a></td> \n    <td>6-stage CRV signal line methodology for vector search</td> \n    <td><a href=\"https://crates.io/crates/ruvector-crv\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-crv.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Maps CRV stages to ruvector subsystems:</strong></p> \n <ul> \n  <li>Stage I (Ideograms) → Poincaré ball hyperbolic embeddings</li> \n  <li>Stage II (Sensory) → Multi-head attention vectors</li> \n  <li>Stage III (Dimensional) → GNN graph topology</li> \n  <li>Stage IV (Emotional) → SNN temporal encoding</li> \n  <li>Stage V (Interrogation) → Differentiable search</li> \n  <li>Stage VI (3D Model) → MinCut partitioning</li> \n </ul> \n <h3>Quantum Simulation Engine (ruQu)</h3> \n <p><a href=\"https://www.npmjs.com/package/@ruvector/ruqu-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/ruqu-wasm.svg?sanitize=true\" /></a></p> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruqu-core\">ruqu-core</a></td> \n    <td>State-vector simulator with SIMD, noise models</td> \n    <td><a href=\"https://crates.io/crates/ruqu-core\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruqu-core.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruqu-algorithms\">ruqu-algorithms</a></td> \n    <td>VQE, Grover's search, QAOA MaxCut, Surface Code QEC</td> \n    <td><a href=\"https://crates.io/crates/ruqu-algorithms\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruqu-algorithms.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruqu-exotic\">ruqu-exotic</a></td> \n    <td>Quantum-classical hybrids: decay, interference, syndrome</td> \n    <td><a href=\"https://crates.io/crates/ruqu-exotic\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruqu-exotic.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruqu-wasm\">ruqu-wasm</a></td> \n    <td>WebAssembly bindings (npm: <code>@ruvector/ruqu-wasm</code>)</td> \n    <td><a href=\"https://crates.io/crates/ruqu-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruqu-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-bash\">npm install @ruvector/ruqu-wasm    # Browser/Node.js\ncargo add ruqu-core                # Rust\n</code></pre> \n <p><strong>Pure Rust quantum simulation</strong> with 25-qubit WASM support:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>State-Vector Simulator</strong></td> \n    <td>Complex128 amplitudes, SIMD acceleration (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/quantum-engine/ADR-QE-001-quantum-engine-core-architecture.md\">QE-001</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>VQE Algorithm</strong></td> \n    <td>Variational Quantum Eigensolver for chemistry (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/quantum-engine/ADR-QE-005-vqe-algorithm-support.md\">QE-005</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>Grover's Search</strong></td> \n    <td>Quadratic speedup for unstructured search (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/quantum-engine/ADR-QE-006-grover-search-implementation.md\">QE-006</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>QAOA MaxCut</strong></td> \n    <td>Quantum approximate optimization (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/quantum-engine/ADR-QE-007-qaoa-maxcut-implementation.md\">QE-007</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>Surface Code QEC</strong></td> \n    <td>Topological error correction (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/quantum-engine/ADR-QE-008-surface-code-error-correction.md\">QE-008</a>)</td> \n   </tr> \n   <tr> \n    <td><strong>MinCut Coherence</strong></td> \n    <td>Quantum-classical integration via dynamic min-cut (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/quantum-engine/ADR-QE-012-mincut-coherence-integration.md\">QE-012</a>)</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-rust\">use ruqu_core::{QuantumState, Gate, Circuit};\n\nlet mut circuit = Circuit::new(3);\ncircuit.add_gate(Gate::H, 0);           // Hadamard\ncircuit.add_gate(Gate::CNOT, 0, 1);     // Entangle\ncircuit.add_gate(Gate::CNOT, 1, 2);     // GHZ state\n\nlet state = circuit.execute()?;\nlet result = state.measure_all();        // Collapse to |000⟩ or |111⟩\n</code></pre> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/adr/quantum-engine/\">Quantum Engine ADRs</a> for full documentation.</p> \n <h3>Distributed Systems (Raft &amp; Replication)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-raft\">ruvector-raft</a></td> \n    <td>Raft consensus with leader election &amp; log replication</td> \n    <td><a href=\"https://crates.io/crates/ruvector-raft\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-raft.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-replication\">ruvector-replication</a></td> \n    <td>Multi-master replication with vector clocks</td> \n    <td><a href=\"https://crates.io/crates/ruvector-replication\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-replication.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-cluster\">ruvector-cluster</a></td> \n    <td>Cluster coordination and sharding</td> \n    <td><a href=\"https://crates.io/crates/ruvector-cluster\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-cluster.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Build distributed vector databases</strong> with strong consistency guarantees:</p> \n <ul> \n  <li><strong>Raft Consensus</strong> — Leader election, log replication, automatic failover</li> \n  <li><strong>Vector Clocks</strong> — Causal ordering for conflict detection</li> \n  <li><strong>Conflict Resolution</strong> — Last-Write-Wins, custom merge functions, CRDT support</li> \n  <li><strong>Change Data Capture</strong> — Stream changes to replicas in real-time</li> \n  <li><strong>Automatic Failover</strong> — Promote replicas on primary failure</li> \n </ul> \n <pre><code class=\"language-typescript\">import { RaftNode, ReplicaSet, VectorClock } from '@ruvector/raft';\nimport { ReplicationManager, ConflictStrategy } from '@ruvector/replication';\n\n// Raft consensus cluster\nconst node = new RaftNode({\n  nodeId: 'node-1',\n  peers: ['node-2', 'node-3'],\n  electionTimeout: [150, 300],\n});\n\nawait node.start();\nconst entry = await node.propose({ op: 'insert', vector: embedding });\n\n// Multi-master replication\nconst replicaSet = new ReplicaSet();\nreplicaSet.addReplica('primary', 'localhost:5001', 'primary');\nreplicaSet.addReplica('replica-1', 'localhost:5002', 'replica');\n\nconst manager = new ReplicationManager(replicaSet, {\n  conflictStrategy: ConflictStrategy.LastWriteWins,\n  syncMode: 'async',\n});\n\nawait manager.write('vectors', { id: 'v1', data: embedding });\n</code></pre> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/npm/packages/raft/README.md\">npm/packages/raft/README.md</a> and <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/npm/packages/replication/README.md\">npm/packages/replication/README.md</a> for full documentation.</p> \n <h3>Standalone Vector Database (rvLite)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvlite\">rvlite</a></td> \n    <td>SQLite-style vector database for browsers &amp; edge</td> \n    <td><a href=\"https://crates.io/crates/rvlite\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvlite.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Runs anywhere JavaScript runs</strong> — browsers, Node.js, Deno, Bun, Cloudflare Workers, Vercel Edge:</p> \n <ul> \n  <li><strong>SQL + SPARQL + Cypher</strong> unified query interface</li> \n  <li><strong>Zero dependencies</strong> — thin orchestration over existing WASM crates</li> \n  <li><strong>Self-learning</strong> via SONA ReasoningBank integration</li> \n </ul> \n <pre><code class=\"language-typescript\">import { RvLite } from '@rvlite/wasm';\n\nconst db = await RvLite.create();\nawait db.sql(`CREATE TABLE docs (id SERIAL, embedding VECTOR(384))`);\nawait db.sparql(`SELECT ?s WHERE { ?s rdf:type ex:Document }`);\nawait db.cypher(`MATCH (d:Doc)-[:SIMILAR]-&gt;(r) RETURN r`);\n</code></pre> \n <h3>Self-Optimizing Neural Architecture (SONA)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n    <th>npm</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/sona\">ruvector-sona</a></td> \n    <td>Runtime-adaptive learning with LoRA, EWC++, and ReasoningBank</td> \n    <td><a href=\"https://crates.io/crates/ruvector-sona\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-sona.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/sona\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/sona.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>SONA</strong> enables AI systems to continuously improve from user feedback without expensive retraining:</p> \n <ul> \n  <li><strong>Two-tier LoRA</strong>: MicroLoRA (rank 1-2) for instant adaptation, BaseLoRA (rank 4-16) for long-term learning</li> \n  <li><strong>EWC++</strong>: Elastic Weight Consolidation prevents catastrophic forgetting</li> \n  <li><strong>ReasoningBank</strong>: K-means++ clustering stores and retrieves successful reasoning patterns</li> \n  <li><strong>Lock-free Trajectories</strong>: ~50ns overhead per step with crossbeam ArrayQueue</li> \n  <li><strong>Sub-millisecond Learning</strong>: &lt;0.8ms per trajectory processing</li> \n </ul> \n <pre><code class=\"language-bash\"># Rust\ncargo add ruvector-sona\n\n# Node.js\nnpm install @ruvector/sona\n</code></pre> \n <pre><code class=\"language-rust\">use ruvector_sona::{SonaEngine, SonaConfig};\n\nlet engine = SonaEngine::new(SonaConfig::default());\nlet traj_id = engine.start_trajectory(query_embedding);\nengine.record_step(traj_id, node_id, 0.85, 150);\nengine.end_trajectory(traj_id, 0.90);\nengine.learn_from_feedback(LearningSignal::positive(50.0, 0.95));\n</code></pre> \n <pre><code class=\"language-javascript\">// Node.js\nconst { SonaEngine } = require('@ruvector/sona');\n\nconst engine = new SonaEngine(256); // 256 hidden dimensions\nconst trajId = engine.beginTrajectory([0.1, 0.2, ...]);\nengine.addTrajectoryStep(trajId, activations, attention, 0.9);\nengine.endTrajectory(trajId, 0.95);\n</code></pre> \n</details> \n<hr /> \n<h2>Platform Features</h2> \n<details> \n <strong>🔀 Self-Learning DAG (Query Optimization)</strong> \n <p><a href=\"https://crates.io/crates/ruvector-dag\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-dag.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/@ruvector/rudag\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rudag.svg?sanitize=true\" /></a></p> \n <p><strong>Make your queries faster automatically.</strong> RuVector DAG learns from every query execution and continuously optimizes performance—no manual tuning required.</p> \n <h3>What is RuVector DAG?</h3> \n <p>A <strong>self-learning query optimization system</strong>—like a \"nervous system\" for your database queries that:</p> \n <ol> \n  <li><strong>Watches</strong> how queries execute and identifies bottlenecks</li> \n  <li><strong>Learns</strong> which optimization strategies work best for different query patterns</li> \n  <li><strong>Adapts</strong> in real-time, switching strategies when conditions change</li> \n  <li><strong>Heals</strong> itself by detecting anomalies and fixing problems before they impact users</li> \n </ol> \n <p>Unlike traditional query optimizers that use static rules, RuVector DAG learns from actual execution patterns and gets smarter over time.</p> \n <h3>Key Benefits</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Benefit</th> \n    <th>What It Does</th> \n    <th>Result</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Automatic Improvement</strong></td> \n    <td>Queries get faster without code changes</td> \n    <td><strong>50-80% latency reduction</strong> after learning</td> \n   </tr> \n   <tr> \n    <td><strong>Zero-Downtime Adaptation</strong></td> \n    <td>Adapts to pattern changes automatically</td> \n    <td>No manual index rebuilds</td> \n   </tr> \n   <tr> \n    <td><strong>Predictive Prevention</strong></td> \n    <td>Detects rising \"tension\" early</td> \n    <td>Intervenes <em>before</em> slowdowns</td> \n   </tr> \n   <tr> \n    <td><strong>Works Everywhere</strong></td> \n    <td>PostgreSQL, Browser (58KB WASM), Embedded</td> \n    <td>Universal deployment</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Use Cases</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>Why RuVector DAG Helps</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Vector Search Applications</strong></td> \n    <td>Optimize similarity searches that traditional databases struggle with</td> \n   </tr> \n   <tr> \n    <td><strong>High-Traffic APIs</strong></td> \n    <td>Automatically adapt to changing query patterns throughout the day</td> \n   </tr> \n   <tr> \n    <td><strong>Real-Time Analytics</strong></td> \n    <td>Learn which aggregation paths are fastest for your specific data</td> \n   </tr> \n   <tr> \n    <td><strong>Edge/Embedded Systems</strong></td> \n    <td>58KB WASM build runs in browsers and IoT devices</td> \n   </tr> \n   <tr> \n    <td><strong>Multi-Tenant Platforms</strong></td> \n    <td>Learn per-tenant query patterns without manual tuning</td> \n   </tr> \n  </tbody> \n </table> \n <h3>How It Works</h3> \n <pre><code>Query comes in → DAG analyzes execution plan → Best attention mechanism selected\n                                                          ↓\nQuery executes → Results returned → Learning system records what worked\n                                                          ↓\n                    Next similar query benefits from learned optimizations\n</code></pre> \n <p>The system maintains a <strong>MinCut tension</strong> score that acts as a health indicator. When tension rises, the system automatically switches to more aggressive optimization strategies and triggers predictive healing.</p> \n <h3>7 DAG Attention Mechanisms</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Mechanism</th> \n    <th>When to Use</th> \n    <th>Trigger</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Topological</strong></td> \n    <td>Default baseline</td> \n    <td>Low variance</td> \n   </tr> \n   <tr> \n    <td><strong>Causal Cone</strong></td> \n    <td>Downstream impact analysis</td> \n    <td>Write-heavy patterns</td> \n   </tr> \n   <tr> \n    <td><strong>Critical Path</strong></td> \n    <td>Latency-bound queries</td> \n    <td>p99 &gt; 2x p50</td> \n   </tr> \n   <tr> \n    <td><strong>MinCut Gated</strong></td> \n    <td>Bottleneck-aware weighting</td> \n    <td>Cut tension rising</td> \n   </tr> \n   <tr> \n    <td><strong>Hierarchical Lorentz</strong></td> \n    <td>Deep hierarchical queries</td> \n    <td>Depth &gt; 10</td> \n   </tr> \n   <tr> \n    <td><strong>Parallel Branch</strong></td> \n    <td>Wide parallel execution</td> \n    <td>Branch count &gt; 3</td> \n   </tr> \n   <tr> \n    <td><strong>Temporal BTSP</strong></td> \n    <td>Time-series workloads</td> \n    <td>Temporal patterns</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <p><strong>Rust:</strong></p> \n <pre><code class=\"language-rust\">use ruvector_dag::{QueryDag, OperatorNode, OperatorType};\nuse ruvector_dag::attention::{TopologicalAttention, DagAttention};\n\n// Build a query DAG\nlet mut dag = QueryDag::new();\nlet scan = dag.add_node(OperatorNode::hnsw_scan(0, \"vectors_idx\", 64));\nlet filter = dag.add_node(OperatorNode::filter(1, \"score &gt; 0.5\"));\nlet result = dag.add_node(OperatorNode::new(2, OperatorType::Result));\n\ndag.add_edge(scan, filter).unwrap();\ndag.add_edge(filter, result).unwrap();\n\n// Compute attention scores\nlet attention = TopologicalAttention::new(Default::default());\nlet scores = attention.forward(&amp;dag).unwrap();\n</code></pre> \n <p><strong>Node.js:</strong></p> \n <pre><code class=\"language-javascript\">import { QueryDag, TopologicalAttention } from '@ruvector/rudag';\n\n// Build DAG\nconst dag = new QueryDag();\nconst scan = dag.addNode({ type: 'hnsw_scan', table: 'vectors', k: 64 });\nconst filter = dag.addNode({ type: 'filter', condition: 'score &gt; 0.5' });\ndag.addEdge(scan, filter);\n\n// Apply attention\nconst attention = new TopologicalAttention();\nconst scores = attention.forward(dag);\nconsole.log('Attention scores:', scores);\n</code></pre> \n <p><strong>Browser (WASM - 58KB):</strong></p> \n <pre><code class=\"language-html\">&lt;script type=\"module\"&gt;\nimport init, { QueryDag, TopologicalAttention } from '@ruvector/rudag-wasm';\n\nawait init();\nconst dag = new QueryDag();\n// ... same API as Node.js\n&lt;/script&gt;\n</code></pre> \n <h3>SONA Learning Integration</h3> \n <p>SONA (Self-Optimizing Neural Architecture) runs post-query in background, never blocking execution:</p> \n <pre><code class=\"language-rust\">use ruvector_dag::sona::{DagSonaEngine, SonaConfig};\n\nlet config = SonaConfig {\n    embedding_dim: 256,\n    lora_rank: 2,           // Rank-2 for &lt;100μs updates\n    ewc_lambda: 5000.0,     // Catastrophic forgetting prevention\n    trajectory_capacity: 10_000,\n};\nlet mut sona = DagSonaEngine::new(config);\n\n// Pre-query: Get enhanced embedding (fast path)\nlet enhanced = sona.pre_query(&amp;dag);\n\n// Execute query...\nlet execution_time = execute_query(&amp;dag);\n\n// Post-query: Record trajectory (async, background)\nsona.post_query(&amp;dag, execution_time, baseline_time, \"topological\");\n</code></pre> \n <h3>Self-Healing</h3> \n <p>Reactive (Z-score anomaly detection) + Predictive (rising MinCut tension triggers early intervention):</p> \n <pre><code class=\"language-rust\">use ruvector_dag::healing::{HealingOrchestrator, AnomalyConfig, PredictiveConfig};\n\nlet mut orchestrator = HealingOrchestrator::new();\n\n// Reactive: Z-score anomaly detection\norchestrator.add_detector(\"query_latency\", AnomalyConfig {\n    z_threshold: 3.0,\n    window_size: 100,\n    min_samples: 10,\n});\n\n// Predictive: Rising cut tension triggers early intervention\norchestrator.enable_predictive(PredictiveConfig {\n    tension_threshold: 0.6,    // Intervene before 0.7 crisis\n    variance_threshold: 1.5,   // Rising variance = trouble coming\n    lookahead_window: 50,      // Predict 50 queries ahead\n});\n</code></pre> \n <h3>Query Convergence Example</h3> \n <p>A slow query converges over several runs:</p> \n <pre><code class=\"language-text\">[run 1] query: SELECT * FROM vectors WHERE embedding &lt;-&gt; $1 &lt; 0.5\n        attention: topological (default)\n        mincut_tension: 0.23\n        latency: 847ms (improvement: 0.4%)\n\n[run 4] mincut_tension: 0.71 &gt; 0.7 (THRESHOLD)\n        --&gt; switching attention: topological -&gt; mincut_gated\n        latency: 412ms (improvement: 51.5%)\n\n[run 10] attention: mincut_gated\n         mincut_tension: 0.22 (stable)\n         latency: 156ms (improvement: 81.6%)\n</code></pre> \n <h3>Performance Targets</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Component</th> \n    <th>Target</th> \n    <th>Notes</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Attention (100 nodes)</td> \n    <td>&lt;100μs</td> \n    <td>All 7 mechanisms</td> \n   </tr> \n   <tr> \n    <td>MicroLoRA adaptation</td> \n    <td>&lt;100μs</td> \n    <td>Rank-2, per-operator</td> \n   </tr> \n   <tr> \n    <td>Pattern search (10K)</td> \n    <td>&lt;2ms</td> \n    <td>K-means++ indexing</td> \n   </tr> \n   <tr> \n    <td>MinCut update</td> \n    <td>O(n^0.12)</td> \n    <td>Subpolynomial amortized</td> \n   </tr> \n   <tr> \n    <td>Anomaly detection</td> \n    <td>&lt;50μs</td> \n    <td>Z-score, streaming</td> \n   </tr> \n   <tr> \n    <td>WASM size</td> \n    <td>58KB</td> \n    <td>Gzipped, browser-ready</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Installation</h3> \n <pre><code class=\"language-bash\"># Rust\ncargo add ruvector-dag\n\n# Node.js\nnpm install @ruvector/rudag\n\n# WASM (browser)\nnpm install @ruvector/rudag-wasm\n</code></pre> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-dag/README.md\">ruvector-dag README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>📦 rvLite - Standalone Edge Database</strong> \n <p><a href=\"https://crates.io/crates/rvlite\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/rvlite.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/@ruvector/rvlite\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/rvlite.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/@ruvector/rvlite\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/rvlite.svg?sanitize=true\" /></a></p> \n <p><strong>A complete vector database that runs anywhere JavaScript runs</strong> — browsers, Node.js, Deno, Bun, Cloudflare Workers, Vercel Edge Functions.</p> \n <h3>What is rvLite?</h3> \n <p>rvLite is a <strong>lightweight, standalone vector database</strong> that runs entirely in WebAssembly. It provides SQL, SPARQL, and Cypher query interfaces, along with graph neural networks and self-learning capabilities—all in under 3MB.</p> \n <h3>Key Features</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>What It Does</th> \n    <th>Why It Matters</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>SQL Interface</strong></td> \n    <td>Familiar <code>SELECT</code>, <code>INSERT</code>, <code>WHERE</code></td> \n    <td>No learning curve</td> \n   </tr> \n   <tr> \n    <td><strong>SPARQL Support</strong></td> \n    <td>W3C-compliant RDF queries</td> \n    <td>Knowledge graphs in browser</td> \n   </tr> \n   <tr> \n    <td><strong>Cypher Queries</strong></td> \n    <td>Neo4j-style graph queries</td> \n    <td>Graph traversals anywhere</td> \n   </tr> \n   <tr> \n    <td><strong>GNN Embeddings</strong></td> \n    <td>Graph neural network layers</td> \n    <td>Self-learning search</td> \n   </tr> \n   <tr> \n    <td><strong>ReasoningBank</strong></td> \n    <td>Trajectory learning</td> \n    <td>Gets smarter over time</td> \n   </tr> \n   <tr> \n    <td><strong>SIMD Optimized</strong></td> \n    <td>Vector operations accelerated</td> \n    <td>Native-like performance</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Runs Everywhere</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Platform</th> \n    <th>Status</th> \n    <th>Use Case</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Browsers</strong></td> \n    <td>✅</td> \n    <td>Offline-first apps</td> \n   </tr> \n   <tr> \n    <td><strong>Node.js</strong></td> \n    <td>✅</td> \n    <td>Server-side</td> \n   </tr> \n   <tr> \n    <td><strong>Deno</strong></td> \n    <td>✅</td> \n    <td>Edge functions</td> \n   </tr> \n   <tr> \n    <td><strong>Bun</strong></td> \n    <td>✅</td> \n    <td>Fast runtime</td> \n   </tr> \n   <tr> \n    <td><strong>Cloudflare Workers</strong></td> \n    <td>✅</td> \n    <td>Edge computing</td> \n   </tr> \n   <tr> \n    <td><strong>Vercel Edge</strong></td> \n    <td>✅</td> \n    <td>Serverless</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Architecture</h3> \n <pre><code>┌─────────────────────────────────────────┐\n│  RvLite (Orchestration)                 │\n│  ├─ SQL executor                        │\n│  ├─ SPARQL executor                     │\n│  ├─ Cypher executor                     │\n│  └─ Unified WASM API                    │\n└──────────────┬──────────────────────────┘\n               │ depends on (100% reuse)\n               ▼\n┌──────────────────────────────────────────┐\n│  Existing WASM Crates                    │\n├──────────────────────────────────────────┤\n│  • ruvector-core (vectors, SIMD)         │\n│  • ruvector-graph-wasm (Cypher)          │\n│  • ruvector-gnn-wasm (GNN layers)        │\n│  • sona (ReasoningBank learning)         │\n│  • micro-hnsw-wasm (ultra-fast HNSW)     │\n└──────────────────────────────────────────┘\n</code></pre> \n <h3>Quick Start</h3> \n <pre><code class=\"language-typescript\">import { RvLite } from '@ruvector/rvlite';\n\n// Create database\nconst db = await RvLite.create();\n\n// SQL with vector search\nawait db.sql(`\n  CREATE TABLE docs (\n    id SERIAL PRIMARY KEY,\n    content TEXT,\n    embedding VECTOR(384)\n  )\n`);\n\nawait db.sql(`\n  SELECT id, content, embedding &lt;=&gt; $1 AS distance\n  FROM docs\n  ORDER BY distance\n  LIMIT 10\n`, [queryVector]);\n\n// Cypher graph queries\nawait db.cypher(`\n  CREATE (a:Person {name: 'Alice'})-[:KNOWS]-&gt;(b:Person {name: 'Bob'})\n`);\n\n// SPARQL RDF queries\nawait db.sparql(`\n  SELECT ?name WHERE {\n    ?person foaf:name ?name .\n  }\n`);\n\n// GNN embeddings\nconst embeddings = await db.gnn.computeEmbeddings('social_network', [\n  db.gnn.createLayer('gcn', { inputDim: 128, outputDim: 64 })\n]);\n\n// Self-learning with ReasoningBank\nawait db.learning.recordTrajectory({ state: [0.1], action: 2, reward: 1.0 });\nawait db.learning.train({ algorithm: 'q-learning', iterations: 1000 });\n</code></pre> \n <h3>Size Budget</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Component</th> \n    <th>Size</th> \n    <th>Purpose</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>ruvector-core</td> \n    <td>~500KB</td> \n    <td>Vectors, SIMD</td> \n   </tr> \n   <tr> \n    <td>SQL parser</td> \n    <td>~200KB</td> \n    <td>Query parsing</td> \n   </tr> \n   <tr> \n    <td>SPARQL executor</td> \n    <td>~300KB</td> \n    <td>RDF queries</td> \n   </tr> \n   <tr> \n    <td>Cypher (graph-wasm)</td> \n    <td>~600KB</td> \n    <td>Graph queries</td> \n   </tr> \n   <tr> \n    <td>GNN layers</td> \n    <td>~300KB</td> \n    <td>Neural networks</td> \n   </tr> \n   <tr> \n    <td>ReasoningBank (sona)</td> \n    <td>~300KB</td> \n    <td>Self-learning</td> \n   </tr> \n   <tr> \n    <td><strong>Total</strong></td> \n    <td><strong>~2.3MB</strong></td> \n    <td>Gzipped</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Installation</h3> \n <pre><code class=\"language-bash\"># npm\nnpm install @ruvector/rvlite\n\n# Rust\ncargo add rvlite\n\n# Build WASM\nwasm-pack build --target web --release\n</code></pre> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvlite/README.md\">rvlite README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🌐 Edge-Net - Collective AI Computing Network</strong> \n <p><a href=\"https://www.npmjs.com/package/@ruvector/edge-net\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/edge-net.svg?sanitize=true\" /></a></p> \n <p><strong>Share, Contribute, Compute Together</strong> — A distributed computing platform that enables collective resource sharing for AI workloads.</p> \n <h3>What is Edge-Net?</h3> \n <p>Edge-Net creates a <strong>collective computing network</strong> where participants share idle browser resources to power distributed AI workloads:</p> \n <pre><code>┌─────────────────────────────────────────────────────────────────────────────┐\n│              EDGE-NET: COLLECTIVE AI COMPUTING NETWORK                      │\n├─────────────────────────────────────────────────────────────────────────────┤\n│   ┌─────────────┐       ┌─────────────┐       ┌─────────────┐              │\n│   │  Your       │       │  Collective │       │  AI Tasks   │              │\n│   │  Browser    │◄─────►│  Network    │◄─────►│  Completed  │              │\n│   │  (Idle CPU) │  P2P  │  (1000s)    │       │  for You    │              │\n│   └─────────────┘       └─────────────┘       └─────────────┘              │\n│         │                     │                     │                       │\n│   Contribute ──────► Earn rUv Units ──────► Use for AI Workloads           │\n└─────────────────────────────────────────────────────────────────────────────┘\n</code></pre> \n <h3>How It Works</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Step</th> \n    <th>What Happens</th> \n    <th>Result</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>1. <strong>Contribute</strong></td> \n    <td>Share unused CPU cycles when browsing</td> \n    <td>Help the network</td> \n   </tr> \n   <tr> \n    <td>2. <strong>Earn</strong></td> \n    <td>Accumulate rUv (Resource Utility Vouchers)</td> \n    <td>Build credits</td> \n   </tr> \n   <tr> \n    <td>3. <strong>Use</strong></td> \n    <td>Spend rUv to run AI tasks</td> \n    <td>Access collective power</td> \n   </tr> \n   <tr> \n    <td>4. <strong>Network Grows</strong></td> \n    <td>More participants = more power</td> \n    <td>Everyone benefits</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Why Collective AI Computing?</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Traditional AI</th> \n    <th>Collective Edge-Net</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Expensive GPU servers</td> \n    <td>Free idle browser CPUs</td> \n   </tr> \n   <tr> \n    <td>Centralized data centers</td> \n    <td>Distributed global network</td> \n   </tr> \n   <tr> \n    <td>Pay-per-use pricing</td> \n    <td>Contribution-based access</td> \n   </tr> \n   <tr> \n    <td>Single point of failure</td> \n    <td>Resilient P2P mesh</td> \n   </tr> \n   <tr> \n    <td>Limited by your hardware</td> \n    <td>Scale with the collective</td> \n   </tr> \n  </tbody> \n </table> \n <h3>AI Intelligence Stack</h3> \n <pre><code>┌─────────────────────────────────────────────────────────────────────────────┐\n│                        AI INTELLIGENCE STACK                                 │\n├─────────────────────────────────────────────────────────────────────────────┤\n│  ┌─────────────────────────────────────────────────────────────────────┐   │\n│  │                    MicroLoRA Adapter Pool (from ruvLLM)              │   │\n│  │  • LRU-managed pool (16 slots) • &lt;50µs rank-1 forward               │   │\n│  │  • 4-bit/8-bit quantization    • 2,236+ ops/sec                     │   │\n│  └─────────────────────────────────────────────────────────────────────┘   │\n│  ┌─────────────────────────────────────────────────────────────────────┐   │\n│  │                    SONA - Self-Optimizing Neural Architecture         │   │\n│  │  • Instant Loop: Per-request MicroLoRA adaptation                    │   │\n│  │  • Background Loop: Hourly K-means consolidation                     │   │\n│  │  • Deep Loop: Weekly EWC++ (prevents catastrophic forgetting)        │   │\n│  └─────────────────────────────────────────────────────────────────────┘   │\n│  ┌──────────────────────┐  ┌──────────────────────┐  ┌─────────────────┐  │\n│  │   HNSW Vector Index  │  │  Federated Learning  │  │ ReasoningBank   │  │\n│  │   • 150x faster      │  │  • Byzantine tolerant│  │ • Pattern learn │  │\n│  │   • O(log N) search  │  │  • Diff privacy      │  │ • 87x energy    │  │\n│  └──────────────────────┘  └──────────────────────┘  └─────────────────┘  │\n└─────────────────────────────────────────────────────────────────────────────┘\n</code></pre> \n <h3>Core AI Tasks</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Task Type</th> \n    <th>Use Case</th> \n    <th>Performance</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Vector Search</strong></td> \n    <td>Find similar items</td> \n    <td>150x speedup via HNSW</td> \n   </tr> \n   <tr> \n    <td><strong>Embeddings</strong></td> \n    <td>Text understanding</td> \n    <td>Semantic vectors</td> \n   </tr> \n   <tr> \n    <td><strong>Semantic Match</strong></td> \n    <td>Intent detection</td> \n    <td>Classify meaning</td> \n   </tr> \n   <tr> \n    <td><strong>LoRA Inference</strong></td> \n    <td>Task adaptation</td> \n    <td>&lt;100µs forward</td> \n   </tr> \n   <tr> \n    <td><strong>Pattern Learning</strong></td> \n    <td>Self-optimization</td> \n    <td>ReasoningBank trajectories</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Pi-Key Identity System</h3> \n <p>Ultra-compact cryptographic identity using mathematical constants:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Key Type</th> \n    <th>Size</th> \n    <th>Purpose</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>π (Pi-Key)</strong></td> \n    <td>40 bytes</td> \n    <td>Permanent identity</td> \n   </tr> \n   <tr> \n    <td><strong>e (Session)</strong></td> \n    <td>34 bytes</td> \n    <td>Encrypted sessions</td> \n   </tr> \n   <tr> \n    <td><strong>φ (Genesis)</strong></td> \n    <td>21 bytes</td> \n    <td>Network origin markers</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Self-Optimizing Features</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Mechanism</th> \n    <th>Benefit</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Task Routing</strong></td> \n    <td>Multi-head attention</td> \n    <td>Work goes to best nodes</td> \n   </tr> \n   <tr> \n    <td><strong>Topology Optimization</strong></td> \n    <td>Self-organizing mesh</td> \n    <td>Network adapts to load</td> \n   </tr> \n   <tr> \n    <td><strong>Q-Learning Security</strong></td> \n    <td>Reinforcement learning</td> \n    <td>Learns to defend threats</td> \n   </tr> \n   <tr> \n    <td><strong>Economic Balance</strong></td> \n    <td>rUv token system</td> \n    <td>Self-sustaining economy</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <p><strong>Add to Your Website:</strong></p> \n <pre><code class=\"language-html\">&lt;script type=\"module\"&gt;\n  import init, { EdgeNetNode, EdgeNetConfig } from '@ruvector/edge-net';\n\n  async function joinCollective() {\n    await init();\n\n    // Join the collective\n    const node = new EdgeNetConfig('my-website')\n      .cpuLimit(0.3)          // Contribute 30% CPU when idle\n      .memoryLimit(256 * 1024 * 1024)  // 256MB max\n      .respectBattery(true)   // Reduce on battery\n      .build();\n\n    node.start();\n\n    // Monitor participation\n    setInterval(() =&gt; {\n      console.log(`Contributed: ${node.ruvBalance()} rUv`);\n    }, 10000);\n  }\n\n  joinCollective();\n&lt;/script&gt;\n</code></pre> \n <p><strong>Use the Collective's AI Power:</strong></p> \n <pre><code class=\"language-javascript\">// Submit an AI task to the collective\nconst result = await node.submitTask('vector_search', {\n  query: embeddings,\n  k: 10,\n  index: 'shared-knowledge-base'\n}, 5);  // Spend up to 5 rUv\n\nconsole.log('Similar items:', result);\n</code></pre> \n <p><strong>Monitor Your Contribution:</strong></p> \n <pre><code class=\"language-javascript\">const stats = node.getStats();\nconsole.log(`\n  rUv Earned: ${stats.ruv_earned}\n  rUv Spent: ${stats.ruv_spent}\n  Tasks Completed: ${stats.tasks_completed}\n  Reputation: ${(stats.reputation * 100).toFixed(1)}%\n`);\n</code></pre> \n <h3>Key Features</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Benefit</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Idle CPU Utilization</strong></td> \n    <td>Use resources that would otherwise be wasted</td> \n   </tr> \n   <tr> \n    <td><strong>Browser-Based</strong></td> \n    <td>No installation, runs in any modern browser</td> \n   </tr> \n   <tr> \n    <td><strong>Adjustable Contribution</strong></td> \n    <td>Control how much you share (10-50% CPU)</td> \n   </tr> \n   <tr> \n    <td><strong>Battery Aware</strong></td> \n    <td>Automatically reduces on battery power</td> \n   </tr> \n   <tr> \n    <td><strong>Fair Distribution</strong></td> \n    <td>Work routed based on capability matching</td> \n   </tr> \n   <tr> \n    <td><strong>Privacy-First</strong></td> \n    <td>Pi-Key cryptographic identity</td> \n   </tr> \n   <tr> \n    <td><strong>Federated Learning</strong></td> \n    <td>Learn collectively without sharing data</td> \n   </tr> \n   <tr> \n    <td><strong>Byzantine Tolerance</strong></td> \n    <td>Resilient to malicious nodes</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Installation</h3> \n <pre><code class=\"language-bash\"># npm\nnpm install @ruvector/edge-net\n\n# Or include directly\n&lt;script src=\"https://unpkg.com/@ruvector/edge-net\"&gt;&lt;/script&gt;\n</code></pre> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge-net/README.md\">edge-net README</a></p> \n </blockquote> \n</details> \n<hr /> \n<h2>AI &amp; Machine Learning</h2> \n<details> \n <strong>🎲 Agentic-Synth - AI Synthetic Data Generation</strong> \n <p><a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/agentic-synth.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/agentic-synth.svg?sanitize=true\" /></a></p> \n <p><strong>AI-Powered Synthetic Data Generation at Scale</strong> — Generate unlimited, high-quality synthetic data for training AI models, testing systems, and building robust agentic applications.</p> \n <h3>Why Agentic-Synth?</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Problem</th> \n    <th>Solution</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Real data is <strong>expensive</strong> to collect</td> \n    <td>Generate <strong>unlimited</strong> synthetic data</td> \n   </tr> \n   <tr> \n    <td><strong>Privacy-sensitive</strong> with compliance risks</td> \n    <td><strong>Fully synthetic</strong>, no PII concerns</td> \n   </tr> \n   <tr> \n    <td><strong>Slow</strong> to generate at scale</td> \n    <td><strong>10-100x faster</strong> than manual creation</td> \n   </tr> \n   <tr> \n    <td><strong>Insufficient</strong> for edge cases</td> \n    <td><strong>Customizable</strong> schemas for any scenario</td> \n   </tr> \n   <tr> \n    <td><strong>Hard to reproduce</strong> across environments</td> \n    <td><strong>Reproducible</strong> with seed values</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Key Features</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Multi-Model Support</strong></td> \n    <td>Gemini, OpenRouter, GPT, Claude, and 50+ models via DSPy.ts</td> \n   </tr> \n   <tr> \n    <td><strong>Context Caching</strong></td> \n    <td>95%+ performance improvement with intelligent LRU cache</td> \n   </tr> \n   <tr> \n    <td><strong>Smart Model Routing</strong></td> \n    <td>Automatic load balancing, failover, and cost optimization</td> \n   </tr> \n   <tr> \n    <td><strong>DSPy.ts Integration</strong></td> \n    <td>Self-learning optimization with 20-25% quality improvement</td> \n   </tr> \n   <tr> \n    <td><strong>Streaming</strong></td> \n    <td>AsyncGenerator for real-time data flow</td> \n   </tr> \n   <tr> \n    <td><strong>Memory Efficient</strong></td> \n    <td>&lt;50MB for datasets up to 10K records</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Data Generation Types</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Type</th> \n    <th>Use Cases</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Time-Series</strong></td> \n    <td>Financial data, IoT sensors, metrics</td> \n   </tr> \n   <tr> \n    <td><strong>Events</strong></td> \n    <td>Logs, user actions, system events</td> \n   </tr> \n   <tr> \n    <td><strong>Structured</strong></td> \n    <td>JSON, CSV, databases, APIs</td> \n   </tr> \n   <tr> \n    <td><strong>Embeddings</strong></td> \n    <td>Vector data for RAG systems</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-bash\"># Install\nnpm install @ruvector/agentic-synth\n\n# Or run instantly with npx\nnpx @ruvector/agentic-synth generate --count 100\n\n# Interactive mode\nnpx @ruvector/agentic-synth interactive\n</code></pre> \n <h3>Basic Usage</h3> \n <pre><code class=\"language-typescript\">import { AgenticSynth } from '@ruvector/agentic-synth';\n\n// Initialize with your preferred model\nconst synth = new AgenticSynth({\n  model: 'gemini-pro',\n  apiKey: process.env.GEMINI_API_KEY\n});\n\n// Generate structured data\nconst users = await synth.generate({\n  schema: {\n    name: 'string',\n    email: 'email',\n    age: 'number:18-65',\n    role: ['admin', 'user', 'guest']\n  },\n  count: 1000\n});\n\n// Generate time-series data\nconst stockData = await synth.timeSeries({\n  fields: ['open', 'high', 'low', 'close', 'volume'],\n  interval: '1h',\n  count: 500,\n  volatility: 0.02\n});\n\n// Stream large datasets\nfor await (const batch of synth.stream({ count: 100000, batchSize: 1000 })) {\n  await processData(batch);\n}\n</code></pre> \n <h3>Self-Learning with DSPy</h3> \n <pre><code class=\"language-typescript\">import { AgenticSynth, DSPyOptimizer } from '@ruvector/agentic-synth';\n\n// Enable self-learning optimization\nconst synth = new AgenticSynth({\n  model: 'gemini-pro',\n  optimizer: new DSPyOptimizer({\n    learningRate: 0.1,\n    qualityThreshold: 0.85\n  })\n});\n\n// Quality improves automatically over time\nconst data = await synth.generate({\n  schema: { ... },\n  count: 1000,\n  optimize: true  // Enable learning\n});\n\nconsole.log(`Quality score: ${data.metrics.quality}`);\n// First run: 0.72\n// After 100 runs: 0.94 (+25% improvement)\n</code></pre> \n <h3>Performance</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Metric</th> \n    <th>Value</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>With caching</strong></td> \n    <td>98.2% faster</td> \n   </tr> \n   <tr> \n    <td><strong>P99 latency</strong></td> \n    <td>2500ms → 45ms</td> \n   </tr> \n   <tr> \n    <td><strong>Memory</strong></td> \n    <td>&lt;50MB for 10K records</td> \n   </tr> \n   <tr> \n    <td><strong>Throughput</strong></td> \n    <td>1000+ records/sec</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Ecosystem Integration</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Purpose</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>RuVector</strong></td> \n    <td>Native vector database for RAG</td> \n   </tr> \n   <tr> \n    <td><strong>DSPy.ts</strong></td> \n    <td>Prompt optimization</td> \n   </tr> \n   <tr> \n    <td><strong>Agentic-Jujutsu</strong></td> \n    <td>Version-controlled generation</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Installation</h3> \n <pre><code class=\"language-bash\"># npm\nnpm install @ruvector/agentic-synth\n\n# Examples package (50+ production examples)\nnpm install @ruvector/agentic-synth-examples\n</code></pre> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/npm/packages/agentic-synth/README.md\">agentic-synth README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>📈 Neural Trader - AI Trading System</strong> \n <p><a href=\"https://www.npmjs.com/package/neural-trader\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/neural-trader.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/neural-trader\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/neural-trader.svg?sanitize=true\" /></a></p> \n <p><strong>Production-ready neural trading system</strong> combining state-of-the-art ML for automated trading, sports betting, and portfolio management. Zero external ML dependencies, sub-millisecond latency.</p> \n <h3>Core AI/ML Engines</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Engine</th> \n    <th>Description</th> \n    <th>Performance</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Fractional Kelly</strong></td> \n    <td>Optimal position sizing with risk-adjusted bet scaling</td> \n    <td>588,885 ops/s</td> \n   </tr> \n   <tr> \n    <td><strong>LSTM-Transformer</strong></td> \n    <td>Deep learning price prediction (temporal + attention)</td> \n    <td>1,468 seq/s</td> \n   </tr> \n   <tr> \n    <td><strong>DRL Portfolio</strong></td> \n    <td>Reinforcement learning ensemble (PPO/SAC/A2C)</td> \n    <td>17,043 steps/s</td> \n   </tr> \n   <tr> \n    <td><strong>Sentiment Alpha</strong></td> \n    <td>Real-time sentiment analysis for alpha generation</td> \n    <td>3,764 pipeline/s</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Why Neural Trader?</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Traditional ML</th> \n    <th>Neural Trader</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>TensorFlow/PyTorch required</td> \n    <td><strong>Zero dependencies</strong></td> \n   </tr> \n   <tr> \n    <td>1.2MB+ bundle size</td> \n    <td><strong>45KB</strong> bundle</td> \n   </tr> \n   <tr> \n    <td>2.1ms LSTM inference</td> \n    <td><strong>0.68ms</strong> inference</td> \n   </tr> \n   <tr> \n    <td>Complex deployment</td> \n    <td><strong>Works in browser &amp; Node.js</strong></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Research-Backed Algorithms</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Algorithm</th> \n    <th>Research Finding</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Kelly Criterion</strong></td> \n    <td>1/5th fractional achieves 98% ROI with 85% less risk of ruin</td> \n   </tr> \n   <tr> \n    <td><strong>LSTM-Transformer</strong></td> \n    <td>Temporal + attention fusion outperforms single architectures</td> \n   </tr> \n   <tr> \n    <td><strong>DRL Ensemble</strong></td> \n    <td>PPO/SAC/A2C voting reduces variance vs single agent</td> \n   </tr> \n   <tr> \n    <td><strong>Sentiment Alpha</strong></td> \n    <td>3% annual excess returns documented in academia</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-javascript\">import { KellyCriterion, HybridLSTMTransformer, DRLPortfolioManager } from 'neural-trader';\n\n// Kelly position sizing\nconst kelly = new KellyCriterion();\nconst stake = kelly.calculateStake(9000, 0.55, 2.0, 0.2);  // 1/5th Kelly\n// → $180 recommended stake (2% of bankroll)\n\n// LSTM-Transformer prediction\nconst model = new HybridLSTMTransformer({\n  lstm: { hiddenSize: 64, layers: 2 },\n  transformer: { heads: 4, layers: 2 }\n});\nconst prediction = model.predict(candles);\n// → { signal: 'BUY', confidence: 0.73, direction: 'bullish' }\n\n// DRL portfolio allocation\nconst manager = new DRLPortfolioManager({ numAssets: 10 });\nawait manager.train(marketData, { episodes: 100 });\nconst allocation = manager.getAction(currentState);\n// → [0.15, 0.12, 0.08, ...] optimal weights\n</code></pre> \n <h3>Use Cases</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Use Case</th> \n    <th>Example</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Stock Trading</strong></td> \n    <td>DAG-based pipeline with parallel execution</td> \n   </tr> \n   <tr> \n    <td><strong>Sports Betting</strong></td> \n    <td>Kelly sizing with ML calibration</td> \n   </tr> \n   <tr> \n    <td><strong>Crypto Trading</strong></td> \n    <td>DRL portfolio for 20+ assets</td> \n   </tr> \n   <tr> \n    <td><strong>News Trading</strong></td> \n    <td>Real-time sentiment stream processing</td> \n   </tr> \n   <tr> \n    <td><strong>Portfolio Rebalancing</strong></td> \n    <td>Reinforcement learning allocation</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Package Ecosystem (20+)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>neural-trader</code></td> \n    <td>Core engine with native HNSW, SIMD</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/core</code></td> \n    <td>Ultra-low latency Rust + Node.js bindings</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/strategies</code></td> \n    <td>Strategy management and backtesting</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/execution</code></td> \n    <td>Trade execution and order management</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/mcp</code></td> \n    <td>MCP server with 87+ trading tools</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/risk</code></td> \n    <td>VaR, stress testing, risk metrics</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/portfolio</code></td> \n    <td>Markowitz, Risk Parity optimization</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/neural</code></td> \n    <td>Neural network training</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/brokers</code></td> \n    <td>Alpaca, Interactive Brokers</td> \n   </tr> \n   <tr> \n    <td><code>@neural-trader/sports-betting</code></td> \n    <td>Arbitrage, Kelly, odds analysis</td> \n   </tr> \n  </tbody> \n </table> \n <h3>CLI Interface</h3> \n <pre><code class=\"language-bash\"># Real-time trading\nnode cli.js run --strategy=hybrid --symbol=AAPL --capital=100000\n\n# Backtest historical performance\nnode cli.js backtest --days=252 --capital=50000 --strategy=drl\n\n# Paper trading simulation\nnode cli.js paper --capital=100000 --strategy=sentiment\n\n# Performance benchmarks\nnode cli.js benchmark --iterations=100\n</code></pre> \n <h3>Exotic Examples</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Example</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Multi-Agent Swarm</strong></td> \n    <td>Distributed trading intelligence with consensus</td> \n   </tr> \n   <tr> \n    <td><strong>GNN Correlation Network</strong></td> \n    <td>Graph neural network correlation analysis</td> \n   </tr> \n   <tr> \n    <td><strong>Attention Regime Detection</strong></td> \n    <td>Transformer-based market regime classification</td> \n   </tr> \n   <tr> \n    <td><strong>Quantum Portfolio</strong></td> \n    <td>QAOA &amp; quantum annealing optimization</td> \n   </tr> \n   <tr> \n    <td><strong>Hyperbolic Embeddings</strong></td> \n    <td>Poincaré disk market embeddings</td> \n   </tr> \n   <tr> \n    <td><strong>Atomic Arbitrage</strong></td> \n    <td>Cross-exchange with MEV protection</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Performance</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Module</th> \n    <th>Latency</th> \n    <th>Throughput</th> \n    <th>Status</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Kelly Engine</td> \n    <td>0.014ms</td> \n    <td>71,295/s</td> \n    <td>✓ Ready</td> \n   </tr> \n   <tr> \n    <td>LSTM-Transformer</td> \n    <td>0.681ms</td> \n    <td>1,468/s</td> \n    <td>✓ Ready</td> \n   </tr> \n   <tr> \n    <td>DRL Portfolio</td> \n    <td>0.059ms</td> \n    <td>17,043/s</td> \n    <td>✓ Ready</td> \n   </tr> \n   <tr> \n    <td>Sentiment Alpha</td> \n    <td>0.266ms</td> \n    <td>3,764/s</td> \n    <td>✓ Ready</td> \n   </tr> \n   <tr> \n    <td>Full Pipeline</td> \n    <td>4.68ms</td> \n    <td>214/s</td> \n    <td>✓ Ready</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Installation</h3> \n <pre><code class=\"language-bash\"># npm\nnpm install neural-trader\n\n# Full ecosystem\nnpm install @neural-trader/core @neural-trader/strategies @neural-trader/mcp\n</code></pre> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/neural-trader/README.md\">neural-trader README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🥋 Agentic-Jujutsu - Quantum-Resistant Version Control</strong> \n <p><a href=\"https://www.npmjs.com/package/agentic-jujutsu\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/agentic-jujutsu.svg?sanitize=true\" /></a> <a href=\"https://opensource.org/licenses/MIT\"><img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true\" /></a></p> \n <h3>What is Agentic-Jujutsu?</h3> \n <p>Agentic-Jujutsu is a <strong>quantum-resistant, self-learning version control system</strong> designed for AI agents. It combines lock-free concurrent operations with ReasoningBank trajectory learning for continuous improvement.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Traditional Git</th> \n    <th>Agentic-Jujutsu</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Lock-based commits</td> \n    <td>Lock-free (23x faster)</td> \n   </tr> \n   <tr> \n    <td>Manual conflict resolution</td> \n    <td>87% automatic resolution</td> \n   </tr> \n   <tr> \n    <td>Static operations</td> \n    <td>Self-learning from patterns</td> \n   </tr> \n   <tr> \n    <td>No quantum protection</td> \n    <td>SHA3-512 + HQC-128</td> \n   </tr> \n   <tr> \n    <td>Sequential agents</td> \n    <td>Concurrent multi-agent</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Key Features</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Performance</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Concurrent Commits</strong></td> \n    <td>350 ops/s</td> \n    <td>23x faster than Git (15 ops/s)</td> \n   </tr> \n   <tr> \n    <td><strong>Context Switching</strong></td> \n    <td>&lt;100ms</td> \n    <td>10x faster than Git (500-1000ms)</td> \n   </tr> \n   <tr> \n    <td><strong>Conflict Resolution</strong></td> \n    <td>87% auto</td> \n    <td>AI-powered pattern matching</td> \n   </tr> \n   <tr> \n    <td><strong>Quantum Security</strong></td> \n    <td>&lt;1ms verify</td> \n    <td>SHA3-512 fingerprints, HQC-128 encryption</td> \n   </tr> \n   <tr> \n    <td><strong>ReasoningBank</strong></td> \n    <td>Continuous</td> \n    <td>Trajectory learning with verdicts</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-bash\"># Install\nnpm install agentic-jujutsu\n\n# Basic usage\nnpx agentic-jujutsu\n</code></pre> \n <pre><code class=\"language-typescript\">import { JjWrapper } from 'agentic-jujutsu';\n\nconst jj = new JjWrapper();\n\n// Start learning trajectory\njj.startTrajectory('Implement feature X');\n\n// Make changes and commit\nawait jj.newCommit('Add authentication module');\njj.addToTrajectory();\n\n// Finalize with success score\njj.finalizeTrajectory(0.9, 'Feature implemented successfully');\n\n// Get AI-powered suggestions\nconst suggestions = await jj.getSuggestions();\n</code></pre> \n <h3>Multi-Agent Coordination</h3> \n <pre><code class=\"language-typescript\">// Concurrent commits without locks\nconst agents = ['agent-1', 'agent-2', 'agent-3'];\nawait Promise.all(agents.map(agent =&gt;\n  jj.newCommit(`Changes from ${agent}`)\n));\n// All commits succeed - no lock waiting!\n</code></pre> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/agentic-jujutsu/README.md\">agentic-jujutsu README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🔬 SciPix - Scientific Document OCR</strong> \n <p><a href=\"https://crates.io/crates/ruvector-scipix\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-scipix.svg?sanitize=true\" /></a> <a href=\"https://docs.rs/ruvector-scipix\"><img alt=\"docs.rs\" src=\"https://docs.rs/ruvector-scipix/badge.svg?sanitize=true\" /></a> <a href=\"https://opensource.org/licenses/MIT\"><img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true\" /></a></p> \n <h3>What is SciPix?</h3> \n <p>SciPix is a <strong>blazing-fast, memory-safe OCR engine</strong> written in pure Rust, purpose-built for scientific documents, mathematical equations, and technical diagrams.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>SciPix</th> \n    <th>Tesseract</th> \n    <th>Mathpix</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Simple Text OCR</td> \n    <td><strong>50ms</strong></td> \n    <td>120ms</td> \n    <td>200ms*</td> \n   </tr> \n   <tr> \n    <td>Math Equation</td> \n    <td><strong>80ms</strong></td> \n    <td>N/A</td> \n    <td>150ms*</td> \n   </tr> \n   <tr> \n    <td>Batch (100 images)</td> \n    <td><strong>2.1s</strong></td> \n    <td>8.5s</td> \n    <td>N/A</td> \n   </tr> \n   <tr> \n    <td>Memory Usage</td> \n    <td><strong>45MB</strong></td> \n    <td>180MB</td> \n    <td>Cloud</td> \n   </tr> \n   <tr> \n    <td>LaTeX Output</td> \n    <td>Yes</td> \n    <td>No</td> \n    <td>Yes</td> \n   </tr> \n  </tbody> \n </table> \n <p>*API latency, not processing time</p> \n <h3>Key Features</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>ONNX Runtime</strong></td> \n    <td>GPU-accelerated with CUDA, TensorRT, CoreML</td> \n   </tr> \n   <tr> \n    <td><strong>LaTeX Output</strong></td> \n    <td>Mathematical equation recognition with LaTeX, MathML, AsciiMath</td> \n   </tr> \n   <tr> \n    <td><strong>SIMD Optimized</strong></td> \n    <td>4x faster image preprocessing with AVX2, SSE4, NEON</td> \n   </tr> \n   <tr> \n    <td><strong>REST API</strong></td> \n    <td>Production-ready HTTP server with rate limiting</td> \n   </tr> \n   <tr> \n    <td><strong>MCP Server</strong></td> \n    <td>Integrate with Claude, ChatGPT via Model Context Protocol</td> \n   </tr> \n   <tr> \n    <td><strong>WebAssembly</strong></td> \n    <td>Run OCR directly in browsers</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-bash\"># Add to Cargo.toml\ncargo add ruvector-scipix\n\n# With features\nruvector-scipix = { version = \"0.1.16\", features = [\"ocr\", \"math\", \"optimize\"] }\n</code></pre> \n <pre><code class=\"language-rust\">use ruvector_scipix::{SciPixOcr, OcrConfig};\n\n// Initialize OCR engine\nlet ocr = SciPixOcr::new(OcrConfig::default())?;\n\n// Process scientific image\nlet result = ocr.process_image(\"equation.png\")?;\nprintln!(\"LaTeX: {}\", result.latex);\nprintln!(\"Confidence: {:.2}%\", result.confidence * 100.0);\n</code></pre> \n <h3>Use Cases</h3> \n <ul> \n  <li><strong>Academic Paper Digitization</strong> - Extract text and equations from scanned research papers</li> \n  <li><strong>Math Homework Assistance</strong> - Convert handwritten equations to LaTeX for AI tutoring</li> \n  <li><strong>Technical Documentation</strong> - Process engineering diagrams and scientific charts</li> \n  <li><strong>AI/LLM Integration</strong> - Feed scientific content to language models via MCP</li> \n </ul> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/scipix/README.md\">scipix README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🧠 Meta-Cognition SNN - Spiking Neural Networks</strong> \n <h3>What is Meta-Cognition SNN?</h3> \n <p>A hybrid AI architecture combining <strong>Spiking Neural Networks (SNN)</strong>, <strong>SIMD-optimized vector operations</strong>, and <strong>5 attention mechanisms</strong> with meta-cognitive self-discovery capabilities.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Capability</th> \n    <th>Performance</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Spiking Neural Networks</strong></td> \n    <td>10-50x faster</td> \n    <td>LIF neurons + STDP learning with N-API SIMD</td> \n   </tr> \n   <tr> \n    <td><strong>SIMD Vector Operations</strong></td> \n    <td>5-54x faster</td> \n    <td>Loop-unrolled distance/dot product calculations</td> \n   </tr> \n   <tr> \n    <td><strong>5 Attention Mechanisms</strong></td> \n    <td>Sub-millisecond</td> \n    <td>Multi-Head, Flash, Linear, Hyperbolic, MoE</td> \n   </tr> \n   <tr> \n    <td><strong>Vector Search</strong></td> \n    <td>150x faster</td> \n    <td>RuVector-powered semantic search</td> \n   </tr> \n   <tr> \n    <td><strong>Meta-Cognition</strong></td> \n    <td>Autonomous</td> \n    <td>Self-discovering emergent capabilities</td> \n   </tr> \n  </tbody> \n </table> \n <h3>SIMD Performance</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Operation</th> \n    <th>Speedup</th> \n    <th>Notes</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>LIF Updates</td> \n    <td><strong>16.7x</strong></td> \n    <td>Leaky integrate-and-fire neurons</td> \n   </tr> \n   <tr> \n    <td>Synaptic Forward</td> \n    <td><strong>14.9x</strong></td> \n    <td>Forward propagation</td> \n   </tr> \n   <tr> \n    <td>STDP Learning</td> \n    <td><strong>26.3x</strong></td> \n    <td>Spike-timing dependent plasticity</td> \n   </tr> \n   <tr> \n    <td>Distance (128d)</td> \n    <td><strong>54x</strong></td> \n    <td>Euclidean distance calculation</td> \n   </tr> \n   <tr> \n    <td>Full Simulation</td> \n    <td><strong>18.4x</strong></td> \n    <td>End-to-end SNN simulation</td> \n   </tr> \n  </tbody> \n </table> \n <h3>5 Attention Mechanisms</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Mechanism</th> \n    <th>Best For</th> \n    <th>Latency</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Flash</strong></td> \n    <td>Long sequences</td> \n    <td>0.023ms</td> \n   </tr> \n   <tr> \n    <td><strong>MoE</strong></td> \n    <td>Specialized domains</td> \n    <td>0.021ms</td> \n   </tr> \n   <tr> \n    <td><strong>Multi-Head</strong></td> \n    <td>Complex patterns</td> \n    <td>0.047ms</td> \n   </tr> \n   <tr> \n    <td><strong>Linear</strong></td> \n    <td>Real-time processing</td> \n    <td>0.075ms</td> \n   </tr> \n   <tr> \n    <td><strong>Hyperbolic</strong></td> \n    <td>Hierarchical data</td> \n    <td>0.222ms</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-bash\"># Install and run demos\ncd examples/meta-cognition-spiking-neural-network\nnpm install\nnode demos/run-all.js\n</code></pre> \n <pre><code class=\"language-javascript\">const { createFeedforwardSNN, rateEncoding } = require('./demos/snn/lib/SpikingNeuralNetwork');\n\n// Create SNN with SIMD optimization\nconst snn = createFeedforwardSNN([100, 50, 10], {\n  dt: 1.0,\n  tau: 20.0,\n  a_plus: 0.005,\n  lateral_inhibition: true\n});\n\n// Train with STDP\nconst input = rateEncoding(pattern, snn.dt, 100);\nsnn.step(input);\n</code></pre> \n <h3>6 Emergent Discoveries</h3> \n <ol> \n  <li>Multi-Scale Attention Hierarchy (Novelty: 5/5)</li> \n  <li>Spike Synchronization Patterns</li> \n  <li>Attention-Gated Spike Propagation</li> \n  <li>Temporal Coherence Emergence</li> \n  <li>Emergent Sparsity (80% fewer active neurons)</li> \n  <li>Meta-Plasticity (faster learning on later tasks)</li> \n </ol> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/meta-cognition-spiking-neural-network/README.md\">meta-cognition-snn README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🤖 RuvLLM - Self-Learning LLM Orchestration</strong> \n <p><a href=\"https://www.rust-lang.org/\"><img alt=\"Rust\" src=\"https://img.shields.io/badge/rust-1.77%2B-orange.svg?sanitize=true\" /></a> <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg?sanitize=true\" /></a> <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/#\"><img alt=\"HuggingFace\" src=\"https://img.shields.io/badge/export-HuggingFace-yellow.svg?sanitize=true\" /></a></p> \n <h3>What is RuvLLM?</h3> \n <p>RuvLLM is a <strong>self-learning language model orchestration system</strong> that combines frozen foundation models with adaptive memory and intelligent routing. Unlike traditional LLMs that rely solely on static parameters, RuvLLM continuously improves from every interaction.</p> \n <blockquote> \n  <p><em>\"The intelligence is not in one model anymore. It is in the loop.\"</em></p> \n </blockquote> \n <h3>SONA: 3-Tier Temporal Learning</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Loop</th> \n    <th>Frequency</th> \n    <th>Latency</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>A: Instant</strong></td> \n    <td>Per-request</td> \n    <td>&lt;100μs</td> \n    <td>MicroLoRA adaptation (rank 1-2)</td> \n   </tr> \n   <tr> \n    <td><strong>B: Background</strong></td> \n    <td>Hourly</td> \n    <td>~1.3ms</td> \n    <td>K-means++ clustering, base LoRA (rank 4-16)</td> \n   </tr> \n   <tr> \n    <td><strong>C: Deep</strong></td> \n    <td>Weekly</td> \n    <td>N/A</td> \n    <td>EWC++ consolidation, concept hierarchies</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Core Components</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Component</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>LFM2 Cortex</strong></td> \n    <td>Frozen reasoning engine (135M-2.6B params)</td> \n   </tr> \n   <tr> \n    <td><strong>Ruvector Memory</strong></td> \n    <td>Adaptive synaptic mesh with HNSW indexing</td> \n   </tr> \n   <tr> \n    <td><strong>FastGRNN Router</strong></td> \n    <td>Intelligent model selection circuit</td> \n   </tr> \n   <tr> \n    <td><strong>Graph Attention</strong></td> \n    <td>8-head attention with edge features</td> \n   </tr> \n   <tr> \n    <td><strong>SONA Engine</strong></td> \n    <td>LoRA + EWC++ + ReasoningBank</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Performance (CPU-Only)</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Metric</th> \n    <th>Value</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Initialization</strong></td> \n    <td>3.71ms</td> \n   </tr> \n   <tr> \n    <td><strong>Average Query</strong></td> \n    <td>0.09ms</td> \n   </tr> \n   <tr> \n    <td><strong>Session Query</strong></td> \n    <td>0.04ms</td> \n   </tr> \n   <tr> \n    <td><strong>Throughput</strong></td> \n    <td>~38,000 q/s</td> \n   </tr> \n   <tr> \n    <td><strong>Memory</strong></td> \n    <td>~50MB</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-rust\">use ruvllm::{RuvLLMOrchestrator, OrchestratorConfig};\n\n// Initialize orchestrator\nlet config = OrchestratorConfig::default();\nlet orchestrator = RuvLLMOrchestrator::new(config)?;\n\n// Query with automatic learning\nlet response = orchestrator.query(\"Explain quantum entanglement\").await?;\nprintln!(\"{}\", response.text);\n\n// Response improves over time as SONA learns patterns\n</code></pre> \n <h3>Federated Learning</h3> \n <pre><code class=\"language-rust\">use rvf_federation::{ExportBuilder, DiffPrivacyEngine, FederationPolicy};\n\n// Build a privacy-preserving federated export\nlet mut dp = DiffPrivacyEngine::gaussian(1.0, 1e-5, 1.0, 10.0)?;\nlet export = ExportBuilder::new(\"contributor_pseudo\".into(), \"code_review\".into())\n    .add_priors(local_engine.extract_priors())\n    .add_weights(sona_weights)\n    .with_policy(FederationPolicy::default())  // quality gate + min observations\n    .build(&amp;mut dp)?;                          // PII strip → DP noise → manifest\n\n// Import and merge federated learning from another contributor\nlet merger = ImportMerger::new();\nmerger.validate(&amp;remote_export)?;              // signature + witness chain check\nmerger.merge_priors(&amp;mut local, &amp;remote_export.priors, 1);  // version-aware merge\n</code></pre> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/rvf-federation\"><code>rvf-federation</code></a> for FedAvg/FedProx aggregation, Byzantine tolerance, RDP privacy accounting, and PII stripping pipeline.</p> \n <h3>Dynamic Embedding Fine-Tuning</h3> \n <p>RuvLLM's adaptive learning system enables real-time model improvement without retraining.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Description</th> \n    <th>Latency</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>MicroLoRA</strong></td> \n    <td>Per-request adaptation (rank 1-2), &lt;50KB adapters</td> \n    <td>&lt;1ms</td> \n   </tr> \n   <tr> \n    <td><strong>Contrastive Training</strong></td> \n    <td>Triplet loss with hard negatives for embedding optimization</td> \n    <td>Batch</td> \n   </tr> \n   <tr> \n    <td><strong>Task-Specific Adapters</strong></td> \n    <td>Pre-tuned for Coder, Researcher, Security, Architect, Reviewer</td> \n    <td>Hot-swap</td> \n   </tr> \n   <tr> \n    <td><strong>EWC++ Consolidation</strong></td> \n    <td>Prevents catastrophic forgetting during continuous learning</td> \n    <td>Background</td> \n   </tr> \n   <tr> \n    <td><strong>Adapter Merging</strong></td> \n    <td>Average, Weighted, SLERP, TIES, DARE strategies</td> \n    <td>On-demand</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-javascript\">// Contrastive fine-tuning for agent routing\nimport { ContrastiveTrainer } from '@ruvector/ruvllm';\n\nconst trainer = new ContrastiveTrainer({\n  margin: 0.5,\n  hardNegativeRatio: 0.7\n});\n\n// Learn: task → correct agent, not wrong agent\ntrainer.addTriplet(taskEmb, correctAgentEmb, wrongAgentEmb, true);\nconst model = trainer.train();\n</code></pre> \n <pre><code class=\"language-rust\">// Task-specific adapter hot-swapping\nuse ruvllm::lora::RuvLtraAdapters;\n\nlet adapters = RuvLtraAdapters::new();\nlet coder = adapters.create_lora(\"coder\", 768)?;      // Rank 16, code patterns\nlet security = adapters.create_lora(\"security\", 768)?; // Rank 16, vulnerability detection\n\n// Hot-swap at runtime without model reload\norchestrator.set_adapter(coder);\nlet code_response = orchestrator.query(\"Implement binary search\").await?;\n\norchestrator.set_adapter(security);\nlet audit_response = orchestrator.query(\"Audit this code for vulnerabilities\").await?;\n</code></pre> \n <h3>Advanced Features</h3> \n <ul> \n  <li><strong>SIMD Inference</strong>: AVX2/AVX512/SSE4.1 optimization</li> \n  <li><strong>Q4 Quantization</strong>: 4-bit weights for memory efficiency</li> \n  <li><strong>HuggingFace Export</strong>: Export LoRA weights and preference pairs</li> \n  <li><strong>Multi-Model Routing</strong>: SmolLM, Qwen2, TinyLlama selection</li> \n  <li><strong>WASM Support</strong>: Run SONA in browsers and edge devices</li> \n  <li><strong>Browser Fine-Tuning</strong>: MicroLoRA WASM with localStorage persistence</li> \n </ul> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/ruvLLM/README.md\">ruvLLM README</a> | <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/ruvllm/FINE_TUNING.md\">Fine-Tuning Guide</a> | <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/training/task_specific_lora_adapters.md\">Task Adapters</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🗜️ REFRAG - Compress-Sense-Expand RAG</strong> \n <h3>What is REFRAG?</h3> \n <p>REFRAG implements the <strong>Compress-Sense-Expand architecture</strong> from <a href=\"https://arxiv.org/abs/2509.01092\">arXiv:2509.01092</a>, achieving <strong>~30x RAG latency reduction</strong> by storing pre-computed representation tensors instead of raw text.</p> \n <h3>Architecture</h3> \n <pre><code>┌────────────────┐    ┌────────────────┐    ┌────────────────┐\n│   COMPRESS     │───▶│     SENSE      │───▶│    EXPAND      │\n│    Layer       │    │     Layer      │    │    Layer       │\n└────────────────┘    └────────────────┘    └────────────────┘\n\nBinary tensor         Policy network        Dimension projection\nstorage with          decides COMPRESS      (768 → 4096 dims)\nzero-copy access      vs EXPAND\n</code></pre> \n <h3>Compression Strategies</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Strategy</th> \n    <th>Compression</th> \n    <th>Use Case</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>None</code></td> \n    <td>1x</td> \n    <td>Maximum precision</td> \n   </tr> \n   <tr> \n    <td><code>Float16</code></td> \n    <td>2x</td> \n    <td>Good balance</td> \n   </tr> \n   <tr> \n    <td><code>Int8</code></td> \n    <td>4x</td> \n    <td>Memory constrained</td> \n   </tr> \n   <tr> \n    <td><code>Binary</code></td> \n    <td>32x</td> \n    <td>Extreme compression</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Policy Networks</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Policy</th> \n    <th>Latency</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>ThresholdPolicy</code></td> \n    <td>~2μs</td> \n    <td>Cosine similarity threshold</td> \n   </tr> \n   <tr> \n    <td><code>LinearPolicy</code></td> \n    <td>~5μs</td> \n    <td>Single layer classifier</td> \n   </tr> \n   <tr> \n    <td><code>MLPPolicy</code></td> \n    <td>~15μs</td> \n    <td>Two-layer neural network</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-bash\"># Run demo\ncargo run --bin refrag-demo\n\n# Run benchmarks\ncargo run --bin refrag-benchmark --release\n</code></pre> \n <pre><code class=\"language-rust\">use refrag_pipeline_example::{RefragStore, RefragEntry};\n\n// Create REFRAG-enabled store\nlet store = RefragStore::new(384, 768)?;\n\n// Insert with representation tensor\nlet entry = RefragEntry::new(\"doc_1\", search_vector, \"The quick brown fox...\")\n    .with_tensor(tensor_bytes, \"llama3-8b\");\nstore.insert(entry)?;\n\n// Hybrid search (policy-based COMPRESS/EXPAND)\nlet results = store.search_hybrid(&amp;query, 10, Some(0.85))?;\n\nfor result in results {\n    match result.response_type {\n        RefragResponseType::Compress =&gt; {\n            // Tensor directly injectable into LLM context\n            println!(\"Tensor: {} dims\", result.tensor_dims.unwrap());\n        }\n        RefragResponseType::Expand =&gt; {\n            // Original text when full context needed\n            println!(\"Text: {}\", result.content.unwrap());\n        }\n    }\n}\n</code></pre> \n <h3>Target LLM Dimensions</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Source</th> \n    <th>Target</th> \n    <th>LLM</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>768</td> \n    <td>4096</td> \n    <td>LLaMA-3 8B</td> \n   </tr> \n   <tr> \n    <td>768</td> \n    <td>8192</td> \n    <td>LLaMA-3 70B</td> \n   </tr> \n   <tr> \n    <td>1536</td> \n    <td>8192</td> \n    <td>GPT-4</td> \n   </tr> \n  </tbody> \n </table> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/refrag-pipeline/README.md\">refrag-pipeline README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🐦 7sense - Bioacoustic Intelligence Platform</strong> \n <p><a href=\"https://www.rust-lang.org\"><img alt=\"Rust\" src=\"https://img.shields.io/badge/rust-1.75+-orange.svg?sanitize=true\" /></a> <a href=\"\"><img alt=\"Tests\" src=\"https://img.shields.io/badge/tests-329%20passed-brightgreen.svg?sanitize=true\" /></a> <a href=\"\"><img alt=\"Coverage\" src=\"https://img.shields.io/badge/coverage-85%25-green.svg?sanitize=true\" /></a></p> \n <h3>What is 7sense?</h3> \n <p>7sense transforms <strong>bird calls into navigable geometric space</strong> using cutting-edge AI and vector search. It converts audio recordings of bird songs into rich, searchable embeddings using Perch 2.0 neural networks and ultra-fast HNSW indexing.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Traditional Monitoring</th> \n    <th>7sense</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Expert human listeners</td> \n    <td>Instant AI species ID</td> \n   </tr> \n   <tr> \n    <td>Basic spectrogram analysis</td> \n    <td>Neural embeddings (1536-dim)</td> \n   </tr> \n   <tr> \n    <td>Limited scale</td> \n    <td>Millions of recordings</td> \n   </tr> \n   <tr> \n    <td>Manual pattern finding</td> \n    <td>Automated discovery</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Performance Targets</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Metric</th> \n    <th>Target</th> \n    <th>Status</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>HNSW Search Speedup</td> \n    <td>150x vs brute force</td> \n    <td>Achieved</td> \n   </tr> \n   <tr> \n    <td>Query Latency (p99)</td> \n    <td>&lt; 50ms</td> \n    <td>Achieved</td> \n   </tr> \n   <tr> \n    <td>Recall@10</td> \n    <td>&gt;= 0.95</td> \n    <td>Achieved</td> \n   </tr> \n   <tr> \n    <td>Embedding Throughput</td> \n    <td>&gt; 100 segments/sec</td> \n    <td>Achieved</td> \n   </tr> \n   <tr> \n    <td>Memory per 1M vectors</td> \n    <td>&lt; 6 GB</td> \n    <td>Achieved</td> \n   </tr> \n  </tbody> \n </table> \n <h3>9 Rust Crates</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>sevensense-core</code></td> \n    <td>Species taxonomy, temporal types</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-audio</code></td> \n    <td>WAV/MP3/FLAC, Mel spectrograms</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-embedding</code></td> \n    <td>Perch 2.0 ONNX, 1536-dim vectors</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-vector</code></td> \n    <td>HNSW with 150x speedup</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-learning</code></td> \n    <td>GNN training, EWC regularization</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-analysis</code></td> \n    <td>HDBSCAN clustering, Markov models</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-interpretation</code></td> \n    <td>Evidence packs, species narratives</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-api</code></td> \n    <td>GraphQL, REST, WebSocket streaming</td> \n   </tr> \n   <tr> \n    <td><code>sevensense-benches</code></td> \n    <td>Criterion.rs performance suites</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-bash\"># Build and run\ncd examples/vibecast-7sense\ncargo build --release\ncargo run -p sevensense-api --release\n</code></pre> \n <pre><code class=\"language-rust\">use sevensense_audio::AudioProcessor;\nuse sevensense_embedding::EmbeddingPipeline;\nuse sevensense_vector::HnswIndex;\n\n// Load and process audio\nlet processor = AudioProcessor::new(Default::default());\nlet segments = processor.process_file(\"recording.wav\").await?;\n\n// Generate Perch 2.0 embeddings\nlet pipeline = EmbeddingPipeline::new(Default::default()).await?;\nlet embeddings = pipeline.embed_segments(&amp;segments).await?;\n\n// Search for similar calls (150x faster)\nlet index = HnswIndex::new(Default::default());\nindex.add_batch(&amp;embeddings)?;\nlet neighbors = index.search(&amp;embeddings[0], 10)?;\n\nprintln!(\"Found {} similar bird calls\", neighbors.len());\n</code></pre> \n <h3>Use Cases</h3> \n <ul> \n  <li><strong>Species Identification</strong> - Instant predictions with confidence scores</li> \n  <li><strong>Pattern Discovery</strong> - Find similar calls across millions of recordings</li> \n  <li><strong>Behavioral Insights</strong> - Detect singing patterns, dialects, anomalies</li> \n  <li><strong>Conservation Monitoring</strong> - Track biodiversity at scale</li> \n </ul> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/vibecast-7sense/README.md\">7sense README</a></p> \n </blockquote> \n</details> \n<details> \n <strong>🧬 EXO-AI - Advanced Cognitive Substrate</strong> \n <p><a href=\"https://crates.io/crates/exo-core\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/exo-core.svg?sanitize=true\" /></a> <a href=\"https://docs.rs/exo-core\"><img alt=\"docs.rs\" src=\"https://docs.rs/exo-core/badge.svg?sanitize=true\" /></a> <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/LICENSE\"><img alt=\"License\" src=\"https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg?sanitize=true\" /></a></p> \n <h3>What is EXO-AI?</h3> \n <p>EXO-AI 2025 is a comprehensive <strong>cognitive substrate</strong> implementing cutting-edge theories from neuroscience, physics, and consciousness research. It provides 9 interconnected Rust crates totaling ~15,800+ lines of research-grade code.</p> \n <blockquote> \n  <p>Traditional AI systems process information. EXO-AI aims to understand it.</p> \n </blockquote> \n <h3>SIMD-Accelerated Performance</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Operation</th> \n    <th>Speedup</th> \n    <th>Notes</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Distance (128d)</td> \n    <td><strong>54x</strong></td> \n    <td>AVX2/NEON optimized</td> \n   </tr> \n   <tr> \n    <td>Cosine Similarity</td> \n    <td><strong>2.73x</strong></td> \n    <td>Batch operations</td> \n   </tr> \n   <tr> \n    <td>Pattern Matching</td> \n    <td><strong>8-54x</strong></td> \n    <td>Loop-unrolled</td> \n   </tr> \n   <tr> \n    <td>Meta-Simulation</td> \n    <td><strong>13+ quadrillion/s</strong></td> \n    <td>From ultra-low-latency-sim</td> \n   </tr> \n  </tbody> \n </table> \n <h3>9 Rust Crates</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>exo-core</code></td> \n    <td>IIT consciousness (Φ) measurement &amp; Landauer thermodynamics</td> \n   </tr> \n   <tr> \n    <td><code>exo-temporal</code></td> \n    <td>Temporal memory with causal tracking &amp; consolidation</td> \n   </tr> \n   <tr> \n    <td><code>exo-hypergraph</code></td> \n    <td>Topological analysis with persistent homology</td> \n   </tr> \n   <tr> \n    <td><code>exo-manifold</code></td> \n    <td>SIREN networks + SIMD-accelerated retrieval</td> \n   </tr> \n   <tr> \n    <td><code>exo-exotic</code></td> \n    <td>10 cutting-edge cognitive experiments</td> \n   </tr> \n   <tr> \n    <td><code>exo-federation</code></td> \n    <td>Post-quantum federated cognitive mesh</td> \n   </tr> \n   <tr> \n    <td><code>exo-backend-classical</code></td> \n    <td>SIMD-accelerated compute backend</td> \n   </tr> \n   <tr> \n    <td><code>exo-wasm</code></td> \n    <td>Browser &amp; edge WASM deployment</td> \n   </tr> \n   <tr> \n    <td><code>exo-node</code></td> \n    <td>Node.js bindings via NAPI-RS</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Key Theories Implemented</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Theory</th> \n    <th>Implementation</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>IIT (Integrated Information Theory)</strong></td> \n    <td>Consciousness level (Φ) measurement</td> \n   </tr> \n   <tr> \n    <td><strong>Landauer's Principle</strong></td> \n    <td>Computational thermodynamics</td> \n   </tr> \n   <tr> \n    <td><strong>Free Energy Principle</strong></td> \n    <td>Friston's predictive processing</td> \n   </tr> \n   <tr> \n    <td><strong>Strange Loops</strong></td> \n    <td>Hofstadter's self-referential patterns</td> \n   </tr> \n   <tr> \n    <td><strong>Morphogenesis</strong></td> \n    <td>Pattern formation emergence</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Quick Start</h3> \n <pre><code class=\"language-toml\">[dependencies]\nexo-core = \"0.1\"\nexo-temporal = \"0.1\"\nexo-exotic = \"0.1\"\nexo-manifold = \"0.1\"  # SIMD acceleration!\n</code></pre> \n <pre><code class=\"language-rust\">use exo_core::consciousness::{ConsciousnessSubstrate, IITConfig};\nuse exo_core::thermodynamics::CognitiveThermometer;\n\n// Measure integrated information (Φ)\nlet substrate = ConsciousnessSubstrate::new(IITConfig::default());\nsubstrate.add_pattern(pattern);\nlet phi = substrate.compute_phi();\nprintln!(\"Consciousness level (Φ): {:.4}\", phi);\n\n// Track computational thermodynamics\nlet thermo = CognitiveThermometer::new(300.0); // Kelvin\nlet cost = thermo.landauer_cost_bits(1024);\nprintln!(\"Landauer cost: {:.2e} J\", cost);\n</code></pre> \n <h3>SIMD Pattern Retrieval</h3> \n <pre><code class=\"language-rust\">use exo_manifold::{ManifoldEngine, cosine_similarity_simd, batch_distances};\n\n// 54x faster similarity search\nlet query = vec![0.5; 768];\nlet results = engine.retrieve(&amp;query, 10)?;\n\n// Batch distance computation\nlet distances = batch_distances(&amp;query, &amp;database);  // 8-54x speedup\n</code></pre> \n <blockquote> \n  <p><strong>Full Documentation</strong>: <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/exo-ai-2025/README.md\">exo-ai README</a></p> \n </blockquote> \n</details> \n<hr /> \n<h2>Database Extensions</h2> \n<details> \n <strong>🐘 PostgreSQL Extension</strong> \n <p><a href=\"https://crates.io/crates/ruvector-postgres\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-postgres.svg?sanitize=true\" /></a> <a href=\"https://www.npmjs.com/package/@ruvector/postgres-cli\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/postgres-cli.svg?sanitize=true\" /></a> <a href=\"https://hub.docker.com/r/ruvnet/ruvector-postgres\"><img alt=\"Docker Hub\" src=\"https://img.shields.io/docker/pulls/ruvnet/ruvector-postgres?label=docker%20pulls\" /></a> <a href=\"https://hub.docker.com/r/ruvnet/ruvector-postgres\"><img alt=\"Docker\" src=\"https://img.shields.io/docker/v/ruvnet/ruvector-postgres?label=docker\" /></a></p> \n <p><strong>The most advanced PostgreSQL vector extension</strong> — a drop-in pgvector replacement with 143 SQL functions, hardware-accelerated SIMD operations, and built-in AI capabilities. Transform your existing PostgreSQL database into a full-featured vector search engine with GNN layers, attention mechanisms, and self-learning capabilities.</p> \n <pre><code class=\"language-bash\"># Quick Install from Docker Hub\ndocker run -d --name ruvector \\\n  -e POSTGRES_PASSWORD=secret \\\n  -p 5432:5432 \\\n  ruvnet/ruvector-postgres:latest\n\n# Connect and use\npsql -h localhost -U ruvector -d ruvector_test\n\n# Create extension\nCREATE EXTENSION ruvector;\n</code></pre> \n <p><strong>Why RuVector Postgres?</strong></p> \n <ul> \n  <li><strong>Zero Migration</strong> — Works with existing pgvector code, just swap the extension</li> \n  <li><strong>10x More Functions</strong> — 143 SQL functions vs pgvector's ~20</li> \n  <li><strong>2x Faster</strong> — AVX-512/AVX2/NEON SIMD acceleration</li> \n  <li><strong>AI-Native</strong> — GNN layers, 46 attention mechanisms, local embeddings</li> \n  <li><strong>Self-Learning</strong> — Improves search quality over time with ReasoningBank</li> \n </ul> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>pgvector</th> \n    <th>RuVector Postgres</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>SQL Functions</td> \n    <td>~20</td> \n    <td><strong>143</strong></td> \n   </tr> \n   <tr> \n    <td>SIMD Acceleration</td> \n    <td>Basic</td> \n    <td>AVX-512/AVX2/NEON (~2x faster)</td> \n   </tr> \n   <tr> \n    <td>Index Types</td> \n    <td>HNSW, IVFFlat</td> \n    <td>HNSW, IVFFlat + Hyperbolic</td> \n   </tr> \n   <tr> \n    <td>Attention Mechanisms</td> \n    <td>❌</td> \n    <td>46 types (Flash, Linear, Graph)</td> \n   </tr> \n   <tr> \n    <td>GNN Layers</td> \n    <td>❌</td> \n    <td>GCN, GraphSAGE, GAT, GIN</td> \n   </tr> \n   <tr> \n    <td>Sparse Vectors</td> \n    <td>❌</td> \n    <td>BM25, TF-IDF, SPLADE</td> \n   </tr> \n   <tr> \n    <td>Self-Learning</td> \n    <td>❌</td> \n    <td>ReasoningBank, trajectory learning</td> \n   </tr> \n   <tr> \n    <td>Local Embeddings</td> \n    <td>❌</td> \n    <td>6 fastembed models built-in</td> \n   </tr> \n   <tr> \n    <td>Multi-Tenancy</td> \n    <td>❌</td> \n    <td>Built-in namespace isolation</td> \n   </tr> \n   <tr> \n    <td>Quantization</td> \n    <td>❌</td> \n    <td>Scalar, Product, Binary (4-32x compression)</td> \n   </tr> \n  </tbody> \n </table> \n <details> \n  <strong>🐳 Docker Hub (Recommended)</strong> \n  <p><strong>Pull from Docker Hub:</strong> <a href=\"https://hub.docker.com/r/ruvnet/ruvector-postgres\">hub.docker.com/r/ruvnet/ruvector-postgres</a></p> \n  <pre><code class=\"language-bash\"># Quick start\ndocker run -d --name ruvector \\\n  -e POSTGRES_PASSWORD=secret \\\n  -p 5432:5432 \\\n  ruvnet/ruvector-postgres:latest\n\n# Connect\npsql -h localhost -U ruvector -d ruvector_test\n\n# Create extension\nCREATE EXTENSION ruvector;\n</code></pre> \n  <p><strong>Environment Variables:</strong></p> \n  <table> \n   <thead> \n    <tr> \n     <th>Variable</th> \n     <th>Default</th> \n     <th>Description</th> \n    </tr> \n   </thead> \n   <tbody> \n    <tr> \n     <td><code>POSTGRES_USER</code></td> \n     <td><code>ruvector</code></td> \n     <td>Database user</td> \n    </tr> \n    <tr> \n     <td><code>POSTGRES_PASSWORD</code></td> \n     <td><code>ruvector</code></td> \n     <td>Database password</td> \n    </tr> \n    <tr> \n     <td><code>POSTGRES_DB</code></td> \n     <td><code>ruvector_test</code></td> \n     <td>Default database</td> \n    </tr> \n   </tbody> \n  </table> \n  <p><strong>Docker Compose:</strong></p> \n  <pre><code class=\"language-yaml\">version: '3.8'\nservices:\n  ruvector-postgres:\n    image: ruvnet/ruvector-postgres:latest\n    environment:\n      POSTGRES_PASSWORD: secret\n      POSTGRES_DB: ruvector_test\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n\nvolumes:\n  pgdata:\n</code></pre> \n  <p><strong>Available Tags:</strong></p> \n  <ul> \n   <li><code>ruvnet/ruvector-postgres:latest</code> - PostgreSQL + RuVector 0.3.0</li> \n   <li><code>ruvnet/ruvector-postgres:0.3.0</code> - Current release (143 SQL functions)</li> \n   <li><code>ruvnet/ruvector-postgres:2.0.0</code> - Previous release</li> \n  </ul> \n </details> \n <details> \n  <strong>📦 npm CLI</strong> \n  <pre><code class=\"language-bash\"># Install globally\nnpm install -g @ruvector/postgres-cli\n\n# Or use npx\nnpx @ruvector/postgres-cli --help\n\n# Commands available as 'ruvector-pg' or 'rvpg'\nruvector-pg --version\nrvpg --help\n</code></pre> \n  <p><strong>CLI Commands:</strong></p> \n  <pre><code class=\"language-bash\"># Install extension to existing PostgreSQL\nruvector-pg install\n\n# Create vector table with HNSW index\nruvector-pg vector create table embeddings --dim 1536 --index hnsw\n\n# Import vectors from file\nruvector-pg vector import embeddings data.json\n\n# Search vectors\nruvector-pg vector search embeddings --query \"0.1,0.2,...\" --limit 10\n\n# Benchmark performance\nruvector-pg bench --iterations 1000\n\n# Check extension status\nruvector-pg status\n</code></pre> \n  <p><strong>Programmatic Usage:</strong></p> \n  <pre><code class=\"language-typescript\">import { RuvectorPG } from '@ruvector/postgres-cli';\n\nconst client = new RuvectorPG({\n  host: 'localhost',\n  port: 5432,\n  database: 'vectors',\n  user: 'postgres',\n  password: 'secret'\n});\n\n// Create table with HNSW index\nawait client.createTable('embeddings', {\n  dimensions: 1536,\n  indexType: 'hnsw',\n  distanceMetric: 'cosine'\n});\n\n// Insert vectors\nawait client.insert('embeddings', {\n  id: '1',\n  vector: [0.1, 0.2, ...],\n  metadata: { source: 'openai' }\n});\n\n// Search\nconst results = await client.search('embeddings', queryVector, { limit: 10 });\n</code></pre> \n </details> \n <details> \n  <strong>🦀 Rust Crate</strong> \n  <pre><code class=\"language-bash\"># Install pgrx (PostgreSQL extension framework)\ncargo install cargo-pgrx --version \"0.12.9\" --locked\ncargo pgrx init\n\n# Build and install extension\ncd crates/ruvector-postgres\ncargo pgrx install --release\n\n# Or install specific PostgreSQL version\ncargo pgrx install --release --pg-config /usr/lib/postgresql/17/bin/pg_config\n</code></pre> \n  <p><strong>Cargo.toml:</strong></p> \n  <pre><code class=\"language-toml\">[dependencies]\nruvector-postgres = \"2.0\"\n\n# Optional features\n[features]\ndefault = [\"pg17\"]\npg16 = [\"ruvector-postgres/pg16\"]\npg15 = [\"ruvector-postgres/pg15\"]\n\n# AI features (opt-in)\nai-complete = [\"ruvector-postgres/ai-complete\"]  # All AI features\nlearning = [\"ruvector-postgres/learning\"]         # Self-learning\nattention = [\"ruvector-postgres/attention\"]       # 46 attention mechanisms\ngnn = [\"ruvector-postgres/gnn\"]                   # Graph neural networks\nhyperbolic = [\"ruvector-postgres/hyperbolic\"]     # Hyperbolic embeddings\nembeddings = [\"ruvector-postgres/embeddings\"]     # Local embedding generation\nsolver = [\"ruvector-postgres/solver\"]                   # Sublinear solvers\nmath-distances = [\"ruvector-postgres/math-distances\"]   # Math distances &amp; spectral\ntda = [\"ruvector-postgres/tda\"]                         # Topological data analysis\nsona-learning = [\"ruvector-postgres/sona-learning\"]     # Sona learning\ndomain-expansion = [\"ruvector-postgres/domain-expansion\"] # Domain expansion\nanalytics-complete = [\"solver\", \"math-distances\", \"tda\"] # All analytics\n</code></pre> \n  <p><strong>Build with all features:</strong></p> \n  <pre><code class=\"language-bash\">cargo pgrx install --release --features \"ai-complete,embeddings,analytics-complete,attention-extended,sona-learning,domain-expansion\"\n</code></pre> \n </details> \n <details> \n  <strong>📝 SQL Examples</strong> \n  <pre><code class=\"language-sql\">-- Enable extension\nCREATE EXTENSION ruvector;\n\n-- Create table with vector column\nCREATE TABLE documents (\n  id SERIAL PRIMARY KEY,\n  content TEXT,\n  embedding VECTOR(1536)\n);\n\n-- Create HNSW index\nCREATE INDEX ON documents USING hnsw (embedding vector_cosine_ops)\n  WITH (m = 16, ef_construction = 200);\n\n-- Insert vectors\nINSERT INTO documents (content, embedding)\nVALUES ('Hello world', '[0.1, 0.2, ...]'::vector);\n\n-- Semantic search (cosine similarity)\nSELECT id, content, embedding &lt;=&gt; '[0.1, 0.2, ...]'::vector AS distance\nFROM documents\nORDER BY distance\nLIMIT 10;\n\n-- Hybrid search (vector + full-text)\nSELECT id, content\nFROM documents\nWHERE to_tsvector(content) @@ to_tsquery('machine &amp; learning')\nORDER BY embedding &lt;=&gt; query_embedding\nLIMIT 10;\n\n-- GNN-enhanced search (with learning)\nSELECT * FROM ruvector_gnn_search(\n  'documents',\n  '[0.1, 0.2, ...]'::vector,\n  10,  -- limit\n  'gcn' -- gnn_type: gcn, graphsage, gat, gin\n);\n\n-- Generate embeddings locally (no API needed)\nSELECT ruvector_embed('all-MiniLM-L6-v2', 'Your text here');\n\n-- Flash attention\nSELECT ruvector_flash_attention(query, key, value);\n</code></pre> \n </details> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-postgres/README.md\">ruvector-postgres README</a> for full SQL API reference (143 functions).</p> \n</details> \n<hr /> \n<h2>Developer Tools</h2> \n<details> \n 🛠️ Tools &amp; Utilities \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-bench\">ruvector-bench</a></td> \n    <td>Benchmarking suite for vector operations</td> \n    <td><a href=\"https://crates.io/crates/ruvector-bench\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-bench.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-metrics\">ruvector-metrics</a></td> \n    <td>Observability, metrics, and monitoring</td> \n    <td><a href=\"https://crates.io/crates/ruvector-metrics\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-metrics.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-filter\">ruvector-filter</a></td> \n    <td>Metadata filtering and query predicates</td> \n    <td><a href=\"https://crates.io/crates/ruvector-filter\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-filter.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-collections\">ruvector-collections</a></td> \n    <td>Multi-tenant collection management</td> \n    <td><a href=\"https://crates.io/crates/ruvector-collections\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-collections.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-snapshot\">ruvector-snapshot</a></td> \n    <td>Point-in-time snapshots and backups</td> \n    <td><a href=\"https://crates.io/crates/ruvector-snapshot\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-snapshot.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/micro-hnsw-wasm\">micro-hnsw-wasm</a></td> \n    <td>Lightweight HNSW implementation for WASM</td> \n    <td><a href=\"https://crates.io/crates/micro-hnsw-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/micro-hnsw-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h3>Embedded &amp; IoT</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>Target</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge\">ruvector-esp32</a></td> \n    <td>ESP32/ESP-IDF vector search</td> \n    <td>ESP32, ESP32-S3</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvlite\">rvlite</a></td> \n    <td>SQLite-style edge DB (no_std compatible)</td> \n    <td>ARM, RISC-V, WASM</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/micro-hnsw-wasm\">micro-hnsw-wasm</a></td> \n    <td>&lt;50KB HNSW for constrained devices</td> \n    <td>WASM, embedded</td> \n   </tr> \n  </tbody> \n </table> \n <pre><code class=\"language-rust\">// ESP32 example (no_std)\n#![no_std]\nuse rvlite::RvLite;\n\nlet db = RvLite::new(128);  // 128-dim vectors\ndb.insert(0, &amp;embedding);\nlet results = db.search(&amp;query, 5);\n</code></pre> \n</details> \n<hr /> \n<h2>Browser &amp; Edge (WASM)</h2> \n<details> \n 🌐 WASM Packages (Browser &amp; Edge) \n <p>Specialized WebAssembly modules for browser and edge deployment. These packages bring advanced AI and distributed computing primitives to JavaScript/TypeScript with near-native performance.</p> \n <h3>Quick Install (All Browser WASM)</h3> \n <pre><code class=\"language-bash\"># Core vector search\nnpm install ruvector-wasm @ruvector/rvlite\n\n# AI &amp; Neural\nnpm install @ruvector/gnn-wasm @ruvector/attention-wasm @ruvector/sona-wasm\n\n# Graph &amp; Algorithms\nnpm install @ruvector/graph-wasm @ruvector/mincut-wasm @ruvector/hyperbolic-hnsw-wasm\n\n# Exotic AI\nnpm install @ruvector/economy-wasm @ruvector/exotic-wasm @ruvector/nervous-system-wasm\n\n# LLM (browser inference)\nnpm install @ruvector/ruvllm-wasm\n</code></pre> \n <table> \n  <thead> \n   <tr> \n    <th>Category</th> \n    <th>Packages</th> \n    <th>Total Size</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Core</strong></td> \n    <td>ruvector-wasm, rvlite</td> \n    <td>~200KB</td> \n   </tr> \n   <tr> \n    <td><strong>AI/Neural</strong></td> \n    <td>gnn, attention, sona</td> \n    <td>~300KB</td> \n   </tr> \n   <tr> \n    <td><strong>Graph</strong></td> \n    <td>graph, mincut, hyperbolic-hnsw</td> \n    <td>~250KB</td> \n   </tr> \n   <tr> \n    <td><strong>Exotic</strong></td> \n    <td>economy, exotic, nervous-system</td> \n    <td>~350KB</td> \n   </tr> \n   <tr> \n    <td><strong>LLM</strong></td> \n    <td>ruvllm-wasm</td> \n    <td>~500KB</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Installation</h3> \n <pre><code class=\"language-bash\"># Install individual packages\nnpm install @ruvector/learning-wasm\nnpm install @ruvector/economy-wasm\nnpm install @ruvector/exotic-wasm\nnpm install @ruvector/nervous-system-wasm\nnpm install @ruvector/attention-unified-wasm\n\n# Or build from source\ncd crates/ruvector-learning-wasm\nwasm-pack build --target web\n</code></pre> \n <h3>ruvector-learning-wasm</h3> \n <p><strong>MicroLoRA, BTSP, and HDC for self-learning AI systems.</strong></p> \n <p>Ultra-fast Low-Rank Adaptation (LoRA) optimized for WASM execution with &lt;100us adaptation latency. Designed for real-time per-operator learning in query optimization and AI agent systems.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Performance</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>MicroLoRA</strong></td> \n    <td>&lt;100us latency</td> \n    <td>Rank-2 LoRA matrices for instant weight adaptation</td> \n   </tr> \n   <tr> \n    <td><strong>Per-Operator Scoping</strong></td> \n    <td>Zero-allocation hot paths</td> \n    <td>Separate adapters for different operator types</td> \n   </tr> \n   <tr> \n    <td><strong>Trajectory Tracking</strong></td> \n    <td>Lock-free buffers</td> \n    <td>Record learning trajectories for replay</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Architecture:</strong></p> \n <pre><code>Input Embedding (256-dim)\n       |\n       v\n  +---------+\n  | A: d x 2 |  Down projection\n  +---------+\n       |\n       v\n  +---------+\n  | B: 2 x d |  Up projection\n  +---------+\n       |\n       v\nDelta W = alpha * (A @ B)\n       |\n       v\nOutput = Input + Delta W\n</code></pre> \n <p><strong>JavaScript/TypeScript Example:</strong></p> \n <pre><code class=\"language-typescript\">import init, { WasmMicroLoRA } from '@ruvector/learning-wasm';\n\nawait init();\n\n// Create MicroLoRA engine (256-dim, alpha=0.1, lr=0.01)\nconst lora = new WasmMicroLoRA(256, 0.1, 0.01);\n\n// Forward pass with adaptation\nconst input = new Float32Array(256).fill(0.5);\nconst output = lora.forward_array(input);\n\n// Adapt based on gradient signal\nconst gradient = new Float32Array(256).fill(0.1);\nlora.adapt_array(gradient);\n\n// Adapt with reward signal for RL\nlora.adapt_with_reward(0.8);  // 80% improvement\n\nconsole.log(`Adaptations: ${lora.adapt_count()}`);\nconsole.log(`Delta norm: ${lora.delta_norm()}`);\n</code></pre> \n <h3>ruvector-economy-wasm</h3> \n <p><strong>CRDT-based autonomous credit economy for distributed compute networks.</strong></p> \n <p>P2P-safe concurrent transactions using Conflict-free Replicated Data Types (CRDTs). Features a 10x-to-1x early adopter contribution curve and stake/slash mechanisms for participation incentives.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>CRDT Ledger</strong></td> \n    <td>G-Counter (earned) + PN-Counter (spent) for P2P consistency</td> \n   </tr> \n   <tr> \n    <td><strong>Contribution Curve</strong></td> \n    <td>10x early adopter multiplier decaying to 1x baseline</td> \n   </tr> \n   <tr> \n    <td><strong>Stake/Slash</strong></td> \n    <td>Participation requirements with slashing for bad actors</td> \n   </tr> \n   <tr> \n    <td><strong>Reputation Scoring</strong></td> \n    <td>Multi-factor: accuracy * uptime * stake_weight</td> \n   </tr> \n   <tr> \n    <td><strong>Merkle Verification</strong></td> \n    <td>SHA-256 state root for quick ledger verification</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Architecture:</strong></p> \n <pre><code>+------------------------+\n|     CreditLedger       |  &lt;-- CRDT-based P2P-safe ledger\n|  +------------------+  |\n|  | G-Counter: Earned|  |  &lt;-- Monotonically increasing\n|  | PN-Counter: Spent|  |  &lt;-- Can handle disputes/refunds\n|  | Stake: Locked    |  |  &lt;-- Participation requirement\n|  | State Root       |  |  &lt;-- Merkle root for verification\n|  +------------------+  |\n+------------------------+\n          |\n          v\n+------------------------+\n|  ContributionCurve     |  &lt;-- Exponential decay: 10x -&gt; 1x\n+------------------------+\n          |\n          v\n+------------------------+\n|   ReputationScore      |  &lt;-- accuracy * uptime * stake_weight\n+------------------------+\n</code></pre> \n <p><strong>JavaScript/TypeScript Example:</strong></p> \n <pre><code class=\"language-typescript\">import init, {\n  CreditLedger,\n  ReputationScore,\n  contribution_multiplier\n} from '@ruvector/economy-wasm';\n\nawait init();\n\n// Create a new ledger for a node\nconst ledger = new CreditLedger(\"node-123\");\n\n// Earn credits (with early adopter multiplier)\nledger.creditWithMultiplier(100, \"task:abc\");\nconsole.log(`Balance: ${ledger.balance()}`);\nconsole.log(`Multiplier: ${ledger.currentMultiplier()}x`);\n\n// Stake for participation\nledger.stake(50);\nconsole.log(`Staked: ${ledger.stakedAmount()}`);\n\n// Check multiplier for network compute hours\nconst mult = contribution_multiplier(50000.0);  // 50K hours\nconsole.log(`Network multiplier: ${mult}x`);  // ~8.5x\n\n// Track reputation\nconst rep = new ReputationScore(0.95, 0.98, 1000);\nconsole.log(`Composite score: ${rep.composite_score()}`);\n\n// P2P merge with another ledger (CRDT operation)\nconst otherEarned = new Uint8Array([/* serialized earned counter */]);\nconst otherSpent = new Uint8Array([/* serialized spent counter */]);\nconst mergedCount = ledger.merge(otherEarned, otherSpent);\n</code></pre> \n <h3>ruvector-exotic-wasm</h3> \n <p><strong>Exotic AI mechanisms for emergent behavior in distributed systems.</strong></p> \n <p>Novel coordination primitives inspired by decentralized governance, developmental biology, and quantum physics.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Mechanism</th> \n    <th>Inspiration</th> \n    <th>Use Case</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Neural Autonomous Organization (NAO)</strong></td> \n    <td>DAOs + oscillatory sync</td> \n    <td>Decentralized AI agent governance</td> \n   </tr> \n   <tr> \n    <td><strong>Morphogenetic Network</strong></td> \n    <td>Developmental biology</td> \n    <td>Emergent network topology</td> \n   </tr> \n   <tr> \n    <td><strong>Time Crystal Coordinator</strong></td> \n    <td>Quantum time crystals</td> \n    <td>Robust distributed coordination</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>NAO Features:</strong></p> \n <ul> \n  <li>Stake-weighted quadratic voting</li> \n  <li>Oscillatory synchronization for coherence</li> \n  <li>Quorum-based consensus (configurable threshold)</li> \n </ul> \n <p><strong>Morphogenetic Network Features:</strong></p> \n <ul> \n  <li>Cellular differentiation through morphogen gradients</li> \n  <li>Emergent network topology via growth/pruning</li> \n  <li>Synaptic pruning for optimization</li> \n </ul> \n <p><strong>Time Crystal Features:</strong></p> \n <ul> \n  <li>Period-doubled oscillations for stable coordination</li> \n  <li>Floquet engineering for noise resilience</li> \n  <li>Phase-locked agent synchronization</li> \n </ul> \n <p><strong>JavaScript/TypeScript Example:</strong></p> \n <pre><code class=\"language-typescript\">import init, {\n  WasmNAO,\n  WasmMorphogeneticNetwork,\n  WasmTimeCrystal,\n  ExoticEcosystem\n} from '@ruvector/exotic-wasm';\n\nawait init();\n\n// Neural Autonomous Organization\nconst nao = new WasmNAO(0.7);  // 70% quorum\nnao.addMember(\"agent_1\", 100);  // 100 stake\nnao.addMember(\"agent_2\", 50);\n\nconst propId = nao.propose(\"Upgrade memory backend\");\nnao.vote(propId, \"agent_1\", 0.9);  // 90% approval weight\nnao.vote(propId, \"agent_2\", 0.6);\n\nif (nao.execute(propId)) {\n  console.log(\"Proposal executed!\");\n}\n\n// Morphogenetic Network\nconst net = new WasmMorphogeneticNetwork(100, 100);  // 100x100 grid\nnet.seedSignaling(50, 50);  // Seed signaling cell at center\n\nfor (let i = 0; i &lt; 1000; i++) {\n  net.grow(0.1);  // 10% growth rate\n}\nnet.differentiate();\nnet.prune(0.1);  // 10% pruning threshold\n\n// Time Crystal Coordinator\nconst crystal = new WasmTimeCrystal(10, 100);  // 10 oscillators, 100ms period\ncrystal.crystallize();\n\nfor (let i = 0; i &lt; 200; i++) {\n  const pattern = crystal.tick();\n  // Use pattern for coordination decisions\n}\n\nconsole.log(`Synchronization: ${crystal.orderParameter()}`);\n\n// Combined Ecosystem (all three working together)\nconst eco = new ExoticEcosystem(5, 50, 8);  // 5 agents, 50x50 grid, 8 oscillators\neco.crystallize();\n\nfor (let i = 0; i &lt; 100; i++) {\n  eco.step();\n}\n\nconsole.log(eco.summaryJson());\n</code></pre> \n <h3>ruvector-nervous-system-wasm</h3> \n <p><strong>Bio-inspired neural system components for browser execution.</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Component</th> \n    <th>Performance</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>BTSP</strong></td> \n    <td>Immediate</td> \n    <td>Behavioral Timescale Synaptic Plasticity for one-shot learning</td> \n   </tr> \n   <tr> \n    <td><strong>HDC</strong></td> \n    <td>&lt;50ns bind, &lt;100ns similarity</td> \n    <td>Hyperdimensional Computing with 10,000-bit vectors</td> \n   </tr> \n   <tr> \n    <td><strong>WTA</strong></td> \n    <td>&lt;1us</td> \n    <td>Winner-Take-All for instant decisions</td> \n   </tr> \n   <tr> \n    <td><strong>K-WTA</strong></td> \n    <td>&lt;10us</td> \n    <td>K-Winner-Take-All for sparse distributed coding</td> \n   </tr> \n   <tr> \n    <td><strong>Global Workspace</strong></td> \n    <td>&lt;10us</td> \n    <td>4-7 item attention bottleneck (Miller's Law)</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Hyperdimensional Computing:</strong></p> \n <ul> \n  <li>10,000-bit binary hypervectors</li> \n  <li>10^40 representational capacity</li> \n  <li>XOR binding (associative, commutative, self-inverse)</li> \n  <li>Hamming distance similarity with SIMD optimization</li> \n </ul> \n <p><strong>Biological References:</strong></p> \n <ul> \n  <li>BTSP: Bittner et al. 2017 - Hippocampal place fields</li> \n  <li>HDC: Kanerva 1988, Plate 2003 - Hyperdimensional computing</li> \n  <li>WTA: Cortical microcircuits - Lateral inhibition</li> \n  <li>Global Workspace: Baars 1988, Dehaene 2014 - Consciousness</li> \n </ul> \n <p><strong>JavaScript/TypeScript Example:</strong></p> \n <pre><code class=\"language-typescript\">import init, {\n  BTSPLayer,\n  Hypervector,\n  HdcMemory,\n  WTALayer,\n  KWTALayer,\n  GlobalWorkspace,\n  WorkspaceItem,\n} from '@ruvector/nervous-system-wasm';\n\nawait init();\n\n// One-shot learning with BTSP\nconst btsp = new BTSPLayer(100, 2000.0);  // 100 dim, 2000ms tau\nconst pattern = new Float32Array(100).fill(0.1);\nbtsp.one_shot_associate(pattern, 1.0);  // Immediate association\nconst output = btsp.forward(pattern);\n\n// Hyperdimensional Computing\nconst apple = Hypervector.random();\nconst orange = Hypervector.random();\nconst fruit = apple.bind(orange);  // XOR binding\n\nconst similarity = apple.similarity(orange);  // ~0.0 (orthogonal)\nconsole.log(`Similarity: ${similarity}`);  // Random vectors are orthogonal\n\n// HDC Memory\nconst memory = new HdcMemory();\nmemory.store(\"apple\", apple);\nmemory.store(\"orange\", orange);\n\nconst results = memory.retrieve(apple, 0.9);  // threshold 0.9\nconst topK = memory.top_k(fruit, 3);  // top-3 similar\n\n// Instant decisions with WTA\nconst wta = new WTALayer(1000, 0.5, 0.8);  // 1000 neurons, threshold, inhibition\nconst activations = new Float32Array(1000);\n// ... fill activations ...\nconst winner = wta.compete(activations);\n\n// Sparse coding with K-WTA\nconst kwta = new KWTALayer(1000, 50);  // 1000 neurons, k=50 winners\nconst winners = kwta.select(activations);\n\n// Attention bottleneck with Global Workspace\nconst workspace = new GlobalWorkspace(7);  // Miller's Law: 7 +/- 2\nconst item = new WorkspaceItem(\n  new Float32Array([1, 2, 3]),  // content\n  0.9,  // salience\n  1,    // source\n  Date.now()  // timestamp\n);\nworkspace.broadcast(item);\n</code></pre> \n <h3>ruvector-attention-unified-wasm</h3> \n <p><strong>Unified API for 18+ attention mechanisms across Neural, DAG, Graph, and SSM domains.</strong></p> \n <p>A single WASM interface that routes to the appropriate attention implementation based on your data structure and requirements.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Category</th> \n    <th>Mechanisms</th> \n    <th>Best For</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Neural</strong></td> \n    <td>Scaled Dot-Product, Multi-Head, Hyperbolic, Linear, Flash, Local-Global, MoE</td> \n    <td>Transformers, sequences</td> \n   </tr> \n   <tr> \n    <td><strong>DAG</strong></td> \n    <td>Topological, Causal Cone, Critical Path, MinCut-Gated, Hierarchical Lorentz, Parallel Branch, Temporal BTSP</td> \n    <td>Query DAGs, workflows</td> \n   </tr> \n   <tr> \n    <td><strong>Graph</strong></td> \n    <td>GAT, GCN, GraphSAGE</td> \n    <td>GNNs, knowledge graphs</td> \n   </tr> \n   <tr> \n    <td><strong>SSM</strong></td> \n    <td>Mamba</td> \n    <td>Long sequences, streaming</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Mechanism Selection:</strong></p> \n <pre><code>+------------------+     +-------------------+\n|   Your Data      | --&gt; | UnifiedAttention  | --&gt; Optimal Mechanism\n+------------------+     +-------------------+\n                               |\n        +----------------------+----------------------+\n        |                      |                      |\n   +----v----+           +-----v-----+          +-----v----+\n   | Neural  |           |    DAG    |          |  Graph   |\n   +---------+           +-----------+          +----------+\n   | dot_prod|           | topological|         | gat      |\n   | multi_hd|           | causal_cone|         | gcn      |\n   | flash   |           | mincut_gtd |         | graphsage|\n   +---------+           +-----------+          +----------+\n</code></pre> \n <p><strong>JavaScript/TypeScript Example:</strong></p> \n <pre><code class=\"language-typescript\">import init, {\n  UnifiedAttention,\n  availableMechanisms,\n  getStats,\n  softmax,\n  temperatureSoftmax,\n  cosineSimilarity,\n  // Neural attention\n  ScaledDotProductAttention,\n  MultiHeadAttention,\n  // DAG attention\n  TopologicalAttention,\n  MinCutGatedAttention,\n  // Graph attention\n  GraphAttention,\n  // SSM\n  MambaSSM,\n} from '@ruvector/attention-unified-wasm';\n\nawait init();\n\n// List all available mechanisms\nconsole.log(availableMechanisms());\n// { neural: [...], dag: [...], graph: [...], ssm: [...] }\n\nconsole.log(getStats());\n// { total_mechanisms: 18, neural_count: 7, dag_count: 7, ... }\n\n// Unified selector - routes to appropriate implementation\nconst attention = new UnifiedAttention(\"multi_head\");\nconsole.log(`Category: ${attention.category}`);  // \"neural\"\nconsole.log(`Supports sequences: ${attention.supportsSequences()}`);  // true\nconsole.log(`Supports graphs: ${attention.supportsGraphs()}`);  // false\n\n// For DAG structures\nconst dagAttention = new UnifiedAttention(\"topological\");\nconsole.log(`Category: ${dagAttention.category}`);  // \"dag\"\nconsole.log(`Supports graphs: ${dagAttention.supportsGraphs()}`);  // true\n\n// Hyperbolic attention for hierarchical data\nconst hypAttention = new UnifiedAttention(\"hierarchical_lorentz\");\nconsole.log(`Supports hyperbolic: ${hypAttention.supportsHyperbolic()}`);  // true\n\n// Utility functions\nconst logits = [1.0, 2.0, 3.0, 4.0];\nconst probs = softmax(logits);\nconsole.log(`Probabilities sum to: ${probs.reduce((a, b) =&gt; a + b)}`);  // 1.0\n\n// Temperature-scaled softmax (lower = more peaked)\nconst sharperProbs = temperatureSoftmax(logits, 0.5);\n\n// Cosine similarity\nconst vecA = [1.0, 0.0, 0.0];\nconst vecB = [1.0, 0.0, 0.0];\nconsole.log(`Similarity: ${cosineSimilarity(vecA, vecB)}`);  // 1.0\n</code></pre> \n <h3>WASM Package Summary</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Size Target</th> \n    <th>Key Features</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>@ruvector/learning-wasm</code></td> \n    <td>&lt;50KB</td> \n    <td>MicroLoRA (&lt;100us), trajectory tracking</td> \n   </tr> \n   <tr> \n    <td><code>@ruvector/economy-wasm</code></td> \n    <td>&lt;100KB</td> \n    <td>CRDT ledger, 10x-&gt;1x curve, stake/slash</td> \n   </tr> \n   <tr> \n    <td><code>@ruvector/exotic-wasm</code></td> \n    <td>&lt;150KB</td> \n    <td>NAO, Morphogenetic, Time Crystal</td> \n   </tr> \n   <tr> \n    <td><code>@ruvector/nervous-system-wasm</code></td> \n    <td>&lt;100KB</td> \n    <td>BTSP, HDC (10K-bit), WTA, Global Workspace</td> \n   </tr> \n   <tr> \n    <td><code>@ruvector/attention-unified-wasm</code></td> \n    <td>&lt;200KB</td> \n    <td>18+ attention mechanisms, unified API</td> \n   </tr> \n   <tr> \n    <td><code>@ruvnet/ruvector-verified-wasm</code></td> \n    <td>&lt;80KB</td> \n    <td>Formal proof verification in browser/edge</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Common Patterns:</strong></p> \n <pre><code class=\"language-typescript\">// All packages follow the same initialization pattern\nimport init, { /* exports */ } from '@ruvector/&lt;package&gt;-wasm';\nawait init();\n\n// Version check\nimport { version } from '@ruvector/&lt;package&gt;-wasm';\nconsole.log(`Version: ${version()}`);\n\n// Feature discovery\nimport { available_mechanisms } from '@ruvector/&lt;package&gt;-wasm';\nconsole.log(available_mechanisms());\n</code></pre> \n</details> \n<hr /> \n<h2>Self-Learning Systems</h2> \n<details> \n 🧠 Self-Learning Intelligence Hooks \n <p><strong>Make your AI assistant smarter over time.</strong></p> \n <p>When you use Claude Code (or any AI coding assistant), it starts fresh every session. It doesn't remember which approaches worked, which files you typically edit together, or what errors you've seen before.</p> \n <p><strong>RuVector Hooks fixes this.</strong> It's a lightweight intelligence layer that:</p> \n <ol> \n  <li><strong>Remembers what works</strong> — Tracks which agent types succeed for different tasks</li> \n  <li><strong>Learns from mistakes</strong> — Records error patterns and suggests fixes you've used before</li> \n  <li><strong>Predicts your workflow</strong> — Knows that after editing <code>api.rs</code>, you usually edit <code>api_test.rs</code></li> \n  <li><strong>Coordinates teams</strong> — Manages multi-agent swarms for complex tasks</li> \n </ol> \n <p>Think of it as giving your AI assistant a memory and intuition about your codebase.</p> \n <h4>How It Works</h4> \n <pre><code>┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐\n│  Claude Code    │────▶│  RuVector Hooks  │────▶│   Intelligence  │\n│  (PreToolUse)   │     │   (pre-edit)     │     │      Layer      │\n└─────────────────┘     └──────────────────┘     └─────────────────┘\n                                                         │\n         ┌───────────────────────────────────────────────┘\n         ▼\n┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐\n│   Q-Learning    │     │  Vector Memory   │     │  Swarm Graph    │\n│   α=0.1 γ=0.95  │     │  64-dim embed    │     │  Coordination   │\n└─────────────────┘     └──────────────────┘     └─────────────────┘\n</code></pre> \n <p>The hooks integrate with Claude Code's event system:</p> \n <ul> \n  <li><strong>PreToolUse</strong> → Provides guidance before edits (agent routing, related files)</li> \n  <li><strong>PostToolUse</strong> → Records outcomes for learning (success/failure, patterns)</li> \n  <li><strong>SessionStart/Stop</strong> → Manages session state and metrics export</li> \n </ul> \n <h4>Technical Specifications</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Component</th> \n    <th>Implementation</th> \n    <th>Details</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Q-Learning</strong></td> \n    <td>Temporal Difference</td> \n    <td>α=0.1, γ=0.95, ε=0.1 (ε-greedy exploration)</td> \n   </tr> \n   <tr> \n    <td><strong>Embeddings</strong></td> \n    <td>Hash-based vectors</td> \n    <td>64 dimensions, normalized, cosine similarity</td> \n   </tr> \n   <tr> \n    <td><strong>LRU Cache</strong></td> \n    <td><code>lru</code> crate</td> \n    <td>1000 entries, ~10x faster Q-value lookups</td> \n   </tr> \n   <tr> \n    <td><strong>Compression</strong></td> \n    <td><code>flate2</code> gzip</td> \n    <td>70-83% storage reduction, fast compression</td> \n   </tr> \n   <tr> \n    <td><strong>Storage</strong></td> \n    <td>JSON / PostgreSQL</td> \n    <td>Auto-fallback, 5000 memory entry limit</td> \n   </tr> \n   <tr> \n    <td><strong>Cross-platform</strong></td> \n    <td>Rust + TypeScript</td> \n    <td>Windows (USERPROFILE), Unix (HOME)</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Performance</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Metric</th> \n    <th>Value</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Q-value lookup (cached)</td> \n    <td>&lt;1µs</td> \n   </tr> \n   <tr> \n    <td>Q-value lookup (uncached)</td> \n    <td>~50µs</td> \n   </tr> \n   <tr> \n    <td>Memory search (1000 entries)</td> \n    <td>&lt;5ms</td> \n   </tr> \n   <tr> \n    <td>Storage compression ratio</td> \n    <td>70-83%</td> \n   </tr> \n   <tr> \n    <td>Session start overhead</td> \n    <td>&lt;10ms</td> \n   </tr> \n  </tbody> \n </table> \n <table> \n  <thead> \n   <tr> \n    <th>Crate/Package</th> \n    <th>Description</th> \n    <th>Status</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-cli\">ruvector-cli hooks</a></td> \n    <td>Rust CLI with 34 hooks commands</td> \n    <td><a href=\"https://crates.io/crates/ruvector-cli\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/npm/packages/cli\">@ruvector/cli hooks</a></td> \n    <td>npm CLI with 29 hooks commands</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/cli\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/cli.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <h4>Quick Start</h4> \n <pre><code class=\"language-bash\"># Rust CLI\ncargo install ruvector-cli\nruvector hooks init\nruvector hooks install\n\n# npm CLI\nnpx @ruvector/cli hooks init\nnpx @ruvector/cli hooks install\n</code></pre> \n <h4>Core Capabilities</h4> \n <table> \n  <thead> \n   <tr> \n    <th>Feature</th> \n    <th>Description</th> \n    <th>Technical Details</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Q-Learning Routing</strong></td> \n    <td>Routes tasks to best agent with learned confidence scores</td> \n    <td>TD learning with α=0.1, γ=0.95, ε-greedy exploration</td> \n   </tr> \n   <tr> \n    <td><strong>Semantic Memory</strong></td> \n    <td>Vector-based memory with embeddings for context retrieval</td> \n    <td>64-dim hash embeddings, cosine similarity, top-k search</td> \n   </tr> \n   <tr> \n    <td><strong>Error Learning</strong></td> \n    <td>Records error patterns and suggests fixes</td> \n    <td>Pattern matching for E0308, E0433, TS2322, etc.</td> \n   </tr> \n   <tr> \n    <td><strong>File Sequences</strong></td> \n    <td>Predicts next files to edit based on historical patterns</td> \n    <td>Markov chain transitions, frequency-weighted suggestions</td> \n   </tr> \n   <tr> \n    <td><strong>Swarm Coordination</strong></td> \n    <td>Registers agents, tracks coordination edges, optimizes</td> \n    <td>Graph-based topology, weighted edges, task assignment</td> \n   </tr> \n   <tr> \n    <td><strong>LRU Cache</strong></td> \n    <td>1000-entry cache for faster Q-value lookups</td> \n    <td>~10x speedup, automatic eviction, RefCell for interior mutability</td> \n   </tr> \n   <tr> \n    <td><strong>Gzip Compression</strong></td> \n    <td>Storage savings with automatic compression</td> \n    <td>flate2 fast mode, 70-83% reduction, transparent load/save</td> \n   </tr> \n   <tr> \n    <td><strong>Batch Saves</strong></td> \n    <td>Dirty flag tracking to reduce disk I/O</td> \n    <td>Only writes when data changes, force_save() override</td> \n   </tr> \n   <tr> \n    <td><strong>Shell Completions</strong></td> \n    <td>Tab completion for all commands</td> \n    <td>bash, zsh, fish, PowerShell support</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Supported Error Codes</h4> \n <p>The intelligence layer has built-in knowledge for common error patterns:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Language</th> \n    <th>Error Codes</th> \n    <th>Auto-Suggested Fixes</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><strong>Rust</strong></td> \n    <td>E0308, E0433, E0425, E0277, E0382</td> \n    <td>Type mismatches, missing imports, borrow checker</td> \n   </tr> \n   <tr> \n    <td><strong>TypeScript</strong></td> \n    <td>TS2322, TS2339, TS2345, TS7006</td> \n    <td>Type assignments, property access, argument types</td> \n   </tr> \n   <tr> \n    <td><strong>Python</strong></td> \n    <td>ImportError, AttributeError, TypeError</td> \n    <td>Module imports, attribute access, type errors</td> \n   </tr> \n   <tr> \n    <td><strong>Go</strong></td> \n    <td>undefined, cannot use, not enough arguments</td> \n    <td>Variable scope, type conversion, function calls</td> \n   </tr> \n  </tbody> \n </table> \n <h4>Commands Reference</h4> \n <pre><code class=\"language-bash\"># Setup\nruvector hooks init [--force] [--postgres]  # Initialize hooks (--postgres for DB schema)\nruvector hooks install                   # Install into Claude settings\n\n# Core\nruvector hooks stats                     # Show intelligence statistics\nruvector hooks session-start [--resume]  # Start/resume a session\nruvector hooks session-end               # End session with metrics\n\n# Memory\nruvector hooks remember -t edit \"content\"  # Store in semantic memory\nruvector hooks recall \"query\" -k 5         # Search memory semantically\n\n# Learning\nruvector hooks learn &lt;state&gt; &lt;action&gt; --reward 0.8  # Record trajectory\nruvector hooks suggest &lt;state&gt; --actions \"a,b,c\"    # Get action suggestion\nruvector hooks route \"implement caching\" --file src/cache.rs  # Route to agent\n\n# Claude Code Hooks\nruvector hooks pre-edit &lt;file&gt;           # Pre-edit intelligence hook\nruvector hooks post-edit &lt;file&gt; --success  # Post-edit learning hook\nruvector hooks pre-command &lt;cmd&gt;         # Pre-command hook\nruvector hooks post-command &lt;cmd&gt; --success  # Post-command hook\nruvector hooks suggest-context           # UserPromptSubmit context injection\nruvector hooks track-notification        # Track notification patterns\nruvector hooks pre-compact [--auto]      # Pre-compact hook (auto/manual)\n\n# Claude Code v2.0.55+ Features\nruvector hooks lsp-diagnostic --file &lt;f&gt; --severity error  # LSP diagnostics\nruvector hooks suggest-ultrathink \"complex task\"  # Recommend extended reasoning\nruvector hooks async-agent --action spawn --agent-id &lt;id&gt;  # Async sub-agents\n\n# Intelligence\nruvector hooks record-error &lt;cmd&gt; &lt;stderr&gt;  # Record error pattern\nruvector hooks suggest-fix E0308           # Get fix for error code\nruvector hooks suggest-next &lt;file&gt; -n 3    # Predict next files\nruvector hooks should-test &lt;file&gt;          # Check if tests needed\n\n# Swarm\nruvector hooks swarm-register &lt;id&gt; &lt;type&gt;  # Register agent\nruvector hooks swarm-coordinate &lt;src&gt; &lt;tgt&gt;  # Record coordination\nruvector hooks swarm-optimize \"task1,task2\"  # Optimize distribution\nruvector hooks swarm-recommend \"rust\"      # Recommend agent for task\nruvector hooks swarm-heal &lt;agent-id&gt;       # Handle agent failure\nruvector hooks swarm-stats                 # Show swarm statistics\n\n# Optimization (Rust only)\nruvector hooks compress                   # Compress storage (70-83% savings)\nruvector hooks cache-stats                # Show LRU cache statistics\nruvector hooks completions bash           # Generate shell completions\n</code></pre> \n <h4>Tutorial: Claude Code Integration</h4> \n <p><strong>1. Initialize and install hooks:</strong></p> \n <pre><code class=\"language-bash\">ruvector hooks init\nruvector hooks install --settings-dir .claude\n</code></pre> \n <p>This creates <code>.claude/settings.json</code> with hook configurations:</p> \n <pre><code class=\"language-json\">{\n  \"hooks\": {\n    \"PreToolUse\": [\n      { \"matcher\": \"Edit|Write|MultiEdit\", \"hooks\": [\"ruvector hooks pre-edit \\\"$TOOL_INPUT_FILE_PATH\\\"\"] },\n      { \"matcher\": \"Bash\", \"hooks\": [\"ruvector hooks pre-command \\\"$TOOL_INPUT_COMMAND\\\"\"] }\n    ],\n    \"PostToolUse\": [\n      { \"matcher\": \"Edit|Write|MultiEdit\", \"hooks\": [\"ruvector hooks post-edit ... --success\"] },\n      { \"matcher\": \"Bash\", \"hooks\": [\"ruvector hooks post-command ... --success\"] }\n    ],\n    \"SessionStart\": [\"ruvector hooks session-start\"],\n    \"Stop\": [\"ruvector hooks session-end --export-metrics\"],\n    \"PreCompact\": [\"ruvector hooks pre-compact\"]\n  }\n}\n</code></pre> \n <p><strong>All 7 Claude Code hooks covered:</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Hook</th> \n    <th>When It Fires</th> \n    <th>What RuVector Does</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>PreToolUse</code></td> \n    <td>Before file edit, command, or Task</td> \n    <td>Suggests agent, shows related files, validates agent assignments</td> \n   </tr> \n   <tr> \n    <td><code>PostToolUse</code></td> \n    <td>After file edit or command</td> \n    <td>Records outcome, updates Q-values, injects context</td> \n   </tr> \n   <tr> \n    <td><code>SessionStart</code></td> \n    <td>When session begins/resumes</td> \n    <td>Loads intelligence, shows stats (startup vs resume)</td> \n   </tr> \n   <tr> \n    <td><code>Stop</code></td> \n    <td>When session ends</td> \n    <td>Saves state, exports metrics</td> \n   </tr> \n   <tr> \n    <td><code>PreCompact</code></td> \n    <td>Before context compaction</td> \n    <td>Preserves critical memories (auto vs manual)</td> \n   </tr> \n   <tr> \n    <td><code>UserPromptSubmit</code></td> \n    <td>Before processing user prompt</td> \n    <td>Injects learned patterns as context</td> \n   </tr> \n   <tr> \n    <td><code>Notification</code></td> \n    <td>On system notifications</td> \n    <td>Tracks notification patterns</td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Advanced Features:</strong></p> \n <ul> \n  <li><strong>Stdin JSON Parsing</strong>: Hooks receive full JSON via stdin (session_id, tool_input, tool_response)</li> \n  <li><strong>Context Injection</strong>: PostToolUse returns <code>additionalContext</code> to inject into Claude's context</li> \n  <li><strong>Timeout Optimization</strong>: All hooks have optimized timeouts (1-5 seconds vs 60s default)</li> \n </ul> \n <p><strong>2. Use routing for intelligent agent selection:</strong></p> \n <pre><code class=\"language-bash\"># Route a task to the best agent\n$ ruvector hooks route \"implement vector search\" --file src/lib.rs\n{\n  \"recommended\": \"rust-developer\",\n  \"confidence\": 0.85,\n  \"reasoning\": \"learned from 47 similar edits\"\n}\n</code></pre> \n <p><strong>3. Learn from outcomes:</strong></p> \n <pre><code class=\"language-bash\"># Record successful outcome\nruvector hooks learn \"edit-rs-lib\" \"rust-developer\" --reward 1.0\n\n# Record failed outcome\nruvector hooks learn \"edit-rs-lib\" \"typescript-dev\" --reward -0.5\n</code></pre> \n <p><strong>4. Get error fix suggestions:</strong></p> \n <pre><code class=\"language-bash\">$ ruvector hooks suggest-fix E0308\n{\n  \"code\": \"E0308\",\n  \"type\": \"type_mismatch\",\n  \"fixes\": [\n    \"Check return type matches function signature\",\n    \"Use .into() or .as_ref() for type conversion\",\n    \"Verify generic type parameters\"\n  ]\n}\n</code></pre> \n <h4>Tutorial: Swarm Coordination</h4> \n <p><strong>1. Register agents:</strong></p> \n <pre><code class=\"language-bash\">ruvector hooks swarm-register agent-1 rust-developer --capabilities \"rust,async,testing\"\nruvector hooks swarm-register agent-2 typescript-dev --capabilities \"ts,react,node\"\nruvector hooks swarm-register agent-3 reviewer --capabilities \"review,security,performance\"\n</code></pre> \n <p><strong>2. Record coordination patterns:</strong></p> \n <pre><code class=\"language-bash\"># Agent-1 hands off to Agent-3 for review\nruvector hooks swarm-coordinate agent-1 agent-3 --weight 0.9\n</code></pre> \n <p><strong>3. Optimize task distribution:</strong></p> \n <pre><code class=\"language-bash\">$ ruvector hooks swarm-optimize \"implement-api,write-tests,code-review\"\n{\n  \"assignments\": {\n    \"implement-api\": \"agent-1\",\n    \"write-tests\": \"agent-1\",\n    \"code-review\": \"agent-3\"\n  }\n}\n</code></pre> \n <p><strong>4. Handle failures with self-healing:</strong></p> \n <pre><code class=\"language-bash\"># Mark agent as failed and redistribute\nruvector hooks swarm-heal agent-2\n</code></pre> \n <h4>PostgreSQL Storage (Optional)</h4> \n <p>For production deployments, use PostgreSQL instead of JSON files:</p> \n <pre><code class=\"language-bash\"># Set connection URL\nexport RUVECTOR_POSTGRES_URL=\"postgres://user:pass@localhost/ruvector\"\n\n# Initialize PostgreSQL schema (automatic)\nruvector hooks init --postgres\n\n# Or apply schema manually\npsql $RUVECTOR_POSTGRES_URL -f crates/ruvector-cli/sql/hooks_schema.sql\n\n# Build CLI with postgres feature\ncargo build -p ruvector-cli --features postgres\n</code></pre> \n <p>The PostgreSQL backend provides:</p> \n <ul> \n  <li>Vector embeddings with native <code>ruvector</code> type</li> \n  <li>Q-learning functions (<code>ruvector_hooks_update_q</code>, <code>ruvector_hooks_best_action</code>)</li> \n  <li>Swarm coordination tables with foreign key relationships</li> \n  <li>Automatic memory cleanup (keeps last 5000 entries)</li> \n </ul> \n</details> \n<hr /> \n<h2>Additional Modules</h2> \n<details> \n 🔬 Scientific OCR (SciPix) \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Install</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/scipix\">ruvector-scipix</a></td> \n    <td>Rust OCR engine for scientific documents</td> \n    <td><code>cargo add ruvector-scipix</code></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/scipix\">@ruvector/scipix</a></td> \n    <td>TypeScript client for SciPix API</td> \n    <td><code>npm install @ruvector/scipix</code></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>SciPix</strong> extracts text and mathematical equations from images, converting them to LaTeX, MathML, or plain text.</p> \n <p><strong>Features:</strong></p> \n <ul> \n  <li><strong>Multi-format output</strong> — LaTeX, MathML, AsciiMath, plain text, structured JSON</li> \n  <li><strong>Batch processing</strong> — Process multiple images with parallel execution</li> \n  <li><strong>Content detection</strong> — Equations, tables, diagrams, mixed content</li> \n  <li><strong>Confidence scoring</strong> — Per-region confidence levels (high/medium/low)</li> \n  <li><strong>PDF support</strong> — Extract from multi-page PDFs with page selection</li> \n </ul> \n <pre><code class=\"language-typescript\">import { SciPixClient, OutputFormat } from '@ruvector/scipix';\n\nconst client = new SciPixClient({\n  baseUrl: 'http://localhost:8080',\n  apiKey: 'your-api-key',\n});\n\n// OCR an image file\nconst result = await client.ocrFile('./equation.png', {\n  formats: [OutputFormat.LaTeX, OutputFormat.MathML],\n  detectEquations: true,\n});\n\nconsole.log('LaTeX:', result.latex);\nconsole.log('Confidence:', result.confidence);\n\n// Quick LaTeX extraction\nconst latex = await client.extractLatex('./math.png');\n\n// Batch processing\nconst batchResult = await client.batchOcr({\n  images: [\n    { source: 'base64...', id: 'eq1' },\n    { source: 'base64...', id: 'eq2' },\n  ],\n  defaultOptions: { formats: [OutputFormat.LaTeX] },\n});\n</code></pre> \n <pre><code class=\"language-bash\"># Rust CLI usage\nscipix-cli ocr --input equation.png --format latex\nscipix-cli serve --port 3000\n\n# MCP server for Claude/AI assistants\nscipix-cli mcp\nclaude mcp add scipix -- scipix-cli mcp\n</code></pre> \n <p>See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/npm/packages/scipix/README.md\">npm/packages/scipix/README.md</a> for full documentation.</p> \n</details> \n<details> \n 🔗 ONNX Embeddings \n <table> \n  <thead> \n   <tr> \n    <th>Example</th> \n    <th>Description</th> \n    <th>Path</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/onnx-embeddings\">ruvector-onnx-embeddings</a></td> \n    <td>Production-ready ONNX embedding generation in pure Rust</td> \n    <td><code>examples/onnx-embeddings</code></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>ONNX Embeddings</strong> provides native embedding generation using ONNX Runtime — no Python required. Supports 8+ pretrained models (all-MiniLM, BGE, E5, GTE), multiple pooling strategies, GPU acceleration (CUDA, TensorRT, CoreML, WebGPU), and direct RuVector index integration for RAG pipelines.</p> \n <pre><code class=\"language-rust\">use ruvector_onnx_embeddings::{Embedder, PretrainedModel};\n\n#[tokio::main]\nasync fn main() -&gt; anyhow::Result&lt;()&gt; {\n    // Create embedder with default model (all-MiniLM-L6-v2)\n    let mut embedder = Embedder::default_model().await?;\n\n    // Generate embedding (384 dimensions)\n    let embedding = embedder.embed_one(\"Hello, world!\")?;\n\n    // Compute semantic similarity\n    let sim = embedder.similarity(\n        \"I love programming in Rust\",\n        \"Rust is my favorite language\"\n    )?;\n    println!(\"Similarity: {:.4}\", sim); // ~0.85\n\n    Ok(())\n}\n</code></pre> \n <p><strong>Supported Models:</strong></p> \n <table> \n  <thead> \n   <tr> \n    <th>Model</th> \n    <th>Dimension</th> \n    <th>Speed</th> \n    <th>Best For</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><code>AllMiniLmL6V2</code></td> \n    <td>384</td> \n    <td>Fast</td> \n    <td>General purpose (default)</td> \n   </tr> \n   <tr> \n    <td><code>BgeSmallEnV15</code></td> \n    <td>384</td> \n    <td>Fast</td> \n    <td>Search &amp; retrieval</td> \n   </tr> \n   <tr> \n    <td><code>AllMpnetBaseV2</code></td> \n    <td>768</td> \n    <td>Accurate</td> \n    <td>Production RAG</td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<details> \n 🔧 Bindings &amp; Tools \n <p><strong>Native bindings and tools</strong> for integrating RuVector into any environment — Node.js, browsers, CLI, or as an HTTP/gRPC server.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Crate</th> \n    <th>Description</th> \n    <th>crates.io</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-node\">ruvector-node</a></td> \n    <td>Native Node.js bindings via napi-rs</td> \n    <td><a href=\"https://crates.io/crates/ruvector-node\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-node.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-wasm\">ruvector-wasm</a></td> \n    <td>WASM bindings for browsers &amp; edge</td> \n    <td><a href=\"https://crates.io/crates/ruvector-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm-wasm\">ruvllm-wasm</a></td> \n    <td>Browser LLM inference with WebGPU</td> \n    <td><a href=\"https://crates.io/crates/ruvllm-wasm\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvllm-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-cli\">ruvector-cli</a></td> \n    <td>Command-line interface</td> \n    <td><a href=\"https://crates.io/crates/ruvector-cli\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvector-server\">ruvector-server</a></td> \n    <td>HTTP/gRPC server</td> \n    <td><a href=\"https://crates.io/crates/ruvector-server\"><img alt=\"crates.io\" src=\"https://img.shields.io/crates/v/ruvector-server.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Node.js (Native Performance)</strong></p> \n <pre><code class=\"language-bash\">npm install @ruvector/node\n</code></pre> \n <pre><code class=\"language-javascript\">const { RuVector } = require('@ruvector/node');\nconst db = new RuVector({ dimensions: 1536 });\ndb.insert('doc1', embedding, { title: 'Hello' });\nconst results = db.search(queryEmbedding, 10);\n</code></pre> \n <p><strong>Browser (WASM)</strong></p> \n <pre><code class=\"language-bash\">npm install @ruvector/wasm\n</code></pre> \n <pre><code class=\"language-javascript\">import { RuVectorWasm } from '@ruvector/wasm';\nconst db = await RuVectorWasm.create({ dimensions: 384 });\nawait db.insert('doc1', embedding);\nconst results = await db.search(query, 5);\n</code></pre> \n <p><strong>CLI</strong></p> \n <pre><code class=\"language-bash\">cargo install ruvector-cli\nruvector init mydb --dim 1536\nruvector insert mydb --file embeddings.json\nruvector search mydb --query \"[0.1, 0.2, ...]\" --limit 10\n</code></pre> \n <p><strong>HTTP Server</strong></p> \n <pre><code class=\"language-bash\">cargo install ruvector-server\nruvector-server --port 8080 --data ./vectors\n\n# REST API\ncurl -X POST http://localhost:8080/search \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"vector\": [0.1, 0.2, ...], \"limit\": 10}'\n</code></pre> \n</details> \n<hr /> \n<h2>Examples &amp; Tutorials</h2> \n<details> \n 📚 Production Examples \n <p>34 production-ready examples demonstrating RuVector integration patterns.</p> \n <table> \n  <thead> \n   <tr> \n    <th>Example</th> \n    <th>Description</th> \n    <th>Type</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/security_hardened.rvf\"><strong>security_hardened.rvf</strong></a></td> \n    <td><strong>Security RVF: 22 capabilities — TEE, AIDefence, eBPF, RBAC, Paranoid policy</strong></td> \n    <td>Rust/RVF</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/agentic-jujutsu\">agentic-jujutsu</a></td> \n    <td>Quantum-resistant version control for AI agents (23x faster than Git)</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/mincut\">mincut</a></td> \n    <td>6 self-organizing network demos: strange loops, time crystals, causal discovery</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/subpolynomial-time\">subpolynomial-time</a></td> \n    <td>n^0.12 subpolynomial algorithm demos</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/exo-ai-2025\">exo-ai-2025</a></td> \n    <td>Cognitive substrate with 9 neural-symbolic crates + 11 research experiments</td> \n    <td>Rust/TS</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/neural-trader\">neural-trader</a></td> \n    <td>AI trading with DRL + sentiment analysis + SONA learning</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/ultra-low-latency-sim\">ultra-low-latency-sim</a></td> \n    <td>13+ quadrillion meta-simulations/sec with SIMD</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/meta-cognition-spiking-neural-network\">meta-cognition-spiking-neural-network</a></td> \n    <td>Spiking neural network with meta-cognitive learning (10-50x speedup)</td> \n    <td>npm</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/spiking-network\">spiking-network</a></td> \n    <td>Biologically-inspired spiking neural networks</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/ruvLLM\">ruvLLM</a></td> \n    <td>LLM integration patterns for RAG and AI agents</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/onnx-embeddings\">onnx-embeddings</a></td> \n    <td>Production ONNX embedding generation without Python</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/onnx-embeddings-wasm\">onnx-embeddings-wasm</a></td> \n    <td>WASM ONNX embeddings for browsers</td> \n    <td>WASM</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/refrag-pipeline\">refrag-pipeline</a></td> \n    <td>RAG pipeline with vector search and document processing</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/scipix\">scipix</a></td> \n    <td>Scientific OCR: equations → LaTeX/MathML with ONNX inference</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/graph\">graph</a></td> \n    <td>Graph database examples with Cypher queries</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge\">edge</a></td> \n    <td>364KB WASM edge deployment</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge-full\">edge-full</a></td> \n    <td>Full-featured edge vector DB</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/edge-net\">edge-net</a></td> \n    <td>Networked edge deployment with zero-cost swarms</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/vibecast-7sense\">vibecast-7sense</a></td> \n    <td>7-sense perception AI application</td> \n    <td>TypeScript</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/apify\">apify</a></td> \n    <td>13 Apify actors: trading, memory engine, synth data, market research</td> \n    <td>npm</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/google-cloud\">google-cloud</a></td> \n    <td>GCP templates for Cloud Run, GKE, Vertex AI</td> \n    <td>Terraform</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/wasm-react\">wasm-react</a></td> \n    <td>React integration with WASM vector operations</td> \n    <td>WASM</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/wasm-vanilla\">wasm-vanilla</a></td> \n    <td>Vanilla JS WASM example for browser vector search</td> \n    <td>WASM</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/wasm\">wasm</a></td> \n    <td>Core WASM examples and bindings</td> \n    <td>WASM</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/nodejs\">nodejs</a></td> \n    <td>Node.js integration examples</td> \n    <td>Node.js</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/rust\">rust</a></td> \n    <td>Core Rust usage examples</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/dna\">dna</a></td> \n    <td>rvDNA: AI-native genomic analysis, variant calling, <code>.rvdna</code> format</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/delta-behavior\">delta-behavior</a></td> \n    <td>Mathematics of systems that refuse to collapse — behavioral change tracking</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/data\">data</a></td> \n    <td>Dataset discovery framework — graph-based pattern finding in massive datasets</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/prime-radiant\">prime-radiant</a></td> \n    <td>Prime-Radiant coherence engine examples and usage demos</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/benchmarks\">benchmarks</a></td> \n    <td>Comprehensive benchmarks for temporal reasoning and vector operations</td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/vwm-viewer\">vwm-viewer</a></td> \n    <td>Visual vector world model viewer (HTML Canvas)</td> \n    <td>HTML</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/verified-applications\"><strong>verified-applications</strong></a></td> \n    <td><strong>10 exotic domains: weapons filter, medical diagnostics, financial routing, agent contracts, sensor swarm, quantization proof, AGI memory, vector signatures, simulation integrity, legal forensics</strong></td> \n    <td>Rust</td> \n   </tr> \n   <tr> \n    <td><a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/examples/rvf-kernel-optimized\">rvf-kernel-optimized</a></td> \n    <td>Verified + hyper-optimized Linux kernel RVF with proof-carrying ingest</td> \n    <td>Rust</td> \n   </tr> \n  </tbody> \n </table> \n</details> \n<details> \n 🎓 Tutorials \n <h3>Tutorial 1: Vector Search in 60 Seconds</h3> \n <pre><code class=\"language-javascript\">import { VectorDB } from 'ruvector';\n\n// Create DB with 384-dimensional vectors\nconst db = new VectorDB(384);\n\n// Add vectors\ndb.insert('doc1', [0.1, 0.2, ...]);  // 384 floats\ndb.insert('doc2', [0.3, 0.1, ...]);\n\n// Search (returns top 5 nearest neighbors)\nconst results = db.search(queryVector, 5);\n// -&gt; [{ id: 'doc1', score: 0.95 }, { id: 'doc2', score: 0.87 }]\n</code></pre> \n <h3>Tutorial 2: Graph Queries with Cypher</h3> \n <pre><code class=\"language-javascript\">import { GraphDB } from 'ruvector';\n\nconst graph = new GraphDB();\n\n// Create nodes and relationships\ngraph.query(`\n  CREATE (a:Person {name: 'Alice', embedding: $emb1})\n  CREATE (b:Person {name: 'Bob', embedding: $emb2})\n  CREATE (a)-[:KNOWS {since: 2020}]-&gt;(b)\n`, { emb1: aliceVector, emb2: bobVector });\n\n// Hybrid query: graph traversal + vector similarity\nconst results = graph.query(`\n  MATCH (p:Person)-[:KNOWS*1..3]-&gt;(friend)\n  WHERE vector.similarity(friend.embedding, $query) &gt; 0.8\n  RETURN friend.name, vector.similarity(friend.embedding, $query) as score\n  ORDER BY score DESC\n`, { query: queryVector });\n</code></pre> \n <h3>Tutorial 3: Self-Learning with SONA</h3> \n <pre><code class=\"language-rust\">use ruvector_sona::{SonaEngine, SonaConfig};\n\n// Initialize SONA with LoRA adapters\nlet sona = SonaEngine::with_config(SonaConfig {\n    hidden_dim: 256,\n    lora_rank: 8,\n    ewc_lambda: 0.4,  // Elastic Weight Consolidation\n    ..Default::default()\n});\n\n// Record successful action\nlet mut trajectory = sona.begin_trajectory(query_embedding);\ntrajectory.add_step(result_embedding, vec![], 1.0);  // reward=1.0\nsona.end_trajectory(trajectory, true);  // success=true\n\n// SONA learns and improves future predictions\nsona.force_learn();\n\n// Later: get improved predictions\nlet prediction = sona.predict(&amp;new_query_embedding);\n</code></pre> \n <h3>Tutorial 4: Dynamic Min-Cut (n^0.12 Updates)</h3> \n <pre><code class=\"language-rust\">use ruvector_mincut::{DynamicMinCut, Graph};\n\n// Build graph\nlet mut graph = Graph::new(100);  // 100 nodes\ngraph.add_edge(0, 1, 10.0);\ngraph.add_edge(1, 2, 5.0);\ngraph.add_edge(0, 2, 15.0);\n\n// Compute initial min-cut\nlet mut mincut = DynamicMinCut::new(&amp;graph);\nlet (value, cut_edges) = mincut.compute();\nprintln!(\"Min-cut value: {}\", value);  // -&gt; 15.0\n\n// Dynamic update - subpolynomial time O(n^0.12)!\ngraph.update_edge(1, 2, 20.0);\nlet (new_value, _) = mincut.recompute();  // Much faster than recomputing from scratch\n</code></pre> \n <h3>Tutorial 5: 39 Attention Mechanisms</h3> \n <pre><code class=\"language-rust\">use ruvector_attention::{\n    Attention, FlashAttention, LinearAttention,\n    HyperbolicAttention, GraphAttention, MinCutGatedAttention\n};\n\n// FlashAttention - O(n) memory, fastest for long sequences\nlet flash = FlashAttention::new(512, 8);  // dim=512, heads=8\nlet output = flash.forward(&amp;query, &amp;key, &amp;value);\n\n// LinearAttention - O(n) time complexity\nlet linear = LinearAttention::new(512, 8);\n\n// HyperbolicAttention - for hierarchical data (Poincaré ball)\nlet hyper = HyperbolicAttention::new(512, 8, Curvature(-1.0));\n\n// GraphAttention - respects graph structure\nlet gat = GraphAttention::new(512, 8, &amp;adjacency_matrix);\n\n// MinCutGatedAttention - 50% compute reduction via sparsity\nlet mincut_gated = MinCutGatedAttention::new(512, 8, sparsity: 0.5);\nlet sparse_output = mincut_gated.forward(&amp;query, &amp;key, &amp;value);\n</code></pre> \n <h3>Tutorial 6: Spiking Neural Networks</h3> \n <pre><code class=\"language-javascript\">import { SpikingNetwork, HDCEncoder } from '@ruvector/spiking-neural';\n\n// High-Dimensional Computing encoder (10K-bit vectors)\nconst encoder = new HDCEncoder(10000);\nconst encoded = encoder.encode(\"hello world\");\n\n// Spiking network with BTSP learning\nconst network = new SpikingNetwork({\n  layers: [784, 256, 10],\n  learning: 'btsp',  // Behavioral Time-Scale Plasticity\n  threshold: 1.0\n});\n\n// Train with spike timing\nnetwork.train(spikes, labels, { epochs: 10 });\n\n// Inference\nconst output = network.forward(inputSpikes);\n</code></pre> \n <h3>Tutorial 7: Claude Code Hooks Integration</h3> \n <pre><code class=\"language-bash\"># 1. Initialize hooks\nnpx @ruvector/cli hooks init\n\n# 2. Install into Claude settings\nnpx @ruvector/cli hooks install\n\n# 3. Hooks now capture:\n#    - File edits (pre/post)\n#    - Commands (pre/post)\n#    - Sessions (start/end)\n#    - Errors and fixes\n\n# 4. Query learned patterns\nnpx @ruvector/cli hooks recall \"authentication error\"\n# -&gt; Returns similar past solutions\n\n# 5. Get AI routing suggestions\nnpx @ruvector/cli hooks route \"implement caching\"\n# -&gt; Suggests: rust-developer (confidence: 0.89)\n</code></pre> \n <h3>Tutorial 8: Edge Deployment with rvLite</h3> \n <pre><code class=\"language-javascript\">import { RvLite } from '@ruvector/rvlite';\n\n// Create persistent edge database (IndexedDB in browser)\nconst db = await RvLite.create({\n  path: 'my-vectors.db',\n  dimensions: 384\n});\n\n// Works offline - all computation local\nawait db.insert('doc1', embedding1, { title: 'Hello' });\nawait db.insert('doc2', embedding2, { title: 'World' });\n\n// Semantic search with metadata filtering\nconst results = await db.search(queryEmbedding, {\n  limit: 10,\n  filter: { title: { $contains: 'Hello' } }\n});\n\n// Sync when online\nawait db.sync('https://api.example.com/vectors');\n</code></pre> \n</details> \n<details> \n 🍕 WASM &amp; Utility Packages \n <table> \n  <thead> \n   <tr> \n    <th>Package</th> \n    <th>Description</th> \n    <th>Version</th> \n    <th>Downloads</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm\">@ruvector/wasm</a></td> \n    <td>WASM core vector DB</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn-wasm\">@ruvector/gnn-wasm</a></td> \n    <td>WASM GNN layers</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/gnn-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/gnn-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/gnn-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-wasm\">@ruvector/graph-wasm</a></td> \n    <td>WASM graph DB</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/graph-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/graph-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-wasm\">@ruvector/attention-wasm</a></td> \n    <td>WASM attention</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/attention-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/attention-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/attention-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer-wasm\">@ruvector/tiny-dancer-wasm</a></td> \n    <td>WASM AI routing</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/tiny-dancer-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/tiny-dancer-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/tiny-dancer-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router-wasm\">@ruvector/router-wasm</a></td> \n    <td>WASM semantic router</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router-wasm\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/router-wasm.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/router-wasm\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/router-wasm.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/postgres-cli\">@ruvector/postgres-cli</a></td> \n    <td>Postgres extension CLI</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/postgres-cli\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/postgres-cli.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/postgres-cli\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/postgres-cli.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\">@ruvector/agentic-synth</a></td> \n    <td>Synthetic data generator</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/agentic-synth.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-synth\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/agentic-synth.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-data-generator\">@ruvector/graph-data-generator</a></td> \n    <td>Graph data generation</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-data-generator\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/graph-data-generator.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/graph-data-generator\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/graph-data-generator.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-integration\">@ruvector/agentic-integration</a></td> \n    <td>Agentic workflows</td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-integration\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/@ruvector/agentic-integration.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/@ruvector/agentic-integration\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/@ruvector/agentic-integration.svg?sanitize=true\" /></a></td> \n   </tr> \n   <tr> \n    <td><a href=\"https://www.npmjs.com/package/rvlite\">rvlite</a></td> \n    <td>SQLite-style edge DB (SQL/SPARQL/Cypher)</td> \n    <td><a href=\"https://www.npmjs.com/package/rvlite\"><img alt=\"npm\" src=\"https://img.shields.io/npm/v/rvlite.svg?sanitize=true\" /></a></td> \n    <td><a href=\"https://www.npmjs.com/package/rvlite\"><img alt=\"downloads\" src=\"https://img.shields.io/npm/dt/rvlite.svg?sanitize=true\" /></a></td> \n   </tr> \n  </tbody> \n </table> \n <p><strong>Platform-specific native bindings</strong> (auto-detected):</p> \n <ul> \n  <li><code>@ruvector/node-linux-x64-gnu</code>, <code>@ruvector/node-linux-arm64-gnu</code>, <code>@ruvector/node-darwin-x64</code>, <code>@ruvector/node-darwin-arm64</code>, <code>@ruvector/node-win32-x64-msvc</code></li> \n  <li><code>@ruvector/gnn-linux-x64-gnu</code>, <code>@ruvector/gnn-linux-arm64-gnu</code>, <code>@ruvector/gnn-darwin-x64</code>, <code>@ruvector/gnn-darwin-arm64</code>, <code>@ruvector/gnn-win32-x64-msvc</code></li> \n  <li><code>@ruvector/tiny-dancer-linux-x64-gnu</code>, <code>@ruvector/tiny-dancer-linux-arm64-gnu</code>, <code>@ruvector/tiny-dancer-darwin-x64</code>, <code>@ruvector/tiny-dancer-darwin-arm64</code>, <code>@ruvector/tiny-dancer-win32-x64-msvc</code></li> \n  <li><code>@ruvector/router-linux-x64-gnu</code>, <code>@ruvector/router-linux-arm64-gnu</code>, <code>@ruvector/router-darwin-x64</code>, <code>@ruvector/router-darwin-arm64</code>, <code>@ruvector/router-win32-x64-msvc</code></li> \n  <li><code>@ruvector/attention-linux-x64-gnu</code>, <code>@ruvector/attention-linux-arm64-gnu</code>, <code>@ruvector/attention-darwin-x64</code>, <code>@ruvector/attention-darwin-arm64</code>, <code>@ruvector/attention-win32-x64-msvc</code></li> \n  <li><code>@ruvector/ruvllm-linux-x64-gnu</code>, <code>@ruvector/ruvllm-linux-arm64-gnu</code>, <code>@ruvector/ruvllm-darwin-x64</code>, <code>@ruvector/ruvllm-darwin-arm64</code>, <code>@ruvector/ruvllm-win32-x64-msvc</code></li> \n </ul> \n <p>See <a href=\"https://github.com/ruvnet/ruvector/issues/20\">GitHub Issue #20</a> for multi-platform npm package roadmap.</p> \n <pre><code class=\"language-bash\"># Install all-in-one package\nnpm install ruvector\n\n# Or install individual packages\nnpm install @ruvector/core @ruvector/gnn @ruvector/graph-node\n\n# List all available packages\nnpx ruvector install\n</code></pre> \n <pre><code class=\"language-javascript\">const ruvector = require('ruvector');\n\n// Vector search\nconst db = new ruvector.VectorDB(128);\ndb.insert('doc1', embedding1);\nconst results = db.search(queryEmbedding, 10);\n\n// Graph queries (Cypher)\ndb.execute(\"CREATE (a:Person {name: 'Alice'})-[:KNOWS]-&gt;(b:Person {name: 'Bob'})\");\ndb.execute(\"MATCH (p:Person)-[:KNOWS]-&gt;(friend) RETURN friend.name\");\n\n// GNN-enhanced search\nconst layer = new ruvector.GNNLayer(128, 256, 4);\nconst enhanced = layer.forward(query, neighbors, weights);\n\n// Compression (2-32x memory savings)\nconst compressed = ruvector.compress(embedding, 0.3);\n\n// Tiny Dancer: AI agent routing\nconst router = new ruvector.Router();\nconst decision = router.route(candidates, { optimize: 'cost' });\n</code></pre> \n</details> \n<details> \n 🦀 Rust Usage Examples \n <pre><code class=\"language-bash\">cargo add ruvector-graph ruvector-gnn\n</code></pre> \n <pre><code class=\"language-rust\">use ruvector_graph::{GraphDB, NodeBuilder};\nuse ruvector_gnn::{RuvectorLayer, differentiable_search};\n\nlet db = GraphDB::new();\n\nlet doc = NodeBuilder::new(\"doc1\")\n    .label(\"Document\")\n    .property(\"embedding\", vec![0.1, 0.2, 0.3])\n    .build();\ndb.create_node(doc)?;\n\n// GNN layer\nlet layer = RuvectorLayer::new(128, 256, 4, 0.1);\nlet enhanced = layer.forward(&amp;query, &amp;neighbors, &amp;weights);\n</code></pre> \n <pre><code class=\"language-rust\">use ruvector_raft::{RaftNode, RaftNodeConfig};\nuse ruvector_cluster::{ClusterManager, ConsistentHashRing};\nuse ruvector_replication::{SyncManager, SyncMode};\n\n// Configure a 5-node Raft cluster\nlet config = RaftNodeConfig {\n    node_id: \"node-1\".into(),\n    cluster_members: vec![\"node-1\", \"node-2\", \"node-3\", \"node-4\", \"node-5\"]\n        .into_iter().map(Into::into).collect(),\n    election_timeout_min: 150,  // ms\n    election_timeout_max: 300,  // ms\n    heartbeat_interval: 50,     // ms\n};\nlet raft = RaftNode::new(config);\n\n// Auto-sharding with consistent hashing (150 virtual nodes per real node)\nlet ring = ConsistentHashRing::new(64, 3); // 64 shards, replication factor 3\nlet shard = ring.get_shard(\"my-vector-key\");\n\n// Multi-master replication with conflict resolution\nlet sync = SyncManager::new(SyncMode::SemiSync { min_replicas: 2 });\n</code></pre> \n</details> \n<details> \n 🎓 RuvLLM Training &amp; RLM Fine-Tuning Tutorials  \n <h4>Hybrid Routing (90% Accuracy)</h4> \n <p>RuvLTRA achieves <strong>90% routing accuracy</strong> using a keyword-first strategy with embedding fallback:</p> \n <pre><code class=\"language-javascript\">// Optimal routing: Keywords first, embeddings as tiebreaker\nfunction routeTask(task, taskEmbedding, agentEmbeddings) {\n  const keywordScores = getKeywordScores(task);\n  const maxKw = Math.max(...Object.values(keywordScores));\n\n  if (maxKw &gt; 0) {\n    const candidates = Object.entries(keywordScores)\n      .filter(([_, score]) =&gt; score === maxKw)\n      .map(([agent]) =&gt; agent);\n\n    if (candidates.length === 1) return { agent: candidates[0] };\n    return pickByEmbedding(candidates, taskEmbedding, agentEmbeddings);\n  }\n\n  return embeddingSimilarity(taskEmbedding, agentEmbeddings);\n}\n</code></pre> \n <p>Run the benchmark: <code>node npm/packages/ruvllm/scripts/hybrid-model-compare.js</code></p> \n <h4>Generate Training Data</h4> \n <pre><code class=\"language-bash\"># Using CLI (recommended)\nnpx @ruvector/ruvllm train stats              # View dataset statistics\nnpx @ruvector/ruvllm train dataset            # Export training data\nnpx @ruvector/ruvllm train contrastive        # Run full training pipeline\n\n# With options\nnpx @ruvector/ruvllm train dataset --output ./my-training\nnpx @ruvector/ruvllm train contrastive --epochs 20 --batch-size 32 --lr 0.0001\n</code></pre> \n <p><strong>Programmatic API:</strong></p> \n <pre><code class=\"language-javascript\">import { ContrastiveTrainer, generateTrainingDataset, getDatasetStats } from '@ruvector/ruvllm';\n\nconst stats = getDatasetStats();\nconsole.log(`${stats.totalExamples} examples, ${stats.agentTypes} agent types`);\n\nconst trainer = new ContrastiveTrainer({ epochs: 10, margin: 0.5 });\ntrainer.addTriplet(anchor, anchorEmb, positive, positiveEmb, negative, negativeEmb, true);\nconst result = trainer.train();\ntrainer.exportTrainingData('./output');\n</code></pre> \n <h4>Fine-Tune with LoRA</h4> \n <pre><code class=\"language-bash\">pip install transformers peft datasets accelerate\n\npython -m peft.lora_train \\\n  --model_name Qwen/Qwen2.5-0.5B-Instruct \\\n  --dataset ./data/training/routing-examples.jsonl \\\n  --output_dir ./ruvltra-routing-lora \\\n  --lora_r 8 --lora_alpha 16 \\\n  --num_train_epochs 3 \\\n  --learning_rate 2e-4\n</code></pre> \n <h4>Convert to GGUF</h4> \n <pre><code class=\"language-bash\"># Merge LoRA weights\npython -c \"\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM\nbase = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-0.5B-Instruct')\nmodel = PeftModel.from_pretrained(base, './ruvltra-routing-lora')\nmodel.merge_and_unload().save_pretrained('./ruvltra-routing-merged')\n\"\n\n# Convert and quantize\npython llama.cpp/convert_hf_to_gguf.py ./ruvltra-routing-merged --outfile ruvltra-routing-f16.gguf\n./llama.cpp/llama-quantize ruvltra-routing-f16.gguf ruvltra-routing-q4_k_m.gguf Q4_K_M\n</code></pre> \n <h4>Contrastive Embedding Training</h4> \n <p><strong>Using RuvLLM CLI (recommended):</strong></p> \n <pre><code class=\"language-bash\"># Full contrastive training pipeline with triplet loss\nnpx @ruvector/ruvllm train contrastive --output ./training-output\n\n# Exports: triplets.jsonl, embeddings.json, lora_config.json, train.sh\n</code></pre> \n <p><strong>Using Python (for GPU training):</strong></p> \n <pre><code class=\"language-python\">from sentence_transformers import SentenceTransformer, losses, InputExample\nfrom torch.utils.data import DataLoader\n\ntrain_examples = [\n    InputExample(texts=[\"implement login\", \"build auth component\"], label=1.0),\n    InputExample(texts=[\"implement login\", \"write unit tests\"], label=0.0),\n]\n\nmodel = SentenceTransformer(\"Qwen/Qwen2.5-0.5B-Instruct\")\ntrain_loss = losses.CosineSimilarityLoss(model)\nmodel.fit([(DataLoader(train_examples, batch_size=16), train_loss)], epochs=5)\n</code></pre> \n <p><strong>Resources:</strong> <a href=\"https://github.com/ruvnet/ruvector/issues/122\">Issue #122</a> | <a href=\"https://arxiv.org/abs/2106.09685\">LoRA Paper</a> | <a href=\"https://www.sbert.net/docs/training/overview.html\">Sentence Transformers</a></p> \n <h4>Rust Training Module</h4> \n <p>For production-scale dataset generation, use the Rust training module (<a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/ruvllm/src/training/README.md\">full docs</a>):</p> \n <pre><code class=\"language-rust\">use ruvllm::training::{DatasetGenerator, DatasetConfig};\n\nlet config = DatasetConfig {\n    examples_per_category: 100,\n    enable_augmentation: true,\n    seed: 42,\n    ..Default::default()\n};\n\nlet dataset = DatasetGenerator::new(config).generate();\nlet (train, val, test) = dataset.split(0.7, 0.15, 0.15, 42);\ndataset.export_jsonl(\"training.jsonl\")?;\n</code></pre> \n <p><strong>Features:</strong></p> \n <ul> \n  <li><strong>5 agent categories</strong>: Coder, Researcher, Security, Architecture, Reviewer (20% each)</li> \n  <li><strong>Model routing</strong>: Haiku (simple) → Sonnet (moderate) → Opus (complex/security)</li> \n  <li><strong>Data augmentation</strong>: Paraphrasing, complexity variations, domain transfer</li> \n  <li><strong>8 technical domains</strong>: Web, Systems, DataScience, Mobile, DevOps, Security, Database, API</li> \n  <li><strong>Quality scores</strong>: 0.80-0.96 based on template quality and category</li> \n  <li><strong>Performance</strong>: ~10,000 examples/second, ~50 MB/s JSONL export</li> \n </ul> \n <pre><code class=\"language-bash\">cargo run --example generate_claude_dataset --release\n# Outputs: train.jsonl, val.jsonl, test.jsonl, stats.json\n</code></pre> \n</details> \n<hr /> \n<h2>Project</h2> \n<details> \n 📁 Project Structure \n <pre><code>crates/\n├── ruvector-core/           # Vector DB engine (HNSW, storage)\n├── ruvector-graph/          # Graph DB + Cypher parser + Hyperedges\n├── ruvector-gnn/            # GNN layers, compression, training\n├── ruvector-tiny-dancer-core/  # AI agent routing (FastGRNN)\n├── ruvector-*-wasm/         # WebAssembly bindings\n├── ruvector-*-node/         # Node.js bindings (napi-rs)\n└── rvf/                     # RVF Cognitive Containers (13 crates)\n    ├── rvf-types/           #   Segment types, headers (no_std)\n    ├── rvf-runtime/         #   Store API, COW engine, compaction\n    ├── rvf-kernel/          #   Linux kernel builder\n    ├── rvf-ebpf/            #   eBPF programs (XDP/TC/socket)\n    ├── rvf-launch/          #   QEMU microvm launcher\n    ├── rvf-cli/             #   CLI with 17 subcommands\n    └── ...                  #   wire, manifest, index, quant, crypto, server, import\n</code></pre> \n</details> \n<h2>Contributing</h2> \n<p>We welcome contributions! See <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/development/CONTRIBUTING.md\">CONTRIBUTING.md</a>.</p> \n<pre><code class=\"language-bash\"># Run tests\ncargo test --workspace\n\n# Run benchmarks\ncargo bench --workspace\n\n# Build WASM\ncargo build -p ruvector-gnn-wasm --target wasm32-unknown-unknown\n</code></pre> \n<h2>License</h2> \n<p>MIT License — free for commercial and personal use.</p> \n<hr /> \n<div align=\"center\"> \n <p><strong>Built by <a href=\"https://ruv.io\">rUv</a></strong> • <a href=\"https://github.com/ruvnet/ruvector\">GitHub</a> • <a href=\"https://npmjs.com/package/ruvector\">npm</a> • <a href=\"https://crates.io/crates/rvf-runtime\">crates.io</a> • <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/docs/\">Docs</a> • <a href=\"https://raw.githubusercontent.com/ruvnet/ruvector/main/crates/rvf/README.md\">RVF</a></p> \n <p><em>Vector search that gets smarter over time — now shipping as cognitive containers.</em></p> \n</div>",
        "source": "mshibanami.github.io",
        "published": "",
        "fetched_at": "2026-02-28T23:17:28.718157Z",
        "tags": [
          {
            "name": "transformation",
            "score": 16
          },
          {
            "name": "boundary_crossing",
            "score": 9
          },
          {
            "name": "visibility_gain",
            "score": 5
          },
          {
            "name": "value_redefinition",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 25
          },
          {
            "name": "ontology_shift",
            "score": 8
          }
        ],
        "structural_score": 67,
        "timeliness_score": 1,
        "final_score": 34.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://github.com/vxcontrol/pentagi",
        "title": "vxcontrol/pentagi",
        "summary": "<p>✨ Fully autonomous AI Agents system capable of performing complex penetration testing tasks</p><hr /><h1>PentAGI</h1> \n<div align=\"center\" style=\"font-size: 1.5em; margin: 20px 0;\"> \n <strong>P</strong>enetration testing \n <strong>A</strong>rtificial \n <strong>G</strong>eneral \n <strong>I</strong>ntelligence \n</div> \n<br /> \n<div align=\"center\"> \n <blockquote> \n  <p>🚀 <strong>Join the Community!</strong> Connect with security researchers, AI enthusiasts, and fellow ethical hackers. Get support, share insights, and stay updated with the latest PentAGI developments.</p> \n </blockquote> \n <p><a href=\"https://discord.gg/2xrMh7qX6m\"><img alt=\"Discord\" src=\"https://img.shields.io/badge/Discord-7289DA?logo=discord&amp;logoColor=white\" /></a>⠀<a href=\"https://t.me/+Ka9i6CNwe71hMWQy\"><img alt=\"Telegram\" src=\"https://img.shields.io/badge/Telegram-2CA5E0?logo=telegram&amp;logoColor=white\" /></a></p> \n <p><a href=\"https://trendshift.io/repositories/15161\" target=\"_blank\"><img alt=\"vxcontrol%2Fpentagi | Trendshift\" height=\"55\" src=\"https://trendshift.io/api/badge/repositories/15161\" style=\"width: 250px; height: 55px;\" width=\"250\" /></a></p> \n</div> \n<h2>📖 Table of Contents</h2> \n<ul> \n <li><a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/#-overview\">Overview</a></li> \n <li><a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/#-features\">Features</a></li> \n <li><a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/#-quick-start\">Quick Start</a></li> \n <li><a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/#-api-access\">API Access</a></li> \n <li><a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/#-advanced-setup\">Advanced Setup</a></li> \n <li><a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/#-development\">Development</a></li> \n <li><a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/#-testing-llm-agents\">Testing LLM Agents</a></li> \n <li><a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/#-embedding-configuration-and-testing\">Embedding Configuration and Testing</a></li> \n <li><a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/#-function-testing-with-ftester\">Function Testing with ftester</a></li> \n <li><a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/#%EF%B8%8F-building\">Building</a></li> \n <li><a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/#-credits\">Credits</a></li> \n <li><a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/#-license\">License</a></li> \n</ul> \n<h2>🎯 Overview</h2> \n<p>PentAGI is an innovative tool for automated security testing that leverages cutting-edge artificial intelligence technologies. The project is designed for information security professionals, researchers, and enthusiasts who need a powerful and flexible solution for conducting penetration tests.</p> \n<p>You can watch the video <strong>PentAGI overview</strong>: <a href=\"https://youtu.be/R70x5Ddzs1o\"><img alt=\"PentAGI Overview Video\" src=\"https://github.com/user-attachments/assets/0828dc3e-15f1-4a1d-858e-9696a146e478\" /></a></p> \n<h2>✨ Features</h2> \n<ul> \n <li>🛡️ Secure &amp; Isolated. All operations are performed in a sandboxed Docker environment with complete isolation.</li> \n <li>🤖 Fully Autonomous. AI-powered agent that automatically determines and executes penetration testing steps.</li> \n <li>🔬 Professional Pentesting Tools. Built-in suite of 20+ professional security tools including nmap, metasploit, sqlmap, and more.</li> \n <li>🧠 Smart Memory System. Long-term storage of research results and successful approaches for future use.</li> \n <li>📚 Knowledge Graph Integration. Graphiti-powered knowledge graph using Neo4j for semantic relationship tracking and advanced context understanding.</li> \n <li>🔍 Web Intelligence. Built-in browser via <a href=\"https://hub.docker.com/r/vxcontrol/scraper\">scraper</a> for gathering latest information from web sources.</li> \n <li>🔎 External Search Systems. Integration with advanced search APIs including <a href=\"https://tavily.com\">Tavily</a>, <a href=\"https://traversaal.ai\">Traversaal</a>, <a href=\"https://www.perplexity.ai\">Perplexity</a>, <a href=\"https://duckduckgo.com/\">DuckDuckGo</a>, <a href=\"https://programmablesearchengine.google.com/\">Google Custom Search</a>, <a href=\"https://sploitus.com\">Sploitus Search</a> and <a href=\"https://searxng.org\">Searxng</a> for comprehensive information gathering.</li> \n <li>👥 Team of Specialists. Delegation system with specialized AI agents for research, development, and infrastructure tasks.</li> \n <li>📊 Comprehensive Monitoring. Detailed logging and integration with Grafana/Prometheus for real-time system observation.</li> \n <li>📝 Detailed Reporting. Generation of thorough vulnerability reports with exploitation guides.</li> \n <li>📦 Smart Container Management. Automatic Docker image selection based on specific task requirements.</li> \n <li>📱 Modern Interface. Clean and intuitive web UI for system management and monitoring.</li> \n <li>🔌 Comprehensive APIs. Full-featured REST and GraphQL APIs with Bearer token authentication for automation and integration.</li> \n <li>💾 Persistent Storage. All commands and outputs are stored in PostgreSQL with <a href=\"https://hub.docker.com/r/vxcontrol/pgvector\">pgvector</a> extension.</li> \n <li>🎯 Scalable Architecture. Microservices-based design supporting horizontal scaling.</li> \n <li>🏠 Self-Hosted Solution. Complete control over your deployment and data.</li> \n <li>🔑 Flexible Authentication. Support for various LLM providers (<a href=\"https://platform.openai.com/\">OpenAI</a>, <a href=\"https://www.anthropic.com/\">Anthropic</a>, <a href=\"https://ollama.com/\">Ollama</a>, <a href=\"https://aws.amazon.com/bedrock/\">AWS Bedrock</a>, <a href=\"https://ai.google.dev/\">Google AI/Gemini</a>, <a href=\"https://deepinfra.com/\">Deep Infra</a>, <a href=\"https://openrouter.ai/\">OpenRouter</a>, <a href=\"https://www.deepseek.com/en\">DeepSeek</a>), <a href=\"https://platform.moonshot.ai/\">Moonshot</a> and custom configurations.</li> \n <li>🔐 API Token Authentication. Secure Bearer token system for programmatic access to REST and GraphQL APIs.</li> \n <li>⚡ Quick Deployment. Easy setup through <a href=\"https://docs.docker.com/compose/\">Docker Compose</a> with comprehensive environment configuration.</li> \n</ul> \n<h2>🏗️ Architecture</h2> \n<h3>System Context</h3> \n<pre><code class=\"language-mermaid\">flowchart TB\n    classDef person fill:#08427B,stroke:#073B6F,color:#fff\n    classDef system fill:#1168BD,stroke:#0B4884,color:#fff\n    classDef external fill:#666666,stroke:#0B4884,color:#fff\n\n    pentester[\"👤 Security Engineer\n    (User of the system)\"]\n\n    pentagi[\"✨ PentAGI\n    (Autonomous penetration testing system)\"]\n\n    target[\"🎯 target-system\n    (System under test)\"]\n    llm[\"🧠 llm-provider\n    (OpenAI/Anthropic/Ollama/Bedrock/Gemini/Custom)\"]\n    search[\"🔍 search-systems\n    (Google/DuckDuckGo/Tavily/Traversaal/Perplexity/Sploitus/Searxng)\"]\n    langfuse[\"📊 langfuse-ui\n    (LLM Observability Dashboard)\"]\n    grafana[\"📈 grafana\n    (System Monitoring Dashboard)\"]\n\n    pentester --&gt; |Uses HTTPS| pentagi\n    pentester --&gt; |Monitors AI HTTPS| langfuse\n    pentester --&gt; |Monitors System HTTPS| grafana\n    pentagi --&gt; |Tests Various protocols| target\n    pentagi --&gt; |Queries HTTPS| llm\n    pentagi --&gt; |Searches HTTPS| search\n    pentagi --&gt; |Reports HTTPS| langfuse\n    pentagi --&gt; |Reports HTTPS| grafana\n\n    class pentester person\n    class pentagi system\n    class target,llm,search,langfuse,grafana external\n\n    linkStyle default stroke:#ffffff,color:#ffffff\n</code></pre> \n<details> \n <b>🔄 Container Architecture</b> (click to expand) \n <pre><code class=\"language-mermaid\">graph TB\n    subgraph Core Services\n        UI[Frontend UI&lt;br/&gt;React + TypeScript]\n        API[Backend API&lt;br/&gt;Go + GraphQL]\n        DB[(Vector Store&lt;br/&gt;PostgreSQL + pgvector)]\n        MQ[Task Queue&lt;br/&gt;Async Processing]\n        Agent[AI Agents&lt;br/&gt;Multi-Agent System]\n    end\n\n    subgraph Knowledge Graph\n        Graphiti[Graphiti&lt;br/&gt;Knowledge Graph API]\n        Neo4j[(Neo4j&lt;br/&gt;Graph Database)]\n    end\n\n    subgraph Monitoring\n        Grafana[Grafana&lt;br/&gt;Dashboards]\n        VictoriaMetrics[VictoriaMetrics&lt;br/&gt;Time-series DB]\n        Jaeger[Jaeger&lt;br/&gt;Distributed Tracing]\n        Loki[Loki&lt;br/&gt;Log Aggregation]\n        OTEL[OpenTelemetry&lt;br/&gt;Data Collection]\n    end\n\n    subgraph Analytics\n        Langfuse[Langfuse&lt;br/&gt;LLM Analytics]\n        ClickHouse[ClickHouse&lt;br/&gt;Analytics DB]\n        Redis[Redis&lt;br/&gt;Cache + Rate Limiter]\n        MinIO[MinIO&lt;br/&gt;S3 Storage]\n    end\n\n    subgraph Security Tools\n        Scraper[Web Scraper&lt;br/&gt;Isolated Browser]\n        PenTest[Security Tools&lt;br/&gt;20+ Pro Tools&lt;br/&gt;Sandboxed Execution]\n    end\n\n    UI --&gt; |HTTP/WS| API\n    API --&gt; |SQL| DB\n    API --&gt; |Events| MQ\n    MQ --&gt; |Tasks| Agent\n    Agent --&gt; |Commands| PenTest\n    Agent --&gt; |Queries| DB\n    Agent --&gt; |Knowledge| Graphiti\n    Graphiti --&gt; |Graph| Neo4j\n\n    API --&gt; |Telemetry| OTEL\n    OTEL --&gt; |Metrics| VictoriaMetrics\n    OTEL --&gt; |Traces| Jaeger\n    OTEL --&gt; |Logs| Loki\n\n    Grafana --&gt; |Query| VictoriaMetrics\n    Grafana --&gt; |Query| Jaeger\n    Grafana --&gt; |Query| Loki\n\n    API --&gt; |Analytics| Langfuse\n    Langfuse --&gt; |Store| ClickHouse\n    Langfuse --&gt; |Cache| Redis\n    Langfuse --&gt; |Files| MinIO\n\n    classDef core fill:#f9f,stroke:#333,stroke-width:2px,color:#000\n    classDef knowledge fill:#ffa,stroke:#333,stroke-width:2px,color:#000\n    classDef monitoring fill:#bbf,stroke:#333,stroke-width:2px,color:#000\n    classDef analytics fill:#bfb,stroke:#333,stroke-width:2px,color:#000\n    classDef tools fill:#fbb,stroke:#333,stroke-width:2px,color:#000\n\n    class UI,API,DB,MQ,Agent core\n    class Graphiti,Neo4j knowledge\n    class Grafana,VictoriaMetrics,Jaeger,Loki,OTEL monitoring\n    class Langfuse,ClickHouse,Redis,MinIO analytics\n    class Scraper,PenTest tools\n</code></pre> \n</details> \n<details> \n <b>📊 Entity Relationship</b> (click to expand) \n <pre><code class=\"language-mermaid\">erDiagram\n    Flow ||--o{ Task : contains\n    Task ||--o{ SubTask : contains\n    SubTask ||--o{ Action : contains\n    Action ||--o{ Artifact : produces\n    Action ||--o{ Memory : stores\n\n    Flow {\n        string id PK\n        string name \"Flow name\"\n        string description \"Flow description\"\n        string status \"active/completed/failed\"\n        json parameters \"Flow parameters\"\n        timestamp created_at\n        timestamp updated_at\n    }\n\n    Task {\n        string id PK\n        string flow_id FK\n        string name \"Task name\"\n        string description \"Task description\"\n        string status \"pending/running/done/failed\"\n        json result \"Task results\"\n        timestamp created_at\n        timestamp updated_at\n    }\n\n    SubTask {\n        string id PK\n        string task_id FK\n        string name \"Subtask name\"\n        string description \"Subtask description\"\n        string status \"queued/running/completed/failed\"\n        string agent_type \"researcher/developer/executor\"\n        json context \"Agent context\"\n        timestamp created_at\n        timestamp updated_at\n    }\n\n    Action {\n        string id PK\n        string subtask_id FK\n        string type \"command/search/analyze/etc\"\n        string status \"success/failure\"\n        json parameters \"Action parameters\"\n        json result \"Action results\"\n        timestamp created_at\n    }\n\n    Artifact {\n        string id PK\n        string action_id FK\n        string type \"file/report/log\"\n        string path \"Storage path\"\n        json metadata \"Additional info\"\n        timestamp created_at\n    }\n\n    Memory {\n        string id PK\n        string action_id FK\n        string type \"observation/conclusion\"\n        vector embedding \"Vector representation\"\n        text content \"Memory content\"\n        timestamp created_at\n    }\n</code></pre> \n</details> \n<details> \n <b>🤖 Agent Interaction</b> (click to expand) \n <pre><code class=\"language-mermaid\">sequenceDiagram\n    participant O as Orchestrator\n    participant R as Researcher\n    participant D as Developer\n    participant E as Executor\n    participant VS as Vector Store\n    participant KB as Knowledge Base\n\n    Note over O,KB: Flow Initialization\n    O-&gt;&gt;VS: Query similar tasks\n    VS--&gt;&gt;O: Return experiences\n    O-&gt;&gt;KB: Load relevant knowledge\n    KB--&gt;&gt;O: Return context\n\n    Note over O,R: Research Phase\n    O-&gt;&gt;R: Analyze target\n    R-&gt;&gt;VS: Search similar cases\n    VS--&gt;&gt;R: Return patterns\n    R-&gt;&gt;KB: Query vulnerabilities\n    KB--&gt;&gt;R: Return known issues\n    R-&gt;&gt;VS: Store findings\n    R--&gt;&gt;O: Research results\n\n    Note over O,D: Planning Phase\n    O-&gt;&gt;D: Plan attack\n    D-&gt;&gt;VS: Query exploits\n    VS--&gt;&gt;D: Return techniques\n    D-&gt;&gt;KB: Load tools info\n    KB--&gt;&gt;D: Return capabilities\n    D--&gt;&gt;O: Attack plan\n\n    Note over O,E: Execution Phase\n    O-&gt;&gt;E: Execute plan\n    E-&gt;&gt;KB: Load tool guides\n    KB--&gt;&gt;E: Return procedures\n    E-&gt;&gt;VS: Store results\n    E--&gt;&gt;O: Execution status\n</code></pre> \n</details> \n<details> \n <b>🧠 Memory System</b> (click to expand) \n <pre><code class=\"language-mermaid\">graph TB\n    subgraph \"Long-term Memory\"\n        VS[(Vector Store&lt;br/&gt;Embeddings DB)]\n        KB[Knowledge Base&lt;br/&gt;Domain Expertise]\n        Tools[Tools Knowledge&lt;br/&gt;Usage Patterns]\n    end\n\n    subgraph \"Working Memory\"\n        Context[Current Context&lt;br/&gt;Task State]\n        Goals[Active Goals&lt;br/&gt;Objectives]\n        State[System State&lt;br/&gt;Resources]\n    end\n\n    subgraph \"Episodic Memory\"\n        Actions[Past Actions&lt;br/&gt;Commands History]\n        Results[Action Results&lt;br/&gt;Outcomes]\n        Patterns[Success Patterns&lt;br/&gt;Best Practices]\n    end\n\n    Context --&gt; |Query| VS\n    VS --&gt; |Retrieve| Context\n\n    Goals --&gt; |Consult| KB\n    KB --&gt; |Guide| Goals\n\n    State --&gt; |Record| Actions\n    Actions --&gt; |Learn| Patterns\n    Patterns --&gt; |Store| VS\n\n    Tools --&gt; |Inform| State\n    Results --&gt; |Update| Tools\n\n    VS --&gt; |Enhance| KB\n    KB --&gt; |Index| VS\n\n    classDef ltm fill:#f9f,stroke:#333,stroke-width:2px,color:#000\n    classDef wm fill:#bbf,stroke:#333,stroke-width:2px,color:#000\n    classDef em fill:#bfb,stroke:#333,stroke-width:2px,color:#000\n\n    class VS,KB,Tools ltm\n    class Context,Goals,State wm\n    class Actions,Results,Patterns em\n</code></pre> \n</details> \n<details> \n <b>🔄 Chain Summarization</b> (click to expand) \n <p>The chain summarization system manages conversation context growth by selectively summarizing older messages. This is critical for preventing token limits from being exceeded while maintaining conversation coherence.</p> \n <pre><code class=\"language-mermaid\">flowchart TD\n    A[Input Chain] --&gt; B{Needs Summarization?}\n    B --&gt;|No| C[Return Original Chain]\n    B --&gt;|Yes| D[Convert to ChainAST]\n    D --&gt; E[Apply Section Summarization]\n    E --&gt; F[Process Oversized Pairs]\n    F --&gt; G[Manage Last Section Size]\n    G --&gt; H[Apply QA Summarization]\n    H --&gt; I[Rebuild Chain with Summaries]\n    I --&gt; J{Is New Chain Smaller?}\n    J --&gt;|Yes| K[Return Optimized Chain]\n    J --&gt;|No| C\n\n    classDef process fill:#bbf,stroke:#333,stroke-width:2px,color:#000\n    classDef decision fill:#bfb,stroke:#333,stroke-width:2px,color:#000\n    classDef output fill:#fbb,stroke:#333,stroke-width:2px,color:#000\n\n    class A,D,E,F,G,H,I process\n    class B,J decision\n    class C,K output\n</code></pre> \n <p>The algorithm operates on a structured representation of conversation chains (ChainAST) that preserves message types including tool calls and their responses. All summarization operations maintain critical conversation flow while reducing context size.</p> \n <h3>Global Summarizer Configuration Options</h3> \n <table> \n  <thead> \n   <tr> \n    <th>Parameter</th> \n    <th>Environment Variable</th> \n    <th>Default</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Preserve Last</td> \n    <td><code>SUMMARIZER_PRESERVE_LAST</code></td> \n    <td><code>true</code></td> \n    <td>Whether to keep all messages in the last section intact</td> \n   </tr> \n   <tr> \n    <td>Use QA Pairs</td> \n    <td><code>SUMMARIZER_USE_QA</code></td> \n    <td><code>true</code></td> \n    <td>Whether to use QA pair summarization strategy</td> \n   </tr> \n   <tr> \n    <td>Summarize Human in QA</td> \n    <td><code>SUMMARIZER_SUM_MSG_HUMAN_IN_QA</code></td> \n    <td><code>false</code></td> \n    <td>Whether to summarize human messages in QA pairs</td> \n   </tr> \n   <tr> \n    <td>Last Section Size</td> \n    <td><code>SUMMARIZER_LAST_SEC_BYTES</code></td> \n    <td><code>51200</code></td> \n    <td>Maximum byte size for last section (50KB)</td> \n   </tr> \n   <tr> \n    <td>Max Body Pair Size</td> \n    <td><code>SUMMARIZER_MAX_BP_BYTES</code></td> \n    <td><code>16384</code></td> \n    <td>Maximum byte size for a single body pair (16KB)</td> \n   </tr> \n   <tr> \n    <td>Max QA Sections</td> \n    <td><code>SUMMARIZER_MAX_QA_SECTIONS</code></td> \n    <td><code>10</code></td> \n    <td>Maximum QA pair sections to preserve</td> \n   </tr> \n   <tr> \n    <td>Max QA Size</td> \n    <td><code>SUMMARIZER_MAX_QA_BYTES</code></td> \n    <td><code>65536</code></td> \n    <td>Maximum byte size for QA pair sections (64KB)</td> \n   </tr> \n   <tr> \n    <td>Keep QA Sections</td> \n    <td><code>SUMMARIZER_KEEP_QA_SECTIONS</code></td> \n    <td><code>1</code></td> \n    <td>Number of recent QA sections to keep without summarization</td> \n   </tr> \n  </tbody> \n </table> \n <h3>Assistant Summarizer Configuration Options</h3> \n <p>Assistant instances can use customized summarization settings to fine-tune context management behavior:</p> \n <table> \n  <thead> \n   <tr> \n    <th>Parameter</th> \n    <th>Environment Variable</th> \n    <th>Default</th> \n    <th>Description</th> \n   </tr> \n  </thead> \n  <tbody> \n   <tr> \n    <td>Preserve Last</td> \n    <td><code>ASSISTANT_SUMMARIZER_PRESERVE_LAST</code></td> \n    <td><code>true</code></td> \n    <td>Whether to preserve all messages in the assistant's last section</td> \n   </tr> \n   <tr> \n    <td>Last Section Size</td> \n    <td><code>ASSISTANT_SUMMARIZER_LAST_SEC_BYTES</code></td> \n    <td><code>76800</code></td> \n    <td>Maximum byte size for assistant's last section (75KB)</td> \n   </tr> \n   <tr> \n    <td>Max Body Pair Size</td> \n    <td><code>ASSISTANT_SUMMARIZER_MAX_BP_BYTES</code></td> \n    <td><code>16384</code></td> \n    <td>Maximum byte size for a single body pair in assistant context (16KB)</td> \n   </tr> \n   <tr> \n    <td>Max QA Sections</td> \n    <td><code>ASSISTANT_SUMMARIZER_MAX_QA_SECTIONS</code></td> \n    <td><code>7</code></td> \n    <td>Maximum QA sections to preserve in assistant context</td> \n   </tr> \n   <tr> \n    <td>Max QA Size</td> \n    <td><code>ASSISTANT_SUMMARIZER_MAX_QA_BYTES</code></td> \n    <td><code>76800</code></td> \n    <td>Maximum byte size for assistant's QA sections (75KB)</td> \n   </tr> \n   <tr> \n    <td>Keep QA Sections</td> \n    <td><code>ASSISTANT_SUMMARIZER_KEEP_QA_SECTIONS</code></td> \n    <td><code>3</code></td> \n    <td>Number of recent QA sections to preserve without summarization</td> \n   </tr> \n  </tbody> \n </table> \n <p>The assistant summarizer configuration provides more memory for context retention compared to the global settings, preserving more recent conversation history while still ensuring efficient token usage.</p> \n <h3>Summarizer Environment Configuration</h3> \n <pre><code class=\"language-bash\"># Default values for global summarizer logic\nSUMMARIZER_PRESERVE_LAST=true\nSUMMARIZER_USE_QA=true\nSUMMARIZER_SUM_MSG_HUMAN_IN_QA=false\nSUMMARIZER_LAST_SEC_BYTES=51200\nSUMMARIZER_MAX_BP_BYTES=16384\nSUMMARIZER_MAX_QA_SECTIONS=10\nSUMMARIZER_MAX_QA_BYTES=65536\nSUMMARIZER_KEEP_QA_SECTIONS=1\n\n# Default values for assistant summarizer logic\nASSISTANT_SUMMARIZER_PRESERVE_LAST=true\nASSISTANT_SUMMARIZER_LAST_SEC_BYTES=76800\nASSISTANT_SUMMARIZER_MAX_BP_BYTES=16384\nASSISTANT_SUMMARIZER_MAX_QA_SECTIONS=7\nASSISTANT_SUMMARIZER_MAX_QA_BYTES=76800\nASSISTANT_SUMMARIZER_KEEP_QA_SECTIONS=3\n</code></pre> \n</details> \n<p>The architecture of PentAGI is designed to be modular, scalable, and secure. Here are the key components:</p> \n<ol> \n <li> <p><strong>Core Services</strong></p> \n  <ul> \n   <li>Frontend UI: React-based web interface with TypeScript for type safety</li> \n   <li>Backend API: Go-based REST and GraphQL APIs with Bearer token authentication for programmatic access</li> \n   <li>Vector Store: PostgreSQL with pgvector for semantic search and memory storage</li> \n   <li>Task Queue: Async task processing system for reliable operation</li> \n   <li>AI Agent: Multi-agent system with specialized roles for efficient testing</li> \n  </ul> </li> \n <li> <p><strong>Knowledge Graph</strong></p> \n  <ul> \n   <li>Graphiti: Knowledge graph API for semantic relationship tracking and contextual understanding</li> \n   <li>Neo4j: Graph database for storing and querying relationships between entities, actions, and outcomes</li> \n   <li>Automatic capturing of agent responses and tool executions for building comprehensive knowledge base</li> \n  </ul> </li> \n <li> <p><strong>Monitoring Stack</strong></p> \n  <ul> \n   <li>OpenTelemetry: Unified observability data collection and correlation</li> \n   <li>Grafana: Real-time visualization and alerting dashboards</li> \n   <li>VictoriaMetrics: High-performance time-series metrics storage</li> \n   <li>Jaeger: End-to-end distributed tracing for debugging</li> \n   <li>Loki: Scalable log aggregation and analysis</li> \n  </ul> </li> \n <li> <p><strong>Analytics Platform</strong></p> \n  <ul> \n   <li>Langfuse: Advanced LLM observability and performance analytics</li> \n   <li>ClickHouse: Column-oriented analytics data warehouse</li> \n   <li>Redis: High-speed caching and rate limiting</li> \n   <li>MinIO: S3-compatible object storage for artifacts</li> \n  </ul> </li> \n <li> <p><strong>Security Tools</strong></p> \n  <ul> \n   <li>Web Scraper: Isolated browser environment for safe web interaction</li> \n   <li>Pentesting Tools: Comprehensive suite of 20+ professional security tools</li> \n   <li>Sandboxed Execution: All operations run in isolated containers</li> \n  </ul> </li> \n <li> <p><strong>Memory Systems</strong></p> \n  <ul> \n   <li>Long-term Memory: Persistent storage of knowledge and experiences</li> \n   <li>Working Memory: Active context and goals for current operations</li> \n   <li>Episodic Memory: Historical actions and success patterns</li> \n   <li>Knowledge Base: Structured domain expertise and tool capabilities</li> \n   <li>Context Management: Intelligently manages growing LLM context windows using chain summarization</li> \n  </ul> </li> \n</ol> \n<p>The system uses Docker containers for isolation and easy deployment, with separate networks for core services, monitoring, and analytics to ensure proper security boundaries. Each component is designed to scale horizontally and can be configured for high availability in production environments.</p> \n<h2>🚀 Quick Start</h2> \n<h3>System Requirements</h3> \n<ul> \n <li>Docker and Docker Compose (or Podman - see <a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/#running-pentagi-with-podman\">Podman configuration</a>)</li> \n <li>Minimum 2 vCPU</li> \n <li>Minimum 4GB RAM</li> \n <li>20GB free disk space</li> \n <li>Internet access for downloading images and updates</li> \n</ul> \n<h3>Using Installer (Recommended)</h3> \n<p>PentAGI provides an interactive installer with a terminal-based UI for streamlined configuration and deployment. The installer guides you through system checks, LLM provider setup, search engine configuration, and security hardening.</p> \n<p><strong>Supported Platforms:</strong></p> \n<ul> \n <li><strong>Linux</strong>: amd64 <a href=\"https://pentagi.com/downloads/linux/amd64/installer-latest.zip\">download</a> | arm64 <a href=\"https://pentagi.com/downloads/linux/arm64/installer-latest.zip\">download</a></li> \n <li><strong>Windows</strong>: amd64 <a href=\"https://pentagi.com/downloads/windows/amd64/installer-latest.zip\">download</a></li> \n <li><strong>macOS</strong>: amd64 (Intel) <a href=\"https://pentagi.com/downloads/darwin/amd64/installer-latest.zip\">download</a> | arm64 (M-series) <a href=\"https://pentagi.com/downloads/darwin/arm64/installer-latest.zip\">download</a></li> \n</ul> \n<p><strong>Quick Installation (Linux amd64):</strong></p> \n<pre><code class=\"language-bash\"># Create installation directory\nmkdir -p pentagi &amp;&amp; cd pentagi\n\n# Download installer\nwget -O installer.zip https://pentagi.com/downloads/linux/amd64/installer-latest.zip\n\n# Extract\nunzip installer.zip\n\n# Run interactive installer\n./installer\n</code></pre> \n<p><strong>Prerequisites &amp; Permissions:</strong></p> \n<p>The installer requires appropriate privileges to interact with the Docker API for proper operation. By default, it uses the Docker socket (<code>/var/run/docker.sock</code>) which requires either:</p> \n<ul> \n <li> <p><strong>Option 1 (Recommended for production):</strong> Run the installer as root:</p> <pre><code class=\"language-bash\">sudo ./installer\n</code></pre> </li> \n <li> <p><strong>Option 2 (Development environments):</strong> Grant your user access to the Docker socket by adding them to the <code>docker</code> group:</p> <pre><code class=\"language-bash\"># Add your user to the docker group\nsudo usermod -aG docker $USER\n\n# Log out and log back in, or activate the group immediately\nnewgrp docker\n\n# Verify Docker access (should run without sudo)\ndocker ps\n</code></pre> <p>⚠️ <strong>Security Note:</strong> Adding a user to the <code>docker</code> group grants root-equivalent privileges. Only do this for trusted users in controlled environments. For production deployments, consider using rootless Docker mode or running the installer with sudo.</p> </li> \n</ul> \n<p>The installer will:</p> \n<ol> \n <li><strong>System Checks</strong>: Verify Docker, network connectivity, and system requirements</li> \n <li><strong>Environment Setup</strong>: Create and configure <code>.env</code> file with optimal defaults</li> \n <li><strong>Provider Configuration</strong>: Set up LLM providers (OpenAI, Anthropic, Gemini, Bedrock, Ollama, Custom)</li> \n <li><strong>Search Engines</strong>: Configure DuckDuckGo, Google, Tavily, Traversaal, Perplexity, Sploitus, Searxng</li> \n <li><strong>Security Hardening</strong>: Generate secure credentials and configure SSL certificates</li> \n <li><strong>Deployment</strong>: Start PentAGI with docker-compose</li> \n</ol> \n<p><strong>For Production &amp; Enhanced Security:</strong></p> \n<p>For production deployments or security-sensitive environments, we <strong>strongly recommend</strong> using a distributed two-node architecture where worker operations are isolated on a separate server. This prevents untrusted code execution and network access issues on your main system.</p> \n<p>👉 <strong>See detailed guide</strong>: <a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/examples/guides/worker_node.md\">Worker Node Setup</a></p> \n<p>The two-node setup provides:</p> \n<ul> \n <li><strong>Isolated Execution</strong>: Worker containers run on dedicated hardware</li> \n <li><strong>Network Isolation</strong>: Separate network boundaries for penetration testing</li> \n <li><strong>Security Boundaries</strong>: Docker-in-Docker with TLS authentication</li> \n <li><strong>OOB Attack Support</strong>: Dedicated port ranges for out-of-band techniques</li> \n</ul> \n<h3>Manual Installation</h3> \n<ol> \n <li>Create a working directory or clone the repository:</li> \n</ol> \n<pre><code class=\"language-bash\">mkdir pentagi &amp;&amp; cd pentagi\n</code></pre> \n<ol start=\"2\"> \n <li>Copy <code>.env.example</code> to <code>.env</code> or download it:</li> \n</ol> \n<pre><code class=\"language-bash\">curl -o .env https://raw.githubusercontent.com/vxcontrol/pentagi/master/.env.example\n</code></pre> \n<ol start=\"3\"> \n <li>Touch examples files (<code>example.custom.provider.yml</code>, <code>example.ollama.provider.yml</code>) or download it:</li> \n</ol> \n<pre><code class=\"language-bash\">curl -o example.custom.provider.yml https://raw.githubusercontent.com/vxcontrol/pentagi/master/examples/configs/custom-openai.provider.yml\ncurl -o example.ollama.provider.yml https://raw.githubusercontent.com/vxcontrol/pentagi/master/examples/configs/ollama-llama318b.provider.yml\n</code></pre> \n<ol start=\"4\"> \n <li>Fill in the required API keys in <code>.env</code> file.</li> \n</ol> \n<pre><code class=\"language-bash\"># Required: At least one of these LLM providers\nOPEN_AI_KEY=your_openai_key\nANTHROPIC_API_KEY=your_anthropic_key\nGEMINI_API_KEY=your_gemini_key\n\n# Optional: AWS Bedrock provider (enterprise-grade models)\nBEDROCK_REGION=us-east-1\nBEDROCK_ACCESS_KEY_ID=your_aws_access_key\nBEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key\n\n# Optional: Local LLM provider (zero-cost inference)\nOLLAMA_SERVER_URL=http://localhost:11434\nOLLAMA_SERVER_MODEL=your_model_name\n\n# Optional: Additional search capabilities\nDUCKDUCKGO_ENABLED=true\nSPLOITUS_ENABLED=true\nGOOGLE_API_KEY=your_google_key\nGOOGLE_CX_KEY=your_google_cx\nTAVILY_API_KEY=your_tavily_key\nTRAVERSAAL_API_KEY=your_traversaal_key\nPERPLEXITY_API_KEY=your_perplexity_key\nPERPLEXITY_MODEL=sonar-pro\nPERPLEXITY_CONTEXT_SIZE=medium\n\n# Searxng meta search engine (aggregates results from multiple sources)\nSEARXNG_URL=http://your-searxng-instance:8080\nSEARXNG_CATEGORIES=general\nSEARXNG_LANGUAGE=\nSEARXNG_SAFESEARCH=0\nSEARXNG_TIME_RANGE=\n\n## Graphiti knowledge graph settings\nGRAPHITI_ENABLED=true\nGRAPHITI_TIMEOUT=30\nGRAPHITI_URL=http://graphiti:8000\nGRAPHITI_MODEL_NAME=gpt-5-mini\n\n# Neo4j settings (used by Graphiti stack)\nNEO4J_USER=neo4j\nNEO4J_DATABASE=neo4j\nNEO4J_PASSWORD=devpassword\nNEO4J_URI=bolt://neo4j:7687\n\n# Assistant configuration\nASSISTANT_USE_AGENTS=false         # Default value for agent usage when creating new assistants\n</code></pre> \n<ol start=\"5\"> \n <li>Change all security related environment variables in <code>.env</code> file to improve security.</li> \n</ol> \n<details> \n Security related environment variables \n <h3>Main Security Settings</h3> \n <ul> \n  <li><code>COOKIE_SIGNING_SALT</code> - Salt for cookie signing, change to random value</li> \n  <li><code>PUBLIC_URL</code> - Public URL of your server (eg. <code>https://pentagi.example.com</code>)</li> \n  <li><code>SERVER_SSL_CRT</code> and <code>SERVER_SSL_KEY</code> - Custom paths to your existing SSL certificate and key for HTTPS (these paths should be used in the docker-compose.yml file to mount as volumes)</li> \n </ul> \n <h3>Scraper Access</h3> \n <ul> \n  <li><code>SCRAPER_PUBLIC_URL</code> - Public URL for scraper if you want to use different scraper server for public URLs</li> \n  <li><code>SCRAPER_PRIVATE_URL</code> - Private URL for scraper (local scraper server in docker-compose.yml file to access it to local URLs)</li> \n </ul> \n <h3>Access Credentials</h3> \n <ul> \n  <li><code>PENTAGI_POSTGRES_USER</code> and <code>PENTAGI_POSTGRES_PASSWORD</code> - PostgreSQL credentials</li> \n  <li><code>NEO4J_USER</code> and <code>NEO4J_PASSWORD</code> - Neo4j credentials (for Graphiti knowledge graph)</li> \n </ul> \n</details> \n<ol start=\"6\"> \n <li>Remove all inline comments from <code>.env</code> file if you want to use it in VSCode or other IDEs as a envFile option:</li> \n</ol> \n<pre><code class=\"language-bash\">perl -i -pe 's/\\s+#.*$//' .env\n</code></pre> \n<ol start=\"7\"> \n <li>Run the PentAGI stack:</li> \n</ol> \n<pre><code class=\"language-bash\">curl -O https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose.yml\ndocker compose up -d\n</code></pre> \n<p>Visit <a href=\"https://localhost:8443\">localhost:8443</a> to access PentAGI Web UI (default is <code>admin@pentagi.com</code> / <code>admin</code>)</p> \n<blockquote> \n <p>[!NOTE] If you caught an error about <code>pentagi-network</code> or <code>observability-network</code> or <code>langfuse-network</code> you need to run <code>docker-compose.yml</code> firstly to create these networks and after that run <code>docker-compose-langfuse.yml</code>, <code>docker-compose-graphiti.yml</code>, and <code>docker-compose-observability.yml</code> to use Langfuse, Graphiti, and Observability services.</p> \n <p>You have to set at least one Language Model provider (OpenAI, Anthropic, Gemini, AWS Bedrock, or Ollama) to use PentAGI. AWS Bedrock provides enterprise-grade access to multiple foundation models from leading AI companies, while Ollama provides zero-cost local inference if you have sufficient computational resources. Additional API keys for search engines are optional but recommended for better results.</p> \n <p><code>LLM_SERVER_*</code> environment variables are experimental feature and will be changed in the future. Right now you can use them to specify custom LLM server URL and one model for all agent types.</p> \n <p><code>PROXY_URL</code> is a global proxy URL for all LLM providers and external search systems. You can use it for isolation from external networks.</p> \n <p>The <code>docker-compose.yml</code> file runs the PentAGI service as root user because it needs access to docker.sock for container management. If you're using TCP/IP network connection to Docker instead of socket file, you can remove root privileges and use the default <code>pentagi</code> user for better security.</p> \n</blockquote> \n<h3>Accessing PentAGI from External Networks</h3> \n<p>By default, PentAGI binds to <code>127.0.0.1</code> (localhost only) for security. To access PentAGI from other machines on your network, you need to configure external access.</p> \n<h4>Configuration Steps</h4> \n<ol> \n <li><strong>Update <code>.env</code> file</strong> with your server's IP address:</li> \n</ol> \n<pre><code class=\"language-bash\"># Network binding - allow external connections\nPENTAGI_LISTEN_IP=0.0.0.0\nPENTAGI_LISTEN_PORT=8443\n\n# Public URL - use your actual server IP or hostname\n# Replace 192.168.1.100 with your server's IP address\nPUBLIC_URL=https://192.168.1.100:8443\n\n# CORS origins - list all URLs that will access PentAGI\n# Include localhost for local access AND your server IP for external access\nCORS_ORIGINS=https://localhost:8443,https://192.168.1.100:8443\n</code></pre> \n<blockquote> \n <p>[!IMPORTANT]</p> \n <ul> \n  <li>Replace <code>192.168.1.100</code> with your actual server's IP address</li> \n  <li>Do NOT use <code>0.0.0.0</code> in <code>PUBLIC_URL</code> or <code>CORS_ORIGINS</code> - use the actual IP address</li> \n  <li>Include both localhost and your server IP in <code>CORS_ORIGINS</code> for flexibility</li> \n </ul> \n</blockquote> \n<ol start=\"2\"> \n <li><strong>Recreate containers</strong> to apply the changes:</li> \n</ol> \n<pre><code class=\"language-bash\">docker compose down\ndocker compose up -d --force-recreate\n</code></pre> \n<ol start=\"3\"> \n <li><strong>Verify port binding:</strong></li> \n</ol> \n<pre><code class=\"language-bash\">docker ps | grep pentagi\n</code></pre> \n<p>You should see <code>0.0.0.0:8443-&gt;8443/tcp</code> or <code>:::8443-&gt;8443/tcp</code>.</p> \n<p>If you see <code>127.0.0.1:8443-&gt;8443/tcp</code>, the environment variable wasn't picked up. In this case, directly edit <code>docker-compose.yml</code> line 31:</p> \n<pre><code class=\"language-yaml\">ports:\n  - \"0.0.0.0:8443:8443\"\n</code></pre> \n<p>Then recreate containers again.</p> \n<ol start=\"4\"> \n <li><strong>Configure firewall</strong> to allow incoming connections on port 8443:</li> \n</ol> \n<pre><code class=\"language-bash\"># Ubuntu/Debian with UFW\nsudo ufw allow 8443/tcp\nsudo ufw reload\n\n# CentOS/RHEL with firewalld\nsudo firewall-cmd --permanent --add-port=8443/tcp\nsudo firewall-cmd --reload\n</code></pre> \n<ol start=\"5\"> \n <li><strong>Access PentAGI:</strong></li> \n</ol> \n<ul> \n <li><strong>Local access:</strong> <code>https://localhost:8443</code></li> \n <li><strong>Network access:</strong> <code>https://your-server-ip:8443</code></li> \n</ul> \n<blockquote> \n <p>[!NOTE] You'll need to accept the self-signed SSL certificate warning in your browser when accessing via IP address.</p> \n</blockquote> \n<hr /> \n<h3>Running PentAGI with Podman</h3> \n<p>PentAGI fully supports Podman as a Docker alternative. However, when using <strong>Podman in rootless mode</strong>, the scraper service requires special configuration because rootless containers cannot bind privileged ports (ports below 1024).</p> \n<h4>Podman Rootless Configuration</h4> \n<p>The default scraper configuration uses port 443 (HTTPS), which is a privileged port. For Podman rootless, reconfigure the scraper to use a non-privileged port:</p> \n<p><strong>1. Edit <code>docker-compose.yml</code></strong> - modify the <code>scraper</code> service (around line 199):</p> \n<pre><code class=\"language-yaml\">scraper:\n  image: vxcontrol/scraper:latest\n  restart: unless-stopped\n  container_name: scraper\n  hostname: scraper\n  expose:\n    - 3000/tcp  # Changed from 443 to 3000\n  ports:\n    - \"${SCRAPER_LISTEN_IP:-127.0.0.1}:${SCRAPER_LISTEN_PORT:-9443}:3000\"  # Map to port 3000\n  environment:\n    - MAX_CONCURRENT_SESSIONS=${LOCAL_SCRAPER_MAX_CONCURRENT_SESSIONS:-10}\n    - USERNAME=${LOCAL_SCRAPER_USERNAME:-someuser}\n    - PASSWORD=${LOCAL_SCRAPER_PASSWORD:-somepass}\n  logging:\n    options:\n      max-size: 50m\n      max-file: \"7\"\n  volumes:\n    - scraper-ssl:/usr/src/app/ssl\n  networks:\n    - pentagi-network\n  shm_size: 2g\n</code></pre> \n<p><strong>2. Update <code>.env</code> file</strong> - change the scraper URL to use HTTP and port 3000:</p> \n<pre><code class=\"language-bash\"># Scraper configuration for Podman rootless\nSCRAPER_PRIVATE_URL=http://someuser:somepass@scraper:3000/\nLOCAL_SCRAPER_USERNAME=someuser\nLOCAL_SCRAPER_PASSWORD=somepass\n</code></pre> \n<blockquote> \n <p>[!IMPORTANT] Key changes for Podman:</p> \n <ul> \n  <li>Use <strong>HTTP</strong> instead of HTTPS for <code>SCRAPER_PRIVATE_URL</code></li> \n  <li>Use port <strong>3000</strong> instead of 443</li> \n  <li>Change internal <code>expose</code> to <code>3000/tcp</code></li> \n  <li>Update port mapping to target <code>3000</code> instead of <code>443</code></li> \n </ul> \n</blockquote> \n<p><strong>3. Recreate containers:</strong></p> \n<pre><code class=\"language-bash\">podman-compose down\npodman-compose up -d --force-recreate\n</code></pre> \n<p><strong>4. Test scraper connectivity:</strong></p> \n<pre><code class=\"language-bash\"># Test from within the pentagi container\npodman exec -it pentagi wget -O- \"http://someuser:somepass@scraper:3000/html?url=http://example.com\"\n</code></pre> \n<p>If you see HTML output, the scraper is working correctly.</p> \n<h4>Podman Rootful Mode</h4> \n<p>If you're running Podman in rootful mode (with sudo), you can use the default configuration without modifications. The scraper will work on port 443 as intended.</p> \n<h4>Docker Compatibility</h4> \n<p>All Podman configurations remain fully compatible with Docker. The non-privileged port approach works identically on both container runtimes.</p> \n<h3>Assistant Configuration</h3> \n<p>PentAGI allows you to configure default behavior for assistants:</p> \n<table> \n <thead> \n  <tr> \n   <th>Variable</th> \n   <th>Default</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><code>ASSISTANT_USE_AGENTS</code></td> \n   <td><code>false</code></td> \n   <td>Controls the default value for agent usage when creating new assistants</td> \n  </tr> \n </tbody> \n</table> \n<p>The <code>ASSISTANT_USE_AGENTS</code> setting affects the initial state of the \"Use Agents\" toggle when creating a new assistant in the UI:</p> \n<ul> \n <li><code>false</code> (default): New assistants are created with agent delegation disabled by default</li> \n <li><code>true</code>: New assistants are created with agent delegation enabled by default</li> \n</ul> \n<p>Note that users can always override this setting by toggling the \"Use Agents\" button in the UI when creating or editing an assistant. This environment variable only controls the initial default state.</p> \n<h2>🔌 API Access</h2> \n<p>PentAGI provides comprehensive programmatic access through both REST and GraphQL APIs, allowing you to integrate penetration testing workflows into your automation pipelines, CI/CD processes, and custom applications.</p> \n<h3>Generating API Tokens</h3> \n<p>API tokens are managed through the PentAGI web interface:</p> \n<ol> \n <li>Navigate to <strong>Settings</strong> → <strong>API Tokens</strong> in the web UI</li> \n <li>Click <strong>Create Token</strong> to generate a new API token</li> \n <li>Configure token properties: \n  <ul> \n   <li><strong>Name</strong> (optional): A descriptive name for the token</li> \n   <li><strong>Expiration Date</strong>: When the token will expire (minimum 1 minute, maximum 3 years)</li> \n  </ul> </li> \n <li>Click <strong>Create</strong> and <strong>copy the token immediately</strong> - it will only be shown once for security reasons</li> \n <li>Use the token as a Bearer token in your API requests</li> \n</ol> \n<p>Each token is associated with your user account and inherits your role's permissions.</p> \n<h3>Using API Tokens</h3> \n<p>Include the API token in the <code>Authorization</code> header of your HTTP requests:</p> \n<pre><code class=\"language-bash\"># GraphQL API example\ncurl -X POST https://your-pentagi-instance:8443/api/v1/graphql \\\n  -H \"Authorization: Bearer YOUR_API_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"{ flows { id title status } }\"}'\n\n# REST API example\ncurl https://your-pentagi-instance:8443/api/v1/flows \\\n  -H \"Authorization: Bearer YOUR_API_TOKEN\"\n</code></pre> \n<h3>API Exploration and Testing</h3> \n<p>PentAGI provides interactive documentation for exploring and testing API endpoints:</p> \n<h4>GraphQL Playground</h4> \n<p>Access the GraphQL Playground at <code>https://your-pentagi-instance:8443/api/v1/graphql/playground</code></p> \n<ol> \n <li>Click the <strong>HTTP Headers</strong> tab at the bottom</li> \n <li>Add your authorization header: <pre><code class=\"language-json\">{\n  \"Authorization\": \"Bearer YOUR_API_TOKEN\"\n}\n</code></pre> </li> \n <li>Explore the schema, run queries, and test mutations interactively</li> \n</ol> \n<h4>Swagger UI</h4> \n<p>Access the REST API documentation at <code>https://your-pentagi-instance:8443/api/v1/swagger/index.html</code></p> \n<ol> \n <li>Click the <strong>Authorize</strong> button</li> \n <li>Enter your token in the format: <code>Bearer YOUR_API_TOKEN</code></li> \n <li>Click <strong>Authorize</strong> to apply</li> \n <li>Test endpoints directly from the Swagger UI</li> \n</ol> \n<h3>Generating API Clients</h3> \n<p>You can generate type-safe API clients for your preferred programming language using the schema files included with PentAGI:</p> \n<h4>GraphQL Clients</h4> \n<p>The GraphQL schema is available at:</p> \n<ul> \n <li><strong>Web UI</strong>: Navigate to Settings to download <code>schema.graphqls</code></li> \n <li><strong>Direct file</strong>: <code>backend/pkg/graph/schema.graphqls</code> in the repository</li> \n</ul> \n<p>Generate clients using tools like:</p> \n<ul> \n <li><strong>GraphQL Code Generator</strong> (JavaScript/TypeScript): <a href=\"https://the-guild.dev/graphql/codegen\">https://the-guild.dev/graphql/codegen</a></li> \n <li><strong>genqlient</strong> (Go): <a href=\"https://github.com/Khan/genqlient\">https://github.com/Khan/genqlient</a></li> \n <li><strong>Apollo iOS</strong> (Swift): <a href=\"https://www.apollographql.com/docs/ios\">https://www.apollographql.com/docs/ios</a></li> \n</ul> \n<h4>REST API Clients</h4> \n<p>The OpenAPI specification is available at:</p> \n<ul> \n <li><strong>Swagger JSON</strong>: <code>https://your-pentagi-instance:8443/api/v1/swagger/doc.json</code></li> \n <li><strong>Swagger YAML</strong>: Available in <code>backend/pkg/server/docs/swagger.yaml</code></li> \n</ul> \n<p>Generate clients using:</p> \n<ul> \n <li> <p><strong>OpenAPI Generator</strong>: <a href=\"https://openapi-generator.tech\">https://openapi-generator.tech</a></p> <pre><code class=\"language-bash\">openapi-generator-cli generate \\\n  -i https://your-pentagi-instance:8443/api/v1/swagger/doc.json \\\n  -g python \\\n  -o ./pentagi-client\n</code></pre> </li> \n <li> <p><strong>Swagger Codegen</strong>: <a href=\"https://github.com/swagger-api/swagger-codegen\">https://github.com/swagger-api/swagger-codegen</a></p> <pre><code class=\"language-bash\">swagger-codegen generate \\\n  -i https://your-pentagi-instance:8443/api/v1/swagger/doc.json \\\n  -l typescript-axios \\\n  -o ./pentagi-client\n</code></pre> </li> \n <li> <p><strong>swagger-typescript-api</strong> (TypeScript): <a href=\"https://github.com/acacode/swagger-typescript-api\">https://github.com/acacode/swagger-typescript-api</a></p> <pre><code class=\"language-bash\">npx swagger-typescript-api \\\n  -p https://your-pentagi-instance:8443/api/v1/swagger/doc.json \\\n  -o ./src/api \\\n  -n pentagi-api.ts\n</code></pre> </li> \n</ul> \n<h3>API Usage Examples</h3> \n<details> \n <b>Creating a New Flow (GraphQL)</b> \n <pre><code class=\"language-graphql\">mutation CreateFlow {\n  createFlow(\n    modelProvider: \"openai\"\n    input: \"Test the security of https://example.com\"\n  ) {\n    id\n    title\n    status\n    createdAt\n  }\n}\n</code></pre> \n</details> \n<details> \n <b>Listing Flows (REST API)</b> \n <pre><code class=\"language-bash\">curl https://your-pentagi-instance:8443/api/v1/flows \\\n  -H \"Authorization: Bearer YOUR_API_TOKEN\" \\\n  | jq '.flows[] | {id, title, status}'\n</code></pre> \n</details> \n<details> \n <b>Python Client Example</b> \n <pre><code class=\"language-python\">import requests\n\nclass PentAGIClient:\n    def __init__(self, base_url, api_token):\n        self.base_url = base_url\n        self.headers = {\n            \"Authorization\": f\"Bearer {api_token}\",\n            \"Content-Type\": \"application/json\"\n        }\n    \n    def create_flow(self, provider, target):\n        query = \"\"\"\n        mutation CreateFlow($provider: String!, $input: String!) {\n          createFlow(modelProvider: $provider, input: $input) {\n            id\n            title\n            status\n          }\n        }\n        \"\"\"\n        response = requests.post(\n            f\"{self.base_url}/api/v1/graphql\",\n            json={\n                \"query\": query,\n                \"variables\": {\n                    \"provider\": provider,\n                    \"input\": target\n                }\n            },\n            headers=self.headers\n        )\n        return response.json()\n    \n    def get_flows(self):\n        response = requests.get(\n            f\"{self.base_url}/api/v1/flows\",\n            headers=self.headers\n        )\n        return response.json()\n\n# Usage\nclient = PentAGIClient(\n    \"https://your-pentagi-instance:8443\",\n    \"your_api_token_here\"\n)\n\n# Create a new flow\nflow = client.create_flow(\"openai\", \"Scan https://example.com for vulnerabilities\")\nprint(f\"Created flow: {flow}\")\n\n# List all flows\nflows = client.get_flows()\nprint(f\"Total flows: {len(flows['flows'])}\")\n</code></pre> \n</details> \n<details> \n <b>TypeScript Client Example</b> \n <pre><code class=\"language-typescript\">import axios, { AxiosInstance } from 'axios';\n\ninterface Flow {\n  id: string;\n  title: string;\n  status: string;\n  createdAt: string;\n}\n\nclass PentAGIClient {\n  private client: AxiosInstance;\n\n  constructor(baseURL: string, apiToken: string) {\n    this.client = axios.create({\n      baseURL: `${baseURL}/api/v1`,\n      headers: {\n        'Authorization': `Bearer ${apiToken}`,\n        'Content-Type': 'application/json',\n      },\n    });\n  }\n\n  async createFlow(provider: string, input: string): Promise&lt;Flow&gt; {\n    const query = `\n      mutation CreateFlow($provider: String!, $input: String!) {\n        createFlow(modelProvider: $provider, input: $input) {\n          id\n          title\n          status\n          createdAt\n        }\n      }\n    `;\n\n    const response = await this.client.post('/graphql', {\n      query,\n      variables: { provider, input },\n    });\n\n    return response.data.data.createFlow;\n  }\n\n  async getFlows(): Promise&lt;Flow[]&gt; {\n    const response = await this.client.get('/flows');\n    return response.data.flows;\n  }\n\n  async getFlow(flowId: string): Promise&lt;Flow&gt; {\n    const response = await this.client.get(`/flows/${flowId}`);\n    return response.data;\n  }\n}\n\n// Usage\nconst client = new PentAGIClient(\n  'https://your-pentagi-instance:8443',\n  'your_api_token_here'\n);\n\n// Create a new flow\nconst flow = await client.createFlow(\n  'openai',\n  'Perform penetration test on https://example.com'\n);\nconsole.log('Created flow:', flow);\n\n// List all flows\nconst flows = await client.getFlows();\nconsole.log(`Total flows: ${flows.length}`);\n</code></pre> \n</details> \n<h3>Security Best Practices</h3> \n<p>When working with API tokens:</p> \n<ul> \n <li><strong>Never commit tokens to version control</strong> - use environment variables or secrets management</li> \n <li><strong>Rotate tokens regularly</strong> - set appropriate expiration dates and create new tokens periodically</li> \n <li><strong>Use separate tokens for different applications</strong> - makes it easier to revoke access if needed</li> \n <li><strong>Monitor token usage</strong> - review API token activity in the Settings page</li> \n <li><strong>Revoke unused tokens</strong> - disable or delete tokens that are no longer needed</li> \n <li><strong>Use HTTPS only</strong> - never send API tokens over unencrypted connections</li> \n</ul> \n<h3>Token Management</h3> \n<ul> \n <li><strong>View tokens</strong>: See all your active tokens in Settings → API Tokens</li> \n <li><strong>Edit tokens</strong>: Update token names or revoke tokens</li> \n <li><strong>Delete tokens</strong>: Permanently remove tokens (this action cannot be undone)</li> \n <li><strong>Token ID</strong>: Each token has a unique ID that can be copied for reference</li> \n</ul> \n<p>The token list shows:</p> \n<ul> \n <li>Token name (if provided)</li> \n <li>Token ID (unique identifier)</li> \n <li>Status (active/revoked/expired)</li> \n <li>Creation date</li> \n <li>Expiration date</li> \n</ul> \n<h3>Custom LLM Provider Configuration</h3> \n<p>When using custom LLM providers with the <code>LLM_SERVER_*</code> variables, you can fine-tune the reasoning format used in requests:</p> \n<table> \n <thead> \n  <tr> \n   <th>Variable</th> \n   <th>Default</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><code>LLM_SERVER_URL</code></td> \n   <td></td> \n   <td>Base URL for the custom LLM API endpoint</td> \n  </tr> \n  <tr> \n   <td><code>LLM_SERVER_KEY</code></td> \n   <td></td> \n   <td>API key for the custom LLM provider</td> \n  </tr> \n  <tr> \n   <td><code>LLM_SERVER_MODEL</code></td> \n   <td></td> \n   <td>Default model to use (can be overridden in provider config)</td> \n  </tr> \n  <tr> \n   <td><code>LLM_SERVER_CONFIG_PATH</code></td> \n   <td></td> \n   <td>Path to the YAML configuration file for agent-specific models</td> \n  </tr> \n  <tr> \n   <td><code>LLM_SERVER_PROVIDER</code></td> \n   <td></td> \n   <td>Provider name prefix for model names (e.g., <code>openrouter</code>, <code>deepseek</code> for LiteLLM proxy)</td> \n  </tr> \n  <tr> \n   <td><code>LLM_SERVER_LEGACY_REASONING</code></td> \n   <td><code>false</code></td> \n   <td>Controls reasoning format in API requests</td> \n  </tr> \n  <tr> \n   <td><code>LLM_SERVER_PRESERVE_REASONING</code></td> \n   <td><code>false</code></td> \n   <td>Preserve reasoning content in multi-turn conversations (required by some providers)</td> \n  </tr> \n </tbody> \n</table> \n<p>The <code>LLM_SERVER_PROVIDER</code> setting is particularly useful when using <strong>LiteLLM proxy</strong>, which adds a provider prefix to model names. For example, when connecting to Moonshot API through LiteLLM, models like <code>kimi-2.5</code> become <code>moonshot/kimi-2.5</code>. By setting <code>LLM_SERVER_PROVIDER=moonshot</code>, you can use the same provider configuration file for both direct API access and LiteLLM proxy access without modifications.</p> \n<p>The <code>LLM_SERVER_LEGACY_REASONING</code> setting affects how reasoning parameters are sent to the LLM:</p> \n<ul> \n <li><code>false</code> (default): Uses modern format where reasoning is sent as a structured object with <code>max_tokens</code> parameter</li> \n <li><code>true</code>: Uses legacy format with string-based <code>reasoning_effort</code> parameter</li> \n</ul> \n<p>This setting is important when working with different LLM providers as they may expect different reasoning formats in their API requests. If you encounter reasoning-related errors with custom providers, try changing this setting.</p> \n<p>The <code>LLM_SERVER_PRESERVE_REASONING</code> setting controls whether reasoning content is preserved in multi-turn conversations:</p> \n<ul> \n <li><code>false</code> (default): Reasoning content is not preserved in conversation history</li> \n <li><code>true</code>: Reasoning content is preserved and sent in subsequent API calls</li> \n</ul> \n<p>This setting is required by some LLM providers (e.g., Moonshot) that return errors like \"thinking is enabled but reasoning_content is missing in assistant tool call message\" when reasoning content is not included in multi-turn conversations. Enable this setting if your provider requires reasoning content to be preserved.</p> \n<h3>Local LLM Provider Configuration</h3> \n<p>PentAGI supports Ollama for local LLM inference, providing zero-cost operation and enhanced privacy:</p> \n<table> \n <thead> \n  <tr> \n   <th>Variable</th> \n   <th>Default</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><code>OLLAMA_SERVER_URL</code></td> \n   <td></td> \n   <td>URL of your Ollama server</td> \n  </tr> \n  <tr> \n   <td><code>OLLAMA_SERVER_MODEL</code></td> \n   <td><code>llama3.1:8b-instruct-q8_0</code></td> \n   <td>Default model for inference</td> \n  </tr> \n  <tr> \n   <td><code>OLLAMA_SERVER_CONFIG_PATH</code></td> \n   <td></td> \n   <td>Path to custom agent configuration file</td> \n  </tr> \n  <tr> \n   <td><code>OLLAMA_SERVER_PULL_MODELS_TIMEOUT</code></td> \n   <td><code>600</code></td> \n   <td>Timeout for model downloads (seconds)</td> \n  </tr> \n  <tr> \n   <td><code>OLLAMA_SERVER_PULL_MODELS_ENABLED</code></td> \n   <td><code>false</code></td> \n   <td>Auto-download models on startup</td> \n  </tr> \n  <tr> \n   <td><code>OLLAMA_SERVER_LOAD_MODELS_ENABLED</code></td> \n   <td><code>false</code></td> \n   <td>Query server for available models</td> \n  </tr> \n </tbody> \n</table> \n<p>Configuration examples:</p> \n<pre><code class=\"language-bash\"># Basic Ollama setup with default model\nOLLAMA_SERVER_URL=http://localhost:11434\nOLLAMA_SERVER_MODEL=llama3.1:8b-instruct-q8_0\n\n# Production setup with auto-pull and model discovery\nOLLAMA_SERVER_URL=http://ollama-server:11434\nOLLAMA_SERVER_PULL_MODELS_ENABLED=true\nOLLAMA_SERVER_PULL_MODELS_TIMEOUT=900\nOLLAMA_SERVER_LOAD_MODELS_ENABLED=true\n\n# Custom configuration with agent-specific models\nOLLAMA_SERVER_CONFIG_PATH=/path/to/ollama-config.yml\n\n# Default configuration file inside docker container\nOLLAMA_SERVER_CONFIG_PATH=/opt/pentagi/conf/ollama-llama318b.provider.yml\n</code></pre> \n<p><strong>Performance Considerations:</strong></p> \n<ul> \n <li><strong>Model Discovery</strong> (<code>OLLAMA_SERVER_LOAD_MODELS_ENABLED=true</code>): Adds 1-2s startup latency querying Ollama API</li> \n <li><strong>Auto-pull</strong> (<code>OLLAMA_SERVER_PULL_MODELS_ENABLED=true</code>): First startup may take several minutes downloading models</li> \n <li><strong>Pull timeout</strong> (<code>OLLAMA_SERVER_PULL_MODELS_TIMEOUT=900</code>): 15 minutes in seconds</li> \n <li><strong>Static Config</strong>: Disable both flags and specify models in config file for fastest startup</li> \n</ul> \n<h4>Creating Custom Ollama Models with Extended Context</h4> \n<p>PentAGI requires models with larger context windows than the default Ollama configurations. You need to create custom models with increased <code>num_ctx</code> parameter through Modelfiles. While typical agent workflows consume around 64K tokens, PentAGI uses 110K context size for safety margin and handling complex penetration testing scenarios.</p> \n<p><strong>Important</strong>: The <code>num_ctx</code> parameter can only be set during model creation via Modelfile - it cannot be changed after model creation or overridden at runtime.</p> \n<h5>Example: Qwen3 32B FP16 with Extended Context</h5> \n<p>Create a Modelfile named <code>Modelfile_qwen3_32b_fp16_tc</code>:</p> \n<pre><code class=\"language-dockerfile\">FROM qwen3:32b-fp16\nPARAMETER num_ctx 110000\nPARAMETER temperature 0.3\nPARAMETER top_p 0.8\nPARAMETER min_p 0.0\nPARAMETER top_k 20\nPARAMETER repeat_penalty 1.1\n</code></pre> \n<p>Build the custom model:</p> \n<pre><code class=\"language-bash\">ollama create qwen3:32b-fp16-tc -f Modelfile_qwen3_32b_fp16_tc\n</code></pre> \n<h5>Example: QwQ 32B FP16 with Extended Context</h5> \n<p>Create a Modelfile named <code>Modelfile_qwq_32b_fp16_tc</code>:</p> \n<pre><code class=\"language-dockerfile\">FROM qwq:32b-fp16\nPARAMETER num_ctx 110000\nPARAMETER temperature 0.2\nPARAMETER top_p 0.7\nPARAMETER min_p 0.0\nPARAMETER top_k 40\nPARAMETER repeat_penalty 1.2\n</code></pre> \n<p>Build the custom model:</p> \n<pre><code class=\"language-bash\">ollama create qwq:32b-fp16-tc -f Modelfile_qwq_32b_fp16_tc\n</code></pre> \n<blockquote> \n <p><strong>Note</strong>: The QwQ 32B FP16 model requires approximately <strong>71.3 GB VRAM</strong> for inference. Ensure your system has sufficient GPU memory before attempting to use this model.</p> \n</blockquote> \n<p>These custom models are referenced in the pre-built provider configuration files (<code>ollama-qwen332b-fp16-tc.provider.yml</code> and <code>ollama-qwq32b-fp16-tc.provider.yml</code>) that are included in the Docker image at <code>/opt/pentagi/conf/</code>.</p> \n<h3>OpenAI Provider Configuration</h3> \n<p>PentAGI supports OpenAI's advanced language models, including the latest reasoning-capable o-series models designed for complex analytical tasks:</p> \n<table> \n <thead> \n  <tr> \n   <th>Variable</th> \n   <th>Default</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><code>OPEN_AI_KEY</code></td> \n   <td></td> \n   <td>API key for OpenAI services</td> \n  </tr> \n  <tr> \n   <td><code>OPEN_AI_SERVER_URL</code></td> \n   <td><code>https://api.openai.com/v1</code></td> \n   <td>OpenAI API endpoint</td> \n  </tr> \n </tbody> \n</table> \n<p>Configuration examples:</p> \n<pre><code class=\"language-bash\"># Basic OpenAI setup\nOPEN_AI_KEY=your_openai_api_key\nOPEN_AI_SERVER_URL=https://api.openai.com/v1\n\n# Using with proxy for enhanced security\nOPEN_AI_KEY=your_openai_api_key\nPROXY_URL=http://your-proxy:8080\n</code></pre> \n<p>The OpenAI provider offers cutting-edge capabilities including:</p> \n<ul> \n <li><strong>Reasoning Models</strong>: Advanced o-series models (o1, o3, o4-mini) with step-by-step analytical thinking</li> \n <li><strong>Latest GPT-4.1 Series</strong>: Flagship models optimized for complex security research and exploit development</li> \n <li><strong>Cost-Effective Options</strong>: From nano models for high-volume scanning to powerful reasoning models for deep analysis</li> \n <li><strong>Versatile Performance</strong>: Fast, intelligent models perfect for multi-step security analysis and penetration testing</li> \n <li><strong>Proven Reliability</strong>: Industry-leading models with consistent performance across diverse security scenarios</li> \n</ul> \n<p>The system automatically selects appropriate OpenAI models based on task complexity, optimizing for both performance and cost-effectiveness.</p> \n<h3>Anthropic Provider Configuration</h3> \n<p>PentAGI integrates with Anthropic's Claude models, known for their exceptional safety, reasoning capabilities, and sophisticated understanding of complex security contexts:</p> \n<table> \n <thead> \n  <tr> \n   <th>Variable</th> \n   <th>Default</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><code>ANTHROPIC_API_KEY</code></td> \n   <td></td> \n   <td>API key for Anthropic services</td> \n  </tr> \n  <tr> \n   <td><code>ANTHROPIC_SERVER_URL</code></td> \n   <td><code>https://api.anthropic.com/v1</code></td> \n   <td>Anthropic API endpoint</td> \n  </tr> \n </tbody> \n</table> \n<p>Configuration examples:</p> \n<pre><code class=\"language-bash\"># Basic Anthropic setup\nANTHROPIC_API_KEY=your_anthropic_api_key\nANTHROPIC_SERVER_URL=https://api.anthropic.com/v1\n\n# Using with proxy for secure environments\nANTHROPIC_API_KEY=your_anthropic_api_key\nPROXY_URL=http://your-proxy:8080\n</code></pre> \n<p>The Anthropic provider delivers superior capabilities including:</p> \n<ul> \n <li><strong>Advanced Reasoning</strong>: Claude 4 series with exceptional reasoning for sophisticated penetration testing</li> \n <li><strong>Extended Thinking</strong>: Claude 3.7 with step-by-step thinking capabilities for methodical security research</li> \n <li><strong>High-Speed Performance</strong>: Claude 3.5 Haiku for blazing-fast vulnerability scans and real-time monitoring</li> \n <li><strong>Comprehensive Analysis</strong>: Claude Sonnet models for complex security analysis and threat hunting</li> \n <li><strong>Safety-First Design</strong>: Built-in safety mechanisms ensuring responsible security testing practices</li> \n</ul> \n<p>The system leverages Claude's advanced understanding of security contexts to provide thorough and responsible penetration testing guidance.</p> \n<h3>Google AI (Gemini) Provider Configuration</h3> \n<p>PentAGI supports Google's Gemini models through the Google AI API, offering state-of-the-art reasoning capabilities and multimodal features:</p> \n<table> \n <thead> \n  <tr> \n   <th>Variable</th> \n   <th>Default</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><code>GEMINI_API_KEY</code></td> \n   <td></td> \n   <td>API key for Google AI services</td> \n  </tr> \n  <tr> \n   <td><code>GEMINI_SERVER_URL</code></td> \n   <td><code>https://generativelanguage.googleapis.com</code></td> \n   <td>Google AI API endpoint</td> \n  </tr> \n </tbody> \n</table> \n<p>Configuration examples:</p> \n<pre><code class=\"language-bash\"># Basic Gemini setup\nGEMINI_API_KEY=your_gemini_api_key\nGEMINI_SERVER_URL=https://generativelanguage.googleapis.com\n\n# Using with proxy\nGEMINI_API_KEY=your_gemini_api_key\nPROXY_URL=http://your-proxy:8080\n</code></pre> \n<p>The Gemini provider offers advanced features including:</p> \n<ul> \n <li><strong>Thinking Capabilities</strong>: Advanced reasoning models (Gemini 2.5 series) with step-by-step analysis</li> \n <li><strong>Multimodal Support</strong>: Text and image processing for comprehensive security assessments</li> \n <li><strong>Large Context Windows</strong>: Up to 2M tokens for analyzing extensive codebases and documentation</li> \n <li><strong>Cost-Effective Options</strong>: From high-performance pro models to economical flash variants</li> \n <li><strong>Security-Focused Models</strong>: Specialized configurations optimized for penetration testing workflows</li> \n</ul> \n<p>The system automatically selects appropriate Gemini models based on agent requirements, balancing performance, capabilities, and cost-effectiveness.</p> \n<h3>AWS Bedrock Provider Configuration</h3> \n<p>PentAGI integrates with Amazon Bedrock, offering access to a wide range of foundation models from leading AI companies including Anthropic, AI21, Cohere, Meta, and Amazon's own models:</p> \n<table> \n <thead> \n  <tr> \n   <th>Variable</th> \n   <th>Default</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><code>BEDROCK_REGION</code></td> \n   <td><code>us-east-1</code></td> \n   <td>AWS region for Bedrock service</td> \n  </tr> \n  <tr> \n   <td><code>BEDROCK_ACCESS_KEY_ID</code></td> \n   <td></td> \n   <td>AWS access key ID for authentication</td> \n  </tr> \n  <tr> \n   <td><code>BEDROCK_SECRET_ACCESS_KEY</code></td> \n   <td></td> \n   <td>AWS secret access key for authentication</td> \n  </tr> \n  <tr> \n   <td><code>BEDROCK_SESSION_TOKEN</code></td> \n   <td></td> \n   <td>AWS session token as alternative way for authentication</td> \n  </tr> \n  <tr> \n   <td><code>BEDROCK_SERVER_URL</code></td> \n   <td></td> \n   <td>Optional custom Bedrock endpoint URL</td> \n  </tr> \n </tbody> \n</table> \n<p>Configuration examples:</p> \n<pre><code class=\"language-bash\"># Basic AWS Bedrock setup with credentials\nBEDROCK_REGION=us-east-1\nBEDROCK_ACCESS_KEY_ID=your_aws_access_key\nBEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key\n\n# Using with proxy for enhanced security\nBEDROCK_REGION=us-east-1\nBEDROCK_ACCESS_KEY_ID=your_aws_access_key\nBEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key\nPROXY_URL=http://your-proxy:8080\n\n# Using custom endpoint (for VPC endpoints or testing)\nBEDROCK_REGION=us-east-1\nBEDROCK_ACCESS_KEY_ID=your_aws_access_key\nBEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key\nBEDROCK_SERVER_URL=https://bedrock-runtime.us-east-1.amazonaws.com\n</code></pre> \n<blockquote> \n <p>[!IMPORTANT] <strong>AWS Bedrock Rate Limits Warning</strong></p> \n <p>The default PentAGI configuration for AWS Bedrock uses two primary models:</p> \n <ul> \n  <li><code>us.anthropic.claude-sonnet-4-20250514-v1:0</code> (for most agents) - <strong>2 requests per minute</strong> for new AWS accounts</li> \n  <li><code>us.anthropic.claude-3-5-haiku-20241022-v1:0</code> (for simple tasks) - <strong>20 requests per minute</strong> for new AWS accounts</li> \n </ul> \n <p>These default rate limits are <strong>extremely restrictive</strong> for comfortable penetration testing scenarios and will significantly impact your workflow. We <strong>strongly recommend</strong>:</p> \n <ol> \n  <li><strong>Request quota increases</strong> for your AWS Bedrock models through the AWS Service Quotas console</li> \n  <li><strong>Use provisioned throughput models</strong> with hourly billing for higher throughput requirements</li> \n  <li><strong>Switch to alternative models</strong> with higher default quotas (e.g., Amazon Nova series, Meta Llama models)</li> \n  <li><strong>Consider using a different LLM provider</strong> (OpenAI, Anthropic, Gemini) if you need immediate high-throughput access</li> \n </ol> \n <p>Without adequate rate limits, you may experience frequent delays, timeouts, and degraded testing performance.</p> \n</blockquote> \n<p>The AWS Bedrock provider delivers comprehensive capabilities including:</p> \n<ul> \n <li><strong>Multi-Provider Access</strong>: Access to models from Anthropic (Claude), AI21 (Jamba), Cohere (Command), Meta (Llama), Amazon (Nova, Titan), and DeepSeek (R1) through a single interface</li> \n <li><strong>Advanced Reasoning</strong>: Support for Claude 4 and other reasoning-capable models with step-by-step thinking</li> \n <li><strong>Multimodal Models</strong>: Amazon Nova series supporting text, image, and video processing for comprehensive security analysis</li> \n <li><strong>Enterprise Security</strong>: AWS-native security controls, VPC integration, and compliance certifications</li> \n <li><strong>Cost Optimization</strong>: Wide range of model sizes and capabilities for cost-effective penetration testing</li> \n <li><strong>Regional Availability</strong>: Deploy models in your preferred AWS region for data residency and performance</li> \n <li><strong>High Performance</strong>: Low-latency inference through AWS's global infrastructure</li> \n</ul> \n<p>The system automatically selects appropriate Bedrock models based on task complexity and requirements, leveraging the full spectrum of available foundation models for optimal security testing results.</p> \n<blockquote> \n <p>[!WARNING] <strong>Converse API Requirements</strong></p> \n <p>PentAGI uses the <strong>Amazon Bedrock Converse API</strong> for model interactions, which requires models to support the following features:</p> \n <ul> \n  <li>✅ <strong>Converse</strong> - Basic conversation API support</li> \n  <li>✅ <strong>ConverseStream</strong> - Streaming response support</li> \n  <li>✅ <strong>Tool use</strong> - Function calling capabilities for penetration testing tools</li> \n  <li>✅ <strong>Streaming tool use</strong> - Real-time tool execution feedback</li> \n </ul> \n <p><strong>Before selecting models</strong>, verify their feature support at: <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html\">Supported models and model features</a></p> \n <p>⚠️ <strong>Important</strong>: Some models like AI21 Jurassic-2 and Cohere Command (Text) have <strong>limited chat support</strong> and may not work properly with PentAGI's multi-turn conversation workflows.</p> \n</blockquote> \n<blockquote> \n <p><strong>Note</strong>: AWS credentials can also be provided through IAM roles, environment variables, or AWS credential files following standard AWS SDK authentication patterns. Ensure your AWS account has appropriate permissions for Amazon Bedrock service access.</p> \n</blockquote> \n<p>For advanced configuration options and detailed setup instructions, please visit our <a href=\"https://docs.pentagi.com\">documentation</a>.</p> \n<h2>🔧 Advanced Setup</h2> \n<h3>Langfuse Integration</h3> \n<p>Langfuse provides advanced capabilities for monitoring and analyzing AI agent operations.</p> \n<ol> \n <li>Configure Langfuse environment variables in existing <code>.env</code> file.</li> \n</ol> \n<details> \n Langfuse valuable environment variables \n <h3>Database Credentials</h3> \n <ul> \n  <li><code>LANGFUSE_POSTGRES_USER</code> and <code>LANGFUSE_POSTGRES_PASSWORD</code> - Langfuse PostgreSQL credentials</li> \n  <li><code>LANGFUSE_CLICKHOUSE_USER</code> and <code>LANGFUSE_CLICKHOUSE_PASSWORD</code> - ClickHouse credentials</li> \n  <li><code>LANGFUSE_REDIS_AUTH</code> - Redis password</li> \n </ul> \n <h3>Encryption and Security Keys</h3> \n <ul> \n  <li><code>LANGFUSE_SALT</code> - Salt for hashing in Langfuse Web UI</li> \n  <li><code>LANGFUSE_ENCRYPTION_KEY</code> - Encryption key (32 bytes in hex)</li> \n  <li><code>LANGFUSE_NEXTAUTH_SECRET</code> - Secret key for NextAuth</li> \n </ul> \n <h3>Admin Credentials</h3> \n <ul> \n  <li><code>LANGFUSE_INIT_USER_EMAIL</code> - Admin email</li> \n  <li><code>LANGFUSE_INIT_USER_PASSWORD</code> - Admin password</li> \n  <li><code>LANGFUSE_INIT_USER_NAME</code> - Admin username</li> \n </ul> \n <h3>API Keys and Tokens</h3> \n <ul> \n  <li><code>LANGFUSE_INIT_PROJECT_PUBLIC_KEY</code> - Project public key (used from PentAGI side too)</li> \n  <li><code>LANGFUSE_INIT_PROJECT_SECRET_KEY</code> - Project secret key (used from PentAGI side too)</li> \n </ul> \n <h3>S3 Storage</h3> \n <ul> \n  <li><code>LANGFUSE_S3_ACCESS_KEY_ID</code> - S3 access key ID</li> \n  <li><code>LANGFUSE_S3_SECRET_ACCESS_KEY</code> - S3 secret access key</li> \n </ul> \n</details> \n<ol start=\"2\"> \n <li>Enable integration with Langfuse for PentAGI service in <code>.env</code> file.</li> \n</ol> \n<pre><code class=\"language-bash\">LANGFUSE_BASE_URL=http://langfuse-web:3000\nLANGFUSE_PROJECT_ID= # default: value from ${LANGFUSE_INIT_PROJECT_ID}\nLANGFUSE_PUBLIC_KEY= # default: value from ${LANGFUSE_INIT_PROJECT_PUBLIC_KEY}\nLANGFUSE_SECRET_KEY= # default: value from ${LANGFUSE_INIT_PROJECT_SECRET_KEY}\n</code></pre> \n<ol start=\"3\"> \n <li>Run the Langfuse stack:</li> \n</ol> \n<pre><code class=\"language-bash\">curl -O https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose-langfuse.yml\ndocker compose -f docker-compose.yml -f docker-compose-langfuse.yml up -d\n</code></pre> \n<p>Visit <a href=\"http://localhost:4000\">localhost:4000</a> to access Langfuse Web UI with credentials from <code>.env</code> file:</p> \n<ul> \n <li><code>LANGFUSE_INIT_USER_EMAIL</code> - Admin email</li> \n <li><code>LANGFUSE_INIT_USER_PASSWORD</code> - Admin password</li> \n</ul> \n<h3>Monitoring and Observability</h3> \n<p>For detailed system operation tracking, integration with monitoring tools is available.</p> \n<ol> \n <li>Enable integration with OpenTelemetry and all observability services for PentAGI in <code>.env</code> file.</li> \n</ol> \n<pre><code class=\"language-bash\">OTEL_HOST=otelcol:8148\n</code></pre> \n<ol start=\"2\"> \n <li>Run the observability stack:</li> \n</ol> \n<pre><code class=\"language-bash\">curl -O https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose-observability.yml\ndocker compose -f docker-compose.yml -f docker-compose-observability.yml up -d\n</code></pre> \n<p>Visit <a href=\"http://localhost:3000\">localhost:3000</a> to access Grafana Web UI.</p> \n<blockquote> \n <p>[!NOTE] If you want to use Observability stack with Langfuse, you need to enable integration in <code>.env</code> file to set <code>LANGFUSE_OTEL_EXPORTER_OTLP_ENDPOINT</code> to <code>http://otelcol:4318</code>.</p> \n <p>To run all available stacks together (Langfuse, Graphiti, and Observability):</p> \n <pre><code class=\"language-bash\">docker compose -f docker-compose.yml -f docker-compose-langfuse.yml -f docker-compose-graphiti.yml -f docker-compose-observability.yml up -d\n</code></pre> \n <p>You can also register aliases for these commands in your shell to run it faster:</p> \n <pre><code class=\"language-bash\">alias pentagi=\"docker compose -f docker-compose.yml -f docker-compose-langfuse.yml -f docker-compose-graphiti.yml -f docker-compose-observability.yml\"\nalias pentagi-up=\"docker compose -f docker-compose.yml -f docker-compose-langfuse.yml -f docker-compose-graphiti.yml -f docker-compose-observability.yml up -d\"\nalias pentagi-down=\"docker compose -f docker-compose.yml -f docker-compose-langfuse.yml -f docker-compose-graphiti.yml -f docker-compose-observability.yml down\"\n</code></pre> \n</blockquote> \n<h3>Knowledge Graph Integration (Graphiti)</h3> \n<p>PentAGI integrates with <a href=\"https://github.com/vxcontrol/pentagi-graphiti\">Graphiti</a>, a temporal knowledge graph system powered by Neo4j, to provide advanced semantic understanding and relationship tracking for AI agent operations. The vxcontrol fork provides custom entity and edge types that are specific to pentesting purposes.</p> \n<h4>What is Graphiti?</h4> \n<p>Graphiti automatically extracts and stores structured knowledge from agent interactions, building a graph of entities, relationships, and temporal context. This enables:</p> \n<ul> \n <li><strong>Semantic Memory</strong>: Store and recall relationships between tools, targets, vulnerabilities, and techniques</li> \n <li><strong>Contextual Understanding</strong>: Track how different pentesting actions relate to each other over time</li> \n <li><strong>Knowledge Reuse</strong>: Learn from past penetration tests and apply insights to new assessments</li> \n <li><strong>Advanced Querying</strong>: Search for complex patterns like \"What tools were effective against similar targets?\"</li> \n</ul> \n<h4>Enabling Graphiti</h4> \n<p>The Graphiti knowledge graph is <strong>optional</strong> and disabled by default. To enable it:</p> \n<ol> \n <li>Configure Graphiti environment variables in <code>.env</code> file:</li> \n</ol> \n<pre><code class=\"language-bash\">## Graphiti knowledge graph settings\nGRAPHITI_ENABLED=true\nGRAPHITI_TIMEOUT=30\nGRAPHITI_URL=http://graphiti:8000\nGRAPHITI_MODEL_NAME=gpt-5-mini\n\n# Neo4j settings (used by Graphiti stack)\nNEO4J_USER=neo4j\nNEO4J_DATABASE=neo4j\nNEO4J_PASSWORD=devpassword\nNEO4J_URI=bolt://neo4j:7687\n\n# OpenAI API key (required by Graphiti for entity extraction)\nOPEN_AI_KEY=your_openai_api_key\n</code></pre> \n<ol start=\"2\"> \n <li>Run the Graphiti stack along with the main PentAGI services:</li> \n</ol> \n<pre><code class=\"language-bash\"># Download the Graphiti compose file if needed\ncurl -O https://raw.githubusercontent.com/vxcontrol/pentagi/master/docker-compose-graphiti.yml\n\n# Start PentAGI with Graphiti\ndocker compose -f docker-compose.yml -f docker-compose-graphiti.yml up -d\n</code></pre> \n<ol start=\"3\"> \n <li>Verify Graphiti is running:</li> \n</ol> \n<pre><code class=\"language-bash\"># Check service health\ndocker compose -f docker-compose.yml -f docker-compose-graphiti.yml ps graphiti neo4j\n\n# View Graphiti logs\ndocker compose -f docker-compose.yml -f docker-compose-graphiti.yml logs -f graphiti\n\n# Access Neo4j Browser (optional)\n# Visit http://localhost:7474 and login with NEO4J_USER/NEO4J_PASSWORD\n\n# Access Graphiti API (optional, for debugging)\n# Visit http://localhost:8000/docs for Swagger API documentation\n</code></pre> \n<blockquote> \n <p>[!NOTE] The Graphiti service is defined in <code>docker-compose-graphiti.yml</code> as a separate stack. You must run both compose files together to enable the knowledge graph functionality. The pre-built Docker image <code>vxcontrol/graphiti:latest</code> is used by default.</p> \n</blockquote> \n<h4>What Gets Stored</h4> \n<p>When enabled, PentAGI automatically captures:</p> \n<ul> \n <li><strong>Agent Responses</strong>: All agent reasoning, analysis, and decisions</li> \n <li><strong>Tool Executions</strong>: Commands executed, tools used, and their results</li> \n <li><strong>Context Information</strong>: Flow, task, and subtask hierarchy</li> \n</ul> \n<h3>GitHub and Google OAuth Integration</h3> \n<p>OAuth integration with GitHub and Google allows users to authenticate using their existing accounts on these platforms. This provides several benefits:</p> \n<ul> \n <li>Simplified login process without need to create separate credentials</li> \n <li>Enhanced security through trusted identity providers</li> \n <li>Access to user profile information from GitHub/Google accounts</li> \n <li>Seamless integration with existing development workflows</li> \n</ul> \n<p>For using GitHub OAuth you need to create a new OAuth application in your GitHub account and set the <code>OAUTH_GITHUB_CLIENT_ID</code> and <code>OAUTH_GITHUB_CLIENT_SECRET</code> in <code>.env</code> file.</p> \n<p>For using Google OAuth you need to create a new OAuth application in your Google account and set the <code>OAUTH_GOOGLE_CLIENT_ID</code> and <code>OAUTH_GOOGLE_CLIENT_SECRET</code> in <code>.env</code> file.</p> \n<h3>Docker Image Configuration</h3> \n<p>PentAGI allows you to configure Docker image selection for executing various tasks. The system automatically chooses the most appropriate image based on the task type, but you can constrain this selection by specifying your preferred images:</p> \n<table> \n <thead> \n  <tr> \n   <th>Variable</th> \n   <th>Default</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><code>DOCKER_DEFAULT_IMAGE</code></td> \n   <td><code>debian:latest</code></td> \n   <td>Default Docker image for general tasks and ambiguous cases</td> \n  </tr> \n  <tr> \n   <td><code>DOCKER_DEFAULT_IMAGE_FOR_PENTEST</code></td> \n   <td><code>vxcontrol/kali-linux</code></td> \n   <td>Default Docker image for security/penetration testing tasks</td> \n  </tr> \n </tbody> \n</table> \n<p>When these environment variables are set, AI agents will be limited to the image choices you specify. This is particularly useful for:</p> \n<ul> \n <li><strong>Security Enforcement</strong>: Restricting usage to only verified and trusted images</li> \n <li><strong>Environment Standardization</strong>: Using corporate or customized images across all operations</li> \n <li><strong>Performance Optimization</strong>: Utilizing pre-built images with necessary tools already installed</li> \n</ul> \n<p>Configuration examples:</p> \n<pre><code class=\"language-bash\"># Using a custom image for general tasks\nDOCKER_DEFAULT_IMAGE=mycompany/custom-debian:latest\n\n# Using a specialized image for penetration testing\nDOCKER_DEFAULT_IMAGE_FOR_PENTEST=mycompany/pentest-tools:v2.0\n</code></pre> \n<blockquote> \n <p>[!NOTE] If a user explicitly specifies a particular Docker image in their task, the system will try to use that exact image, ignoring these settings. These variables only affect the system's automatic image selection process.</p> \n</blockquote> \n<h2>💻 Development</h2> \n<h3>Development Requirements</h3> \n<ul> \n <li>golang</li> \n <li>nodejs</li> \n <li>docker</li> \n <li>postgres</li> \n <li>commitlint</li> \n</ul> \n<h3>Environment Setup</h3> \n<h4>Backend Setup</h4> \n<p>Run once <code>cd backend &amp;&amp; go mod download</code> to install needed packages.</p> \n<p>For generating swagger files have to run</p> \n<pre><code class=\"language-bash\">swag init -g ../../pkg/server/router.go -o pkg/server/docs/ --parseDependency --parseInternal --parseDepth 2 -d cmd/pentagi\n</code></pre> \n<p>before installing <code>swag</code> package via</p> \n<pre><code class=\"language-bash\">go install github.com/swaggo/swag/cmd/swag@v1.8.7\n</code></pre> \n<p>For generating graphql resolver files have to run</p> \n<pre><code class=\"language-bash\">go run github.com/99designs/gqlgen --config ./gqlgen/gqlgen.yml\n</code></pre> \n<p>after that you can see the generated files in <code>pkg/graph</code> folder.</p> \n<p>For generating ORM methods (database package) from sqlc configuration</p> \n<pre><code class=\"language-bash\">docker run --rm -v $(pwd):/src -w /src --network pentagi-network -e DATABASE_URL=\"{URL}\" sqlc/sqlc generate -f sqlc/sqlc.yml\n</code></pre> \n<p>For generating Langfuse SDK from OpenAPI specification</p> \n<pre><code class=\"language-bash\">fern generate --local\n</code></pre> \n<p>and to install fern-cli</p> \n<pre><code class=\"language-bash\">npm install -g fern-api\n</code></pre> \n<h4>Testing</h4> \n<p>For running tests <code>cd backend &amp;&amp; go test -v ./...</code></p> \n<h4>Frontend Setup</h4> \n<p>Run once <code>cd frontend &amp;&amp; npm install</code> to install needed packages.</p> \n<p>For generating graphql files have to run <code>npm run graphql:generate</code> which using <code>graphql-codegen.ts</code> file.</p> \n<p>Be sure that you have <code>graphql-codegen</code> installed globally:</p> \n<pre><code class=\"language-bash\">npm install -g graphql-codegen\n</code></pre> \n<p>After that you can run:</p> \n<ul> \n <li><code>npm run prettier</code> to check if your code is formatted correctly</li> \n <li><code>npm run prettier:fix</code> to fix it</li> \n <li><code>npm run lint</code> to check if your code is linted correctly</li> \n <li><code>npm run lint:fix</code> to fix it</li> \n</ul> \n<p>For generating SSL certificates you need to run <code>npm run ssl:generate</code> which using <code>generate-ssl.ts</code> file or it will be generated automatically when you run <code>npm run dev</code>.</p> \n<h4>Backend Configuration</h4> \n<p>Edit the configuration for <code>backend</code> in <code>.vscode/launch.json</code> file:</p> \n<ul> \n <li><code>DATABASE_URL</code> - PostgreSQL database URL (eg. <code>postgres://postgres:postgres@localhost:5432/pentagidb?sslmode=disable</code>)</li> \n <li><code>DOCKER_HOST</code> - Docker SDK API (eg. for macOS <code>DOCKER_HOST=unix:///Users/&lt;my-user&gt;/Library/Containers/com.docker.docker/Data/docker.raw.sock</code>) <a href=\"https://stackoverflow.com/a/62757128/5922857\">more info</a></li> \n</ul> \n<p>Optional:</p> \n<ul> \n <li><code>SERVER_PORT</code> - Port to run the server (default: <code>8443</code>)</li> \n <li><code>SERVER_USE_SSL</code> - Enable SSL for the server (default: <code>false</code>)</li> \n</ul> \n<h4>Frontend Configuration</h4> \n<p>Edit the configuration for <code>frontend</code> in <code>.vscode/launch.json</code> file:</p> \n<ul> \n <li><code>VITE_API_URL</code> - Backend API URL. <em>Omit</em> the URL scheme (e.g., <code>localhost:8080</code> <em>NOT</em> <code>http://localhost:8080</code>)</li> \n <li><code>VITE_USE_HTTPS</code> - Enable SSL for the server (default: <code>false</code>)</li> \n <li><code>VITE_PORT</code> - Port to run the server (default: <code>8000</code>)</li> \n <li><code>VITE_HOST</code> - Host to run the server (default: <code>0.0.0.0</code>)</li> \n</ul> \n<h3>Running the Application</h3> \n<h4>Backend</h4> \n<p>Run the command(s) in <code>backend</code> folder:</p> \n<ul> \n <li>Use <code>.env</code> file to set environment variables like a <code>source .env</code></li> \n <li>Run <code>go run cmd/pentagi/main.go</code> to start the server</li> \n</ul> \n<blockquote> \n <p>[!NOTE] The first run can take a while as dependencies and docker images need to be downloaded to setup the backend environment.</p> \n</blockquote> \n<h4>Frontend</h4> \n<p>Run the command(s) in <code>frontend</code> folder:</p> \n<ul> \n <li>Run <code>npm install</code> to install the dependencies</li> \n <li>Run <code>npm run dev</code> to run the web app</li> \n <li>Run <code>npm run build</code> to build the web app</li> \n</ul> \n<p>Open your browser and visit the web app URL.</p> \n<h2>🧪 Testing LLM Agents</h2> \n<p>PentAGI includes a powerful utility called <code>ctester</code> for testing and validating LLM agent capabilities. This tool helps ensure your LLM provider configurations work correctly with different agent types, allowing you to optimize model selection for each specific agent role.</p> \n<p>The utility features parallel testing of multiple agents, detailed reporting, and flexible configuration options.</p> \n<h3>Key Features</h3> \n<ul> \n <li><strong>Parallel Testing</strong>: Tests multiple agents simultaneously for faster results</li> \n <li><strong>Comprehensive Test Suite</strong>: Evaluates basic completion, JSON responses, function calling, and penetration testing knowledge</li> \n <li><strong>Detailed Reporting</strong>: Generates markdown reports with success rates and performance metrics</li> \n <li><strong>Flexible Configuration</strong>: Test specific agents or test groups as needed</li> \n <li><strong>Specialized Test Groups</strong>: Includes domain-specific tests for cybersecurity and penetration testing scenarios</li> \n</ul> \n<h3>Usage Scenarios</h3> \n<h4>For Developers (with local Go environment)</h4> \n<p>If you've cloned the repository and have Go installed:</p> \n<pre><code class=\"language-bash\"># Default configuration with .env file\ncd backend\ngo run cmd/ctester/*.go -verbose\n\n# Custom provider configuration\ngo run cmd/ctester/*.go -config ../examples/configs/openrouter.provider.yml -verbose\n\n# Generate a report file\ngo run cmd/ctester/*.go -config ../examples/configs/deepinfra.provider.yml -report ../test-report.md\n\n# Test specific agent types only\ngo run cmd/ctester/*.go -agents simple,simple_json,primary_agent -verbose\n\n# Test specific test groups only\ngo run cmd/ctester/*.go -groups basic,advanced -verbose\n</code></pre> \n<h4>For Users (using Docker image)</h4> \n<p>If you prefer to use the pre-built Docker image without setting up a development environment:</p> \n<pre><code class=\"language-bash\"># Using Docker to test with default environment\ndocker run --rm -v $(pwd)/.env:/opt/pentagi/.env vxcontrol/pentagi /opt/pentagi/bin/ctester -verbose\n\n# Test with your custom provider configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  -v $(pwd)/my-config.yml:/opt/pentagi/config.yml \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/config.yml -agents simple,primary_agent,coder -verbose\n\n# Generate a detailed report\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  -v $(pwd):/opt/pentagi/output \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -report /opt/pentagi/output/report.md\n</code></pre> \n<h4>Using Pre-configured Providers</h4> \n<p>The Docker image comes with built-in support for major providers (OpenAI, Anthropic, Gemini, Ollama) and pre-configured provider files for additional services (OpenRouter, DeepInfra, DeepSeek, Moonshot):</p> \n<pre><code class=\"language-bash\"># Test with OpenRouter configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/openrouter.provider.yml\n\n# Test with DeepInfra configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/deepinfra.provider.yml\n\n# Test with DeepSeek configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/deepseek.provider.yml\n\n# Test with Moonshot configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/moonshot.provider.yml\n\n# Test with OpenAI configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -type openai\n\n# Test with Anthropic configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -type anthropic\n\n# Test with Gemini configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -type gemini\n\n# Test with AWS Bedrock configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -type bedrock\n\n# Test with Custom OpenAI configuration\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/custom-openai.provider.yml\n\n# Test with Ollama configuration (local inference)\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/ollama-llama318b.provider.yml\n\n# Test with Ollama Qwen3 32B configuration (requires custom model creation)\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/ollama-qwen332b-fp16-tc.provider.yml\n\n# Test with Ollama QwQ 32B configuration (requires custom model creation and 71.3GB VRAM)\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/ollama-qwq32b-fp16-tc.provider.yml\n</code></pre> \n<p>To use these configurations, your <code>.env</code> file only needs to contain:</p> \n<pre><code>LLM_SERVER_URL=https://openrouter.ai/api/v1      # or https://api.deepinfra.com/v1/openai or https://api.deepseek.com or https://api.openai.com/v1 or https://api.moonshot.ai/v1\nLLM_SERVER_KEY=your_api_key\nLLM_SERVER_MODEL=                                # Leave empty, as models are specified in the config\nLLM_SERVER_CONFIG_PATH=/opt/pentagi/conf/openrouter.provider.yml  # or deepinfra.provider.yml or deepseek.provider.yml or custom-openai.provider.yml or moonshot.provider.yml\nLLM_SERVER_PROVIDER=                             # Provider name for LiteLLM proxy (e.g., openrouter, deepseek, moonshot)\nLLM_SERVER_LEGACY_REASONING=false                # Controls reasoning format, for OpenAI must be true (default: false)\nLLM_SERVER_PRESERVE_REASONING=false              # Preserve reasoning content in multi-turn conversations (required by Moonshot, default: false)\n\n# For OpenAI (official API)\nOPEN_AI_KEY=your_openai_api_key                  # Your OpenAI API key\nOPEN_AI_SERVER_URL=https://api.openai.com/v1     # OpenAI API endpoint\n\n# For Anthropic (Claude models)\nANTHROPIC_API_KEY=your_anthropic_api_key         # Your Anthropic API key\nANTHROPIC_SERVER_URL=https://api.anthropic.com/v1  # Anthropic API endpoint\n\n# For Gemini (Google AI)\nGEMINI_API_KEY=your_gemini_api_key               # Your Google AI API key\nGEMINI_SERVER_URL=https://generativelanguage.googleapis.com  # Google AI API endpoint\n\n# For AWS Bedrock (enterprise foundation models)\nBEDROCK_REGION=us-east-1                         # AWS region for Bedrock service\nBEDROCK_ACCESS_KEY_ID=your_aws_access_key        # AWS access key ID\nBEDROCK_SECRET_ACCESS_KEY=your_aws_secret_key    # AWS secret access key\nBEDROCK_SESSION_TOKEN=your_aws_session_token     # AWS session token (alternative auth method)\nBEDROCK_SERVER_URL=                              # Optional custom Bedrock endpoint\n\n# For Ollama (local inference)\nOLLAMA_SERVER_URL=http://localhost:11434\nOLLAMA_SERVER_MODEL=llama3.1:8b-instruct-q8_0\nOLLAMA_SERVER_CONFIG_PATH=/opt/pentagi/conf/ollama-llama318b.provider.yml\nOLLAMA_SERVER_PULL_MODELS_ENABLED=false\nOLLAMA_SERVER_LOAD_MODELS_ENABLED=false\n</code></pre> \n<h4>Using OpenAI with Unverified Organizations</h4> \n<p>For OpenAI accounts with unverified organizations that don't have access to the latest reasoning models (o1, o3, o4-mini), you need to use a custom configuration.</p> \n<p>To use OpenAI with unverified organization accounts, configure your <code>.env</code> file as follows:</p> \n<pre><code class=\"language-bash\">LLM_SERVER_URL=https://api.openai.com/v1\nLLM_SERVER_KEY=your_openai_api_key\nLLM_SERVER_MODEL=                                # Leave empty, models are specified in config\nLLM_SERVER_CONFIG_PATH=/opt/pentagi/conf/custom-openai.provider.yml\nLLM_SERVER_LEGACY_REASONING=true                 # Required for OpenAI reasoning format\n</code></pre> \n<p>This configuration uses the pre-built <code>custom-openai.provider.yml</code> file that maps all agent types to models available for unverified organizations, using <code>o3-mini</code> instead of models like <code>o1</code>, <code>o3</code>, and <code>o4-mini</code>.</p> \n<p>You can test this configuration using:</p> \n<pre><code class=\"language-bash\"># Test with custom OpenAI configuration for unverified accounts\ndocker run --rm \\\n  -v $(pwd)/.env:/opt/pentagi/.env \\\n  vxcontrol/pentagi /opt/pentagi/bin/ctester -config /opt/pentagi/conf/custom-openai.provider.yml\n</code></pre> \n<blockquote> \n <p>[!NOTE] The <code>LLM_SERVER_LEGACY_REASONING=true</code> setting is crucial for OpenAI compatibility as it ensures reasoning parameters are sent in the format expected by OpenAI's API.</p> \n</blockquote> \n<h4>Using LiteLLM Proxy</h4> \n<p>When using LiteLLM proxy to access various LLM providers, model names are prefixed with the provider name (e.g., <code>moonshot/kimi-2.5</code> instead of <code>kimi-2.5</code>). To use the same provider configuration files with both direct API access and LiteLLM proxy, set the <code>LLM_SERVER_PROVIDER</code> variable:</p> \n<pre><code class=\"language-bash\"># Direct access to Moonshot API\nLLM_SERVER_URL=https://api.moonshot.ai/v1\nLLM_SERVER_KEY=your_moonshot_api_key\nLLM_SERVER_CONFIG_PATH=/opt/pentagi/conf/moonshot.provider.yml\nLLM_SERVER_PROVIDER=                             # Empty for direct access\n\n# Access via LiteLLM proxy\nLLM_SERVER_URL=http://litellm-proxy:4000\nLLM_SERVER_KEY=your_litellm_api_key\nLLM_SERVER_CONFIG_PATH=/opt/pentagi/conf/moonshot.provider.yml\nLLM_SERVER_PROVIDER=moonshot                     # Provider prefix for LiteLLM\n</code></pre> \n<p>With <code>LLM_SERVER_PROVIDER=moonshot</code>, the system automatically prefixes all model names from the configuration file with <code>moonshot/</code>, making them compatible with LiteLLM's model naming convention.</p> \n<p><strong>Supported provider names for LiteLLM:</strong></p> \n<ul> \n <li><code>openai</code> - for OpenAI models via LiteLLM</li> \n <li><code>anthropic</code> - for Anthropic/Claude models via LiteLLM</li> \n <li><code>gemini</code> - for Google Gemini models via LiteLLM</li> \n <li><code>openrouter</code> - for OpenRouter aggregator</li> \n <li><code>deepseek</code> - for DeepSeek models</li> \n <li><code>deepinfra</code> - for DeepInfra hosting</li> \n <li><code>moonshot</code> - for Moonshot AI (Kimi)</li> \n <li>Any other provider name configured in your LiteLLM instance</li> \n</ul> \n<p>This approach allows you to:</p> \n<ul> \n <li>Use the same configuration files for both direct and proxied access</li> \n <li>Switch between providers without modifying configuration files</li> \n <li>Easily test different routing strategies with LiteLLM</li> \n</ul> \n<h4>Running Tests in a Production Environment</h4> \n<p>If you already have a running PentAGI container and want to test the current configuration:</p> \n<pre><code class=\"language-bash\"># Run ctester in an existing container using current environment variables\ndocker exec -it pentagi /opt/pentagi/bin/ctester -verbose\n\n# Test specific agent types with deterministic ordering\ndocker exec -it pentagi /opt/pentagi/bin/ctester -agents simple,primary_agent,pentester -groups basic,knowledge -verbose\n\n# Generate a report file inside the container\ndocker exec -it pentagi /opt/pentagi/bin/ctester -report /opt/pentagi/data/agent-test-report.md\n\n# Access the report from the host\ndocker cp pentagi:/opt/pentagi/data/agent-test-report.md ./\n</code></pre> \n<h3>Command-line Options</h3> \n<p>The utility accepts several options:</p> \n<ul> \n <li><code>-env &lt;path&gt;</code> - Path to environment file (default: <code>.env</code>)</li> \n <li><code>-type &lt;provider&gt;</code> - Provider type: <code>custom</code>, <code>openai</code>, <code>anthropic</code>, <code>ollama</code>, <code>bedrock</code>, <code>gemini</code> (default: <code>custom</code>)</li> \n <li><code>-config &lt;path&gt;</code> - Path to custom provider config (default: from <code>LLM_SERVER_CONFIG_PATH</code> env variable)</li> \n <li><code>-tests &lt;path&gt;</code> - Path to custom tests YAML file (optional)</li> \n <li><code>-report &lt;path&gt;</code> - Path to write the report file (optional)</li> \n <li><code>-agents &lt;list&gt;</code> - Comma-separated list of agent types to test (default: <code>all</code>)</li> \n <li><code>-groups &lt;list&gt;</code> - Comma-separated list of test groups to run (default: <code>all</code>)</li> \n <li><code>-verbose</code> - Enable verbose output with detailed test results for each agent</li> \n</ul> \n<h3>Available Agent Types</h3> \n<p>Agents are tested in the following deterministic order:</p> \n<ol> \n <li><strong>simple</strong> - Basic completion tasks</li> \n <li><strong>simple_json</strong> - JSON-structured responses</li> \n <li><strong>primary_agent</strong> - Main reasoning agent</li> \n <li><strong>assistant</strong> - Interactive assistant mode</li> \n <li><strong>generator</strong> - Content generation</li> \n <li><strong>refiner</strong> - Content refinement and improvement</li> \n <li><strong>adviser</strong> - Expert advice and consultation</li> \n <li><strong>reflector</strong> - Self-reflection and analysis</li> \n <li><strong>searcher</strong> - Information gathering and search</li> \n <li><strong>enricher</strong> - Data enrichment and expansion</li> \n <li><strong>coder</strong> - Code generation and analysis</li> \n <li><strong>installer</strong> - Installation and setup tasks</li> \n <li><strong>pentester</strong> - Penetration testing and security assessment</li> \n</ol> \n<h3>Available Test Groups</h3> \n<ul> \n <li><strong>basic</strong> - Fundamental completion and prompt response tests</li> \n <li><strong>advanced</strong> - Complex reasoning and function calling tests</li> \n <li><strong>json</strong> - JSON format validation and structure tests (specifically designed for <code>simple_json</code> agent)</li> \n <li><strong>knowledge</strong> - Domain-specific cybersecurity and penetration testing knowledge tests</li> \n</ul> \n<blockquote> \n <p><strong>Note</strong>: The <code>json</code> test group is specifically designed for the <code>simple_json</code> agent type, while all other agents are tested with <code>basic</code>, <code>advanced</code>, and <code>knowledge</code> groups. This specialization ensures optimal testing coverage for each agent's intended purpose.</p> \n</blockquote> \n<h3>Example Provider Configuration</h3> \n<p>Provider configuration defines which models to use for different agent types:</p> \n<pre><code class=\"language-yaml\">simple:\n  model: \"provider/model-name\"\n  temperature: 0.7\n  top_p: 0.95\n  n: 1\n  max_tokens: 4000\n\nsimple_json:\n  model: \"provider/model-name\"\n  temperature: 0.7\n  top_p: 1.0\n  n: 1\n  max_tokens: 4000\n  json: true\n\n# ... other agent types ...\n</code></pre> \n<h3>Optimization Workflow</h3> \n<ol> \n <li><strong>Create a baseline</strong>: Run tests with default configuration to establish benchmark performance</li> \n <li><strong>Analyze agent-specific performance</strong>: Review the deterministic agent ordering to identify underperforming agents</li> \n <li><strong>Test specialized configurations</strong>: Experiment with different models for each agent type using provider-specific configs</li> \n <li><strong>Focus on domain knowledge</strong>: Pay special attention to knowledge group tests for cybersecurity expertise</li> \n <li><strong>Validate function calling</strong>: Ensure tool-based tests pass consistently for critical agent types</li> \n <li><strong>Compare results</strong>: Look for the best success rate and performance across all test groups</li> \n <li><strong>Deploy optimal configuration</strong>: Use in production with your optimized setup</li> \n</ol> \n<p>This tool helps ensure your AI agents are using the most effective models for their specific tasks, improving reliability while optimizing costs.</p> \n<h2>🧮 Embedding Configuration and Testing</h2> \n<p>PentAGI uses vector embeddings for semantic search, knowledge storage, and memory management. The system supports multiple embedding providers that can be configured according to your needs and preferences.</p> \n<h3>Supported Embedding Providers</h3> \n<p>PentAGI supports the following embedding providers:</p> \n<ul> \n <li><strong>OpenAI</strong> (default): Uses OpenAI's text embedding models</li> \n <li><strong>Ollama</strong>: Local embedding model through Ollama</li> \n <li><strong>Mistral</strong>: Mistral AI's embedding models</li> \n <li><strong>Jina</strong>: Jina AI's embedding service</li> \n <li><strong>HuggingFace</strong>: Models from HuggingFace</li> \n <li><strong>GoogleAI</strong>: Google's embedding models</li> \n <li><strong>VoyageAI</strong>: VoyageAI's embedding models</li> \n</ul> \n<details> \n <b>Embedding Provider Configuration</b> (click to expand) \n <h3>Environment Variables</h3> \n <p>To configure the embedding provider, set the following environment variables in your <code>.env</code> file:</p> \n <pre><code class=\"language-bash\"># Primary embedding configuration\nEMBEDDING_PROVIDER=openai       # Provider type (openai, ollama, mistral, jina, huggingface, googleai, voyageai)\nEMBEDDING_MODEL=text-embedding-3-small  # Model name to use\nEMBEDDING_URL=                  # Optional custom API endpoint\nEMBEDDING_KEY=                  # API key for the provider (if required)\nEMBEDDING_BATCH_SIZE=100        # Number of documents to process in a batch\nEMBEDDING_STRIP_NEW_LINES=true  # Whether to remove new lines from text before embedding\n\n# Advanced settings\nPROXY_URL=                      # Optional proxy for all API calls\n\n# SSL/TLS Certificate Configuration (for external communication with LLM backends and tool servers)\nEXTERNAL_SSL_CA_PATH=           # Path to custom CA certificate file (PEM format) inside the container\n                                # Must point to /opt/pentagi/ssl/ directory (e.g., /opt/pentagi/ssl/ca-bundle.pem)\nEXTERNAL_SSL_INSECURE=false     # Skip certificate verification (use only for testing)\n</code></pre> \n <details> \n  <b>How to Add Custom CA Certificates</b> (click to expand) \n  <p>If you see this error: <code>tls: failed to verify certificate: x509: certificate signed by unknown authority</code></p> \n  <p><strong>Step 1:</strong> Get your CA certificate bundle in PEM format (can contain multiple certificates)</p> \n  <p><strong>Step 2:</strong> Place the file in the SSL directory on your host machine:</p> \n  <pre><code class=\"language-bash\"># Default location (if PENTAGI_SSL_DIR is not set)\ncp ca-bundle.pem ./pentagi-ssl/\n\n# Or custom location (if using PENTAGI_SSL_DIR in docker-compose.yml)\ncp ca-bundle.pem /path/to/your/ssl/dir/\n</code></pre> \n  <p><strong>Step 3:</strong> Set the path in <code>.env</code> file (path must be inside the container):</p> \n  <pre><code class=\"language-bash\"># The volume pentagi-ssl is mounted to /opt/pentagi/ssl inside the container\nEXTERNAL_SSL_CA_PATH=/opt/pentagi/ssl/ca-bundle.pem\nEXTERNAL_SSL_INSECURE=false\n</code></pre> \n  <p><strong>Step 4:</strong> Restart PentAGI:</p> \n  <pre><code class=\"language-bash\">docker compose restart pentagi\n</code></pre> \n  <p><strong>Notes:</strong></p> \n  <ul> \n   <li>The <code>pentagi-ssl</code> volume is mounted to <code>/opt/pentagi/ssl</code> inside the container</li> \n   <li>You can change host directory using <code>PENTAGI_SSL_DIR</code> variable in docker-compose.yml</li> \n   <li>File supports multiple certificates and intermediate CAs in one PEM file</li> \n   <li>Use <code>EXTERNAL_SSL_INSECURE=true</code> only for testing (not recommended for production)</li> \n  </ul> \n </details> \n <h3>Provider-Specific Limitations</h3> \n <p>Each provider has specific limitations and supported features:</p> \n <ul> \n  <li><strong>OpenAI</strong>: Supports all configuration options</li> \n  <li><strong>Ollama</strong>: Does not support <code>EMBEDDING_KEY</code> as it uses local models</li> \n  <li><strong>Mistral</strong>: Does not support <code>EMBEDDING_MODEL</code> or custom HTTP client</li> \n  <li><strong>Jina</strong>: Does not support custom HTTP client</li> \n  <li><strong>HuggingFace</strong>: Requires <code>EMBEDDING_KEY</code> and supports all other options</li> \n  <li><strong>GoogleAI</strong>: Does not support <code>EMBEDDING_URL</code>, requires <code>EMBEDDING_KEY</code></li> \n  <li><strong>VoyageAI</strong>: Supports all configuration options</li> \n </ul> \n <p>If <code>EMBEDDING_URL</code> and <code>EMBEDDING_KEY</code> are not specified, the system will attempt to use the corresponding LLM provider settings (e.g., <code>OPEN_AI_KEY</code> when <code>EMBEDDING_PROVIDER=openai</code>).</p> \n <h3>Why Consistent Embedding Providers Matter</h3> \n <p>It's crucial to use the same embedding provider consistently because:</p> \n <ol> \n  <li><strong>Vector Compatibility</strong>: Different providers produce vectors with different dimensions and mathematical properties</li> \n  <li><strong>Semantic Consistency</strong>: Changing providers can break semantic similarity between previously embedded documents</li> \n  <li><strong>Memory Corruption</strong>: Mixed embeddings can lead to poor search results and broken knowledge base functionality</li> \n </ol> \n <p>If you change your embedding provider, you should flush and reindex your entire knowledge base (see <code>etester</code> utility below).</p> \n</details> \n<h3>Embedding Tester Utility (etester)</h3> \n<p>PentAGI includes a specialized <code>etester</code> utility for testing, managing, and debugging embedding functionality. This tool is essential for diagnosing and resolving issues related to vector embeddings and knowledge storage.</p> \n<details> \n <b>Etester Commands</b> (click to expand) \n <pre><code class=\"language-bash\"># Test embedding provider and database connection\ncd backend\ngo run cmd/etester/main.go test -verbose\n\n# Show statistics about the embedding database\ngo run cmd/etester/main.go info\n\n# Delete all documents from the embedding database (use with caution!)\ngo run cmd/etester/main.go flush\n\n# Recalculate embeddings for all documents (after changing provider)\ngo run cmd/etester/main.go reindex\n\n# Search for documents in the embedding database\ngo run cmd/etester/main.go search -query \"How to install PostgreSQL\" -limit 5\n</code></pre> \n <h3>Using Docker</h3> \n <p>If you're running PentAGI in Docker, you can use etester from within the container:</p> \n <pre><code class=\"language-bash\"># Test embedding provider\ndocker exec -it pentagi /opt/pentagi/bin/etester test\n\n# Show detailed database information\ndocker exec -it pentagi /opt/pentagi/bin/etester info -verbose\n</code></pre> \n <h3>Advanced Search Options</h3> \n <p>The <code>search</code> command supports various filters to narrow down results:</p> \n <pre><code class=\"language-bash\"># Filter by document type\ndocker exec -it pentagi /opt/pentagi/bin/etester search -query \"Security vulnerability\" -doc_type guide -threshold 0.8\n\n# Filter by flow ID\ndocker exec -it pentagi /opt/pentagi/bin/etester search -query \"Code examples\" -doc_type code -flow_id 42\n\n# All available search options\ndocker exec -it pentagi /opt/pentagi/bin/etester search -help\n</code></pre> \n <p>Available search parameters:</p> \n <ul> \n  <li><code>-query STRING</code>: Search query text (required)</li> \n  <li><code>-doc_type STRING</code>: Filter by document type (answer, memory, guide, code)</li> \n  <li><code>-flow_id NUMBER</code>: Filter by flow ID (positive number)</li> \n  <li><code>-answer_type STRING</code>: Filter by answer type (guide, vulnerability, code, tool, other)</li> \n  <li><code>-guide_type STRING</code>: Filter by guide type (install, configure, use, pentest, development, other)</li> \n  <li><code>-limit NUMBER</code>: Maximum number of results (default: 3)</li> \n  <li><code>-threshold NUMBER</code>: Similarity threshold (0.0-1.0, default: 0.7)</li> \n </ul> \n <h3>Common Troubleshooting Scenarios</h3> \n <ol> \n  <li><strong>After changing embedding provider</strong>: Always run <code>flush</code> or <code>reindex</code> to ensure consistency</li> \n  <li><strong>Poor search results</strong>: Try adjusting the similarity threshold or check if embeddings are correctly generated</li> \n  <li><strong>Database connection issues</strong>: Verify PostgreSQL is running with pgvector extension installed</li> \n  <li><strong>Missing API keys</strong>: Check environment variables for your chosen embedding provider</li> \n </ol> \n</details> \n<h2>🔍 Function Testing with ftester</h2> \n<p>PentAGI includes a versatile utility called <code>ftester</code> for debugging, testing, and developing specific functions and AI agent behaviors. While <code>ctester</code> focuses on testing LLM model capabilities, <code>ftester</code> allows you to directly invoke individual system functions and AI agent components with precise control over execution context.</p> \n<h3>Key Features</h3> \n<ul> \n <li><strong>Direct Function Access</strong>: Test individual functions without running the entire system</li> \n <li><strong>Mock Mode</strong>: Test functions without a live PentAGI deployment using built-in mocks</li> \n <li><strong>Interactive Input</strong>: Fill function arguments interactively for exploratory testing</li> \n <li><strong>Detailed Output</strong>: Color-coded terminal output with formatted responses and errors</li> \n <li><strong>Context-Aware Testing</strong>: Debug AI agents within the context of specific flows, tasks, and subtasks</li> \n <li><strong>Observability Integration</strong>: All function calls are logged to Langfuse and Observability stack</li> \n</ul> \n<h3>Usage Modes</h3> \n<h4>Command Line Arguments</h4> \n<p>Run ftester with specific function and arguments directly from the command line:</p> \n<pre><code class=\"language-bash\"># Basic usage with mock mode\ncd backend\ngo run cmd/ftester/main.go [function_name] -[arg1] [value1] -[arg2] [value2]\n\n# Example: Test terminal command in mock mode\ngo run cmd/ftester/main.go terminal -command \"ls -la\" -message \"List files\"\n\n# Using a real flow context\ngo run cmd/ftester/main.go -flow 123 terminal -command \"whoami\" -message \"Check user\"\n\n# Testing AI agent in specific task/subtask context\ngo run cmd/ftester/main.go -flow 123 -task 456 -subtask 789 pentester -message \"Find vulnerabilities\"\n</code></pre> \n<h4>Interactive Mode</h4> \n<p>Run ftester without arguments for a guided interactive experience:</p> \n<pre><code class=\"language-bash\"># Start interactive mode\ngo run cmd/ftester/main.go [function_name]\n\n# For example, to interactively fill browser tool arguments\ngo run cmd/ftester/main.go browser\n</code></pre> \n<details> \n <b>Available Functions</b> (click to expand) \n <h3>Environment Functions</h3> \n <ul> \n  <li><strong>terminal</strong>: Execute commands in a container and return the output</li> \n  <li><strong>file</strong>: Perform file operations (read, write, list) in a container</li> \n </ul> \n <h3>Search Functions</h3> \n <ul> \n  <li><strong>browser</strong>: Access websites and capture screenshots</li> \n  <li><strong>google</strong>: Search the web using Google Custom Search</li> \n  <li><strong>duckduckgo</strong>: Search the web using DuckDuckGo</li> \n  <li><strong>tavily</strong>: Search using Tavily AI search engine</li> \n  <li><strong>traversaal</strong>: Search using Traversaal AI search engine</li> \n  <li><strong>perplexity</strong>: Search using Perplexity AI</li> \n  <li><strong>sploitus</strong>: Search for security exploits, vulnerabilities (CVEs), and pentesting tools</li> \n  <li><strong>searxng</strong>: Search using Searxng meta search engine (aggregates results from multiple engines)</li> \n </ul> \n <h3>Vector Database Functions</h3> \n <ul> \n  <li><strong>search_in_memory</strong>: Search for information in vector database</li> \n  <li><strong>search_guide</strong>: Find guidance documents in vector database</li> \n  <li><strong>search_answer</strong>: Find answers to questions in vector database</li> \n  <li><strong>search_code</strong>: Find code examples in vector database</li> \n </ul> \n <h3>AI Agent Functions</h3> \n <ul> \n  <li><strong>advice</strong>: Get expert advice from an AI agent</li> \n  <li><strong>coder</strong>: Request code generation or modification</li> \n  <li><strong>maintenance</strong>: Run system maintenance tasks</li> \n  <li><strong>memorist</strong>: Store and organize information in vector database</li> \n  <li><strong>pentester</strong>: Perform security tests and vulnerability analysis</li> \n  <li><strong>search</strong>: Complex search across multiple sources</li> \n </ul> \n <h3>Utility Functions</h3> \n <ul> \n  <li><strong>describe</strong>: Show information about flows, tasks, and subtasks</li> \n </ul> \n</details> \n<details> \n <b>Debugging Flow Context</b> (click to expand) \n <p>The <code>describe</code> function provides detailed information about tasks and subtasks within a flow. This is particularly useful for diagnosing issues when PentAGI encounters problems or gets stuck.</p> \n <pre><code class=\"language-bash\"># List all flows in the system\ngo run cmd/ftester/main.go describe\n\n# Show all tasks and subtasks for a specific flow\ngo run cmd/ftester/main.go -flow 123 describe\n\n# Show detailed information for a specific task\ngo run cmd/ftester/main.go -flow 123 -task 456 describe\n\n# Show detailed information for a specific subtask\ngo run cmd/ftester/main.go -flow 123 -task 456 -subtask 789 describe\n\n# Show verbose output with full descriptions and results\ngo run cmd/ftester/main.go -flow 123 describe -verbose\n</code></pre> \n <p>This function allows you to identify the exact point where a flow might be stuck and resume processing by directly invoking the appropriate agent function.</p> \n</details> \n<details> \n <b>Function Help and Discovery</b> (click to expand) \n <p>Each function has a help mode that shows available parameters:</p> \n <pre><code class=\"language-bash\"># Get help for a specific function\ngo run cmd/ftester/main.go [function_name] -help\n\n# Examples:\ngo run cmd/ftester/main.go terminal -help\ngo run cmd/ftester/main.go browser -help\ngo run cmd/ftester/main.go describe -help\n</code></pre> \n <p>You can also run ftester without arguments to see a list of all available functions:</p> \n <pre><code class=\"language-bash\">go run cmd/ftester/main.go\n</code></pre> \n</details> \n<details> \n <b>Output Format</b> (click to expand) \n <p>The <code>ftester</code> utility uses color-coded output to make interpretation easier:</p> \n <ul> \n  <li><strong>Blue headers</strong>: Section titles and key names</li> \n  <li><strong>Cyan [INFO]</strong>: General information messages</li> \n  <li><strong>Green [SUCCESS]</strong>: Successful operations</li> \n  <li><strong>Red [ERROR]</strong>: Error messages</li> \n  <li><strong>Yellow [WARNING]</strong>: Warning messages</li> \n  <li><strong>Yellow [MOCK]</strong>: Indicates mock mode operation</li> \n  <li><strong>Magenta values</strong>: Function arguments and results</li> \n </ul> \n <p>JSON and Markdown responses are automatically formatted for readability.</p> \n</details> \n<details> \n <b>Advanced Usage Scenarios</b> (click to expand) \n <h3>Debugging Stuck AI Flows</h3> \n <p>When PentAGI gets stuck in a flow:</p> \n <ol> \n  <li>Pause the flow through the UI</li> \n  <li>Use <code>describe</code> to identify the current task and subtask</li> \n  <li>Directly invoke the agent function with the same task/subtask IDs</li> \n  <li>Examine the detailed output to identify the issue</li> \n  <li>Resume the flow or manually intervene as needed</li> \n </ol> \n <h3>Testing Environment Variables</h3> \n <p>Verify that API keys and external services are configured correctly:</p> \n <pre><code class=\"language-bash\"># Test Google search API configuration\ngo run cmd/ftester/main.go google -query \"pentesting tools\"\n\n# Test browser access to external websites\ngo run cmd/ftester/main.go browser -url \"https://example.com\"\n</code></pre> \n <h3>Developing New AI Agent Behaviors</h3> \n <p>When developing new prompt templates or agent behaviors:</p> \n <ol> \n  <li>Create a test flow in the UI</li> \n  <li>Use ftester to directly invoke the agent with different prompts</li> \n  <li>Observe responses and adjust prompts accordingly</li> \n  <li>Check Langfuse for detailed traces of all function calls</li> \n </ol> \n <h3>Verifying Docker Container Setup</h3> \n <p>Ensure containers are properly configured:</p> \n <pre><code class=\"language-bash\">go run cmd/ftester/main.go -flow 123 terminal -command \"env | grep -i proxy\" -message \"Check proxy settings\"\n</code></pre> \n</details> \n<details> \n <b>Docker Container Usage</b> (click to expand) \n <p>If you have PentAGI running in Docker, you can use ftester from within the container:</p> \n <pre><code class=\"language-bash\"># Run ftester inside the running PentAGI container\ndocker exec -it pentagi /opt/pentagi/bin/ftester [arguments]\n\n# Examples:\ndocker exec -it pentagi /opt/pentagi/bin/ftester -flow 123 describe\ndocker exec -it pentagi /opt/pentagi/bin/ftester -flow 123 terminal -command \"ps aux\" -message \"List processes\"\n</code></pre> \n <p>This is particularly useful for production deployments where you don't have a local development environment.</p> \n</details> \n<details> \n <b>Integration with Observability Tools</b> (click to expand) \n <p>All function calls made through ftester are logged to:</p> \n <ol> \n  <li><strong>Langfuse</strong>: Captures the entire AI agent interaction chain, including prompts, responses, and function calls</li> \n  <li><strong>OpenTelemetry</strong>: Records metrics, traces, and logs for system performance analysis</li> \n  <li><strong>Terminal Output</strong>: Provides immediate feedback on function execution</li> \n </ol> \n <p>To access detailed logs:</p> \n <ul> \n  <li>Check Langfuse UI for AI agent traces (typically at <code>http://localhost:4000</code>)</li> \n  <li>Use Grafana dashboards for system metrics (typically at <code>http://localhost:3000</code>)</li> \n  <li>Examine terminal output for immediate function results and errors</li> \n </ul> \n</details> \n<h3>Command-line Options</h3> \n<p>The main utility accepts several options:</p> \n<ul> \n <li><code>-env &lt;path&gt;</code> - Path to environment file (optional, default: <code>.env</code>)</li> \n <li><code>-provider &lt;type&gt;</code> - Provider type to use (default: <code>custom</code>, options: <code>openai</code>, <code>anthropic</code>, <code>ollama</code>, <code>bedrock</code>, <code>gemini</code>, <code>custom</code>)</li> \n <li><code>-flow &lt;id&gt;</code> - Flow ID for testing (0 means using mocks, default: <code>0</code>)</li> \n <li><code>-task &lt;id&gt;</code> - Task ID for agent context (optional)</li> \n <li><code>-subtask &lt;id&gt;</code> - Subtask ID for agent context (optional)</li> \n</ul> \n<p>Function-specific arguments are passed after the function name using <code>-name value</code> format.</p> \n<h2>🏗️ Building</h2> \n<h3>Building Docker Image</h3> \n<p>The Docker build process automatically embeds version information from git tags. To properly version your build, use the provided scripts:</p> \n<h4>Linux/macOS</h4> \n<pre><code class=\"language-bash\"># Load version variables\nsource ./scripts/version.sh\n\n# Standard build\ndocker build \\\n  --build-arg PACKAGE_VER=$PACKAGE_VER \\\n  --build-arg PACKAGE_REV=$PACKAGE_REV \\\n  -t pentagi:$PACKAGE_VER .\n\n# Multi-platform build\ndocker buildx build \\\n  --platform linux/amd64,linux/arm64 \\\n  --build-arg PACKAGE_VER=$PACKAGE_VER \\\n  --build-arg PACKAGE_REV=$PACKAGE_REV \\\n  -t pentagi:$PACKAGE_VER .\n\n# Build and push\ndocker buildx build \\\n  --platform linux/amd64,linux/arm64 \\\n  --build-arg PACKAGE_VER=$PACKAGE_VER \\\n  --build-arg PACKAGE_REV=$PACKAGE_REV \\\n  -t myregistry/pentagi:$PACKAGE_VER \\\n  --push .\n</code></pre> \n<h4>Windows (PowerShell)</h4> \n<pre><code class=\"language-powershell\"># Load version variables\n. .\\scripts\\version.ps1\n\n# Standard build\ndocker build `\n  --build-arg PACKAGE_VER=$env:PACKAGE_VER `\n  --build-arg PACKAGE_REV=$env:PACKAGE_REV `\n  -t pentagi:$env:PACKAGE_VER .\n\n# Multi-platform build\ndocker buildx build `\n  --platform linux/amd64,linux/arm64 `\n  --build-arg PACKAGE_VER=$env:PACKAGE_VER `\n  --build-arg PACKAGE_REV=$env:PACKAGE_REV `\n  -t pentagi:$env:PACKAGE_VER .\n</code></pre> \n<h4>Quick build without version</h4> \n<p>For development builds without version tracking:</p> \n<pre><code class=\"language-bash\">docker build -t pentagi:dev .\n</code></pre> \n<blockquote> \n <p>[!NOTE]</p> \n <ul> \n  <li>The build scripts automatically determine version from git tags</li> \n  <li>Release builds (on tag commit) have no revision suffix</li> \n  <li>Development builds (after tag) include commit hash as revision (e.g., <code>1.1.0-bc6e800</code>)</li> \n  <li>To use the built image locally, update the image name in <code>docker-compose.yml</code> or use the <code>build</code> option</li> \n </ul> \n</blockquote> \n<h2>👏 Credits</h2> \n<p>This project is made possible thanks to the following research and developments:</p> \n<ul> \n <li><a href=\"https://lilianweng.github.io/posts/2023-06-23-agent\">Emerging Architectures for LLM Applications</a></li> \n <li><a href=\"https://arxiv.org/abs/2403.08299\">A Survey of Autonomous LLM Agents</a></li> \n</ul> \n<h2>📄 License</h2> \n<h3>PentAGI Core License</h3> \n<p><strong>PentAGI Core</strong>: Licensed under <a href=\"https://raw.githubusercontent.com/vxcontrol/pentagi/master/LICENSE\">MIT License</a><br /> Copyright (c) 2025 PentAGI Development Team</p> \n<h3>VXControl Cloud SDK Integration</h3> \n<p><strong>VXControl Cloud SDK Integration</strong>: This repository integrates <a href=\"https://github.com/vxcontrol/cloud\">VXControl Cloud SDK</a> under a <strong>special licensing exception</strong> that applies <strong>ONLY</strong> to the official PentAGI project.</p> \n<h4>✅ Official PentAGI Project</h4> \n<ul> \n <li>This official repository: <code>https://github.com/vxcontrol/pentagi</code></li> \n <li>Official releases distributed by VXControl LLC</li> \n <li>Code used under direct authorization from VXControl LLC</li> \n</ul> \n<h4>⚠️ Important for Forks and Third-Party Use</h4> \n<p>If you fork this project or create derivative works, the VXControl SDK components are subject to <strong>AGPL-3.0</strong> license terms. You must either:</p> \n<ol> \n <li><strong>Remove VXControl SDK integration</strong></li> \n <li><strong>Open source your entire application</strong> (comply with AGPL-3.0 copyleft terms)</li> \n <li><strong>Obtain a commercial license</strong> from VXControl LLC</li> \n</ol> \n<h4>Commercial Licensing</h4> \n<p>For commercial use of VXControl Cloud SDK in proprietary applications, contact:</p> \n<ul> \n <li><strong>Email</strong>: <a href=\"mailto:info@vxcontrol.com\">info@vxcontrol.com</a></li> \n <li><strong>Subject</strong>: \"VXControl Cloud SDK Commercial License\"</li> \n</ul>",
        "source": "mshibanami.github.io",
        "published": "",
        "fetched_at": "2026-02-28T23:17:28.718174Z",
        "tags": [
          {
            "name": "transformation",
            "score": 16
          },
          {
            "name": "boundary_crossing",
            "score": 9
          },
          {
            "name": "scale_shift",
            "score": 17
          },
          {
            "name": "ontology_shift",
            "score": 8
          }
        ],
        "structural_score": 50,
        "timeliness_score": 1,
        "final_score": 25.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://github.com/moonshine-ai/moonshine",
        "title": "moonshine-ai/moonshine",
        "summary": "<p>Fast and accurate automatic speech recognition (ASR) for edge devices</p><hr /><p><img alt=\"Moonshine Voice Logo\" src=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/images/logo.png\" /></p> \n<h1>Moonshine Voice</h1> \n<p><strong>Voice Interfaces for Everyone</strong></p> \n<ul> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#quickstart\">Quickstart</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#when-should-you-choose-moonshine-over-whisper\">When should you choose Moonshine over Whisper?</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#using-the-library\">Using the Library</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#models\">Models</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#api-reference\">API Reference</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#support\">Support</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#roadmap\">Roadmap</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#acknowledgements\">Acknowledgements</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#license\">License</a></li> \n</ul> \n<p><a href=\"https://moonshine.ai\">Moonshine</a> Voice is an open source AI toolkit for developers building real-time voice applications.</p> \n<ul> \n <li>Everything runs on-device, so it's fast, private, and you don't need an account, credit card, or API keys.</li> \n <li>The framework and models are optimized for live streaming applications, offering low latency responses by doing a lot of the work while the user is still talking.</li> \n <li>All models are based on our <a href=\"https://arxiv.org/abs/2602.12241\">cutting edge research</a> and trained from scratch, so we can offer <a href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\">higher accuracy than Whisper Large V3 at the top end</a>, down to tiny 26MB models for constrained deployments.</li> \n <li>It's easy to integrate across platforms, with the same library running on <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#python\">Python</a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#ios\">iOS</a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#android\">Android</a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#macos\">MacOS</a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#linux\">Linux</a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#windows\">Windows</a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#raspberry-pi\">Raspberry Pis</a>, <a href=\"https://www.linkedin.com/posts/petewarden_most-of-the-recent-news-about-ai-seems-to-activity-7384664255242932224-v6Mr/\">IoT devices</a>, and wearables.</li> \n <li>Batteries are included. Its high-level APIs offer complete solutions for common tasks like transcription, speaker identification (diarization) and command recognition, so you don't need to be an expert to build a voice application.</li> \n <li>It supports multiple languages, including English, Spanish, Mandarin, Japanese, Korean, Vietnamese, Ukrainian, and Arabic.</li> \n</ul> \n<h2>Quickstart</h2> \n<p><a href=\"https://discord.gg/27qp9zSRXF\">Join our community on Discord to get live support</a>.</p> \n<h3>Python</h3> \n<pre><code class=\"language-bash\">pip install moonshine-voice\npython -m moonshine_voice.mic_transcriber --language en\n</code></pre> \n<p>Listens to the microphone and prints updates to the transcript as they come in.</p> \n<pre><code class=\"language-bash\">python -m moonshine_voice.intent_recognizer\n</code></pre> \n<p>Listens for user-defined action phrases, like \"Turn on the lights\", using semantic matching so natural language variations are recognized. For more, check out <a href=\"https://bit.ly/moonshine-colab\">our \"Getting Started\" Colab notebook</a> and <a href=\"https://www.youtube.com/watch?v=WH-AGvHmtoM\">video</a>.</p> \n<h3>iOS</h3> \n<p>Download <a href=\"https://github.com/moonshine-ai/moonshine/releases/latest/download/ios-examples.tar.gz\">github.com/moonshine-ai/moonshine/releases/latest/download/ios-examples.tar.gz</a>, extract it, and then open the <code>Transcriber/Transcriber.xcodeproj</code> project in Xcode.</p> \n<h3>Android</h3> \n<p>Download <a href=\"https://github.com/moonshine-ai/moonshine/releases/latest/download/android-examples.tar.gz\">github.com/moonshine-ai/moonshine/releases/latest/download/android-examples.tar.gz</a>, extract it, and then open the <code>Transcriber</code> folder in Android Studio.</p> \n<h3>Linux</h3> \n<p><a href=\"https://github.com/moonshine-ai/moonshine/archive/refs/heads/main.zip\">Download</a> or <code>git clone</code> this repository and then run:</p> \n<pre><code class=\"language-bash\">cd core\nmkdir build\ncmake ..\ncmake --build .\n./moonshine-cpp-test\n</code></pre> \n<h3>MacOS</h3> \n<p>Download <a href=\"https://github.com/moonshine-ai/moonshine/releases/latest/download/macos-examples.tar.gz\">github.com/moonshine-ai/moonshine/releases/latest/download/macos-examples.tar.gz</a>, extract it, and then open the <code>MicTranscription/MicTranscription.xcodeproj</code> project in Xcode.</p> \n<h3>Windows</h3> \n<p>Download <a href=\"https://github.com/moonshine-ai/moonshine/releases/latest/download/windows-examples.tar.gz\">github.com/moonshine-ai/moonshine/releases/latest/download/windows-examples.tar.gz</a>, extract it, and then open the <code>cli-transcriber\\cli-transcriber.vcxproj</code> project in Visual Studio.</p> \n<p><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#python\">Install Moonshine in Python</a> for model downloading.</p> \n<p>In the terminal:</p> \n<pre><code class=\"language-batch\">pip install moonshine-voice\ncd examples\\windows\\cli-transcriber\n.\\download-lib.bat\nmsbuild cli-transcriber.sln /p:Configuration=Release /p:Platform=x64\npython -m moonshine_voice.download --language en\nx64\\Release\\cli-transcriber.exe --model-path &lt;path from the download command&gt; --model-arch &lt;number from the download command&gt;\n</code></pre> \n<h3>Raspberry Pi</h3> \n<p>You'll need a USB microphone plugged in to get audio input, but the Python pip package has been optimized for the Pi, so you can run:</p> \n<pre><code class=\"language-bash\"> sudo pip install --break-system-packages moonshine-voice\n python -m moonshine_voice.mic_transcriber --language en\n</code></pre> \n<p>I've recorded <a href=\"https://www.youtube.com/watch?v=NNcqx1wFxl0\">a screencast on YouTube</a> to help you get started, and you can also download <a href=\"https://github.com/moonshine-ai/moonshine/releases/latest/download/raspberry-pi-examples.tar.gz\">github.com/moonshine-ai/moonshine/releases/latest/download/raspberry-pi-examples.tar.gz</a> for some fun, Pi-specific examples. <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/raspberry-pi/my-dalek/README.md\">The README</a> has information about using a virtual environment for the Python install if you don't want to use <code>--break-system-packages</code>.</p> \n<h2>When should you choose Moonshine over Whisper?</h2> \n<p>TL;DR - When you're working with live speech.</p> \n<table> \n <thead> \n  <tr> \n   <th>Model</th> \n   <th>WER</th> \n   <th># Parameters</th> \n   <th>MacBook Pro</th> \n   <th>Linux x86</th> \n   <th>R. Pi 5</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>Moonshine Medium Streaming</td> \n   <td>6.65%</td> \n   <td>245 million</td> \n   <td>107ms</td> \n   <td>269ms</td> \n   <td>802ms</td> \n  </tr> \n  <tr> \n   <td>Whisper Large v3</td> \n   <td>7.44%</td> \n   <td>1.5 billion</td> \n   <td>11,286ms</td> \n   <td>16,919ms</td> \n   <td>N/A</td> \n  </tr> \n  <tr> \n   <td>Moonshine Small Streaming</td> \n   <td>7.84%</td> \n   <td>123 million</td> \n   <td>73ms</td> \n   <td>165ms</td> \n   <td>527ms</td> \n  </tr> \n  <tr> \n   <td>Whisper Small</td> \n   <td>8.59%</td> \n   <td>244 million</td> \n   <td>1940ms</td> \n   <td>3,425ms</td> \n   <td>10,397ms</td> \n  </tr> \n  <tr> \n   <td>Moonshine Tiny Streaming</td> \n   <td>12.00%</td> \n   <td>34 million</td> \n   <td>34ms</td> \n   <td>69ms</td> \n   <td>237ms</td> \n  </tr> \n  <tr> \n   <td>Whisper Tiny</td> \n   <td>12.81%</td> \n   <td>39 million</td> \n   <td>277ms</td> \n   <td>1,141ms</td> \n   <td>5,863ms</td> \n  </tr> \n </tbody> \n</table> \n<p><em>See <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#benchmarks\">benchmarks</a> for how these numbers were measured.</em></p> \n<p><a href=\"\">OpenAI's release of their Whisper family of models</a> was a massive step forward for open-source speech to text. They offered a range of sizes, allowing developers to trade off compute and storage space against accuracy to fit their applications. Their biggest models, like Large v3, also gave accuracy scores that were higher than anything available outside of large tech companies like Google or Apple. At Moonshine we were early and enthusiastic adopters of Whisper, and we still remain big fans of the models and the great frameworks like <a href=\"https://github.com/SYSTRAN/faster-whisper\">FasterWhisper</a> and others that have been built around them.</p> \n<p>However, as we built applications that needed a live voice interface we found we needed features that weren't available through Whisper:</p> \n<ul> \n <li><strong>Whisper always operates on a 30-second input window</strong>. This isn't an issue when you're processing audio in large batches, you can usually just look ahead in the file and find a 30-second-ish chunk of speech to apply it to. Voice interfaces can't look ahead to create larger chunks from their input stream, and phrases are seldom longer than five to ten seconds. This means there's a lot of wasted computation encoding zero padding in the encoder and decoder, which means longer latency in returning results. Since one of the most important requirements for any interface is responsiveness, usually defined as latency below 200ms, this hurts the user experience even on platforms that have compute to spare, and makes it unusable on more constrained devices.</li> \n <li><strong>Whisper doesn't cache anything</strong>. Another common requirement for voice interfaces is that they display feedback as the user is talking, so that they know the app is listening and understanding them. This means calling the speech to text model repeatedly over time as a sentence is spoken. Most of the audio input is the same, with only a short addition to the end. Even though a lot of the input is constant, Whisper starts from scratch every time, doing a lot of redundant work on audio that it has seen before. Like the fixed input window, this unnecessary latency impairs the user experience.</li> \n <li><strong>Whisper supports a lot of languages poorly</strong>. Whisper's multilingual support is an incredible feat of engineering, and demonstrated a single model could handle many languages, and even offer translations. This chart from OpenAI (<a href=\"https://cdn.openai.com/papers/whisper.pdf\">raw data in Appendix D-2.4</a>) shows the drop-off in Word Error Rate (WER) with the very largest 1.5 billion parameter model.</li> \n</ul> \n<p><img alt=\"Language Chart\" src=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/images/lang-chart.png\" /></p> \n<p>82 languages are listed, but only 33 have sub-20% WER (what we consider usable). For the Base model size commonly used on edge devices, only 5 languages are under 20% WER. Asian languages like Korean and Japanese stand out as the native tongue of large markets with a lot of tech innovation, but Whisper doesn't offer good enough accuracy to use in most applications The proprietary in-house versions of Whisper that are available through OpenAI's cloud API seem to offer better accuracy, but aren't available as open models.</p> \n<ul> \n <li><strong>Fragmented edge support</strong>. A fantastic ecosystem has grown up around Whisper, there are a lot of mature frameworks you can use to deploy the models. However these often tend to be focused on desktop-class machines and operating systems. There are projects you can use across edge platforms like iOS, Android, or Raspberry Pi OS, but they tend to have different interfaces, capabilities, and levels of optimization. This made building applications that need to run on a variety of devices unnecessarily difficult.</li> \n</ul> \n<p>All these limitations drove us to create our own family of models that better meet the needs of live voice interfaces. It took us some time since the combined size of the open speech datasets available is tiny compared to the amount of web-derived text data, but after extensive data-gathering work, we were able to release <a href=\"https://arxiv.org/abs/2410.15608\">the first generation of Moonshine models</a>. These removed the fixed-input window limitation along with some other architectural improvements, and gave significantly lower latency than Whisper in live speech applications, often running 5x faster or more.</p> \n<p>However we kept encountering applications that needed even lower latencies on even more constrained platforms. We also wanted to offer higher accuracy than the Base-equivalent that was the top end of the initial models. That led us to this second generation of Moonshine models, which offer:</p> \n<ul> \n <li><strong>Flexible input windows</strong>. You can supply any length of audio (though we recommend staying below around 30 seconds) and the model will only spend compute on that input, no zero-padding required. This gives us a significant latency boost.</li> \n <li><strong>Caching for streaming</strong>. Our models now support incremental addition of audio over time, and they cache the input encoding and part of the decoder's state so that we're able to skip even more of the compute, driving latency down dramatically.</li> \n <li><strong>Language-specific models</strong>. We have gathered data and trained models for multiple languages, including Arabic, Japanese, Korean, Spanish, Ukrainian, Vietnamese, and Chinese. As we discuss in our <a href=\"https://arxiv.org/abs/2509.02523\">Flavors of Moonshine paper</a>, we've found that we can get much higher accuracy for the same size and compute if we restrict a model to focus on just one language, compared to training one model across many.</li> \n <li><strong>Cross-platform library support</strong>. We're building applications ourselves, and needed to be able to deploy these models across Linux, MacOS, Windows, iOS, and Android, as well as use them from languages like Python, Swift, Java, and C++. To support this we architected a portable C++ core library that handles all of the processing, uses OnnxRuntime for good performance across systems, and then built native interfaces for all the required high-level languages. This allows developers to learn one API, and then deploy it almost anywhere they want to run.</li> \n <li><strong>Better accuracy than Whisper V3 Large</strong>. On <a href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\">HuggingFace's OpenASR leaderboard</a>, our newest streaming model for English, Medium Streaming, achieves a lower word-error rate than the most-accurate Whisper model from OpenAI. This is despite Moonshine's version using 250 million parameters, versus Large v3's 1.5 billion, making it much easier to deploy on the edge.</li> \n</ul> \n<p>Hopefully this gives you a good idea of how Moonshine compares to Whisper. If you're working with GPUs in the cloud on data in bulk where throughput is most important then Whisper (or Nvidia alternatives like Parakeet) offer advantages like batch processing, but we believe we can't be beat for live speech. We've built the framework and models we wished we'd had when we first started building applications with voice interfaces, so if you're working with live voice inputs, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#quickstart\">give Moonshine a try</a>.</p> \n<h2>Using the Library</h2> \n<p>The Moonshine API is designed to take care of the details around capturing and transcribing live speech, giving application developers a high-level API focused on actionable events. I'll use Python to illustrate how it works, but the API is consistent across all the supported languages.</p> \n<ul> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#architecture\">Architecture</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#concepts\">Concepts</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#getting-started-with-transcription\">Getting Started with Transcription</a> \n  <ul> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcription-event-flow\">Transcription Event Flow</a></li> \n  </ul> </li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#getting-started-with-command-recognition\">Getting Started with Command Recognition</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#examples\">Examples</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#adding-the-library-to-your-own-app\">Adding the Library to your own App</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#python-1\">Python</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#ios-or-macos\">iOS or MacOS</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#android-1\">Android</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#windowsc\">Windows</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#debugging\">Debugging</a> \n  <ul> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#console-logs\">Console Logs</a></li> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#input-saving\">Input Saving</a></li> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#api-call-logging\">API Call Logging</a></li> \n  </ul> </li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#building-from-source\">Building from Source</a> \n  <ul> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#cmake\">Cmake</a></li> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#language-bindings\">Language Bindings</a></li> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#porting\">Porting</a></li> \n  </ul> </li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#downloading-models\">Downloading Models</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#benchmarking\">Benchmarking</a></li> \n</ul> \n<h3>Architecture</h3> \n<p>Our goal is to build a framework that any developer can pick up and use, even with no previous experience of speech technologies. We've abstracted away a lot of the unnecessary details and provide a simple interface that lets you focus on building your application, and that's reflected in our system architecture.</p> \n<p>The basic flow is:</p> \n<ul> \n <li>Create a <code>Transcriber</code> or <code>IntentRecognizer</code> object, depending on whether you want the text that's spoken, or just to know that a user has requested an action.</li> \n <li>Attach an <code>EventListener</code> that gets called when important things occur, like the end of a phrase or an action being triggered, so your application can respond.</li> \n</ul> \n<p>Traditionally, adding a voice interface to an application or product required integrating a lot of different libraries to handle all the processing that's needed to capture audio and turn it into something actionable. The main steps involved are microphone capture, voice activity detection (to break a continuous stream of audio into sections of speech), speech to text, speaker identification, and intent recognition. Each of these steps typically involved a different framework, which greatly increased the complexity of integrating, optimizing, and maintaining these dependencies.</p> \n<p>Moonshine Voice includes all of these stages in a single library, and abstracts away everything but the essential information your application needs to respond to user speech, whether you want to transcribe it or trigger actions.</p> \n<p><img alt=\"Moonshine Voice Architecture\" src=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/images/moonshine-voice-architecture.png\" /></p> \n<p>Most developers should be able to treat the library as a black box that tells them when something interesting has happened, using our event-based classes to implement application logic. Of course the framework is fully open source, so speech experts can dive as deep under the hood as they'd like, but it's not necessary to use it.</p> \n<h3>Concepts</h3> \n<p>A <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/src/moonshine_voice/transcriber.py#L66\"><strong>Transcriber</strong></a> takes in audio input and turns any speech into text. This is the first object you'll need to create to use Moonshine, and you'll give it a path to <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#downloading-models\">the models you've downloaded</a>.</p> \n<p>A <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/src/moonshine_voice/mic_transcriber.py#L10\"><strong>MicTranscriber</strong></a> is a helper class based on the general transcriber that takes care of connecting to a microphone using your platform's built-in support (for example sounddevice in Python) and then feeding the audio in as it's captured.</p> \n<p>A <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/src/moonshine_voice/transcriber.py#L297\"><strong>Stream</strong></a> is a handler for audio input. The reason streams exist is because you may want to process multiple audio inputs at once, and a transcriber can support those through multiple streams, without duplicating the model resources. If you only have one input, the transcriber class includes the same methods (start/stop/add_audio) as a stream, and you can use that interface instead and forget about streams.</p> \n<p>A <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/src/moonshine_voice/moonshine_api.py#L51\"><strong>TranscriptLine</strong></a> is a data structure holding information about one line in the transcript. When someone is speaking, the library waits for short pauses (where punctuation might go in written language) and starts a new line. These aren't exactly sentences, since a speech pause isn't a sure sign of the end of a sentence, but this does break the spoken audio into segments that can be considered phrases. A line includes state such as whether the line has just started, is still being spoken, or is complete, along with its start time and duration.</p> \n<p>A <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/src/moonshine_voice/moonshine_api.py#67\"><strong>Transcript</strong></a> is a list of lines in time order holding information about what text has already been recognized, along with other state like when it was captured.</p> \n<p>A <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/src/moonshine_voice/transcriber.py#L22\"><strong>TranscriptEvent</strong></a> contains information about changes to the transcript. Events include a new line being started, the text in a line being updated, and a line being completed. The event object includes the transcript line it's referring to as a member, holding the latest state of that line.</p> \n<p>A <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/src/moonshine_voice/transcriber.py#L266\"><strong>TranscriptEventListener</strong></a> is a protocol that allows app-defined functions to be called when transcript events happen. This is the main way that most applications interact with the results of the transcription. When live speech is happening, applications usually need to respond or display results as new speech is recognized, and this approach allows you to handle those changes in a similar way to events from traditional user interfaces like touch screen gestures or mouse clicks on buttons.</p> \n<p>An <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/src/moonshine_voice/intent_recognizer.py#L44\"><strong>IntentRecognizer</strong></a> is a type of TranscriptEventListener that allows you to invoke different callback functions when preprogrammed intents are detected. This is useful for building voice command recognition features.</p> \n<h3>Getting Started with Transcription</h3> \n<p>We have <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#examples\">examples</a> for most platforms so as a first step I recommend checking out what we have for the systems you're targeting.</p> \n<p>Next, you'll need to <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#adding-the-library-to-your-own-app\">add the library to your project</a>. We aim to provide pre-built binaries for all major platforms using their native package managers. On Python this means a pip install, for Android it's a Maven package, and for MacOS and iOS we provide a Swift package through SPM.</p> \n<p>The transcriber needs access to the files for the model you're using, so after <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#downloading-models\">downloading them</a> you'll need to place them somewhere the application can find them, and make a note of the path. This usually means adding them as resources in your IDE if you're planning to distribute the app, or you can use hard-wired paths if you're just experimenting. The download script gives you the location of the models and their architecture type on your drive after it completes.</p> \n<p>Now you can try creating a transcriber. Here's what that looks like in Python:</p> \n<pre><code class=\"language-python\">transcriber = Transcriber(model_path=model_path, model_arch=model_arch)\n</code></pre> \n<p>If the model isn't found, or if there's any other error, this will throw an exception with information about the problem. You can also check the console for logs from the core library, these are printed to <code>stderr</code> or your system's equivalent.</p> \n<p>Now we'll create a listener that contains the app logic that you want triggered when the transcript updates, and attach it to your transcriber:</p> \n<pre><code class=\"language-python\">class TestListener(TranscriptEventListener):\n    def on_line_started(self, event):\n        print(f\"Line started: {event.line.text}\")\n\n    def on_line_text_changed(self, event):\n        print(f\"Line text changed: {event.line.text}\")\n\n    def on_line_completed(self, event):\n        print(f\"Line completed: {event.line.text}\")\n\ntranscriber.add_listener(listener)\n</code></pre> \n<p>The transcriber needs some audio data to work with. If you want to try it with the microphone you can update your transcriber creation line to use a MicTranscriber instead, but if you want to start with a .wav file for testing purposes here's how you feed that in:</p> \n<pre><code class=\"language-python\">    audio_data, sample_rate = load_wav_file(wav_path)\n\n    transcriber.start()\n\n    # Loop through the audio data in chunks to simulate live streaming\n    # from a microphone or other source.\n    chunk_duration = 0.1\n    chunk_size = int(chunk_duration * sample_rate)\n    for i in range(0, len(audio_data), chunk_size):\n        chunk = audio_data[i: i + chunk_size]\n        transcriber.add_audio(chunk, sample_rate)\n\n    transcriber.stop()\n</code></pre> \n<p>The important things to notice here are:</p> \n<ul> \n <li>We create an array of mono audio data from a wav file, using the convenience <code>load_wav_file()</code> function that's part of the Moonshine library.</li> \n <li>We start the transcriber to activate its processing code.</li> \n <li>The loop adds audio in chunks. These chunks can be any length and any sample rate, the library takes care of all the housekeeping.</li> \n <li>As audio is added, the event listener you added will be called, giving information about the latest speech.</li> \n</ul> \n<p>In a real application you'd be calling <code>add_audio()</code> from an audio handler that's receiving it from your source. Since the library can handle arbitrary durations and sample rates, just make sure it's mono and otherwise feed it in as-is.</p> \n<p>The transcriber analyses the speech at a default interval of every 500ms of input. You can change this with the <code>update_interval</code> argument to the transcriber constructor. For streaming models most of the work is done as the audio is being added, and it's automatically done at the end of a phrase, so changing this won't usually affect the workload or latency massively.</p> \n<p>The key takeaway is that you usually don't need to worry about the transcript data structure itself, the event system tells you when something important happens. You can manually trigger a transcript update by calling <code>update_transcription()</code> which returns a transcript object with all of the information about the current session if you do need to examine the state.</p> \n<p>By calling <code>start()</code> and <code>stop()</code> on a transcriber (or stream) we're beginning and ending a session. Each session has one transcript document associated with it, and it is started fresh on every <code>start()</code> call, so you should make copies of any data you need from the transcript object before that.</p> \n<p>The transcriber class also offers a simpler <code>transcribe_without_streaming()</code> method, for when you have an array of data from the past that you just want to analyse, such as a file or recording.</p> \n<p>We also offer a specialization of the base <code>Transcriber</code> class called <code>MicTranscriber</code>. How this is implemented will depend on the language and platform, but it should provide a transcriber that's automatically attached to the main microphone on the system. This makes it straightforward to start transcribing speech from that common source, since it supports all of the same listener callbacks as the base class.</p> \n<h4>Transcription Event Flow</h4> \n<p>The main communication channel between the library and your application is through events that are passed to any listener functions you have registered. There are four major event types:</p> \n<ul> \n <li><code>LineStarted</code>. This is sent to listeners when the beginning of a new speech segment is detected. It may or may not contain any text, but since it's dispatched near the start of an utterance, that text is likely to change over time.</li> \n <li><code>LineUpdated</code>. Called whenever any of the information about a line changes, including the duration, audio data, and text.</li> \n <li><code>LineTextChanged</code>. Called only when the text associated with a line is updated. This is a subset of <code>LineUpdated</code> that focuses on the common need to refresh the text shown to users as often as possible to keep the experience interactive.</li> \n <li><code>LineCompleted</code>. Sent when we detect that someone has paused speaking, and we've ended the current segment. The line data structure has the final values for the text, duration, and speaker ID.</li> \n</ul> \n<p>We offer some guarantees about these events:</p> \n<ul> \n <li><code>LineStarted</code> is always called exactly once for any segment.</li> \n <li><code>LineCompleted</code> is always called exactly once after <code>LineStarted</code> for any segment.</li> \n <li><code>LineUpdated</code> and <code>LineTextChanged</code> will only ever be called after the <code>LineStarted</code> and before the <code>LineCompleted</code> events for a segment.</li> \n <li>Those update events are not guaranteed to be called (and in practice can be disabled by setting <code>update_interval</code> to a very large value).</li> \n <li>There will only be one line active at any one time for any given stream.</li> \n <li>Once <code>LineCompleted</code> has been called, the library will never alter that line's data again.</li> \n <li>If <code>stop()</code> is called on a transcriber or stream, any active lines will have <code>LineCompleted</code> called.</li> \n <li>Each line has a 64-bit <code>lineId</code> that is designed to be unique enough to avoid collisions.</li> \n <li>This <code>lineId</code> remains the same for the line over time, from the first <code>LineStarted</code> event onwards.</li> \n</ul> \n<h3>Getting Started with Command Recognition</h3> \n<p>If you want your application to respond when users talk, you need to understand what they're saying. The previous generation of voice interfaces could only recognize speech that was phrased in exactly the form they expected. For example \"Alexa, turn on living-room lights\" might work, but \"Alexa, lights on in the living room please\" might not. The general problem of figuring out what a user wants from natural speech is known as intent recognition. There have been decades of research into this area, but the rise of transformer-based LLMs has given us new tools. We have integrated some of these advances into Moonshine Voice's command recognition API.</p> \n<p>The basic idea is that your application registers some general actions you're interested in, like \"Turn the lights on\" or \"Move left\", and then Moonshine sends an event when the user says something that matches the meaning of those phrases. It works a lot like a graphical user interface - you define a button (action) and an event callback that is triggered when the user presses that button.</p> \n<p>To give it a try for yourself, run this built-in example:</p> \n<pre><code class=\"language-bash\">python -m moonshine_voice.intent_recognizer\n</code></pre> \n<p>This will present you with a menu of command phrases, and then start listening to the microphone. If you say something that's a variant on one of the phrases you'll see a \"triggered\" log message telling you which action was matched, along with how confident the system is in the match.</p> \n<pre><code class=\"language-bash\">📝 Let there be light.\n'TURN ON THE LIGHTS' triggered by 'Let there be light.' with 76% confidence\n</code></pre> \n<p>To show that you can modify these at run time, try supplying your own list of phrases as a comma-separated string argument to <code>--intents</code>.</p> \n<pre><code class=\"language-bash\">python -m moonshine_voice.intent_recognizer --intents \"Turn left, turn right, go backwards, go forward\"\n</code></pre> \n<p>This could be the core command set to control a robot's movement for example. It's worth spending a bit of time experimenting with different wordings of the command phrases, and different variations on the user side, to get a feel for how the system works.</p> \n<p>Under the hood this is all accomplished using two main classes. We've met the <code>MicTranscriber</code> above, the new addition is <code>IntentRecognizer</code>. This listens to the results of the transcriber, fuzzily matches completed lines against any intents that have been registered with it, and calls back the client-supplied code.</p> \n<p>The fuzzy matching uses a sentence-embedding model based on Gemma300m, so the first step is downloading it and getting the path:</p> \n<pre><code class=\"language-python\">embedding_model_path, embedding_model_arch = get_embedding_model(\n    args.embedding_model, args.quantization\n)\n</code></pre> \n<p>Once we have the model's location, we create an <code>IntentRecognizer</code> using that path. The only other argument is the <code>threshold</code> we use for fuzzy matching. It's between 0 and 1, with low numbers producing more matches but at the cost of less accuracy, and vice versa for high values.</p> \n<pre><code class=\"language-python\">intent_recognizer = IntentRecognizer(\n    model_path=embedding_model_path,\n    model_arch=embedding_model_arch,\n    model_variant=args.quantization,\n    threshold=args.threshold,\n)\n</code></pre> \n<p>Next we tell the recognizer what kinds of phrases to listen out for, and what to do when there's a match.</p> \n<pre><code class=\"language-python\">def on_intent_triggered_on(trigger: str, utterance: str, similarity: float):\n    print(f\"\\n'{trigger.upper()}' triggered by '{utterance}' with {similarity:.0%} confidence\")\n\nfor intent in intents:\n    intent_recognizer.register_intent(intent, on_intent_triggered_on)\n</code></pre> \n<p>The recognizer supports the transcript event listener interface, so the final stage is adding it as a listener to the <code>MicTranscriber</code>.</p> \n<pre><code class=\"language-python\">mic_transcriber.add_listener(intent_recognizer)\n</code></pre> \n<p>Once you start the transcriber, it will listen out for any variations on the supplied phrases, and call <code>on_intent_triggered_on()</code> whenever there's a match.</p> \n<p>The current intent recognition is designed for full-sentence matching, which works well for straightforward commands, but we will be expanding into more advanced \"slot filling\" techniques in the future, to handle extracting the quantity from \"I want ten bananas\" for example.</p> \n<h3>Examples</h3> \n<p>The <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/\"><code>examples</code></a> folder has code samples organized by platform. We offer these for <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/android/\">Android</a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/c++/\">portable C++</a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/ios/\">iOS</a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/macos/\">MacOS</a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/python\">Python</a>, and <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/windows/\">Windows</a>. We have tried to use the most common build system for each platform, so Android uses Android Studio and Maven, iOS and MacOS use Xcode and Swift, while Windows uses Visual Studio.</p> \n<p>The examples usually include one minimal project that just creates a transcriber and then feeds it data from a WAV file, and another that's pulling audio from a microphone using the platform's default framework for accessing audio devices.</p> \n<h3>Adding the Library to your own App</h3> \n<p>We distribute the library through the most widely-used package managers for each platform. Here's how you can use these to add the framework to an existing project on different systems.</p> \n<h4>Python</h4> \n<p>The Python package is <a href=\"https://pypi.org/project/moonshine-voice/\">hosted on PyPi</a>, so all you should need to do to install it is <code>pip install moonshine-voice</code>, and then <code>import moonshine_voice</code> in your project.</p> \n<h4>iOS or MacOS</h4> \n<p>For iOS we use the Swift Package Manager, with <a href=\"https://github.com/moonshine-ai/moonshine-swift/\">an auto-updated GitHub repository</a> holding each version. To use this right-click on the file view sidebar in Xcode and choose \"Add Package Dependencies...\" from the menu. A dialog should open up, paste <code>https://github.com/moonshine-ai/moonshine-swift/</code> into the top search box and you should see <code>moonshine-swift</code>. Select it and choose \"Add Package\", and it should be added to your project. You should now be able to <code>import MoonshineVoice</code> and use the library. You will need to add any model files you use to your app bundle and ensure they're copied during the deployment phase, so they can be accessed on-device.</p> \n<p>For reference purposes you can find Xcode projects with these changes applied in <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/ios/Transcriber\"><code>examples/ios/Transcriber</code></a> and <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/macos/BasicTranscription/\"><code>examples/macos/BasicTranscription</code></a>.</p> \n<h4>Android</h4> \n<p>On Android we publish <a href=\"https://mvnrepository.com/artifact/ai.moonshine/moonshine-voice\">the package to Maven</a>. To include it in your project using Android Studio and Gradle, first add the version number you want to the <code>gradle/libs.versions.toml</code> file by inserting a line in the <code>[versions]</code> section, for example <code>moonshineVoice = \"0.0.49\"</code>. Then in the <code>[libraries]</code> part, add a reference to the package: <code>moonshine-voice = { group = \"ai.moonshine\", name = \"moonshine-voice\", version.ref = \"moonshineVoice\" }</code>.</p> \n<p>Finally, in your <code>app/build.gradle.kts</code> add the library to the <code>dependencies</code> list: <code>implementation(libs.moonshine.voice)</code>. You can find a working example of all these changes in [<code>examples/android/Transcriber</code>].</p> \n<h4>Windows/C++</h4> \n<p>We couldn't find a single package manager that is used by most Windows developers, so instead we've made the raw library and headers available as a download. The script in <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/windows/cli-transcriber/download-lib.bat\"><code>examples/windows/cli-transcriber/download-lib.bat</code></a> will fetch these for you. You'll see an <code>include</code> folder that you should add to the include search paths in your project settings, and a <code>lib</code> directory that you should add to the include search paths. Then add all of the library files in the <code>lib</code> folder to your project's linker dependencies.</p> \n<p>The recommended interface to use on Windows is the C++ language binding. This is a header-only library that offers a higher-level API than the underlying C version. You can <code>#include \"moonshine-cpp.h\"</code> to access Moonshine from your C++ code. If you want to see an example of all these changes together, take a look at <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/examples/windows/cli-transcriber\"><code>examples/windows/cli-transcriber</code></a>.</p> \n<h3>Debugging</h3> \n<h4>Console Logs</h4> \n<p>The library is designed to help you understand what's going wrong when you hit an issue. If something isn't working as expected, the first place to look is the console for log messages. Whenever there's a failure point or an exception within the core library, you should see a message that adds more information about what went wrong. Your language bindings should also recognize when the core library has returned an error and raise an appropriate exception, but sometimes the logs can be helpful because they contain more details.</p> \n<h4>Input Saving</h4> \n<p>If no errors are being reported but the quality of the transcription isn't what you expect, it's worth ruling out an issue with the audio data that the transcriber is receiving. To make this easier, you can pass in the <code>save_input_wav_path</code> option when you create a transcriber. That will save any audio received into .wav files in the folder you specify. Here's a Python example:</p> \n<pre><code class=\"language-python\">python -m moonshine_voice.transcriber --options='save_input_wav_path=.'\n</code></pre> \n<p>This will run test audio through a transcriber, and write out the audio it has received into an <code>input_1.wav</code> file in the current directory. If you're running multiple streams, you'll see <code>input_2.wav</code>, etc for each additional one. These wavs only contain the audio data from the latest session, and are overwritten after each one is started. Listening to these files should help you confirm that the input you're providing is as you expect it, and not distorted or corrupted.</p> \n<h4>API Call Logging</h4> \n<p>If you're running into errors it can be hard to keep track of the timeline of your interactions with the library. The <code>log_api_calls</code> option will print out the underlying API calls that have been triggered to the console, so you can investigate any ordering or timing issues.</p> \n<pre><code class=\"language-python\">uv run -m moonshine_voice.transcriber --options='log_api_calls=true'\n</code></pre> \n<h3>Building from Source</h3> \n<p>If you want to debug into the library internals, or add instrumentation to help understand its operation, or add improvements or customizations, all of the source is available for you to build it for yourself.</p> \n<h4>Cmake</h4> \n<p>The core engine of the library is contained in the <code>core</code> folder of this repo. It's written in C++ with a C interface for easy integration with other languages. We use cmake to build on all our platforms, and so the easiest way to get started is something like this:</p> \n<pre><code class=\"language-bash\">cd core\nmkdir -p build\ncd build\ncmake ..\ncmake --build .\n</code></pre> \n<p>After that completes you should have a set of binary executables you can run on your own system. These executables are all unit tests, and expect to be run from the <code>test-assets</code> folder. You can run the build and test process in one step using the <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/scripts/run-core-tests.sh\"><code>scripts/run-core-tests.sh</code></a>, or <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/scripts/run-core-tests.bat\"><code>scripts/run-core-tests.bat</code></a> for Windows. All tests should compile and run without any errors.</p> \n<h4>Language Bindings</h4> \n<p>There are various scripts for building for different platforms and languages, but to see examples of how to build for all of the supported systems you should look at <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/scripts/build-all-platforms.sh\"><code>scripts/build-all-platforms.sh</code></a>. This is the script we call for every release, and it builds all of the artifacts we upload to the various package manager systems.</p> \n<p>The different platforms and languages have a layer on top of the C interfaces to enable idiomatic use of the library within the different environments. The major systems have their own top-level folders in this repo, for example: <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/python/\"><code>python</code></a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/android/\"><code>android</code></a>, and <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/swift/\"><code>swift</code></a> for iOS and MacOS. This is where you'll find the code that calls the underlying core library routines, and handles the event system for each platform.</p> \n<h4>Porting</h4> \n<p>If you have a device that isn't supported, you can try <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#cmake\">building using cmake</a> on your system. The only major dependency that the C++ core library has is <a href=\"https://github.com/microsoft/onnxruntime\">the Onnx Runtime</a>. We include <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/core/third-party/onnxruntime/lib/\">pre-built binary library files</a> for all our supported systems, but you'll need to find or build your own version if the libraries we offer don't cover your use case.</p> \n<p>If you want to call this library from a language we don't support, then you should take a look at <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/core/moonshine-c-api.h\">the C interface bindings</a>. Most languages have some way to call into C functions, so you can use these and the binding examples for other languages to guide your implementation.</p> \n<h3>Downloading Models</h3> \n<p>The easiest way to get the model files is using the Python module. After <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#python\">installing it</a> run the downloader like this:</p> \n<pre><code class=\"language-bash\">python -m moonshine_voice.download --language en\n</code></pre> \n<p>You can use either the two-letter code or the English name for the <code>language</code> argument. If you want to see which languages are supported by your current version they're <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#available-models\">listed below</a>, or you can supply a bogus language as the argument to this command:</p> \n<pre><code class=\"language-bash\">python -m moonshine_voice.download --language foo\n</code></pre> \n<p>You can also optionally request a specific model architecture using the <code>model-arch</code> flag, chosen from the numbers in <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/core/moonshine-c-api.h\">moonshine-c-api.h</a>. If no architecture is set, the script will load the highest-quality model available.</p> \n<p>The download script will log the location of the downloaded model files and the model architecture, for example:</p> \n<pre><code class=\"language-bash\">encoder_model.ort: 100%|███████████████████████████████████████████████████████| 29.9M/29.9M [00:00&lt;00:00, 34.5MB/s]\ndecoder_model_merged.ort: 100%|██████████████████████████████████████████████████| 104M/104M [00:02&lt;00:00, 52.6MB/s]\ntokenizer.bin: 100%|█████████████████████████████████████████████████████████████| 244k/244k [00:00&lt;00:00, 1.44MB/s]\nModel download url: https://download.moonshine.ai/model/base-en/quantized/base-en\nModel components: ['encoder_model.ort', 'decoder_model_merged.ort', 'tokenizer.bin']\nModel arch: 1\nDownloaded model path: /Users/petewarden/Library/Caches/moonshine_voice/download.moonshine.ai/model/base-en/quantized/base-en\n</code></pre> \n<p>The last two lines tell you which model architecture is being used, and where the model files are on disk. By default it uses your user cache directory, which is <code>~/Library/Caches/moonshine_voice</code> on MacOS, but you can use a different location by setting the <code>MOONSHINE_VOICE_CACHE</code> environment variable before running the script.</p> \n<h3>Benchmarks</h3> \n<p>The core library includes a benchmarking tool that simulates processing live audio by loading a .wav audio file and feeding it in chunks to the model. To run it:</p> \n<pre><code>cd core\nmd build\ncd build\ncmake ..\ncmake --build . --config Release\n./benchmark\n</code></pre> \n<p>This will report the absolute time taken to process the audio, what percentage of the audio file's duration that is, and the average latency for a response.</p> \n<p>The percentage is helpful because it approximates how much of a compute load the model will be on your hardware. For example, if it shows 20% then that means the speech processing will take a fifth of the compute time when running in your application, leaving 80% for the rest of your code.</p> \n<p>The latency metric needs a bit of explanation. What most applications care about is how soon they are notified about a phrase after the user has finished talking, since this determines how fast the product can respond. As with any user interface, the time between speech ending and the app doing something determines how responsive the voice interface feels, with a goal of keeping it below 200ms. The latency figure logged here is the average time between when the library determines the user has stopped talking and the delivery of the final transcript of that phrase to the client. This is where streaming models have the most impact, since they do a lot of their work upfront, while speech is still happening, so they can usually finish very quickly.</p> \n<p>By default the benchmark binary uses the Tiny English model that's embedded in the framework, but you can pass in the <code>--model-path</code> and <code>--model-arch</code> parameters to choose <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#downloading-models\">one that you've downloaded</a>.</p> \n<p>You can also choose how often the transcript should be updated using the <code>--transcription-interval</code> argument. This defaults to 0.5 seconds, but the right value will depend on how fast your application needs updates. Longer intervals reduce the compute required a bit, at the cost of slower updates.</p> \n<h4>Whisper Comparisons</h4> \n<p>For platforms that support Python, you can run the <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/scripts/run-benchmarks.py\"><code>scripts/run-benchmarks.py</code></a> script which will evaluate similar metrics, with the advantage that it can also download the models so you don't need to worry about path handling.</p> \n<p>It also evaluates equivalent Whisper models. This is a pretty opinionated benchmark that looks at the latency and total compute cost of the two families of models in a situation that is representative of many common real-time voice applications' requirements:</p> \n<ul> \n <li>Speech needs to be responded to as quickly as possible once a user completes a phrase.</li> \n <li>The phrases are of durations between a range of one to ten seconds.</li> \n</ul> \n<p>These are very different requirements from bulk offline processing scenarios, where the overall throughput of the system is more important, and so the latency on a single segment of speech is less important than the overall throughput of the system. This allows optimizations like batch processing.</p> \n<p>We are not claiming that Whisper is not a great model for offline processing, but we do want to highlight the advantages we that Moonshine offers for live speech applications with real-time latency requirements.</p> \n<p>The experimental setup is as follows:</p> \n<ul> \n <li>We use the two_cities.wav audio file as a test case, since it has a mix of short and long phrases. You can vary this by passing in your own audio file with the --wav_path argument.</li> \n <li>We use the Moonshine Tiny, Base, Tiny Streaming, Small Streaming, and Medium Streaming models.</li> \n <li>We compare these to the Whisper Tiny, Base, Small, and Large v3 models. Since the Moonshine Medium Streaming model achieves lower WER than Whisper Large v3 we compare those two, otherwise we compare each with their namesake.</li> \n <li>We use the Moonshine VAD segmenter to split the audio into phrases, and feed each phrase to Whisper for transcription.</li> \n <li>Response latency for both models is measured as the time between a phrase being identified as complete by the VAD segmenter and the transcribed text being returned. For Whisper this means the full transcription time, but since the Moonshine models are streaming we can do a lot of the work while speech is still happening, so the latency is much lower.</li> \n <li>We measure the total compute cost of the models by totalling the duration of the audio processing times for each model, and then expressing that as a percentage of the total audio duration. This is the inverse of the commonly used real-time factor (RTF) metric, but it reflects the compute load required for a real-time application.</li> \n <li>We're using faster-whisper for Whisper, since that seems to provide the best cross-platform performance. We're also sticking with the CPU, since most applications can't rely on GPU or NPU acceleration being present on all the platforms they target. We know there are a lot of great GPU/NPU-accelerated Whisper implementations out there, but these aren't portable enough to be useful for the applications we care about.</li> \n</ul> \n<h2>Models</h2> \n<p>Moonshine Voice is based on a family of speech to text models created by the team at Moonshine AI. If you want to download models to use with the framework, you can use <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#downloading-models\">the Python package to access them</a>. This section contains more information about the history and characteristics of the models we offer.</p> \n<ul> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#papers\">Papers</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#available-models\">Available Models</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#domain-customization\">Domain Customization</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#quantization\">Quantization</a></li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#huggingface\">HuggingFace</a></li> \n</ul> \n<h3>Papers</h3> \n<p>These research papers are a good resource for understanding the architectures and performance strategies behind the models:</p> \n<ul> \n <li><a href=\"https://arxiv.org/abs/2410.15608\"><strong>Moonshine: Speech Recognition for Live Transcription and Voice Commands</strong></a>: Describes the first-generation model architecture, which enabled flexible-duration input windows, improving on Whisper's fixed 30 second requirement.</li> \n <li><a href=\"https://arxiv.org/abs/2509.02523\"><strong>Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices</strong></a>: How we improved accuracy for non-English languages by training mono-lingual models.</li> \n <li><a href=\"https://arxiv.org/abs/2602.12241\"><strong>Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications</strong></a>: Introduces our approach to streaming, and the advantages it offers for live voice applications.</li> \n</ul> \n<h3>Available Models</h3> \n<p>Here are the models currently available. See <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#downloading-models\">Downloading Models</a> for how to obtain them. This library uses the Onnx model format, converted to the memory-mappable OnnxRuntime (<code>.ort</code>) flatbuffer encoding. For <code>safetensor</code> versions, see the <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#huggingface\">HuggingFace</a> section.</p> \n<table> \n <thead> \n  <tr> \n   <th>Language</th> \n   <th>Architecture</th> \n   <th># Parameters</th> \n   <th>WER/CER</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>English</td> \n   <td>Tiny</td> \n   <td>26 million</td> \n   <td>12.66%</td> \n  </tr> \n  <tr> \n   <td>English</td> \n   <td>Tiny Streaming</td> \n   <td>34 million</td> \n   <td>12.00%</td> \n  </tr> \n  <tr> \n   <td>English</td> \n   <td>Base</td> \n   <td>58 million</td> \n   <td>10.07%</td> \n  </tr> \n  <tr> \n   <td>English</td> \n   <td>Small Streaming</td> \n   <td>123 million</td> \n   <td>7.84%</td> \n  </tr> \n  <tr> \n   <td>English</td> \n   <td>Medium Streaming</td> \n   <td>245 million</td> \n   <td>6.65%</td> \n  </tr> \n  <tr> \n   <td>Arabic</td> \n   <td>Base</td> \n   <td>58 million</td> \n   <td>5.63%</td> \n  </tr> \n  <tr> \n   <td>Japanese</td> \n   <td>Base</td> \n   <td>58 million</td> \n   <td>13.62%</td> \n  </tr> \n  <tr> \n   <td>Korean</td> \n   <td>Tiny</td> \n   <td>26 million</td> \n   <td>6.46%</td> \n  </tr> \n  <tr> \n   <td>Mandarin</td> \n   <td>Base</td> \n   <td>58 million</td> \n   <td>25.76%</td> \n  </tr> \n  <tr> \n   <td>Spanish</td> \n   <td>Base</td> \n   <td>58 million</td> \n   <td>4.33%</td> \n  </tr> \n  <tr> \n   <td>Ukrainian</td> \n   <td>Base</td> \n   <td>58 million</td> \n   <td>14.55%</td> \n  </tr> \n  <tr> \n   <td>Vietnamese</td> \n   <td>Base</td> \n   <td>58 million</td> \n   <td>8.82%</td> \n  </tr> \n </tbody> \n</table> \n<p>The English evaluations were done using the <a href=\"https://huggingface.co/spaces/hf-audio/open_asr_leaderboard\">HuggingFace OpenASR Leaderboard</a> datasets and methodology. The other languages were evaluated using the FLEURS dataset and the <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/scripts/eval-model-accuracy.py\"><code>scripts/eval-model-accuracy</code></a> script, with the character or word error rate chosen per language.</p> \n<p>One common issue to watch out for if you're using models that don't use the Latin alphabet (so any languages except English and Spanish) is that you'll need to set the <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber-options\"><code>max_tokens_per_second</code> option</a> to 13.0 when you create the transcriber. This is because the most common pattern for hallucinations is endlessly repeating the last few words, and our heuristic to detect this is to check if there's an unusually high number of tokens for the duration of a segment. Unfortunately the base number of tokens per second for non-Latin languages is much higher than for English, thanks to how we're tokenizing, so you have to manually set the threshold higher to avoid cutting off valid outputs.</p> \n<h3>Domain Customization</h3> \n<p>It's often useful to be able to calibrate a speech to text model towards certain words that you're expecting to hear in your application, whether it's technical terms, slang, or a particular dialect or accent. <a href=\"mailto:contact@moonshine.ai\">Moonshine AI offers full retraining using our internal dataset for customization as a commercial service</a> and we do hope to support free lighter-weight approaches in the future. You can find a community project working on this at <a href=\"https://github.com/pierre-cheneau/finetune-moonshine-asr\">github.com/pierre-cheneau/finetune-moonshine-asr</a>.</p> \n<h3>Quantization</h3> \n<p>We typically quantize our models to eight-bit weights across the board, and eight-bit calculations for heavy operations like MatMul. This is all post-training quantization, using a combination of OnnxRuntime's tools and <a href=\"https://pypi.org/project/onnx-shrink-ray/\">my Onnx Shrink Ray utility</a>. The only anomaly in the process is the treatment of the frontend, which uses convolution layers to generate features, which produces results similar to the more traditional MEL spectrogram preprocessing, but in a learned way with standard ML operations. The inputs to this initial stage correspond to 16-bit signed integers from the raw audio data (though they're encoded as floats) so we've found it necessary to leave the convolution operations in at least B16 float precision.</p> \n<p>You can see the options we use for the conversions in <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/scripts/quantize-streaming-model.sh\">scripts/quantize-streaming-model.sh</a>.</p> \n<h3>HuggingFace</h3> \n<p>We have <code>safetensors</code> versions of the models linked from our organization on HF, <a href=\"https://huggingface.co/UsefulSensors/models\">huggingface.co/UsefulSensors/models</a>. The organization name is from an earlier incarnation of the company, when we were focused on supplying complete voice interface solutions integrated onto a low-cost chip with a built-in microphone. These are all floating-point checkpoints exported from our training pipeline</p> \n<h2>API Reference</h2> \n<p>This documentation covers the Python API, but the same functions and classes are present in all the other supported languages, just with native adaptations (for example CamelCase). You should be able to use this as a reference for all platforms the library runs on.</p> \n<ul> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#data-structures\">Data Structures</a> \n  <ul> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriberline\">TranscriberLine</a></li> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcript\">Transcript</a></li> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriptevent\">TranscriptEvent</a></li> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#intentmatch\">IntentMatch</a></li> \n  </ul> </li> \n <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#classes\">Classes</a> \n  <ul> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber\">Transcriber</a></li> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#mictranscriber\">MicTranscriber</a></li> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#stream\">Stream</a></li> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcripteventlistener\">TranscriptEventListener</a></li> \n   <li><a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#intentrecognizer\">IntentRecognizer</a></li> \n  </ul> </li> \n</ul> \n<h3>Data Structures</h3> \n<h4>TranscriberLine</h4> \n<p>Represents a single \"line\" or speech segment in a transcript. It includes information about the timing, speaker, and text content of the utterance, as well as state such as whether the speech is ongoing or done. If you're building an application that involves transcription, this data structure has all of the information available about each line of speech. Be aware that each line can be updated multiple times with new text and other information as the user keeps speaking.</p> \n<ul> \n <li> <p><code>text</code>: A string containing the UTF-8 encoded text that has been extracted from the audio of this segment.</p> </li> \n <li> <p><code>start_time</code>: A float value representing the time in seconds since the start of the current session that the current utterance was first detected.</p> </li> \n <li> <p><code>duration</code>: A float that represents the duration in seconds of the current utterance.</p> </li> \n <li> <p><code>line_id</code>: An unsigned 64-bit integer that represents a line in a collision-resistant way, for use in storage and ensuring the application can keep track of lines as they change over time. See <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcription-event-flow\">Transcription Event Flow</a> for more details.</p> </li> \n <li> <p><code>is_complete</code>: A boolean that is false until the segment has been completed, and true for the remainder of the line's lifetime.</p> </li> \n <li> <p><code>is_updated</code>: A boolean that's true if any information about the line has changed since the last time the transcript was updated. Since the transcript will be periodically updated internally by the library as you add audio chunks, you can't rely on polling this to detect changes. You should rely on the event/listener flow to catch modifications instead. This applies to all of the booleans below too.</p> </li> \n <li> <p><code>is_new</code>: A boolean indicating whether the line has been added to the transcript by the last update call.</p> </li> \n <li> <p><code>has_text_changed</code>: A boolean that's set if the contents of the line's text was modified by the last transcript update. If this is set, <code>is_updated</code> will always be set too, but if other properties of the line (for example the duration or the audio data) have changed but the text remains the same, then <code>is_updated</code> can be true while <code>has_text_changed</code> is false.</p> </li> \n <li> <p><code>has_speaker_id</code>: Whether a speaker has been identified for this line. Unless the <code>identify_speakers</code> option passed to the Transcriber is set to false, this will always be true by the time the line is complete, and potentially it may be set earlier. The speaker identification process is still experimental, so the current accuracy may not be reliable enough for some applications.</p> </li> \n <li> <p><code>speaker_id</code>: A unique-ish unsigned 64-bit integer that is designed for storage or used to identify the same speaker across multiple sessions.</p> </li> \n <li> <p><code>speaker_index</code>: An integer that represents the order in which the speaker appeared in the transcript, to make it easy to give speakers default names like \"Speaker 1:\", etc.</p> </li> \n <li> <p><code>audio_data</code>: An array of 32-bit floats representing the raw audio data that the line is based on, as 16KHz mono PCM data between 0.0 and 1.0. This can be useful for further processing (for example to drive a visual indicator or to feed into a specialized speech to text model after the line is complete).</p> </li> \n</ul> \n<h4>Transcript</h4> \n<p>A Transcript contains a list of TranscriberLines, arranged in descending time order. The transcript is reset at every <code>Transcriber.start()</code> call, so if you need to retain information from it, you should make explicit copies. Most applications won't work with this structure, since all of the same information is available through event callbacks.</p> \n<h4>TranscriptEvent</h4> \n<p>Contains information about a change to the transcript. It has four subclasses, which are explained in more detail in <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcription-event-flow\">the transcription event flow section</a>. Most of the information is contained in the <code>line</code> member, but there's also a <code>stream_handle</code> that your application can use to tell the source of a line if you're running multiple streams.</p> \n<h4>IntentMatch</h4> \n<p>This event is sent to any listeners you have registered when an <code>IntentRecognizer</code> finds a match to a command you've specified.</p> \n<ul> \n <li><code>trigger_phrase</code>: The string representing the canonical command, exactly as you registered it with the recognizer.</li> \n <li><code>utterance</code>: The text of the utterance that triggered the match.</li> \n <li><code>similarity</code>: A float value that reflects how confident the recognizer is that the utterance has the same meaning as the command, with zero being the least confident and one the most.</li> \n</ul> \n<h3>Classes</h3> \n<h4>Transcriber</h4> \n<p>Handles the speech to text pipeline.</p> \n<ul> \n <li> <p><a id=\"transcriber-init\"></a><code>__init__()</code>: Loads and initializes the transcriber.</p> \n  <ul> \n   <li><code>model_path</code>: The path to the directory holding the component model files needed for the complete flow. Note that this is a path to the <strong>folder</strong>, not an individual <strong>file</strong>. You can download and get a path to a cached version of the standard models using the <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#downloading-models\">download_model()</a> function.</li> \n   <li><code>model_arch</code>: The architecture of the model to load, from the selection defined in <code>ModelArch</code>.</li> \n   <li><code>update_interval</code>: By default the transcriber will periodically run text transcription as new audio data is fed, so that update events can be triggered. This value is how often the speech to text model should be run. You can set this to a large duration to suppress updates between a line starting and ending, but because the streaming models do a lot of their work before the final speech to text stage, this may not reduce overall latency by much.</li> \n   <li><a id=\"transcriber-options\"></a><code>options</code>: These are flags that affect how the transcription process works inside the library, often enabling performance optimizations or debug logging. They are passed as a dictionary mapping strings to strings, even if the values are to be interpreted as numbers - for example <code>{\"max_tokens_per_second\", \"15\"}</code>. \n    <ul> \n     <li><code>skip_transcription</code>: If you only want the voice-activity detection and segmentation, but want to do further processing in your app, you can set this to \"true\" and then use the <code>audioData</code> array in each line.</li> \n     <li><code>max_tokens_per_second</code>: The models occassionally get caught in an infinite decoder loop, where the same words are repeated over and over again. As a heuristic to catch this we compare the number of tokens in the current run to the duration of the audio, and if there seem to be too many tokens we truncate the decoding. By default this is set to 6.5, but for non-English languages where the models produce a lot more raw tokens per second, you may want to bump this to 13.0.</li> \n     <li><code>transcription_interval</code>: How often to run transcription, in seconds.</li> \n     <li><code>vad_threshold</code>: Controls the sensitivity of the initial voice-activity detection stage that decides how to break raw audio into segments. This defaults to 0.5, with lower values creating longer segments, potentially with more background noise sections, and higher values breaking up speech into smaller chunks, at the risk of losing some actual speech by clipping.</li> \n     <li><code>save_input_wav_path</code>: One of the most common causes of poor transcription quality is incorrect conversion or corruption of the audio that's fed into the pipeline. If you set this option to a folder path, the transcriber will save out exactly what it has received as 16KHz mono WAV files, so you can ensure that your input audio is as you expect.</li> \n     <li><code>log_api_calls</code>: Another debugging option, turning this on causes all calls to the C API entry points in the library to write out information on their arguments to stderr or the console each time they're run.</li> \n     <li><code>log_ort_runs</code>: Prints information about the ONNXRuntime inference runs and how long they take.</li> \n     <li><code>vad_window_duration</code>: The VAD runs every 30ms, but to get higher-confidence values we average the results over time. This value is the time in seconds to average over. The default is 0.5s, shorter durations will spot speech faster at the cost of lower accuracy, higher values may increase accuracy, but at the cost of missing shorter utterances.</li> \n     <li><code>vad_look_behind_sample_count</code>: Because we're averaging over time, the mean VAD signal will lag behind the initial speech detection. To compensate for that, when speech is detected we pull in some of the audio immediately before the average passed the threshold. This value is the number of samples to prepend, and defaults to 8192 (all at 16KHz).</li> \n     <li><code>vad_max_segment_duration</code>: It can be hard to find gaps in rapid-fire speech, but a lot of applications want their text in chunks that aren't endless. This option sets the longest duration a line can be before it's marked as complete and a new segment is started. The default is 15 seconds, and to increase the chance that a natural break is found, the <code>vad_threshold</code> is linearly decreased over time from two thirds of the maximum duration until the maximum is reached.</li> \n     <li><code>identify_speakers</code>: A boolean that controls whether to run the speaker identification stage in the pipeline.</li> \n     <li><code>return_audio_data</code>: By default the transcriber returns the segment of audio data corresponding to a line of text along with the transcription. You can disable this if you want to reduce memory overhead.</li> \n     <li><code>log_output_text</code>: If this is enabled then the results of the speech to text model will be logged to the console.</li> \n    </ul> </li> \n  </ul> </li> \n <li> <p><a id=\"transcriber-transcribe-without-streaming\"></a><code>transcribe_without_streaming()</code>: A convenience function to extract text from a non-live audio source, such as a file. We optimize for streaming use cases, so you're probably better off using libraries that specialize in bulk, batched transcription if you use this a lot and have performance constraints. This will still call any registered event listeners as it processes the lines, so this can be useful to test your application using pre-recorded files, or to easily integrate offline audio sources.</p> \n  <ul> \n   <li><code>audio_data</code>: An array of 32-bit float values, representing mono PCM audio between -1.0 and 1.0, to be analyzed for speech.</li> \n   <li><code>sample_rate</code>: The number of samples per second. The library uses this to convert to its working rate (16KHz) internally.</li> \n   <li><code>flags</code>: Integer, currently unused.</li> \n  </ul> </li> \n <li> <p><a id=\"transcriber-start\"></a><code>start()</code>: Begins a new transcription session. You need to call this after you've created the <code>Transcriber</code> and before you add any audio.</p> </li> \n <li> <p><a id=\"transcriber-stop\"></a><code>stop()</code>: Ends a transcription session. If a speech segment was still active, it's marked as complete and the appropriate event handlers are called.</p> </li> \n <li> <p><a id=\"transcriber-add-audio\"></a><code>add_audio()</code>: Call this every time you have a new chunk of audio from your input, to begin processing. The size and sample rate of the audio should be whatever's natural for your source, since the library will handle all conversions.</p> \n  <ul> \n   <li><code>audio_data</code>: Array of 32-bit floats representing a mono PCM chunk of audio.</li> \n   <li><code>sample_rate</code>: How many samples per second are present in the input audio. The library uses this to convert the data to its preferred rate.</li> \n  </ul> </li> \n <li> <p><a id=\"transcriber-update-transcription\"></a><code>update_transcription</code>: The transcript is usually updated periodically as audio data is added, but if you need to trigger one yourself, for example when a user presses refresh, or want access to the complete transcript, you can call this manually.</p> \n  <ul> \n   <li><code>flags</code>: Integer holding flags that are combined using bitwise or (<code>|</code>). \n    <ul> \n     <li><code>MOONSHINE_FLAG_FORCE_UPDATE</code>: By default the transcriber returns a cached version of the transcript if less than 200ms of new audio has come in since the last transcription, but by setting this you can ensure that a transcription happens regardless.</li> \n    </ul> </li> \n  </ul> </li> \n <li> <p><a id=\"transcriber-create-stream\"></a><code>create_stream()</code>: If your application is taking audio input from multiple sources, for example a microphone and system audio, then you'll want to create multiple streams on a single transcriber to avoid loading multiple copies of the models. Each stream has its own transcript, and line events are tagged with the stream handle they came from. You don't need to worry about this if you only need to deal with a single input though, just use the <code>Transcriber</code> class's <code>start()</code>, <code>stop()</code>, etc. This function returns <code>Stream</code> class object.</p> \n  <ul> \n   <li><code>flags</code>: Integer, reserved for future expansion.</li> \n   <li><code>update_interval</code>: Period in seconds between transcription updates.</li> \n  </ul> </li> \n <li> <p><a id=\"transcriber-add-listener\"></a><code>add_listener()</code>: Registers a callable object with the transcriber. This object will be called back as audio is fed in and text is extracted.</p> \n  <ul> \n   <li><code>listener</code>: This is often a subclass of <code>TranscriptEventListener</code>, but can be a plain function. It defines what code is called when a speech event happens.</li> \n  </ul> </li> \n <li> <p><a id=\"transcriber-remove-listener\"></a><code>remove_listener()</code>: Deletes a listener so that it no longer receives events.</p> \n  <ul> \n   <li><code>listener</code>: An object you previously passed into <code>add_listener()</code>.</li> \n  </ul> </li> \n <li> <p><a id=\"transcriber-remove-all-listeners\"></a><code>remove_all_listeners()</code>: Deletes all registered listeners so than none of them receive events anymore.</p> </li> \n</ul> \n<h4>MicTranscriber</h4> \n<p>This class supports the []<code>start()</code>](#transcriber-start), <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber-stop\"><code>stop()</code></a> and listener functions of <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber\"><code>Transcriber</code></a>, but internally creates and attaches to the system's microphone input, so you don't need to call <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber-add-audio\"><code>add_audio()</code></a> yourself. In Python this uses the <a href=\"\"><code>sounddevice</code> library</a>, but in other languages the class uses the native audio API under the hood.</p> \n<h4>Stream</h4> \n<p>The access point for when you need to feed multiple audio inputs into a single transcriber. Supports <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber-start\"><code>start()</code></a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber-stop\"><code>stop()</code></a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber-add-audio\"><code>add_audio()</code></a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber-update-transcription\"><code>update_transcription()</code></a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber-add-listener\"><code>add_listener()</code></a>, <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber-remove-listener\"><code>remove_listener()</code></a>, and <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber-remove-all-listeners\"><code>remove_all_listeners()</code></a> as documented in the <a href=\"https://raw.githubusercontent.com/moonshine-ai/moonshine/main/#transcriber\"><code>Transcriber</code></a> class.</p> \n<h4>TranscriptEventListener</h4> \n<p>A convenience class to derive from to create your own listener code. Override any or all of <code>on_line_started()</code>, <code>on_line_updated()</code>, <code>on_line_text_changed()</code>, and <code>on_line_completed()</code>, and they'll be called back when the corresponding event occurs.</p> \n<h4>IntentRecognizer</h4> \n<p>A specialized kind of event listener that you add as a listener to a <code>Transcriber</code>, and it then analyzes the transcription results to determine if any of the specified commands have been spoken, using natural-language fuzzy matching.</p> \n<ul> \n <li><a id=\"intentrecognizer-init\"></a><code>__init__()</code>: Constructs a new recognizer, loading required models. \n  <ul> \n   <li><code>model_path</code>: String holding a path to a folder that contains the required embedding model files. You can download and obtain a path by calling <code>download_embedding_model()</code>.</li> \n   <li><code>model_arch</code>: An <code>EmbeddingModelArch</code>, obtained from the <code>download_embedding_model()</code> function.</li> \n   <li><code>model_variant</code>: The precision to run the model at. \"q4\" is recommended.</li> \n   <li><code>threshold</code>: How close an utterance has to be to the target sentence to trigger an event.</li> \n  </ul> </li> \n <li><a id=\"intentrecognizer-register-intent\"></a><code>register_intent()</code>: Asks the recognizer to look for utterances that match a given command, and call back into the application when one is found. \n  <ul> \n   <li><code>trigger_phrase</code>: The canonical command sentence to match against.</li> \n   <li><code>handler</code>: A callable function or object that contains code you want to trigger when the command is recognized.</li> \n  </ul> </li> \n <li><a id=\"intentrecognizer-unregister-intent\"></a><code>unregister_intent()</code>: Removes an intent handler from the event callback process. \n  <ul> \n   <li><code>handler</code>: A handler that had previously been registered with the recognizer.</li> \n  </ul> </li> \n <li><a id=\"intentrecognizer-clear-intents\"></a><code>clear_intents()</code>: Removes all intent listeners from the recognizer.</li> \n <li><a id=\"intentrecognizer-set-on-intent\"></a><code>set_on_intent()</code>: Sets a callable that is called when any registered action is triggered, not just a single command as for <code>register_intent()</code>.</li> \n</ul> \n<h2>Support</h2> \n<p>Our primary support channel is <a href=\"https://discord.gg/27qp9zSRXF\">the Moonshine Discord</a>. We make our best efforts to respond to questions there, and other channels like <a href=\"https://github.com/moonshine-ai/moonshine/issues\">GitHub issues</a>. We also offer paid support for commercial customers who need porting or acceleration on other platforms, model customization, more languages, or any other services, please <a href=\"mailto:contact@moonshine.ai\">get in touch</a>.</p> \n<h2>Roadmap</h2> \n<p>This library is in active development, and we aim to implement:</p> \n<ul> \n <li>Binary size reduction for mobile deployment.</li> \n <li>More languages.</li> \n <li>More streaming models.</li> \n <li>Improved speaker identification.</li> \n <li>Lightweight domain customization.</li> \n</ul> \n<h2>Acknowledgements</h2> \n<p>We're grateful to:</p> \n<ul> \n <li>Lambda and Stephen Balaban for supporting our model training through <a href=\"https://lambda.ai/research\">their foundational model grants</a>.</li> \n <li>The ONNX Runtime community for building <a href=\"https://github.com/microsoft/onnxruntime\">a fast, cross-platform inference engine</a>.</li> \n <li><a href=\"https://github.com/snakers4\">Alexander Veysov</a> for the great <a href=\"https://github.com/snakers4/silero-vad\">Silero Voice Activity Detector</a>.</li> \n <li><a href=\"https://github.com/onqtam\">Viktor Kirilov</a> for <a href=\"https://github.com/doctest/doctest\">his fantastic DocTest C++ testing framework</a>.</li> \n <li><a href=\"https://github.com/nemtrif\">Nemanja Trifunovic</a> for <a href=\"https://github.com/nemtrif/utfcpp\">his very helpful UTF8 CPP library</a>.</li> \n <li>The <a href=\"https://www.pyannote.ai/\">Pyannote team</a> for making available their speaker embedding model.</li> \n</ul> \n<h2>License</h2> \n<p>This code, apart from the source in <code>core/third-party</code>, is licensed under the MIT License, see LICENSE in this repository.</p> \n<p>The English-language models are also released under the MIT License. Models for other languages are released under the <a href=\"https://moonshine.ai\">Moonshine Community License</a>, which is a non-commercial license.</p> \n<p>The code in <code>core/third-party</code> is licensed according to the terms of the open source projects it originates from, with details in a LICENSE file in each subfolder.</p>",
        "source": "mshibanami.github.io",
        "published": "",
        "fetched_at": "2026-02-28T23:17:27.212454Z",
        "tags": [
          {
            "name": "transformation",
            "score": 6
          },
          {
            "name": "boundary_crossing",
            "score": 9
          },
          {
            "name": "visibility_gain",
            "score": 5
          },
          {
            "name": "scale_shift",
            "score": 12
          }
        ],
        "structural_score": 32,
        "timeliness_score": 1,
        "final_score": 16.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering",
        "title": "muratcankoylan/Agent-Skills-for-Context-Engineering",
        "summary": "<p>A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.</p><hr /><h1>Agent Skills for Context Engineering</h1> \n<p>A comprehensive, open collection of Agent Skills focused on context engineering principles for building production-grade AI agent systems. These skills teach the art and science of curating context to maximize agent effectiveness across any agent platform.</p> \n<h2>What is Context Engineering?</h2> \n<p>Context engineering is the discipline of managing the language model's context window. Unlike prompt engineering, which focuses on crafting effective instructions, context engineering addresses the holistic curation of all information that enters the model's limited attention budget: system prompts, tool definitions, retrieved documents, message history, and tool outputs.</p> \n<p>The fundamental challenge is that context windows are constrained not by raw token capacity but by attention mechanics. As context length increases, models exhibit predictable degradation patterns: the \"lost-in-the-middle\" phenomenon, U-shaped attention curves, and attention scarcity. Effective context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.</p> \n<h2>Recognition</h2> \n<p>This repository is cited in academic research as foundational work on static skill architecture:</p> \n<blockquote> \n <p>\"While static skills are well-recognized [Anthropic, 2025b; Muratcan Koylan, 2025], MCE is among the first to dynamically evolve them, bridging manual skill engineering and autonomous self-improvement.\"</p> \n</blockquote> \n<p>— <a href=\"https://arxiv.org/pdf/2601.21557\">Meta Context Engineering via Agentic Skill Evolution</a>, Peking University State Key Laboratory of General Artificial Intelligence (2026)</p> \n<h2>Skills Overview</h2> \n<h3>Foundational Skills</h3> \n<p>These skills establish the foundational understanding required for all subsequent context engineering work.</p> \n<table> \n <thead> \n  <tr> \n   <th>Skill</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/context-fundamentals/\">context-fundamentals</a></td> \n   <td>Understand what context is, why it matters, and the anatomy of context in agent systems</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/context-degradation/\">context-degradation</a></td> \n   <td>Recognize patterns of context failure: lost-in-middle, poisoning, distraction, and clash</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/context-compression/\">context-compression</a></td> \n   <td>Design and evaluate compression strategies for long-running sessions</td> \n  </tr> \n </tbody> \n</table> \n<h3>Architectural Skills</h3> \n<p>These skills cover the patterns and structures for building effective agent systems.</p> \n<table> \n <thead> \n  <tr> \n   <th>Skill</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/multi-agent-patterns/\">multi-agent-patterns</a></td> \n   <td>Master orchestrator, peer-to-peer, and hierarchical multi-agent architectures</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/memory-systems/\">memory-systems</a></td> \n   <td>Design short-term, long-term, and graph-based memory architectures</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/tool-design/\">tool-design</a></td> \n   <td>Build tools that agents can use effectively</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/filesystem-context/\">filesystem-context</a></td> \n   <td>Use filesystems for dynamic context discovery, tool output offloading, and plan persistence</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/hosted-agents/\">hosted-agents</a></td> \n   <td><strong>NEW</strong> Build background coding agents with sandboxed VMs, pre-built images, multiplayer support, and multi-client interfaces</td> \n  </tr> \n </tbody> \n</table> \n<h3>Operational Skills</h3> \n<p>These skills address the ongoing operation and optimization of agent systems.</p> \n<table> \n <thead> \n  <tr> \n   <th>Skill</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/context-optimization/\">context-optimization</a></td> \n   <td>Apply compaction, masking, and caching strategies</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/evaluation/\">evaluation</a></td> \n   <td>Build evaluation frameworks for agent systems</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/advanced-evaluation/\">advanced-evaluation</a></td> \n   <td>Master LLM-as-a-Judge techniques: direct scoring, pairwise comparison, rubric generation, and bias mitigation</td> \n  </tr> \n </tbody> \n</table> \n<h3>Development Methodology</h3> \n<p>These skills cover the meta-level practices for building LLM-powered projects.</p> \n<table> \n <thead> \n  <tr> \n   <th>Skill</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/project-development/\">project-development</a></td> \n   <td>Design and build LLM projects from ideation through deployment, including task-model fit analysis, pipeline architecture, and structured output design</td> \n  </tr> \n </tbody> \n</table> \n<h3>Cognitive Architecture Skills</h3> \n<p>These skills cover formal cognitive modeling for rational agent systems.</p> \n<table> \n <thead> \n  <tr> \n   <th>Skill</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/bdi-mental-states/\">bdi-mental-states</a></td> \n   <td><strong>NEW</strong> Transform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns for deliberative reasoning and explainability</td> \n  </tr> \n </tbody> \n</table> \n<h2>Design Philosophy</h2> \n<h3>Progressive Disclosure</h3> \n<p>Each skill is structured for efficient context use. At startup, agents load only skill names and descriptions. Full content loads only when a skill is activated for relevant tasks.</p> \n<h3>Platform Agnosticism</h3> \n<p>These skills focus on transferable principles rather than vendor-specific implementations. The patterns work across Claude Code, Cursor, and any agent platform that supports skills or allows custom instructions.</p> \n<h3>Conceptual Foundation with Practical Examples</h3> \n<p>Scripts and examples demonstrate concepts using Python pseudocode that works across environments without requiring specific dependency installations.</p> \n<h2>Usage</h2> \n<h3>Usage with Claude Code</h3> \n<p>This repository is a <strong>Claude Code Plugin Marketplace</strong> containing context engineering skills that Claude automatically discovers and activates based on your task context.</p> \n<h3>Installation</h3> \n<p><strong>Step 1: Add the Marketplace</strong></p> \n<p>Run this command in Claude Code to register this repository as a plugin source:</p> \n<pre><code>/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering\n</code></pre> \n<p><strong>Step 2: Browse and Install</strong></p> \n<p>Option A - Browse available plugins:</p> \n<ol> \n <li>Select <code>Browse and install plugins</code></li> \n <li>Select <code>context-engineering-marketplace</code></li> \n <li>Choose a plugin (e.g., <code>context-engineering-fundamentals</code>, <code>agent-architecture</code>)</li> \n <li>Select <code>Install now</code></li> \n</ol> \n<p>Option B - Direct install via command:</p> \n<pre><code>/plugin install context-engineering-fundamentals@context-engineering-marketplace\n/plugin install agent-architecture@context-engineering-marketplace\n/plugin install agent-evaluation@context-engineering-marketplace\n/plugin install agent-development@context-engineering-marketplace\n/plugin install cognitive-architecture@context-engineering-marketplace\n</code></pre> \n<h3>Available Plugins</h3> \n<table> \n <thead> \n  <tr> \n   <th>Plugin</th> \n   <th>Skills Included</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><code>context-engineering-fundamentals</code></td> \n   <td>context-fundamentals, context-degradation, context-compression, context-optimization</td> \n  </tr> \n  <tr> \n   <td><code>agent-architecture</code></td> \n   <td>multi-agent-patterns, memory-systems, tool-design, filesystem-context, hosted-agents</td> \n  </tr> \n  <tr> \n   <td><code>agent-evaluation</code></td> \n   <td>evaluation, advanced-evaluation</td> \n  </tr> \n  <tr> \n   <td><code>agent-development</code></td> \n   <td>project-development</td> \n  </tr> \n  <tr> \n   <td><code>cognitive-architecture</code></td> \n   <td>bdi-mental-states</td> \n  </tr> \n </tbody> \n</table> \n<h3>Skill Triggers</h3> \n<table> \n <thead> \n  <tr> \n   <th>Skill</th> \n   <th>Triggers On</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><code>context-fundamentals</code></td> \n   <td>\"understand context\", \"explain context windows\", \"design agent architecture\"</td> \n  </tr> \n  <tr> \n   <td><code>context-degradation</code></td> \n   <td>\"diagnose context problems\", \"fix lost-in-middle\", \"debug agent failures\"</td> \n  </tr> \n  <tr> \n   <td><code>context-compression</code></td> \n   <td>\"compress context\", \"summarize conversation\", \"reduce token usage\"</td> \n  </tr> \n  <tr> \n   <td><code>context-optimization</code></td> \n   <td>\"optimize context\", \"reduce token costs\", \"implement KV-cache\"</td> \n  </tr> \n  <tr> \n   <td><code>multi-agent-patterns</code></td> \n   <td>\"design multi-agent system\", \"implement supervisor pattern\"</td> \n  </tr> \n  <tr> \n   <td><code>memory-systems</code></td> \n   <td>\"implement agent memory\", \"build knowledge graph\", \"track entities\"</td> \n  </tr> \n  <tr> \n   <td><code>tool-design</code></td> \n   <td>\"design agent tools\", \"reduce tool complexity\", \"implement MCP tools\"</td> \n  </tr> \n  <tr> \n   <td><code>filesystem-context</code></td> \n   <td>\"offload context to files\", \"dynamic context discovery\", \"agent scratch pad\", \"file-based context\"</td> \n  </tr> \n  <tr> \n   <td><code>hosted-agents</code></td> \n   <td>\"build background agent\", \"create hosted coding agent\", \"sandboxed execution\", \"multiplayer agent\", \"Modal sandboxes\"</td> \n  </tr> \n  <tr> \n   <td><code>evaluation</code></td> \n   <td>\"evaluate agent performance\", \"build test framework\", \"measure quality\"</td> \n  </tr> \n  <tr> \n   <td><code>advanced-evaluation</code></td> \n   <td>\"implement LLM-as-judge\", \"compare model outputs\", \"mitigate bias\"</td> \n  </tr> \n  <tr> \n   <td><code>project-development</code></td> \n   <td>\"start LLM project\", \"design batch pipeline\", \"evaluate task-model fit\"</td> \n  </tr> \n  <tr> \n   <td><code>bdi-mental-states</code></td> \n   <td>\"model agent mental states\", \"implement BDI architecture\", \"transform RDF to beliefs\", \"build cognitive agent\"</td> \n  </tr> \n </tbody> \n</table> \n<img alt=\"Screenshot 2025-12-26 at 12 34 47 PM\" height=\"894\" src=\"https://github.com/user-attachments/assets/f79aaf03-fd2d-4c71-a630-7027adeb9bfe\" width=\"1014\" /> \n<h3>For Cursor &amp; Codex &amp; IDE</h3> \n<p>Copy skill content into <code>.rules</code> or create project-specific Skills folders. The skills provide the context and guidelines that agent needs for effective context engineering and agent design.</p> \n<h3>For Custom Implementations</h3> \n<p>Extract the principles and patterns from any skill and implement them in your agent framework. The skills are deliberately platform-agnostic.</p> \n<h2>Examples</h2> \n<p>The <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/\">examples</a> folder contains complete system designs that demonstrate how multiple skills work together in practice.</p> \n<table> \n <thead> \n  <tr> \n   <th>Example</th> \n   <th>Description</th> \n   <th>Skills Applied</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/digital-brain-skill/\">digital-brain-skill</a></td> \n   <td><strong>NEW</strong> Personal operating system for founders and creators. Complete Claude Code skill with 6 modules, 4 automation scripts</td> \n   <td>context-fundamentals, context-optimization, memory-systems, tool-design, multi-agent-patterns, evaluation, project-development</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/x-to-book-system/\">x-to-book-system</a></td> \n   <td>Multi-agent system that monitors X accounts and generates daily synthesized books</td> \n   <td>multi-agent-patterns, memory-systems, context-optimization, tool-design, evaluation</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/llm-as-judge-skills/\">llm-as-judge-skills</a></td> \n   <td>Production-ready LLM evaluation tools with TypeScript implementation, 19 passing tests</td> \n   <td>advanced-evaluation, tool-design, context-fundamentals, evaluation</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/book-sft-pipeline/\">book-sft-pipeline</a></td> \n   <td>Train models to write in any author's style. Includes Gertrude Stein case study with 70% human score on Pangram, $2 total cost</td> \n   <td>project-development, context-compression, multi-agent-patterns, evaluation</td> \n  </tr> \n </tbody> \n</table> \n<p>Each example includes:</p> \n<ul> \n <li>Complete PRD with architecture decisions</li> \n <li>Skills mapping showing which concepts informed each decision</li> \n <li>Implementation guidance</li> \n</ul> \n<h3>Digital Brain Skill Example</h3> \n<p>The <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/digital-brain-skill/\">digital-brain-skill</a> example is a complete personal operating system demonstrating comprehensive skills application:</p> \n<ul> \n <li><strong>Progressive Disclosure</strong>: 3-level loading (SKILL.md → MODULE.md → data files)</li> \n <li><strong>Module Isolation</strong>: 6 independent modules (identity, content, knowledge, network, operations, agents)</li> \n <li><strong>Append-Only Memory</strong>: JSONL files with schema-first lines for agent-friendly parsing</li> \n <li><strong>Automation Scripts</strong>: 4 consolidated tools (weekly_review, content_ideas, stale_contacts, idea_to_draft)</li> \n</ul> \n<p>Includes detailed traceability in <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/digital-brain-skill/HOW-SKILLS-BUILT-THIS.md\">HOW-SKILLS-BUILT-THIS.md</a> mapping every architectural decision to specific skill principles.</p> \n<h3>LLM-as-Judge Skills Example</h3> \n<p>The <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/llm-as-judge-skills/\">llm-as-judge-skills</a> example is a complete TypeScript implementation demonstrating:</p> \n<ul> \n <li><strong>Direct Scoring</strong>: Evaluate responses against weighted criteria with rubric support</li> \n <li><strong>Pairwise Comparison</strong>: Compare responses with position bias mitigation</li> \n <li><strong>Rubric Generation</strong>: Create domain-specific evaluation standards</li> \n <li><strong>EvaluatorAgent</strong>: High-level agent combining all evaluation capabilities</li> \n</ul> \n<h3>Book SFT Pipeline Example</h3> \n<p>The <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/book-sft-pipeline/\">book-sft-pipeline</a> example demonstrates training small models (8B) to write in any author's style:</p> \n<ul> \n <li><strong>Intelligent Segmentation</strong>: Two-tier chunking with overlap for maximum training examples</li> \n <li><strong>Prompt Diversity</strong>: 15+ templates to prevent memorization and force style learning</li> \n <li><strong>Tinker Integration</strong>: Complete LoRA training workflow with $2 total cost</li> \n <li><strong>Validation Methodology</strong>: Modern scenario testing proves style transfer vs content memorization</li> \n</ul> \n<p>Integrates with context engineering skills: project-development, context-compression, multi-agent-patterns, evaluation.</p> \n<h2>Star History</h2> \n<img alt=\"star-history-2026224\" height=\"2648\" src=\"https://github.com/user-attachments/assets/b3bdbf23-4b6a-4774-ae85-42ef4d9b2d79\" width=\"3664\" /> \n<h2>Structure</h2> \n<p>Each skill follows the Agent Skills specification:</p> \n<pre><code>skill-name/\n├── SKILL.md              # Required: instructions + metadata\n├── scripts/              # Optional: executable code demonstrating concepts\n└── references/           # Optional: additional documentation and resources\n</code></pre> \n<p>See the <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/template/\">template</a> folder for the canonical skill structure.</p> \n<h2>Contributing</h2> \n<p>This repository follows the Agent Skills open development model. Contributions are welcome from the broader ecosystem. When contributing:</p> \n<ol> \n <li>Follow the skill template structure</li> \n <li>Provide clear, actionable instructions</li> \n <li>Include working examples where appropriate</li> \n <li>Document trade-offs and potential issues</li> \n <li>Keep SKILL.md under 500 lines for optimal performance</li> \n</ol> \n<p>Feel free to contact <a href=\"https://x.com/koylanai\">Muratcan Koylan</a> for collaboration opportunities or any inquiries.</p> \n<h2>License</h2> \n<p>MIT License - see LICENSE file for details.</p> \n<h2>References</h2> \n<p>The principles in these skills are derived from research and production experience at leading AI labs and framework developers. Each skill includes references to the underlying research and case studies that inform its recommendations.</p>",
        "source": "mshibanami.github.io",
        "published": "",
        "fetched_at": "2026-02-28T23:17:27.212409Z",
        "tags": [
          {
            "name": "transformation",
            "score": 8
          },
          {
            "name": "boundary_crossing",
            "score": 7
          },
          {
            "name": "value_redefinition",
            "score": 4
          },
          {
            "name": "ontology_shift",
            "score": 8
          }
        ],
        "structural_score": 27,
        "timeliness_score": 1,
        "final_score": 14.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering",
        "title": "muratcankoylan/Agent-Skills-for-Context-Engineering",
        "summary": "<p>A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.</p><hr /><h1>Agent Skills for Context Engineering</h1> \n<p>A comprehensive, open collection of Agent Skills focused on context engineering principles for building production-grade AI agent systems. These skills teach the art and science of curating context to maximize agent effectiveness across any agent platform.</p> \n<h2>What is Context Engineering?</h2> \n<p>Context engineering is the discipline of managing the language model's context window. Unlike prompt engineering, which focuses on crafting effective instructions, context engineering addresses the holistic curation of all information that enters the model's limited attention budget: system prompts, tool definitions, retrieved documents, message history, and tool outputs.</p> \n<p>The fundamental challenge is that context windows are constrained not by raw token capacity but by attention mechanics. As context length increases, models exhibit predictable degradation patterns: the \"lost-in-the-middle\" phenomenon, U-shaped attention curves, and attention scarcity. Effective context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.</p> \n<h2>Recognition</h2> \n<p>This repository is cited in academic research as foundational work on static skill architecture:</p> \n<blockquote> \n <p>\"While static skills are well-recognized [Anthropic, 2025b; Muratcan Koylan, 2025], MCE is among the first to dynamically evolve them, bridging manual skill engineering and autonomous self-improvement.\"</p> \n</blockquote> \n<p>— <a href=\"https://arxiv.org/pdf/2601.21557\">Meta Context Engineering via Agentic Skill Evolution</a>, Peking University State Key Laboratory of General Artificial Intelligence (2026)</p> \n<h2>Skills Overview</h2> \n<h3>Foundational Skills</h3> \n<p>These skills establish the foundational understanding required for all subsequent context engineering work.</p> \n<table> \n <thead> \n  <tr> \n   <th>Skill</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/context-fundamentals/\">context-fundamentals</a></td> \n   <td>Understand what context is, why it matters, and the anatomy of context in agent systems</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/context-degradation/\">context-degradation</a></td> \n   <td>Recognize patterns of context failure: lost-in-middle, poisoning, distraction, and clash</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/context-compression/\">context-compression</a></td> \n   <td>Design and evaluate compression strategies for long-running sessions</td> \n  </tr> \n </tbody> \n</table> \n<h3>Architectural Skills</h3> \n<p>These skills cover the patterns and structures for building effective agent systems.</p> \n<table> \n <thead> \n  <tr> \n   <th>Skill</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/multi-agent-patterns/\">multi-agent-patterns</a></td> \n   <td>Master orchestrator, peer-to-peer, and hierarchical multi-agent architectures</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/memory-systems/\">memory-systems</a></td> \n   <td>Design short-term, long-term, and graph-based memory architectures</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/tool-design/\">tool-design</a></td> \n   <td>Build tools that agents can use effectively</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/filesystem-context/\">filesystem-context</a></td> \n   <td>Use filesystems for dynamic context discovery, tool output offloading, and plan persistence</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/hosted-agents/\">hosted-agents</a></td> \n   <td><strong>NEW</strong> Build background coding agents with sandboxed VMs, pre-built images, multiplayer support, and multi-client interfaces</td> \n  </tr> \n </tbody> \n</table> \n<h3>Operational Skills</h3> \n<p>These skills address the ongoing operation and optimization of agent systems.</p> \n<table> \n <thead> \n  <tr> \n   <th>Skill</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/context-optimization/\">context-optimization</a></td> \n   <td>Apply compaction, masking, and caching strategies</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/evaluation/\">evaluation</a></td> \n   <td>Build evaluation frameworks for agent systems</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/advanced-evaluation/\">advanced-evaluation</a></td> \n   <td>Master LLM-as-a-Judge techniques: direct scoring, pairwise comparison, rubric generation, and bias mitigation</td> \n  </tr> \n </tbody> \n</table> \n<h3>Development Methodology</h3> \n<p>These skills cover the meta-level practices for building LLM-powered projects.</p> \n<table> \n <thead> \n  <tr> \n   <th>Skill</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/project-development/\">project-development</a></td> \n   <td>Design and build LLM projects from ideation through deployment, including task-model fit analysis, pipeline architecture, and structured output design</td> \n  </tr> \n </tbody> \n</table> \n<h3>Cognitive Architecture Skills</h3> \n<p>These skills cover formal cognitive modeling for rational agent systems.</p> \n<table> \n <thead> \n  <tr> \n   <th>Skill</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/skills/bdi-mental-states/\">bdi-mental-states</a></td> \n   <td><strong>NEW</strong> Transform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns for deliberative reasoning and explainability</td> \n  </tr> \n </tbody> \n</table> \n<h2>Design Philosophy</h2> \n<h3>Progressive Disclosure</h3> \n<p>Each skill is structured for efficient context use. At startup, agents load only skill names and descriptions. Full content loads only when a skill is activated for relevant tasks.</p> \n<h3>Platform Agnosticism</h3> \n<p>These skills focus on transferable principles rather than vendor-specific implementations. The patterns work across Claude Code, Cursor, and any agent platform that supports skills or allows custom instructions.</p> \n<h3>Conceptual Foundation with Practical Examples</h3> \n<p>Scripts and examples demonstrate concepts using Python pseudocode that works across environments without requiring specific dependency installations.</p> \n<h2>Usage</h2> \n<h3>Usage with Claude Code</h3> \n<p>This repository is a <strong>Claude Code Plugin Marketplace</strong> containing context engineering skills that Claude automatically discovers and activates based on your task context.</p> \n<h3>Installation</h3> \n<p><strong>Step 1: Add the Marketplace</strong></p> \n<p>Run this command in Claude Code to register this repository as a plugin source:</p> \n<pre><code>/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering\n</code></pre> \n<p><strong>Step 2: Browse and Install</strong></p> \n<p>Option A - Browse available plugins:</p> \n<ol> \n <li>Select <code>Browse and install plugins</code></li> \n <li>Select <code>context-engineering-marketplace</code></li> \n <li>Choose a plugin (e.g., <code>context-engineering-fundamentals</code>, <code>agent-architecture</code>)</li> \n <li>Select <code>Install now</code></li> \n</ol> \n<p>Option B - Direct install via command:</p> \n<pre><code>/plugin install context-engineering-fundamentals@context-engineering-marketplace\n/plugin install agent-architecture@context-engineering-marketplace\n/plugin install agent-evaluation@context-engineering-marketplace\n/plugin install agent-development@context-engineering-marketplace\n/plugin install cognitive-architecture@context-engineering-marketplace\n</code></pre> \n<h3>Available Plugins</h3> \n<table> \n <thead> \n  <tr> \n   <th>Plugin</th> \n   <th>Skills Included</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><code>context-engineering-fundamentals</code></td> \n   <td>context-fundamentals, context-degradation, context-compression, context-optimization</td> \n  </tr> \n  <tr> \n   <td><code>agent-architecture</code></td> \n   <td>multi-agent-patterns, memory-systems, tool-design, filesystem-context, hosted-agents</td> \n  </tr> \n  <tr> \n   <td><code>agent-evaluation</code></td> \n   <td>evaluation, advanced-evaluation</td> \n  </tr> \n  <tr> \n   <td><code>agent-development</code></td> \n   <td>project-development</td> \n  </tr> \n  <tr> \n   <td><code>cognitive-architecture</code></td> \n   <td>bdi-mental-states</td> \n  </tr> \n </tbody> \n</table> \n<h3>Skill Triggers</h3> \n<table> \n <thead> \n  <tr> \n   <th>Skill</th> \n   <th>Triggers On</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><code>context-fundamentals</code></td> \n   <td>\"understand context\", \"explain context windows\", \"design agent architecture\"</td> \n  </tr> \n  <tr> \n   <td><code>context-degradation</code></td> \n   <td>\"diagnose context problems\", \"fix lost-in-middle\", \"debug agent failures\"</td> \n  </tr> \n  <tr> \n   <td><code>context-compression</code></td> \n   <td>\"compress context\", \"summarize conversation\", \"reduce token usage\"</td> \n  </tr> \n  <tr> \n   <td><code>context-optimization</code></td> \n   <td>\"optimize context\", \"reduce token costs\", \"implement KV-cache\"</td> \n  </tr> \n  <tr> \n   <td><code>multi-agent-patterns</code></td> \n   <td>\"design multi-agent system\", \"implement supervisor pattern\"</td> \n  </tr> \n  <tr> \n   <td><code>memory-systems</code></td> \n   <td>\"implement agent memory\", \"build knowledge graph\", \"track entities\"</td> \n  </tr> \n  <tr> \n   <td><code>tool-design</code></td> \n   <td>\"design agent tools\", \"reduce tool complexity\", \"implement MCP tools\"</td> \n  </tr> \n  <tr> \n   <td><code>filesystem-context</code></td> \n   <td>\"offload context to files\", \"dynamic context discovery\", \"agent scratch pad\", \"file-based context\"</td> \n  </tr> \n  <tr> \n   <td><code>hosted-agents</code></td> \n   <td>\"build background agent\", \"create hosted coding agent\", \"sandboxed execution\", \"multiplayer agent\", \"Modal sandboxes\"</td> \n  </tr> \n  <tr> \n   <td><code>evaluation</code></td> \n   <td>\"evaluate agent performance\", \"build test framework\", \"measure quality\"</td> \n  </tr> \n  <tr> \n   <td><code>advanced-evaluation</code></td> \n   <td>\"implement LLM-as-judge\", \"compare model outputs\", \"mitigate bias\"</td> \n  </tr> \n  <tr> \n   <td><code>project-development</code></td> \n   <td>\"start LLM project\", \"design batch pipeline\", \"evaluate task-model fit\"</td> \n  </tr> \n  <tr> \n   <td><code>bdi-mental-states</code></td> \n   <td>\"model agent mental states\", \"implement BDI architecture\", \"transform RDF to beliefs\", \"build cognitive agent\"</td> \n  </tr> \n </tbody> \n</table> \n<img alt=\"Screenshot 2025-12-26 at 12 34 47 PM\" height=\"894\" src=\"https://github.com/user-attachments/assets/f79aaf03-fd2d-4c71-a630-7027adeb9bfe\" width=\"1014\" /> \n<h3>For Cursor &amp; Codex &amp; IDE</h3> \n<p>Copy skill content into <code>.rules</code> or create project-specific Skills folders. The skills provide the context and guidelines that agent needs for effective context engineering and agent design.</p> \n<h3>For Custom Implementations</h3> \n<p>Extract the principles and patterns from any skill and implement them in your agent framework. The skills are deliberately platform-agnostic.</p> \n<h2>Examples</h2> \n<p>The <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/\">examples</a> folder contains complete system designs that demonstrate how multiple skills work together in practice.</p> \n<table> \n <thead> \n  <tr> \n   <th>Example</th> \n   <th>Description</th> \n   <th>Skills Applied</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/digital-brain-skill/\">digital-brain-skill</a></td> \n   <td><strong>NEW</strong> Personal operating system for founders and creators. Complete Claude Code skill with 6 modules, 4 automation scripts</td> \n   <td>context-fundamentals, context-optimization, memory-systems, tool-design, multi-agent-patterns, evaluation, project-development</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/x-to-book-system/\">x-to-book-system</a></td> \n   <td>Multi-agent system that monitors X accounts and generates daily synthesized books</td> \n   <td>multi-agent-patterns, memory-systems, context-optimization, tool-design, evaluation</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/llm-as-judge-skills/\">llm-as-judge-skills</a></td> \n   <td>Production-ready LLM evaluation tools with TypeScript implementation, 19 passing tests</td> \n   <td>advanced-evaluation, tool-design, context-fundamentals, evaluation</td> \n  </tr> \n  <tr> \n   <td><a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/book-sft-pipeline/\">book-sft-pipeline</a></td> \n   <td>Train models to write in any author's style. Includes Gertrude Stein case study with 70% human score on Pangram, $2 total cost</td> \n   <td>project-development, context-compression, multi-agent-patterns, evaluation</td> \n  </tr> \n </tbody> \n</table> \n<p>Each example includes:</p> \n<ul> \n <li>Complete PRD with architecture decisions</li> \n <li>Skills mapping showing which concepts informed each decision</li> \n <li>Implementation guidance</li> \n</ul> \n<h3>Digital Brain Skill Example</h3> \n<p>The <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/digital-brain-skill/\">digital-brain-skill</a> example is a complete personal operating system demonstrating comprehensive skills application:</p> \n<ul> \n <li><strong>Progressive Disclosure</strong>: 3-level loading (SKILL.md → MODULE.md → data files)</li> \n <li><strong>Module Isolation</strong>: 6 independent modules (identity, content, knowledge, network, operations, agents)</li> \n <li><strong>Append-Only Memory</strong>: JSONL files with schema-first lines for agent-friendly parsing</li> \n <li><strong>Automation Scripts</strong>: 4 consolidated tools (weekly_review, content_ideas, stale_contacts, idea_to_draft)</li> \n</ul> \n<p>Includes detailed traceability in <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/digital-brain-skill/HOW-SKILLS-BUILT-THIS.md\">HOW-SKILLS-BUILT-THIS.md</a> mapping every architectural decision to specific skill principles.</p> \n<h3>LLM-as-Judge Skills Example</h3> \n<p>The <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/llm-as-judge-skills/\">llm-as-judge-skills</a> example is a complete TypeScript implementation demonstrating:</p> \n<ul> \n <li><strong>Direct Scoring</strong>: Evaluate responses against weighted criteria with rubric support</li> \n <li><strong>Pairwise Comparison</strong>: Compare responses with position bias mitigation</li> \n <li><strong>Rubric Generation</strong>: Create domain-specific evaluation standards</li> \n <li><strong>EvaluatorAgent</strong>: High-level agent combining all evaluation capabilities</li> \n</ul> \n<h3>Book SFT Pipeline Example</h3> \n<p>The <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/examples/book-sft-pipeline/\">book-sft-pipeline</a> example demonstrates training small models (8B) to write in any author's style:</p> \n<ul> \n <li><strong>Intelligent Segmentation</strong>: Two-tier chunking with overlap for maximum training examples</li> \n <li><strong>Prompt Diversity</strong>: 15+ templates to prevent memorization and force style learning</li> \n <li><strong>Tinker Integration</strong>: Complete LoRA training workflow with $2 total cost</li> \n <li><strong>Validation Methodology</strong>: Modern scenario testing proves style transfer vs content memorization</li> \n</ul> \n<p>Integrates with context engineering skills: project-development, context-compression, multi-agent-patterns, evaluation.</p> \n<h2>Star History</h2> \n<img alt=\"star-history-2026224\" height=\"2648\" src=\"https://github.com/user-attachments/assets/b3bdbf23-4b6a-4774-ae85-42ef4d9b2d79\" width=\"3664\" /> \n<h2>Structure</h2> \n<p>Each skill follows the Agent Skills specification:</p> \n<pre><code>skill-name/\n├── SKILL.md              # Required: instructions + metadata\n├── scripts/              # Optional: executable code demonstrating concepts\n└── references/           # Optional: additional documentation and resources\n</code></pre> \n<p>See the <a href=\"https://raw.githubusercontent.com/muratcankoylan/Agent-Skills-for-Context-Engineering/main/template/\">template</a> folder for the canonical skill structure.</p> \n<h2>Contributing</h2> \n<p>This repository follows the Agent Skills open development model. Contributions are welcome from the broader ecosystem. When contributing:</p> \n<ol> \n <li>Follow the skill template structure</li> \n <li>Provide clear, actionable instructions</li> \n <li>Include working examples where appropriate</li> \n <li>Document trade-offs and potential issues</li> \n <li>Keep SKILL.md under 500 lines for optimal performance</li> \n</ol> \n<p>Feel free to contact <a href=\"https://x.com/koylanai\">Muratcan Koylan</a> for collaboration opportunities or any inquiries.</p> \n<h2>License</h2> \n<p>MIT License - see LICENSE file for details.</p> \n<h2>References</h2> \n<p>The principles in these skills are derived from research and production experience at leading AI labs and framework developers. Each skill includes references to the underlying research and case studies that inform its recommendations.</p>",
        "source": "mshibanami.github.io",
        "published": "",
        "fetched_at": "2026-02-28T23:17:28.718109Z",
        "tags": [
          {
            "name": "transformation",
            "score": 8
          },
          {
            "name": "boundary_crossing",
            "score": 7
          },
          {
            "name": "value_redefinition",
            "score": 4
          },
          {
            "name": "ontology_shift",
            "score": 8
          }
        ],
        "structural_score": 27,
        "timeliness_score": 1,
        "final_score": 14.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://github.com/ruvnet/wifi-densepose",
        "title": "ruvnet/wifi-densepose",
        "summary": "<p>Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that enables real-time full-body tracking through walls using commodity mesh routers</p><hr /><h1>WiFi DensePose</h1> \n<p><a href=\"https://www.python.org/downloads/\"><img alt=\"Python 3.8+\" src=\"https://img.shields.io/badge/python-3.8+-blue.svg?sanitize=true\" /></a> <a href=\"https://fastapi.tiangolo.com/\"><img alt=\"FastAPI\" src=\"https://img.shields.io/badge/FastAPI-0.95+-green.svg?sanitize=true\" /></a> <a href=\"https://opensource.org/licenses/MIT\"><img alt=\"License: MIT\" src=\"https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true\" /></a> <a href=\"https://pypi.org/project/wifi-densepose/\"><img alt=\"PyPI version\" src=\"https://img.shields.io/pypi/v/wifi-densepose.svg?sanitize=true\" /></a> <a href=\"https://pypi.org/project/wifi-densepose/\"><img alt=\"PyPI downloads\" src=\"https://img.shields.io/pypi/dm/wifi-densepose.svg?sanitize=true\" /></a> <a href=\"https://github.com/ruvnet/wifi-densepose\"><img alt=\"Test Coverage\" src=\"https://img.shields.io/badge/coverage-100%25-brightgreen.svg?sanitize=true\" /></a> <a href=\"https://hub.docker.com/r/ruvnet/wifi-densepose\"><img alt=\"Docker\" src=\"https://img.shields.io/badge/docker-ready-blue.svg?sanitize=true\" /></a></p> \n<p>A cutting-edge WiFi-based human pose estimation system that leverages Channel State Information (CSI) data and advanced machine learning to provide real-time, privacy-preserving pose detection without cameras.</p> \n<h2>🚀 Key Features</h2> \n<ul> \n <li><strong>Privacy-First</strong>: No cameras required - uses WiFi signals for pose detection</li> \n <li><strong>Real-Time Processing</strong>: Sub-50ms latency with 30 FPS pose estimation</li> \n <li><strong>Multi-Person Tracking</strong>: Simultaneous tracking of up to 10 individuals</li> \n <li><strong>Domain-Specific Optimization</strong>: Healthcare, fitness, smart home, and security applications</li> \n <li><strong>Enterprise-Ready</strong>: Production-grade API with authentication, rate limiting, and monitoring</li> \n <li><strong>Hardware Agnostic</strong>: Works with standard WiFi routers and access points</li> \n <li><strong>Comprehensive Analytics</strong>: Fall detection, activity recognition, and occupancy monitoring</li> \n <li><strong>WebSocket Streaming</strong>: Real-time pose data streaming for live applications</li> \n <li><strong>100% Test Coverage</strong>: Thoroughly tested with comprehensive test suite</li> \n</ul> \n<h2>🦀 Rust Implementation (v2)</h2> \n<p>A high-performance Rust port is available in <code>/rust-port/wifi-densepose-rs/</code>:</p> \n<h3>Performance Benchmarks (Validated)</h3> \n<table> \n <thead> \n  <tr> \n   <th>Operation</th> \n   <th>Python (v1)</th> \n   <th>Rust (v2)</th> \n   <th>Speedup</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>CSI Preprocessing (4x64)</td> \n   <td>~5ms</td> \n   <td><strong>5.19 µs</strong></td> \n   <td>~1000x</td> \n  </tr> \n  <tr> \n   <td>Phase Sanitization (4x64)</td> \n   <td>~3ms</td> \n   <td><strong>3.84 µs</strong></td> \n   <td>~780x</td> \n  </tr> \n  <tr> \n   <td>Feature Extraction (4x64)</td> \n   <td>~8ms</td> \n   <td><strong>9.03 µs</strong></td> \n   <td>~890x</td> \n  </tr> \n  <tr> \n   <td>Motion Detection</td> \n   <td>~1ms</td> \n   <td><strong>186 ns</strong></td> \n   <td>~5400x</td> \n  </tr> \n  <tr> \n   <td><strong>Full Pipeline</strong></td> \n   <td>~15ms</td> \n   <td><strong>18.47 µs</strong></td> \n   <td>~810x</td> \n  </tr> \n </tbody> \n</table> \n<h3>Throughput Metrics</h3> \n<table> \n <thead> \n  <tr> \n   <th>Component</th> \n   <th>Throughput</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>CSI Preprocessing</td> \n   <td>49-66 Melem/s</td> \n  </tr> \n  <tr> \n   <td>Phase Sanitization</td> \n   <td>67-85 Melem/s</td> \n  </tr> \n  <tr> \n   <td>Feature Extraction</td> \n   <td>7-11 Melem/s</td> \n  </tr> \n  <tr> \n   <td>Full Pipeline</td> \n   <td><strong>~54,000 fps</strong></td> \n  </tr> \n </tbody> \n</table> \n<h3>Resource Comparison</h3> \n<table> \n <thead> \n  <tr> \n   <th>Feature</th> \n   <th>Python (v1)</th> \n   <th>Rust (v2)</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td>Memory Usage</td> \n   <td>~500MB</td> \n   <td>~100MB</td> \n  </tr> \n  <tr> \n   <td>WASM Support</td> \n   <td>❌</td> \n   <td>✅</td> \n  </tr> \n  <tr> \n   <td>Binary Size</td> \n   <td>N/A</td> \n   <td>~10MB</td> \n  </tr> \n  <tr> \n   <td>Test Coverage</td> \n   <td>100%</td> \n   <td>107 tests</td> \n  </tr> \n </tbody> \n</table> \n<p><strong>Quick Start (Rust):</strong></p> \n<pre><code class=\"language-bash\">cd rust-port/wifi-densepose-rs\ncargo build --release\ncargo test --workspace\ncargo bench --package wifi-densepose-signal\n</code></pre> \n<h3>Validation Tests</h3> \n<p>Mathematical correctness validated:</p> \n<ul> \n <li>✅ Phase unwrapping: 0.000000 radians max error</li> \n <li>✅ Amplitude RMS: Exact match</li> \n <li>✅ Doppler shift: 33.33 Hz (exact)</li> \n <li>✅ Correlation: 1.0 for identical signals</li> \n <li>✅ Phase coherence: 1.0 for coherent signals</li> \n</ul> \n<p>See <a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/rust-port/wifi-densepose-rs/docs/\">Rust Port Documentation</a> for ADRs and DDD patterns.</p> \n<h2>🚨 WiFi-Mat: Disaster Response Module</h2> \n<p>A specialized extension for <strong>search and rescue operations</strong> - detecting and localizing survivors trapped in rubble, earthquakes, and natural disasters.</p> \n<h3>Key Capabilities</h3> \n<table> \n <thead> \n  <tr> \n   <th>Feature</th> \n   <th>Description</th> \n  </tr> \n </thead> \n <tbody> \n  <tr> \n   <td><strong>Vital Signs Detection</strong></td> \n   <td>Breathing (4-60 BPM), heartbeat via micro-Doppler</td> \n  </tr> \n  <tr> \n   <td><strong>3D Localization</strong></td> \n   <td>Position estimation through debris up to 5m depth</td> \n  </tr> \n  <tr> \n   <td><strong>START Triage</strong></td> \n   <td>Automatic Immediate/Delayed/Minor/Deceased classification</td> \n  </tr> \n  <tr> \n   <td><strong>Real-time Alerts</strong></td> \n   <td>Priority-based notifications with escalation</td> \n  </tr> \n </tbody> \n</table> \n<h3>Use Cases</h3> \n<ul> \n <li>Earthquake search and rescue</li> \n <li>Building collapse response</li> \n <li>Avalanche victim location</li> \n <li>Mine collapse detection</li> \n <li>Flood rescue operations</li> \n</ul> \n<h3>Quick Example</h3> \n<pre><code class=\"language-rust\">use wifi_densepose_mat::{DisasterResponse, DisasterConfig, DisasterType, ScanZone, ZoneBounds};\n\nlet config = DisasterConfig::builder()\n    .disaster_type(DisasterType::Earthquake)\n    .sensitivity(0.85)\n    .max_depth(5.0)\n    .build();\n\nlet mut response = DisasterResponse::new(config);\nresponse.initialize_event(location, \"Building collapse\")?;\nresponse.add_zone(ScanZone::new(\"North Wing\", ZoneBounds::rectangle(0.0, 0.0, 30.0, 20.0)))?;\nresponse.start_scanning().await?;\n\n// Get survivors prioritized by triage status\nlet immediate = response.survivors_by_triage(TriageStatus::Immediate);\nprintln!(\"{} survivors require immediate rescue\", immediate.len());\n</code></pre> \n<h3>Documentation</h3> \n<ul> \n <li><strong><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/docs/wifi-mat-user-guide.md\">WiFi-Mat User Guide</a></strong> - Complete setup, configuration, and field deployment</li> \n <li><strong><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/docs/adr/ADR-001-wifi-mat-disaster-detection.md\">Architecture Decision Record</a></strong> - Design decisions and rationale</li> \n <li><strong><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/docs/ddd/wifi-mat-domain-model.md\">Domain Model</a></strong> - DDD bounded contexts and entities</li> \n</ul> \n<p><strong>Build:</strong></p> \n<pre><code class=\"language-bash\">cd rust-port/wifi-densepose-rs\ncargo build --release --package wifi-densepose-mat\ncargo test --package wifi-densepose-mat\n</code></pre> \n<h2>📋 Table of Contents</h2> \n<table> \n <tbody>\n  <tr> \n   <td width=\"50%\"> <p><strong>🚀 Getting Started</strong></p> \n    <ul> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-key-features\">Key Features</a></li> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-rust-implementation-v2\">Rust Implementation (v2)</a></li> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-wifi-mat-disaster-response-module\">WiFi-Mat Disaster Response</a></li> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#%EF%B8%8F-system-architecture\">System Architecture</a></li> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-installation\">Installation</a> \n      <ul> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#using-pip-recommended\">Using pip (Recommended)</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#from-source\">From Source</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#using-docker\">Using Docker</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#system-requirements\">System Requirements</a></li> \n      </ul> </li> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-quick-start\">Quick Start</a> \n      <ul> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#1-basic-setup\">Basic Setup</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#2-start-the-system\">Start the System</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#3-using-the-rest-api\">Using the REST API</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#4-real-time-streaming\">Real-time Streaming</a></li> \n      </ul> </li> \n    </ul> <p><strong>🖥️ Usage &amp; Configuration</strong></p> \n    <ul> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#%EF%B8%8F-cli-usage\">CLI Usage</a> \n      <ul> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#cli-installation\">Installation</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#basic-commands\">Basic Commands</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#configuration-commands\">Configuration Commands</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#cli-examples\">Examples</a></li> \n      </ul> </li> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-documentation\">Documentation</a> \n      <ul> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-core-documentation\">Core Documentation</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-quick-links\">Quick Links</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-api-overview\">API Overview</a></li> \n      </ul> </li> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-hardware-setup\">Hardware Setup</a> \n      <ul> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#supported-hardware\">Supported Hardware</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#physical-setup\">Physical Setup</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#network-configuration\">Network Configuration</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#environment-calibration\">Environment Calibration</a></li> \n      </ul> </li> \n    </ul> </td> \n   <td width=\"50%\"> <p><strong>⚙️ Advanced Topics</strong></p> \n    <ul> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#%EF%B8%8F-configuration\">Configuration</a> \n      <ul> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#environment-variables\">Environment Variables</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#domain-specific-configurations\">Domain-Specific Configurations</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#advanced-configuration\">Advanced Configuration</a></li> \n      </ul> </li> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-testing\">Testing</a> \n      <ul> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#running-tests\">Running Tests</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#test-categories\">Test Categories</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#mock-testing\">Mock Testing</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#continuous-integration\">Continuous Integration</a></li> \n      </ul> </li> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-deployment\">Deployment</a> \n      <ul> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#production-deployment\">Production Deployment</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#infrastructure-as-code\">Infrastructure as Code</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#monitoring-and-logging\">Monitoring and Logging</a></li> \n      </ul> </li> \n    </ul> <p><strong>📊 Performance &amp; Community</strong></p> \n    <ul> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-performance-metrics\">Performance Metrics</a> \n      <ul> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#benchmark-results\">Benchmark Results</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#performance-optimization\">Performance Optimization</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#load-testing\">Load Testing</a></li> \n      </ul> </li> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-contributing\">Contributing</a> \n      <ul> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#development-setup\">Development Setup</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#code-standards\">Code Standards</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#contribution-process\">Contribution Process</a></li> \n       <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#code-review-checklist\">Code Review Checklist</a></li> \n      </ul> </li> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-license\">License</a></li> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-acknowledgments\">Acknowledgments</a></li> \n     <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/#-support\">Support</a></li> \n    </ul> </td> \n  </tr> \n </tbody>\n</table> \n<h2>🏗️ System Architecture</h2> \n<p>WiFi DensePose consists of several key components working together:</p> \n<pre><code>┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n│   WiFi Router   │    │   WiFi Router   │    │   WiFi Router   │\n│   (CSI Source)  │    │   (CSI Source)  │    │   (CSI Source)  │\n└─────────┬───────┘    └─────────┬───────┘    └─────────┬───────┘\n          │                      │                      │\n          └──────────────────────┼──────────────────────┘\n                                 │\n                    ┌─────────────▼─────────────┐\n                    │     CSI Data Collector    │\n                    │   (Hardware Interface)    │\n                    └─────────────┬─────────────┘\n                                  │\n                    ┌─────────────▼─────────────┐\n                    │    Signal Processor       │\n                    │  (Phase Sanitization)     │\n                    └─────────────┬─────────────┘\n                                  │\n                    ┌─────────────▼─────────────┐\n                    │   Neural Network Model    │\n                    │    (DensePose Head)       │\n                    └─────────────┬─────────────┘\n                                  │\n                    ┌─────────────▼─────────────┐\n                    │   Person Tracker          │\n                    │  (Multi-Object Tracking)  │\n                    └─────────────┬─────────────┘\n                                  │\n          ┌───────────────────────┼───────────────────────┐\n          │                       │                       │\n┌─────────▼─────────┐   ┌─────────▼─────────┐   ┌─────────▼─────────┐\n│   REST API        │   │  WebSocket API    │   │   Analytics       │\n│  (CRUD Operations)│   │ (Real-time Stream)│   │  (Fall Detection) │\n└───────────────────┘   └───────────────────┘   └───────────────────┘\n</code></pre> \n<h3>Core Components</h3> \n<ul> \n <li><strong>CSI Processor</strong>: Extracts and processes Channel State Information from WiFi signals</li> \n <li><strong>Phase Sanitizer</strong>: Removes hardware-specific phase offsets and noise</li> \n <li><strong>DensePose Neural Network</strong>: Converts CSI data to human pose keypoints</li> \n <li><strong>Multi-Person Tracker</strong>: Maintains consistent person identities across frames</li> \n <li><strong>REST API</strong>: Comprehensive API for data access and system control</li> \n <li><strong>WebSocket Streaming</strong>: Real-time pose data broadcasting</li> \n <li><strong>Analytics Engine</strong>: Advanced analytics including fall detection and activity recognition</li> \n</ul> \n<h2>📦 Installation</h2> \n<h3>Using pip (Recommended)</h3> \n<p>WiFi-DensePose is now available on PyPI for easy installation:</p> \n<pre><code class=\"language-bash\"># Install the latest stable version\npip install wifi-densepose\n\n# Install with specific version\npip install wifi-densepose==1.0.0\n\n# Install with optional dependencies\npip install wifi-densepose[gpu]  # For GPU acceleration\npip install wifi-densepose[dev]  # For development\npip install wifi-densepose[all]  # All optional dependencies\n</code></pre> \n<h3>From Source</h3> \n<pre><code class=\"language-bash\">git clone https://github.com/ruvnet/wifi-densepose.git\ncd wifi-densepose\npip install -r requirements.txt\npip install -e .\n</code></pre> \n<h3>Using Docker</h3> \n<pre><code class=\"language-bash\">docker pull ruvnet/wifi-densepose:latest\ndocker run -p 8000:8000 ruvnet/wifi-densepose:latest\n</code></pre> \n<h3>System Requirements</h3> \n<ul> \n <li><strong>Python</strong>: 3.8 or higher</li> \n <li><strong>Operating System</strong>: Linux (Ubuntu 18.04+), macOS (10.15+), Windows 10+</li> \n <li><strong>Memory</strong>: Minimum 4GB RAM, Recommended 8GB+</li> \n <li><strong>Storage</strong>: 2GB free space for models and data</li> \n <li><strong>Network</strong>: WiFi interface with CSI capability</li> \n <li><strong>GPU</strong>: Optional but recommended (NVIDIA GPU with CUDA support)</li> \n</ul> \n<h2>🚀 Quick Start</h2> \n<h3>1. Basic Setup</h3> \n<pre><code class=\"language-bash\"># Install the package\npip install wifi-densepose\n\n# Copy example configuration\ncp example.env .env\n\n# Edit configuration (set your WiFi interface)\nnano .env\n</code></pre> \n<h3>2. Start the System</h3> \n<pre><code class=\"language-python\">from wifi_densepose import WiFiDensePose\n\n# Initialize with default configuration\nsystem = WiFiDensePose()\n\n# Start pose estimation\nsystem.start()\n\n# Get latest pose data\nposes = system.get_latest_poses()\nprint(f\"Detected {len(poses)} persons\")\n\n# Stop the system\nsystem.stop()\n</code></pre> \n<h3>3. Using the REST API</h3> \n<pre><code class=\"language-bash\"># Start the API server\nwifi-densepose start\n\n# Start with custom configuration\nwifi-densepose -c /path/to/config.yaml start\n\n# Start with verbose logging\nwifi-densepose -v start\n\n# Check server status\nwifi-densepose status\n</code></pre> \n<p>The API will be available at <code>http://localhost:8000</code></p> \n<ul> \n <li><strong>API Documentation</strong>: <a href=\"http://localhost:8000/docs\">http://localhost:8000/docs</a></li> \n <li><strong>Health Check</strong>: <a href=\"http://localhost:8000/api/v1/health\">http://localhost:8000/api/v1/health</a></li> \n <li><strong>Latest Poses</strong>: <a href=\"http://localhost:8000/api/v1/pose/latest\">http://localhost:8000/api/v1/pose/latest</a></li> \n</ul> \n<h3>4. Real-time Streaming</h3> \n<pre><code class=\"language-python\">import asyncio\nimport websockets\nimport json\n\nasync def stream_poses():\n    uri = \"ws://localhost:8000/ws/pose/stream\"\n    async with websockets.connect(uri) as websocket:\n        while True:\n            data = await websocket.recv()\n            poses = json.loads(data)\n            print(f\"Received poses: {len(poses['persons'])} persons detected\")\n\n# Run the streaming client\nasyncio.run(stream_poses())\n</code></pre> \n<h2>🖥️ CLI Usage</h2> \n<p>WiFi DensePose provides a comprehensive command-line interface for easy system management, configuration, and monitoring.</p> \n<h3>CLI Installation</h3> \n<p>The CLI is automatically installed with the package:</p> \n<pre><code class=\"language-bash\"># Install WiFi DensePose with CLI\npip install wifi-densepose\n\n# Verify CLI installation\nwifi-densepose --help\nwifi-densepose version\n</code></pre> \n<h3>Basic Commands</h3> \n<p>The WiFi-DensePose CLI provides the following commands:</p> \n<pre><code class=\"language-bash\">wifi-densepose [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  -c, --config PATH  Path to configuration file\n  -v, --verbose      Enable verbose logging\n  --debug            Enable debug mode\n  --help             Show this message and exit.\n\nCommands:\n  config   Configuration management commands.\n  db       Database management commands.\n  start    Start the WiFi-DensePose API server.\n  status   Show the status of the WiFi-DensePose API server.\n  stop     Stop the WiFi-DensePose API server.\n  tasks    Background task management commands.\n  version  Show version information.\n</code></pre> \n<h4>Server Management</h4> \n<pre><code class=\"language-bash\"># Start the WiFi-DensePose API server\nwifi-densepose start\n\n# Start with custom configuration\nwifi-densepose -c /path/to/config.yaml start\n\n# Start with verbose logging\nwifi-densepose -v start\n\n# Start with debug mode\nwifi-densepose --debug start\n\n# Check server status\nwifi-densepose status\n\n# Stop the server\nwifi-densepose stop\n\n# Show version information\nwifi-densepose version\n</code></pre> \n<h3>Configuration Commands</h3> \n<h4>Configuration Management</h4> \n<pre><code class=\"language-bash\"># Configuration management commands\nwifi-densepose config [SUBCOMMAND]\n\n# Examples:\n# Show current configuration\nwifi-densepose config show\n\n# Validate configuration file\nwifi-densepose config validate\n\n# Create default configuration\nwifi-densepose config init\n\n# Edit configuration\nwifi-densepose config edit\n</code></pre> \n<h4>Database Management</h4> \n<pre><code class=\"language-bash\"># Database management commands\nwifi-densepose db [SUBCOMMAND]\n\n# Examples:\n# Initialize database\nwifi-densepose db init\n\n# Run database migrations\nwifi-densepose db migrate\n\n# Check database status\nwifi-densepose db status\n\n# Backup database\nwifi-densepose db backup\n\n# Restore database\nwifi-densepose db restore\n</code></pre> \n<h4>Background Tasks</h4> \n<pre><code class=\"language-bash\"># Background task management commands\nwifi-densepose tasks [SUBCOMMAND]\n\n# Examples:\n# List running tasks\nwifi-densepose tasks list\n\n# Start background tasks\nwifi-densepose tasks start\n\n# Stop background tasks\nwifi-densepose tasks stop\n\n# Check task status\nwifi-densepose tasks status\n</code></pre> \n<h3>Command Examples</h3> \n<h4>Complete CLI Reference</h4> \n<pre><code class=\"language-bash\"># Show help for main command\nwifi-densepose --help\n\n# Show help for specific command\nwifi-densepose start --help\nwifi-densepose config --help\nwifi-densepose db --help\n\n# Use global options with commands\nwifi-densepose -v status          # Verbose status check\nwifi-densepose --debug start      # Start with debug logging\nwifi-densepose -c custom.yaml start  # Start with custom config\n</code></pre> \n<h4>Common Usage Patterns</h4> \n<pre><code class=\"language-bash\"># Basic server lifecycle\nwifi-densepose start              # Start the server\nwifi-densepose status             # Check if running\nwifi-densepose stop               # Stop the server\n\n# Configuration management\nwifi-densepose config show        # View current config\nwifi-densepose config validate    # Check config validity\n\n# Database operations\nwifi-densepose db init            # Initialize database\nwifi-densepose db migrate         # Run migrations\nwifi-densepose db status          # Check database health\n\n# Task management\nwifi-densepose tasks list         # List background tasks\nwifi-densepose tasks status       # Check task status\n\n# Version and help\nwifi-densepose version            # Show version info\nwifi-densepose --help             # Show help message\n</code></pre> \n<h3>CLI Examples</h3> \n<h4>Complete Setup Workflow</h4> \n<pre><code class=\"language-bash\"># 1. Check version and help\nwifi-densepose version\nwifi-densepose --help\n\n# 2. Initialize configuration\nwifi-densepose config init\n\n# 3. Initialize database\nwifi-densepose db init\n\n# 4. Start the server\nwifi-densepose start\n\n# 5. Check status\nwifi-densepose status\n</code></pre> \n<h4>Development Workflow</h4> \n<pre><code class=\"language-bash\"># Start with debug logging\nwifi-densepose --debug start\n\n# Use custom configuration\nwifi-densepose -c dev-config.yaml start\n\n# Check database status\nwifi-densepose db status\n\n# Manage background tasks\nwifi-densepose tasks start\nwifi-densepose tasks list\n</code></pre> \n<h4>Production Workflow</h4> \n<pre><code class=\"language-bash\"># Start with production config\nwifi-densepose -c production.yaml start\n\n# Check system status\nwifi-densepose status\n\n# Manage database\nwifi-densepose db migrate\nwifi-densepose db backup\n\n# Monitor tasks\nwifi-densepose tasks status\n</code></pre> \n<h4>Troubleshooting</h4> \n<pre><code class=\"language-bash\"># Enable verbose logging\nwifi-densepose -v status\n\n# Check configuration\nwifi-densepose config validate\n\n# Check database health\nwifi-densepose db status\n\n# Restart services\nwifi-densepose stop\nwifi-densepose start\n</code></pre> \n<h2>📚 Documentation</h2> \n<p>Comprehensive documentation is available to help you get started and make the most of WiFi-DensePose:</p> \n<h3>📖 Core Documentation</h3> \n<ul> \n <li><strong><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/docs/user_guide.md\">User Guide</a></strong> - Complete guide covering installation, setup, basic usage, and examples</li> \n <li><strong><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/docs/api_reference.md\">API Reference</a></strong> - Detailed documentation of all public classes, methods, and endpoints</li> \n <li><strong><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/docs/deployment.md\">Deployment Guide</a></strong> - Production deployment, Docker setup, Kubernetes, and scaling strategies</li> \n <li><strong><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/docs/troubleshooting.md\">Troubleshooting Guide</a></strong> - Common issues, solutions, and diagnostic procedures</li> \n</ul> \n<h3>🚀 Quick Links</h3> \n<ul> \n <li><strong>Interactive API Docs</strong>: <a href=\"http://localhost:8000/docs\">http://localhost:8000/docs</a> (when running)</li> \n <li><strong>Health Check</strong>: <a href=\"http://localhost:8000/api/v1/health\">http://localhost:8000/api/v1/health</a></li> \n <li><strong>Latest Poses</strong>: <a href=\"http://localhost:8000/api/v1/pose/latest\">http://localhost:8000/api/v1/pose/latest</a></li> \n <li><strong>System Status</strong>: <a href=\"http://localhost:8000/api/v1/system/status\">http://localhost:8000/api/v1/system/status</a></li> \n</ul> \n<h3>📋 API Overview</h3> \n<p>The system provides a comprehensive REST API and WebSocket streaming:</p> \n<h4>Key REST Endpoints</h4> \n<pre><code class=\"language-bash\"># Pose estimation\nGET /api/v1/pose/latest          # Get latest pose data\nGET /api/v1/pose/history         # Get historical data\nGET /api/v1/pose/zones/{zone_id} # Get zone-specific data\n\n# System management\nGET /api/v1/system/status        # System health and status\nPOST /api/v1/system/calibrate    # Calibrate environment\nGET /api/v1/analytics/summary    # Analytics dashboard data\n</code></pre> \n<h4>WebSocket Streaming</h4> \n<pre><code class=\"language-javascript\">// Real-time pose data\nws://localhost:8000/ws/pose/stream\n\n// Analytics events (falls, alerts)\nws://localhost:8000/ws/analytics/events\n\n// System status updates\nws://localhost:8000/ws/system/status\n</code></pre> \n<h4>Python SDK Quick Example</h4> \n<pre><code class=\"language-python\">from wifi_densepose import WiFiDensePoseClient\n\n# Initialize client\nclient = WiFiDensePoseClient(base_url=\"http://localhost:8000\")\n\n# Get latest poses with confidence filtering\nposes = client.get_latest_poses(min_confidence=0.7)\nprint(f\"Detected {len(poses)} persons\")\n\n# Get zone occupancy\noccupancy = client.get_zone_occupancy(\"living_room\")\nprint(f\"Living room occupancy: {occupancy.person_count}\")\n</code></pre> \n<p>For complete API documentation with examples, see the <a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/docs/api_reference.md\">API Reference Guide</a>.</p> \n<h2>🔧 Hardware Setup</h2> \n<h3>Supported Hardware</h3> \n<p>WiFi DensePose works with standard WiFi equipment that supports CSI extraction:</p> \n<h4>Recommended Routers</h4> \n<ul> \n <li><strong>ASUS AX6000</strong> (RT-AX88U) - Excellent CSI quality</li> \n <li><strong>Netgear Nighthawk AX12</strong> - High performance</li> \n <li><strong>TP-Link Archer AX73</strong> - Budget-friendly option</li> \n <li><strong>Ubiquiti UniFi 6 Pro</strong> - Enterprise grade</li> \n</ul> \n<h4>CSI-Capable Devices</h4> \n<ul> \n <li>Intel WiFi cards (5300, 7260, 8260, 9260)</li> \n <li>Atheros AR9300 series</li> \n <li>Broadcom BCM4366 series</li> \n <li>Qualcomm QCA9984 series</li> \n</ul> \n<h3>Physical Setup</h3> \n<ol> \n <li><strong>Router Placement</strong>: Position routers to create overlapping coverage areas</li> \n <li><strong>Height</strong>: Mount routers 2-3 meters high for optimal coverage</li> \n <li><strong>Spacing</strong>: 5-10 meter spacing between routers depending on environment</li> \n <li><strong>Orientation</strong>: Ensure antennas are positioned for maximum signal diversity</li> \n</ol> \n<h3>Network Configuration</h3> \n<pre><code class=\"language-bash\"># Configure WiFi interface for CSI extraction\nsudo iwconfig wlan0 mode monitor\nsudo iwconfig wlan0 channel 6\n\n# Set up CSI extraction (Intel 5300 example)\necho 0x4101 | sudo tee /sys/kernel/debug/ieee80211/phy0/iwlwifi/iwldvm/debug/monitor_tx_rate\n</code></pre> \n<h3>Environment Calibration</h3> \n<pre><code class=\"language-python\">from wifi_densepose import Calibrator\n\n# Run environment calibration\ncalibrator = Calibrator()\ncalibrator.calibrate_environment(\n    duration_minutes=10,\n    environment_id=\"room_001\"\n)\n\n# Apply calibration\ncalibrator.apply_calibration()\n</code></pre> \n<h2>⚙️ Configuration</h2> \n<h3>Environment Variables</h3> \n<p>Copy <code>example.env</code> to <code>.env</code> and configure:</p> \n<pre><code class=\"language-bash\"># Application Settings\nAPP_NAME=WiFi-DensePose API\nVERSION=1.0.0\nENVIRONMENT=production  # development, staging, production\nDEBUG=false\n\n# Server Settings\nHOST=0.0.0.0\nPORT=8000\nWORKERS=4\n\n# Security Settings\nSECRET_KEY=your-secure-secret-key-here\nJWT_ALGORITHM=HS256\nJWT_EXPIRE_HOURS=24\n\n# Hardware Settings\nWIFI_INTERFACE=wlan0\nCSI_BUFFER_SIZE=1000\nHARDWARE_POLLING_INTERVAL=0.1\n\n# Pose Estimation Settings\nPOSE_CONFIDENCE_THRESHOLD=0.7\nPOSE_PROCESSING_BATCH_SIZE=32\nPOSE_MAX_PERSONS=10\n\n# Feature Flags\nENABLE_AUTHENTICATION=true\nENABLE_RATE_LIMITING=true\nENABLE_WEBSOCKETS=true\nENABLE_REAL_TIME_PROCESSING=true\nENABLE_HISTORICAL_DATA=true\n</code></pre> \n<h3>Domain-Specific Configurations</h3> \n<h4>Healthcare Configuration</h4> \n<pre><code class=\"language-python\">config = {\n    \"domain\": \"healthcare\",\n    \"detection\": {\n        \"confidence_threshold\": 0.8,\n        \"max_persons\": 5,\n        \"enable_tracking\": True\n    },\n    \"analytics\": {\n        \"enable_fall_detection\": True,\n        \"enable_activity_recognition\": True,\n        \"alert_thresholds\": {\n            \"fall_confidence\": 0.9,\n            \"inactivity_timeout\": 300\n        }\n    },\n    \"privacy\": {\n        \"data_retention_days\": 30,\n        \"anonymize_data\": True,\n        \"enable_encryption\": True\n    }\n}\n</code></pre> \n<h4>Fitness Configuration</h4> \n<pre><code class=\"language-python\">config = {\n    \"domain\": \"fitness\",\n    \"detection\": {\n        \"confidence_threshold\": 0.6,\n        \"max_persons\": 20,\n        \"enable_tracking\": True\n    },\n    \"analytics\": {\n        \"enable_activity_recognition\": True,\n        \"enable_form_analysis\": True,\n        \"metrics\": [\"rep_count\", \"form_score\", \"intensity\"]\n    }\n}\n</code></pre> \n<h3>Advanced Configuration</h3> \n<pre><code class=\"language-python\">from wifi_densepose.config import Settings\n\n# Load custom configuration\nsettings = Settings(\n    pose_model_path=\"/path/to/custom/model.pth\",\n    neural_network={\n        \"batch_size\": 64,\n        \"enable_gpu\": True,\n        \"inference_timeout\": 500\n    },\n    tracking={\n        \"max_age\": 30,\n        \"min_hits\": 3,\n        \"iou_threshold\": 0.3\n    }\n)\n</code></pre> \n<h2>🧪 Testing</h2> \n<p>WiFi DensePose maintains 100% test coverage with comprehensive testing:</p> \n<h3>Running Tests</h3> \n<pre><code class=\"language-bash\"># Run all tests\npytest\n\n# Run with coverage report\npytest --cov=wifi_densepose --cov-report=html\n\n# Run specific test categories\npytest tests/unit/          # Unit tests\npytest tests/integration/   # Integration tests\npytest tests/e2e/          # End-to-end tests\npytest tests/performance/  # Performance tests\n</code></pre> \n<h3>Test Categories</h3> \n<h4>Unit Tests (95% coverage)</h4> \n<ul> \n <li>CSI processing algorithms</li> \n <li>Neural network components</li> \n <li>Tracking algorithms</li> \n <li>API endpoints</li> \n <li>Configuration validation</li> \n</ul> \n<h4>Integration Tests</h4> \n<ul> \n <li>Hardware interface integration</li> \n <li>Database operations</li> \n <li>WebSocket connections</li> \n <li>Authentication flows</li> \n</ul> \n<h4>End-to-End Tests</h4> \n<ul> \n <li>Complete pose estimation pipeline</li> \n <li>Multi-person tracking scenarios</li> \n <li>Real-time streaming</li> \n <li>Analytics generation</li> \n</ul> \n<h4>Performance Tests</h4> \n<ul> \n <li>Latency benchmarks</li> \n <li>Throughput testing</li> \n <li>Memory usage profiling</li> \n <li>Stress testing</li> \n</ul> \n<h3>Mock Testing</h3> \n<p>For development without hardware:</p> \n<pre><code class=\"language-bash\"># Enable mock mode\nexport MOCK_HARDWARE=true\nexport MOCK_POSE_DATA=true\n\n# Run tests with mocked hardware\npytest tests/ --mock-hardware\n</code></pre> \n<h3>Continuous Integration</h3> \n<pre><code class=\"language-yaml\"># .github/workflows/test.yml\nname: Test Suite\non: [push, pull_request]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: 3.8\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n          pip install -e .\n      - name: Run tests\n        run: pytest --cov=wifi_densepose --cov-report=xml\n      - name: Upload coverage\n        uses: codecov/codecov-action@v1\n</code></pre> \n<h2>🚀 Deployment</h2> \n<h3>Production Deployment</h3> \n<h4>Using Docker</h4> \n<pre><code class=\"language-bash\"># Build production image\ndocker build -t wifi-densepose:latest .\n\n# Run with production configuration\ndocker run -d \\\n  --name wifi-densepose \\\n  -p 8000:8000 \\\n  -v /path/to/data:/app/data \\\n  -v /path/to/models:/app/models \\\n  -e ENVIRONMENT=production \\\n  -e SECRET_KEY=your-secure-key \\\n  wifi-densepose:latest\n</code></pre> \n<h4>Using Docker Compose</h4> \n<pre><code class=\"language-yaml\"># docker-compose.yml\nversion: '3.8'\nservices:\n  wifi-densepose:\n    image: wifi-densepose:latest\n    ports:\n      - \"8000:8000\"\n    environment:\n      - ENVIRONMENT=production\n      - DATABASE_URL=postgresql://user:pass@db:5432/wifi_densepose\n      - REDIS_URL=redis://redis:6379/0\n    volumes:\n      - ./data:/app/data\n      - ./models:/app/models\n    depends_on:\n      - db\n      - redis\n\n  db:\n    image: postgres:13\n    environment:\n      POSTGRES_DB: wifi_densepose\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: password\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\n  redis:\n    image: redis:6-alpine\n    volumes:\n      - redis_data:/data\n\nvolumes:\n  postgres_data:\n  redis_data:\n</code></pre> \n<h4>Kubernetes Deployment</h4> \n<pre><code class=\"language-yaml\"># k8s/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: wifi-densepose\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: wifi-densepose\n  template:\n    metadata:\n      labels:\n        app: wifi-densepose\n    spec:\n      containers:\n      - name: wifi-densepose\n        image: wifi-densepose:latest\n        ports:\n        - containerPort: 8000\n        env:\n        - name: ENVIRONMENT\n          value: \"production\"\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: wifi-densepose-secrets\n              key: database-url\n        resources:\n          requests:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n          limits:\n            memory: \"4Gi\"\n            cpu: \"2000m\"\n</code></pre> \n<h3>Infrastructure as Code</h3> \n<h4>Terraform (AWS)</h4> \n<pre><code class=\"language-hcl\"># terraform/main.tf\nresource \"aws_ecs_cluster\" \"wifi_densepose\" {\n  name = \"wifi-densepose\"\n}\n\nresource \"aws_ecs_service\" \"wifi_densepose\" {\n  name            = \"wifi-densepose\"\n  cluster         = aws_ecs_cluster.wifi_densepose.id\n  task_definition = aws_ecs_task_definition.wifi_densepose.arn\n  desired_count   = 3\n\n  load_balancer {\n    target_group_arn = aws_lb_target_group.wifi_densepose.arn\n    container_name   = \"wifi-densepose\"\n    container_port   = 8000\n  }\n}\n</code></pre> \n<h4>Ansible Playbook</h4> \n<pre><code class=\"language-yaml\"># ansible/playbook.yml\n- hosts: servers\n  become: yes\n  tasks:\n    - name: Install Docker\n      apt:\n        name: docker.io\n        state: present\n\n    - name: Deploy WiFi DensePose\n      docker_container:\n        name: wifi-densepose\n        image: wifi-densepose:latest\n        ports:\n          - \"8000:8000\"\n        env:\n          ENVIRONMENT: production\n          DATABASE_URL: \"{{ database_url }}\"\n        restart_policy: always\n</code></pre> \n<h3>Monitoring and Logging</h3> \n<h4>Prometheus Metrics</h4> \n<pre><code class=\"language-yaml\"># monitoring/prometheus.yml\nglobal:\n  scrape_interval: 15s\n\nscrape_configs:\n  - job_name: 'wifi-densepose'\n    static_configs:\n      - targets: ['localhost:8000']\n    metrics_path: '/metrics'\n</code></pre> \n<h4>Grafana Dashboard</h4> \n<pre><code class=\"language-json\">{\n  \"dashboard\": {\n    \"title\": \"WiFi DensePose Monitoring\",\n    \"panels\": [\n      {\n        \"title\": \"Pose Detection Rate\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(pose_detections_total[5m])\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Processing Latency\",\n        \"type\": \"graph\",\n        \"targets\": [\n          {\n            \"expr\": \"histogram_quantile(0.95, pose_processing_duration_seconds_bucket)\"\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre> \n<h2>📊 Performance Metrics</h2> \n<h3>Benchmark Results</h3> \n<h4>Latency Performance</h4> \n<ul> \n <li><strong>Average Processing Time</strong>: 45.2ms per frame</li> \n <li><strong>95th Percentile</strong>: 67ms</li> \n <li><strong>99th Percentile</strong>: 89ms</li> \n <li><strong>Real-time Capability</strong>: 30 FPS sustained</li> \n</ul> \n<h4>Accuracy Metrics</h4> \n<ul> \n <li><strong>Pose Detection Accuracy</strong>: 94.2% (compared to camera-based systems)</li> \n <li><strong>Person Tracking Accuracy</strong>: 91.8%</li> \n <li><strong>Fall Detection Sensitivity</strong>: 96.5%</li> \n <li><strong>Fall Detection Specificity</strong>: 94.1%</li> \n</ul> \n<h4>Resource Usage</h4> \n<ul> \n <li><strong>CPU Usage</strong>: 65% (4-core system)</li> \n <li><strong>Memory Usage</strong>: 2.1GB RAM</li> \n <li><strong>GPU Usage</strong>: 78% (NVIDIA RTX 3080)</li> \n <li><strong>Network Bandwidth</strong>: 15 Mbps (CSI data)</li> \n</ul> \n<h4>Scalability</h4> \n<ul> \n <li><strong>Maximum Concurrent Users</strong>: 1000+ WebSocket connections</li> \n <li><strong>API Throughput</strong>: 10,000 requests/minute</li> \n <li><strong>Data Storage</strong>: 50GB/month (with compression)</li> \n <li><strong>Multi-Environment Support</strong>: Up to 50 simultaneous environments</li> \n</ul> \n<h3>Performance Optimization</h3> \n<h4>Hardware Optimization</h4> \n<pre><code class=\"language-python\"># Enable GPU acceleration\nconfig = {\n    \"neural_network\": {\n        \"enable_gpu\": True,\n        \"batch_size\": 64,\n        \"mixed_precision\": True\n    },\n    \"processing\": {\n        \"num_workers\": 4,\n        \"prefetch_factor\": 2\n    }\n}\n</code></pre> \n<h4>Software Optimization</h4> \n<pre><code class=\"language-python\"># Enable performance optimizations\nconfig = {\n    \"caching\": {\n        \"enable_redis\": True,\n        \"cache_ttl\": 300\n    },\n    \"database\": {\n        \"connection_pool_size\": 20,\n        \"enable_query_cache\": True\n    }\n}\n</code></pre> \n<h3>Load Testing</h3> \n<pre><code class=\"language-bash\"># API load testing with Apache Bench\nab -n 10000 -c 100 http://localhost:8000/api/v1/pose/latest\n\n# WebSocket load testing\npython scripts/websocket_load_test.py --connections 1000 --duration 300\n</code></pre> \n<h2>🤝 Contributing</h2> \n<p>We welcome contributions to WiFi DensePose! Please follow these guidelines:</p> \n<h3>Development Setup</h3> \n<pre><code class=\"language-bash\"># Clone the repository\ngit clone https://github.com/ruvnet/wifi-densepose.git\ncd wifi-densepose\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install development dependencies\npip install -r requirements-dev.txt\npip install -e .\n\n# Install pre-commit hooks\npre-commit install\n</code></pre> \n<h3>Code Standards</h3> \n<ul> \n <li><strong>Python Style</strong>: Follow PEP 8, enforced by Black and Flake8</li> \n <li><strong>Type Hints</strong>: Use type hints for all functions and methods</li> \n <li><strong>Documentation</strong>: Comprehensive docstrings for all public APIs</li> \n <li><strong>Testing</strong>: Maintain 100% test coverage for new code</li> \n <li><strong>Security</strong>: Follow OWASP guidelines for security</li> \n</ul> \n<h3>Contribution Process</h3> \n<ol> \n <li><strong>Fork</strong> the repository</li> \n <li><strong>Create</strong> a feature branch (<code>git checkout -b feature/amazing-feature</code>)</li> \n <li><strong>Commit</strong> your changes (<code>git commit -m 'Add amazing feature'</code>)</li> \n <li><strong>Push</strong> to the branch (<code>git push origin feature/amazing-feature</code>)</li> \n <li><strong>Open</strong> a Pull Request</li> \n</ol> \n<h3>Code Review Checklist</h3> \n<ul> \n <li><input disabled=\"disabled\" type=\"checkbox\" /> Code follows style guidelines</li> \n <li><input disabled=\"disabled\" type=\"checkbox\" /> Tests pass and coverage is maintained</li> \n <li><input disabled=\"disabled\" type=\"checkbox\" /> Documentation is updated</li> \n <li><input disabled=\"disabled\" type=\"checkbox\" /> Security considerations addressed</li> \n <li><input disabled=\"disabled\" type=\"checkbox\" /> Performance impact assessed</li> \n <li><input disabled=\"disabled\" type=\"checkbox\" /> Backward compatibility maintained</li> \n</ul> \n<h3>Issue Templates</h3> \n<h4>Bug Report</h4> \n<pre><code class=\"language-markdown\">**Describe the bug**\nA clear description of the bug.\n\n**To Reproduce**\nSteps to reproduce the behavior.\n\n**Expected behavior**\nWhat you expected to happen.\n\n**Environment**\n- OS: [e.g., Ubuntu 20.04]\n- Python version: [e.g., 3.8.10]\n- WiFi DensePose version: [e.g., 1.0.0]\n</code></pre> \n<h4>Feature Request</h4> \n<pre><code class=\"language-markdown\">**Feature Description**\nA clear description of the feature.\n\n**Use Case**\nDescribe the use case and benefits.\n\n**Implementation Ideas**\nAny ideas on how to implement this feature.\n</code></pre> \n<h2>📄 License</h2> \n<p>This project is licensed under the MIT License - see the <a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/LICENSE\">LICENSE</a> file for details.</p> \n<pre><code>MIT License\n\nCopyright (c) 2025 WiFi DensePose Contributors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre> \n<h2>🙏 Acknowledgments</h2> \n<ul> \n <li><strong>Research Foundation</strong>: Based on groundbreaking research in WiFi-based human sensing</li> \n <li><strong>Open Source Libraries</strong>: Built on PyTorch, FastAPI, and other excellent open source projects</li> \n <li><strong>Community</strong>: Thanks to all contributors and users who make this project possible</li> \n <li><strong>Hardware Partners</strong>: Special thanks to router manufacturers for CSI support</li> \n</ul> \n<h2>📞 Support</h2> \n<ul> \n <li><strong>Documentation</strong>: \n  <ul> \n   <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/docs/user_guide.md\">User Guide</a> - Complete setup and usage guide</li> \n   <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/docs/api_reference.md\">API Reference</a> - Detailed API documentation</li> \n   <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/docs/deployment.md\">Deployment Guide</a> - Production deployment instructions</li> \n   <li><a href=\"https://raw.githubusercontent.com/ruvnet/wifi-densepose/main/docs/troubleshooting.md\">Troubleshooting Guide</a> - Common issues and solutions</li> \n  </ul> </li> \n <li><strong>Issues</strong>: <a href=\"https://github.com/ruvnet/wifi-densepose/issues\">GitHub Issues</a></li> \n <li><strong>Discussions</strong>: <a href=\"https://github.com/ruvnet/wifi-densepose/discussions\">GitHub Discussions</a></li> \n <li><strong>PyPI Package</strong>: <a href=\"https://pypi.org/project/wifi-densepose/\">https://pypi.org/project/wifi-densepose/</a></li> \n <li><strong>Email</strong>: <a href=\"mailto:support@wifi-densepose.com\">support@wifi-densepose.com</a></li> \n <li><strong>Discord</strong>: <a href=\"https://discord.gg/wifi-densepose\">Join our community</a></li> \n</ul> \n<hr /> \n<p><strong>WiFi DensePose</strong> - Revolutionizing human pose estimation through privacy-preserving WiFi technology.</p>",
        "source": "mshibanami.github.io",
        "published": "",
        "fetched_at": "2026-02-28T23:17:27.212448Z",
        "tags": [
          {
            "name": "transformation",
            "score": 3
          },
          {
            "name": "boundary_crossing",
            "score": 9
          },
          {
            "name": "visibility_gain",
            "score": 5
          },
          {
            "name": "scale_shift",
            "score": 9
          }
        ],
        "structural_score": 26,
        "timeliness_score": 1,
        "final_score": 8.5,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://dev.to/programmerspace/16-api-concepts-you-need-to-master-48d0",
        "title": "16 API Concepts You Need to Master",
        "summary": "<p>Hey readers,</p>\n\n<p>I still remember my first “real” system design interview. I was ready to code algorithms, but then the interviewer asked me how I’d handle a payment retry without charging the customer twice. </p>\n\n<p>I froze. I knew <em>how</em> to write code, but I didn’t fully grasp the architectural glue that holds big systems together.</p>\n\n<p>That glue is APIs.</p>\n\n<p>APIs look messy when you stare at a massive distributed system diagram. But here’s the secret: the whole thing usually boils down to a handful of core ideas. </p>\n\n<p>Once you get these sixteen concepts down, you stop seeing “complexity” and start seeing patterns.</p>\n\n<blockquote>\n<p>You can get the post as PDF file from <a href=\"https://10xdev.blog/16-api-concepts-you-need-to-master/\" rel=\"noopener noreferrer\">https://10xdev.blog/16-api-concepts-you-need-to-master/</a></p>\n</blockquote>\n\n<p>Whether you are preparing for a backend interview or trying to explain to a junior dev why we can’t just “return all the data at once,” this guide is for you.</p>\n\n<p>This a breakdown of 16 essential API concepts every software engineer should understand, followed by clear explanations for each.</p>\n\n<p><a href=\"https://substackcdn.com/image/fetch/$s_!C1l2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa3589b28-faf1-4249-a956-6c410326a9bb_2816x1536.png\" rel=\"noopener noreferrer\"></a></p>\n\n<p><a class=\"article-body-image-wrapper\" href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftd1h356hc43fdudhgoiq.png\"><img height=\"436\" src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ftd1h356hc43fdudhgoiq.png\" width=\"800\" /></a></p>\n\n<p>Let’s get into the mechanics of how modern systems actually talk to each other.</p>\n\n\n\n\n<h2>\n  \n  \n  The “Big Three” Communication Styles\n</h2>\n\n<p>Before we worry about data formats or security, we have to decide how our servers are going to exchange information.</p>\n\n<blockquote>\n<p>There isn’t a single “best” way, just the right tool for the job.</p>\n</blockquote>\n\n<h3>\n  \n  \n  1. REST (The Reliable Standard)\n</h3>\n\n<p>Think of REST (Representational State Transfer) as the plain English of the web. It’s what you’ll see 90% of the time. A REST API lets a client talk to a server using standard HTTP actions that actually mean what they say.</p>\n\n<ul>\n<li><p><strong>GET:</strong> “Give me this data.”</p></li>\n<li><p><strong>POST:</strong> “Create this new thing.”</p></li>\n<li><p><strong>PUT:</strong> “Update this existing thing.”</p></li>\n<li><p><strong>DELETE:</strong> “Get rid of this.”</p></li>\n</ul>\n\n<blockquote>\n<p>Imagine an online bookstore. You hit <strong>GET /books</strong> to see the catalog. You hit <strong>POST /books</strong> to add a new title. It’s predictable. Each URL is a specific “resource” (like a user or a product), and the server usually hits you back with <strong>JSON</strong>. Because it works across browsers, mobile apps, and backends without fuss, it’s usually the default choice for public APIs.</p>\n</blockquote>\n\n<p><a href=\"https://substackcdn.com/image/fetch/$s_!_fo2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2995b997-1234-4960-9b6c-b843e7fd23af_1818x1917.png\" rel=\"noopener noreferrer\"></a></p>\n\n<p><a class=\"article-body-image-wrapper\" href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjhcwshq71ijfu7azmkzd.png\"><img height=\"843\" src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fjhcwshq71ijfu7azmkzd.png\" width=\"800\" /></a></p>\n\n<h3>\n  \n  \n  2. GraphQL (The “Picky Eater”)\n</h3>\n\n<p>REST is great, but it has a habit of over-sharing. You ask for a user’s profile, and it sends you their name, address, shoe size, and purchase history—even if you just wanted their name.</p>\n\n<p>GraphQL changes that dynamic. It lets the client ask for <em>exactly</em> what it needs, and nothing else.</p>\n\n<p><em><strong>Why it matters:</strong></em></p>\n\n<p>Say you’re building a mobile app. Bandwidth is tight. Instead of making three different REST calls to get a user’s name, their last order, and their wishlist, you send one GraphQL query. You define the shape of the data, and the server fills it in. This cuts down on round trips and keeps your mobile app snappy.  </p>\n\n<h3>\n  \n  \n  3. gRPC (The Speed Demon)\n</h3>\n\n<p>If REST is a polite conversation and GraphQL is a specific order, gRPC is a high-speed binary stream. It’s rarely used in browsers but is a powerhouse for backend services talking to other backend services.</p>\n\n<p>It uses <strong>Protobufs</strong> (Protocol Buffers) instead of text-based JSON. This binary data is tiny and incredibly fast to parse.</p>\n\n<p><em><strong>When to use it:</strong></em></p>\n\n<p>I worked on a system once where a recommendation engine had to talk to a ranking service thousands of times. JSON was too slow. We switched to gRPC, and the latency dropped like a rock. It supports streaming and strict type checks, making it safer and faster for internal microservices.</p>\n\n<h2>\n  \n  \n  The Gatekeepers: Control &amp; Security\n</h2>\n\n<p>Once your API is live, you can’t just let anyone walk in and do whatever they want. You need bouncers.</p>\n\n<h3>\n  \n  \n  4. API Gateway\n</h3>\n\n<p>In a microservices architecture, you might have fifty different services (Billing, Users, Inventory, Shipping). You definitely don’t want your mobile app trying to keep track of many addresses.</p>\n\n<p>An API Gateway is the single entry point. It sits in front of everything.</p>\n\n<ul>\n<li>  <strong>Routing:</strong> It sends the request to the right service.</li>\n<li>  <strong>Security:</strong> It handles the login check so the individual services don’t have to.</li>\n<li>  <strong>Housekeeping:</strong> It handles rate limits and logging.</li>\n</ul>\n\n<p>It keeps your backend organized and your clients happy because they only need to know one URL.</p>\n\n<p><a href=\"https://substackcdn.com/image/fetch/$s_!_M0N!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbbe4e483-1f9d-489d-823e-1951adf04713_1986x1137.png\" rel=\"noopener noreferrer\"></a></p>\n\n<p><a class=\"article-body-image-wrapper\" href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frriy21krt84u8vwlhgkj.png\"><img height=\"458\" src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Frriy21krt84u8vwlhgkj.png\" width=\"800\" /></a></p>\n\n<h3>\n  \n  \n  5. Authentication (Who are you?)\n</h3>\n\n<p>This is security step one. Authentication is simply the system asking, “ID, please.” We usually verify this using tokens (like JWTs), passwords, or API keys.</p>\n\n<p>If you don’t have a valid token, you don’t get in. The request is rejected immediately (usually with a 401 status code) before it even touches any business logic.</p>\n\n<p><a href=\"https://substackcdn.com/image/fetch/$s_!d-oD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36fc9584-8861-40a6-bee0-61c29ae3d352_2184x2193.png\" rel=\"noopener noreferrer\"></a></p>\n\n<p><a class=\"article-body-image-wrapper\" href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Flqn1gwxxgdsgnkm6y1go.png\"><img height=\"803\" src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Flqn1gwxxgdsgnkm6y1go.png\" width=\"800\" /></a></p>\n\n<h3>\n  \n  \n  6. Authorization (What are you allowed to do?)\n</h3>\n\n<p>Here is where people get mixed up. Authentication says you <em>are</em> John Doe. Authorization says what John Doe is <em>allowed</em> to touch.</p>\n\n<p><em><strong>The difference:</strong></em></p>\n\n<p>I might log in (Authentication successful), but if I try to delete the production database, the system stops me (Authorization failed).</p>\n\n<ul>\n<li>  <strong>Regular User:</strong> Can edit their own profile.</li>\n<li>\n<p><strong>Admin</strong>: Can edit any profile.</p>\n\n<p>Both are authenticated, but their permissions differ.</p>\n</li>\n</ul>\n\n<h3>\n  \n  \n  7. Rate Limiting\n</h3>\n\n<p>Imagine your service is a nightclub with a fire code capacity. Rate limiting restricts how many requests a user can send in a specific timeframe (e.g., 100 requests per minute).</p>\n\n<p>If a user—or a bot—tries to hammer your API with 1,000 requests in a second, the rate limiter steps in and blocks the extras.</p>\n\n<blockquote>\n<p>This is non-negotiable for public APIs to prevent abuse and keep costs in check.</p>\n</blockquote>\n\n<h3>\n  \n  \n  8. Throttling\n</h3>\n\n<p>Throttling is the polite cousin of rate limiting. Instead of slamming the door shut when a user sends too many requests, the server just slows them down.</p>\n\n<p>The logic:</p>\n\n<p>If a client is sending too much traffic, we might delay their response by a few hundred milliseconds. It tells the client, “Hey, back off a bit,” without completely breaking their functionality. It maintains fair usage across the system.  </p>\n\n\n\n\n<h2>\n  \n  \n  Performance &amp; Reliability (The “Safety Nets”)\n</h2>\n\n<p>Things go wrong in distributed systems. Networks fail. Servers hang. Here is how we handle the chaos.</p>\n\n<h3>\n  \n  \n  9. Idempotency\n</h3>\n\n<p>This is the concept I messed up in that interview. Idempotency guarantees that if you make the same request multiple times, the result is the same as if you made it once.</p>\n\n<p><em><strong>Why it saves you:</strong></em></p>\n\n<p>A user is on a shaky subway connection. They hit “Pay Now.” The request goes through, but the confirmation response gets lost. The user panics and hits “Pay Now” again.</p>\n\n<ul>\n<li><p><strong>Without Idempotency:</strong> You just charged them twice. Support ticket incoming.</p></li>\n<li><p><strong>With Idempotency:</strong> The server sees the same “Idempotency Key” (a unique ID for that transaction), realizes it already processed this payment, and just returns the success message again without charging the card.</p></li>\n</ul>\n\n<p><a href=\"https://substackcdn.com/image/fetch/$s_!ubZe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc0ad5f-6204-47d0-825c-0a5dec8a3645_3142x2103.png\" rel=\"noopener noreferrer\"></a></p>\n\n<p><a class=\"article-body-image-wrapper\" href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fici8n62bifzyba63lbhd.png\"><img height=\"535\" src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fici8n62bifzyba63lbhd.png\" width=\"800\" /></a></p>\n\n<h3>\n  \n  \n  10. Timeouts\n</h3>\n\n<p>Never let a request wait forever. A “timeout” is a rule that says, “If this database doesn’t answer in 2 seconds, give up.”</p>\n\n<p>If you don’t have timeouts, a slow service can cause a pile-up. One stuck request becomes ten, then a thousand, and suddenly your whole system runs out of threads and crashes. Timeouts free up resources so the rest of the system stays alive.</p>\n\n<p><a href=\"https://substackcdn.com/image/fetch/$s_!UNf6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F770fd38c-5fe3-428d-86e5-652063407605_2202x1923.png\" rel=\"noopener noreferrer\"></a></p>\n\n<p><a class=\"article-body-image-wrapper\" href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fipy8ubft6ov5wcn7tads.png\"><img height=\"698\" src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fipy8ubft6ov5wcn7tads.png\" width=\"800\" /></a></p>\n\n<h3>\n  \n  \n  11. Caching Headers\n</h3>\n\n<p>The fastest request is the one you don’t have to make. Caching headers tell the client (or a proxy) how long they can keep a copy of the data.</p>\n\n<p>If you send <code>Cache-Control: max-age=3600</code>, you are telling the browser, “This data is good for an hour. Don’t bother asking me for it again until then.” This drastically reduces the load on your servers and makes the app feel instant for the user.  </p>\n\n<h3>\n  \n  \n  12. Pagination\n</h3>\n\n<p>You have 50,000 products in your database. If someone hits <code>GET /products</code>, and you try to return all of them at once, your server will choke, and the user’s browser will crash trying to render it.</p>\n\n<p>Pagination breaks that data into chunks (pages). You send the first 20 items. If they want more, they ask for page 2. It saves memory and bandwidth.</p>\n\n\n\n\n<h2>\n  \n  \n  The Mechanics: Speaking the Language\n</h2>\n\n<p>Finally, let’s look at the nuts and bolts of the data exchange.</p>\n\n<h3>\n  \n  \n  13. JSON (JavaScript Object Notation)\n</h3>\n\n<p>XML used to be the king, but JSON took the throne because it is readable by humans. It stores data in key-value pairs: <code>{”id”: 1, “name”: “Alex”}</code>.</p>\n\n<p>It is lightweight, parses easily in every major programming language, and is the default response format for almost every modern API.  </p>\n\n<h3>\n  \n  \n  14. HTTP Status Codes\n</h3>\n\n<p>These are the traffic lights of the web. They tell the client instantly what happened, so they don’t have to parse the body of the response to guess.</p>\n\n<ul>\n<li><p><strong>200s (Success):</strong> “We’re good.” (200 OK, 201 Created).</p></li>\n<li><p><strong>400s (Client Error):</strong> “You messed up.” (400 Bad Request, 401 Unauthenticated, 404 Not Found).</p></li>\n<li><p><strong>500s (Server Error):</strong> “I messed up.” (500 Internal Server Error).</p></li>\n</ul>\n\n<h3>\n  \n  \n  15. Webhooks\n</h3>\n\n<p>Most APIs are “poll” based—you keep asking, “Is there new data?”</p>\n\n<p>Webhooks flip this. They are “push” based. You tell the server, “Here is my URL. Call me when something happens.”</p>\n\n<p><em><strong>Example:</strong></em></p>\n\n<p>Stripe (payments) uses this heavily. Instead of your server asking Stripe every second “Did the user pay yet?”, Stripe sends a message to your webhook URL the moment the payment succeeds. It makes systems real-time and event-driven.  </p>\n\n<h3>\n  \n  \n  16. Versioning\n</h3>\n\n<p>You built a great API. Now you need to change it. But if you change the data structure, you will break the mobile app that people haven’t updated yet.</p>\n\n<p>Versioning solves this. you keep <code>v1</code> running for the old apps while you launch <code>v2</code> for the new ones.</p>\n\n<ul>\n<li><p><strong>v1:</strong> Returns just a name.</p></li>\n<li>\n<p>v2: Returns name, photo, and bio.</p>\n\n<p>It allows you to improve your system without breaking the experience for legacy users.</p>\n</li>\n</ul>\n\n\n\n\n<h2>\n  \n  \n  The Big Picture: How It All Connects\n</h2>\n\n<p>Now that we have the pieces, here is what a modern system actually looks like when you put them together.</p>\n\n<p><a href=\"https://substackcdn.com/image/fetch/$s_!G-5_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78796310-2271-4074-b09d-6ff3ff5f3837_2754x2227.png\" rel=\"noopener noreferrer\"></a></p>\n\n<p><a class=\"article-body-image-wrapper\" href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9r24mwixe3cjdynyf7os.png\"><img height=\"646\" src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9r24mwixe3cjdynyf7os.png\" width=\"800\" /></a></p>\n\n<h3>\n  \n  \n  So, what’s the next move?\n</h3>\n\n<p>These concepts are the building blocks. You don’t need to implement all of them tomorrow, but you do need to spot them when you see them.</p>\n\n<p><strong>Here is a challenge for you:</strong> Open up the “Network” tab in your browser right now (F12) and refresh this page. Look at the API calls. Can you spot the <strong>Status Codes</strong>? Do you see any <strong>Caching Headers</strong>? Are they using <strong>GraphQL</strong> or <strong>REST</strong>?</p>\n\n<p>Once you start looking, you’ll see these patterns everywhere.</p>\n\n<p>Thanks for reading!</p>\n\n<p><em><strong>Bou~codes and Naima from 10xdev blog.</strong></em></p>\n\n<p>This is a diagram to help you remember all those 16 concepts:</p>\n\n<p><a href=\"https://substackcdn.com/image/fetch/$s_!JhKJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac696183-9b4d-493f-aaf8-b34105296ecb_1218x1958.png\" rel=\"noopener noreferrer\"></a></p>\n\n<p><a class=\"article-body-image-wrapper\" href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ffigfofb4f9v15ghfs0ho.png\"><img height=\"1286\" src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Ffigfofb4f9v15ghfs0ho.png\" width=\"800\" /></a></p>\n\n<h2>\n  \n  \n  PDF Roadmaps:\n</h2>\n\n<ol>\n<li><p><a href=\"https://10xdev.blog/Guide-AI-Agents.pdf\" rel=\"noopener noreferrer\">A PDF Guide for Learning AI Agents</a></p></li>\n<li><p><a href=\"https://10xdev.blog/Guide-FrontEnd.pdf\" rel=\"noopener noreferrer\">A PDF Guide for Learning Frontend Development</a></p></li>\n<li><p><a href=\"https://10xdev.blog/Guide-JavaScript.pdf\" rel=\"noopener noreferrer\">A PDF guide for becoming a JavaScript Developer.</a></p></li>\n<li><p><a href=\"https://10xdev.blog/Guide-Backend.pdf\" rel=\"noopener noreferrer\">A PDF guide for becoming a Backend Developer.</a></p></li>\n</ol>",
        "source": "dev.to",
        "published": "Sat, 28 Feb 2026 22:30:37 +0000",
        "fetched_at": "2026-02-28T23:17:32.737376Z",
        "tags": [
          {
            "name": "transformation",
            "score": 3
          },
          {
            "name": "boundary_crossing",
            "score": 4
          },
          {
            "name": "value_redefinition",
            "score": 4
          },
          {
            "name": "scale_shift",
            "score": 8
          }
        ],
        "structural_score": 19,
        "timeliness_score": 2,
        "final_score": 7.1,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://github.com/obra/superpowers",
        "title": "obra/superpowers",
        "summary": "<p>An agentic skills framework & software development methodology that works.</p><hr /><h1>Superpowers</h1> \n<p>Superpowers is a complete software development workflow for your coding agents, built on top of a set of composable \"skills\" and some initial instructions that make sure your agent uses them.</p> \n<h2>How it works</h2> \n<p>It starts from the moment you fire up your coding agent. As soon as it sees that you're building something, it <em>doesn't</em> just jump into trying to write code. Instead, it steps back and asks you what you're really trying to do.</p> \n<p>Once it's teased a spec out of the conversation, it shows it to you in chunks short enough to actually read and digest.</p> \n<p>After you've signed off on the design, your agent puts together an implementation plan that's clear enough for an enthusiastic junior engineer with poor taste, no judgement, no project context, and an aversion to testing to follow. It emphasizes true red/green TDD, YAGNI (You Aren't Gonna Need It), and DRY.</p> \n<p>Next up, once you say \"go\", it launches a <em>subagent-driven-development</em> process, having agents work through each engineering task, inspecting and reviewing their work, and continuing forward. It's not uncommon for Claude to be able to work autonomously for a couple hours at a time without deviating from the plan you put together.</p> \n<p>There's a bunch more to it, but that's the core of the system. And because the skills trigger automatically, you don't need to do anything special. Your coding agent just has Superpowers.</p> \n<h2>Sponsorship</h2> \n<p>If Superpowers has helped you do stuff that makes money and you are so inclined, I'd greatly appreciate it if you'd consider <a href=\"https://github.com/sponsors/obra\">sponsoring my opensource work</a>.</p> \n<p>Thanks!</p> \n<ul> \n <li>Jesse</li> \n</ul> \n<h2>Installation</h2> \n<p><strong>Note:</strong> Installation differs by platform. Claude Code or Cursor have built-in plugin marketplaces. Codex and OpenCode require manual setup.</p> \n<h3>Claude Code (via Plugin Marketplace)</h3> \n<p>In Claude Code, register the marketplace first:</p> \n<pre><code class=\"language-bash\">/plugin marketplace add obra/superpowers-marketplace\n</code></pre> \n<p>Then install the plugin from this marketplace:</p> \n<pre><code class=\"language-bash\">/plugin install superpowers@superpowers-marketplace\n</code></pre> \n<h3>Cursor (via Plugin Marketplace)</h3> \n<p>In Cursor Agent chat, install from marketplace:</p> \n<pre><code class=\"language-text\">/plugin-add superpowers\n</code></pre> \n<h3>Codex</h3> \n<p>Tell Codex:</p> \n<pre><code>Fetch and follow instructions from https://raw.githubusercontent.com/obra/superpowers/refs/heads/main/.codex/INSTALL.md\n</code></pre> \n<p><strong>Detailed docs:</strong> <a href=\"https://raw.githubusercontent.com/obra/superpowers/main/docs/README.codex.md\">docs/README.codex.md</a></p> \n<h3>OpenCode</h3> \n<p>Tell OpenCode:</p> \n<pre><code>Fetch and follow instructions from https://raw.githubusercontent.com/obra/superpowers/refs/heads/main/.opencode/INSTALL.md\n</code></pre> \n<p><strong>Detailed docs:</strong> <a href=\"https://raw.githubusercontent.com/obra/superpowers/main/docs/README.opencode.md\">docs/README.opencode.md</a></p> \n<h3>Verify Installation</h3> \n<p>Start a new session in your chosen platform and ask for something that should trigger a skill (for example, \"help me plan this feature\" or \"let's debug this issue\"). The agent should automatically invoke the relevant superpowers skill.</p> \n<h2>The Basic Workflow</h2> \n<ol> \n <li> <p><strong>brainstorming</strong> - Activates before writing code. Refines rough ideas through questions, explores alternatives, presents design in sections for validation. Saves design document.</p> </li> \n <li> <p><strong>using-git-worktrees</strong> - Activates after design approval. Creates isolated workspace on new branch, runs project setup, verifies clean test baseline.</p> </li> \n <li> <p><strong>writing-plans</strong> - Activates with approved design. Breaks work into bite-sized tasks (2-5 minutes each). Every task has exact file paths, complete code, verification steps.</p> </li> \n <li> <p><strong>subagent-driven-development</strong> or <strong>executing-plans</strong> - Activates with plan. Dispatches fresh subagent per task with two-stage review (spec compliance, then code quality), or executes in batches with human checkpoints.</p> </li> \n <li> <p><strong>test-driven-development</strong> - Activates during implementation. Enforces RED-GREEN-REFACTOR: write failing test, watch it fail, write minimal code, watch it pass, commit. Deletes code written before tests.</p> </li> \n <li> <p><strong>requesting-code-review</strong> - Activates between tasks. Reviews against plan, reports issues by severity. Critical issues block progress.</p> </li> \n <li> <p><strong>finishing-a-development-branch</strong> - Activates when tasks complete. Verifies tests, presents options (merge/PR/keep/discard), cleans up worktree.</p> </li> \n</ol> \n<p><strong>The agent checks for relevant skills before any task.</strong> Mandatory workflows, not suggestions.</p> \n<h2>What's Inside</h2> \n<h3>Skills Library</h3> \n<p><strong>Testing</strong></p> \n<ul> \n <li><strong>test-driven-development</strong> - RED-GREEN-REFACTOR cycle (includes testing anti-patterns reference)</li> \n</ul> \n<p><strong>Debugging</strong></p> \n<ul> \n <li><strong>systematic-debugging</strong> - 4-phase root cause process (includes root-cause-tracing, defense-in-depth, condition-based-waiting techniques)</li> \n <li><strong>verification-before-completion</strong> - Ensure it's actually fixed</li> \n</ul> \n<p><strong>Collaboration</strong></p> \n<ul> \n <li><strong>brainstorming</strong> - Socratic design refinement</li> \n <li><strong>writing-plans</strong> - Detailed implementation plans</li> \n <li><strong>executing-plans</strong> - Batch execution with checkpoints</li> \n <li><strong>dispatching-parallel-agents</strong> - Concurrent subagent workflows</li> \n <li><strong>requesting-code-review</strong> - Pre-review checklist</li> \n <li><strong>receiving-code-review</strong> - Responding to feedback</li> \n <li><strong>using-git-worktrees</strong> - Parallel development branches</li> \n <li><strong>finishing-a-development-branch</strong> - Merge/PR decision workflow</li> \n <li><strong>subagent-driven-development</strong> - Fast iteration with two-stage review (spec compliance, then code quality)</li> \n</ul> \n<p><strong>Meta</strong></p> \n<ul> \n <li><strong>writing-skills</strong> - Create new skills following best practices (includes testing methodology)</li> \n <li><strong>using-superpowers</strong> - Introduction to the skills system</li> \n</ul> \n<h2>Philosophy</h2> \n<ul> \n <li><strong>Test-Driven Development</strong> - Write tests first, always</li> \n <li><strong>Systematic over ad-hoc</strong> - Process over guessing</li> \n <li><strong>Complexity reduction</strong> - Simplicity as primary goal</li> \n <li><strong>Evidence over claims</strong> - Verify before declaring success</li> \n</ul> \n<p>Read more: <a href=\"https://blog.fsck.com/2025/10/09/superpowers/\">Superpowers for Claude Code</a></p> \n<h2>Contributing</h2> \n<p>Skills live directly in this repository. To contribute:</p> \n<ol> \n <li>Fork the repository</li> \n <li>Create a branch for your skill</li> \n <li>Follow the <code>writing-skills</code> skill for creating and testing new skills</li> \n <li>Submit a PR</li> \n</ol> \n<p>See <code>skills/writing-skills/SKILL.md</code> for the complete guide.</p> \n<h2>Updating</h2> \n<p>Skills update automatically when you update the plugin:</p> \n<pre><code class=\"language-bash\">/plugin update superpowers\n</code></pre> \n<h2>License</h2> \n<p>MIT License - see LICENSE file for details</p> \n<h2>Support</h2> \n<ul> \n <li><strong>Issues</strong>: <a href=\"https://github.com/obra/superpowers/issues\">https://github.com/obra/superpowers/issues</a></li> \n <li><strong>Marketplace</strong>: <a href=\"https://github.com/obra/superpowers-marketplace\">https://github.com/obra/superpowers-marketplace</a></li> \n</ul>",
        "source": "mshibanami.github.io",
        "published": "",
        "fetched_at": "2026-02-28T23:17:28.718163Z",
        "tags": [
          {
            "name": "boundary_crossing",
            "score": 9
          },
          {
            "name": "scale_shift",
            "score": 4
          }
        ],
        "structural_score": 13,
        "timeliness_score": 1,
        "final_score": 7.0,
        "reddit_score": null,
        "reddit_comments": null
      },
      {
        "url": "https://dev.to/sain_bux_dev/from-github-to-knowledge-panel-a-developers-digital-identity-stack-2i2c",
        "title": "From GitHub to Knowledge Panel: A Developer’s Digital Identity Stack",
        "summary": "<h2>\n  \n  \n  In 2026, every developer has a digital footprint.\n</h2>\n\n<p>But not every developer has a structured digital identity.</p>\n\n<p><strong>There’s a difference.</strong></p>\n\n<ul>\n<li>A digital footprint is accidental.</li>\n<li>A digital identity stack is intentional.</li>\n</ul>\n\n<p>If you think about it architecturally, your online presence behaves like a distributed system. Multiple nodes (platforms) publish data about you. Search engines aggregate, reconcile, and structure that data into entities.</p>\n\n<p>When done correctly, this can lead to strong authority signals — sometimes even a knowledge panel powered by systems like Google Knowledge Graph.</p>\n\n<p>Let’s break down what a developer’s digital identity stack actually looks like.</p>\n\n<h3>\n  \n  \n  Layer 1: The Code Layer (Source of Truth)\n</h3>\n\n<p>Everything starts with verifiable output.</p>\n\n<p>Platforms like GitHub are foundational because they provide:</p>\n\n<p>Timestamped commits</p>\n\n<p>Public repositories</p>\n\n<p>Contribution graphs</p>\n\n<p>Open collaboration</p>\n\n<p>Search engines and recruiters treat GitHub as high-trust infrastructure because activity is hard to fake at scale.</p>\n\n<p>But not all GitHub profiles are equal.</p>\n\n<p>Strong signals include:</p>\n\n<p>Clean repository structure</p>\n\n<p>Meaningful README documentation</p>\n\n<p>Real-world projects (not just tutorial clones)</p>\n\n<p>Consistent contribution over time</p>\n\n<p>Think of GitHub as your database layer.<br />\nIf the data here is weak, higher layers won’t hold.</p>\n\n<h3>\n  \n  \n  Layer 2: The Context Layer (Explaining the Code)\n</h3>\n\n<p>Code alone doesn’t communicate intent.</p>\n\n<p>This is where technical writing matters.</p>\n\n<p>Platforms like dev.to allow you to:</p>\n\n<p>Explain architecture decisions</p>\n\n<p>Share debugging processes</p>\n\n<p>Document trade-offs</p>\n\n<p>Clarify your specialization</p>\n\n<p>When you publish consistently under the same name, you’re building semantic consistency.</p>\n\n<p>Search systems, including Google, rely on entity matching. If your name, specialization, and topics are aligned across platforms, your identity becomes easier to interpret algorithmically.</p>\n\n<p>In simple terms:<br />\nCode shows what you built.<br />\nContent explains why and how.</p>\n\n<h3>\n  \n  \n  Layer 3: The Structured Data Layer\n</h3>\n\n<p>Most developers ignore this layer.</p>\n\n<p>Structured data (like schema markup) tells search engines explicitly:</p>\n\n<p>Who you are</p>\n\n<p>What you do</p>\n\n<p>What you’re known for</p>\n\n<p>Which platforms belong to you</p>\n\n<p>This is where personal websites become powerful.</p>\n\n<p>Using schema types such as:</p>\n\n<p>Person</p>\n\n<p>SameAs</p>\n\n<p>Author</p>\n\n<p>You create machine-readable connections between:</p>\n\n<p><a href=\"https://github.com/Sainbux/\" rel=\"noopener noreferrer\">GitHub</a></p>\n\n<p><a href=\"https://sainbux.blogspot.com/\" rel=\"noopener noreferrer\">Blog profiles</a></p>\n\n<p><a href=\"https://sainbux.github.io/\" rel=\"noopener noreferrer\">Portfolio site</a></p>\n\n<p><a href=\"https://www.instagram.com/sainbuxdev/\" rel=\"noopener noreferrer\">Social platforms</a></p>\n\n<p>Now your identity is not just visible — it’s structured.</p>\n\n<h3>\n  \n  \n  Layer 4: The Authority Signals Layer\n</h3>\n\n<p>Authority isn’t declared. It’s inferred.</p>\n\n<p>Search engines look for:</p>\n\n<p>Cross-platform consistency</p>\n\n<p>Mentions from other credible sources</p>\n\n<p>Topical depth</p>\n\n<p>Longevity</p>\n\n<p>If your articles reference advanced topics (APIs, architecture, AI systems) and your GitHub repositories reflect similar themes, your entity begins to cluster around that specialization.</p>\n\n<p>Over time, systems like the Google Knowledge Graph may associate your name with specific domains (e.g., full-stack development, entity SEO, API architecture).</p>\n\n<p>This is not a hack.<br />\nIt’s cumulative signal alignment.</p>\n\n<h3>\n  \n  \n  Layer 5: The Knowledge Panel (Emergent Property)\n</h3>\n\n<p>A knowledge panel is not something you request in most cases.<br />\nIt’s something that emerges when:</p>\n\n<p>Your identity is consistent</p>\n\n<p>Your content is structured</p>\n\n<p>Your name has sufficient search volume</p>\n\n<p>Your authority signals are stable</p>\n\n<p>It represents a high-confidence entity recognition event.</p>\n\n<p>But here’s the important part:</p>\n\n<p>The goal should not be the panel.<br />\nThe goal should be clarity.</p>\n\n<p>When clarity is strong, structured recognition follows.</p>\n\n<p>The Digital Identity Stack Model</p>\n\n<p>You can think of it like this:</p>\n\n<ul>\n<li>Infrastructure: GitHub</li>\n<li>Documentation: Technical writing</li>\n<li>Schema: Structured data</li>\n<li>Consistency: Unified naming</li>\n<li>Authority: Time + depth</li>\n<li>Recognition: Knowledge graph inclusion</li>\n</ul>\n\n<p>This is less about branding and more about systems thinking.</p>\n\n<p>Common Mistakes Developers Make</p>\n\n<p>Using different names across platforms</p>\n\n<p>Jumping between unrelated topics every month</p>\n\n<p>Publishing inconsistently</p>\n\n<p>Ignoring structured data</p>\n\n<p>Building projects without explaining them</p>\n\n<p>Digital identity fragmentation reduces entity clarity.</p>\n\n<p>A Practical Action Plan</p>\n\n<p>If you want to build a clean digital identity stack:</p>\n\n<p><strong>1. Standardize your name everywhere</strong><br />\nSame username, same format.</p>\n\n<p><strong>2. Pick 1–2 core specializations</strong><br />\nFull-stack architecture, AI integration, API design, etc.</p>\n\n<p><strong>3. Ship real projects publicly</strong></p>\n\n<p><strong>4. Publish technical breakdowns regularly</strong></p>\n\n<p><strong>5. Connect everything through your personal website using schema</strong></p>\n\n<p>Think long-term. Search engines reward consistency more than bursts of activity.</p>\n\n<h4>\n  \n  \n  Final Thought\n</h4>\n\n<p>In distributed systems, clarity reduces entropy.</p>\n\n<p>The same applies to your online presence.</p>\n\n<p>From GitHub repositories to structured schema, each layer either strengthens or weakens your identity as an engineer.</p>\n\n<p>Your digital identity is already being built.</p>\n\n<p>The only question is whether it’s being architected intentionally.</p>",
        "source": "dev.to",
        "published": "Sat, 28 Feb 2026 22:39:37 +0000",
        "fetched_at": "2026-02-28T23:17:32.737358Z",
        "tags": [
          {
            "name": "transformation",
            "score": 3
          },
          {
            "name": "boundary_crossing",
            "score": 2
          },
          {
            "name": "value_redefinition",
            "score": 4
          },
          {
            "name": "ontology_shift",
            "score": 8
          }
        ],
        "structural_score": 17,
        "timeliness_score": 2,
        "final_score": 6.5,
        "reddit_score": null,
        "reddit_comments": null
      }
    ]
  }
}